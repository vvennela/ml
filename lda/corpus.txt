amazon ec2 provides the following networking features. topics 
amazon elastic compute cloud (amazon ec2) conforms to the aws , which includes regulations and guidelines for data protection. aws is responsible for protecting the global infrastructure that runs all aws services. aws maintains control over data hosted on this infrastructure, including the security configuration controls for handling customer content and personal data. aws customers and apn partners, acting either as data controllers or data processors, are responsible for any personal data that they put in the aws cloud. for data protection purposes, we recommend that you protect aws account credentials and set up individual user accounts with aws identity and access management (iam), so that each user is given only the permissions necessary to fulfill their job duties. we also recommend that you secure your data in the following ways: use multi-factor authentication (mfa) with each account.use tls to communicate with aws resources.set up api and user activity logging with aws cloudtrail.use aws encryption solutions, along with all default security controls within aws services.use advanced managed security services such as amazon macie, which assists in discovering and securing personal data that is stored in amazon s3.we strongly recommend that you never put sensitive identifying information, such as your customers' account numbers, into free-form fields or metadata, such as function names and tags. any data that you enter into metadata might get picked up for inclusion in diagnostic logs. when you provide a url to an external server, don't include credential information in the url to validate your request to that server. for more information about data protection, see the  blog post on the aws security blog. amazon ebs encryption is an encryption solution for your ebs volumes and snapshots. it uses aws key management service (aws kms) customer master keys (cmk). for more information, see . the data on nvme instance store volumes is encrypted using an xts-aes-256 cipher implemented on a hardware module on the instance. the encryption keys are generated using the hardware module and are unique to each nvme instance storage device. all encryption keys are destroyed when the instance is stopped or terminated and cannot be recovered. you cannot disable this encryption and you cannot provide your own encryption key. ssh provides a secure communications channel for remote access to your linux instances. remote access to your instances using aws systems manager session manager and run command is encrypted using tls 1.2, and requests to create a connection are signed using sigv4. use an encryption protocol such as transport layer security (tls) to encrypt sensitive data in transit between clients and your instances. aws provides secure and private connectivity between ec2 instances of all types. in addition, some instance types use the offload capabilities of the underlying hardware to automatically encrypt in-transit traffic between instances, using aead algorithms with 256-bit encryption. there is no impact on network performance. the following requirements must be met to ensure the additional in-transit traffic encryption: the instances use the following instance types: c5n, c5a, g4, i3en, m5dn, m5n, p3dn, r5dn, and r5n.the instances are in the same region.the instances are in the same vpc or peered vpcs, and the traffic does not pass through a virtual network device, such as a load balancer or a transit gateway.
amazon ec2 provides amazon cloudwatch metrics that you can use to monitor your spot fleet. importantto ensure accuracy, we recommend that you enable detailed monitoring when using these metrics. for more information, see . for more information about cloudwatch metrics provided by amazon ec2, see . the  namespace includes the following metrics, plus the cloudwatch metrics for the spot instances in your fleet. for more information, see . if the unit of measure for a metric is , the most useful statistic is . to filter the data for your spot fleet, use the following dimensions. you can view the cloudwatch metrics for your spot fleet using the amazon cloudwatch console. these metrics are displayed as monitoring graphs. these graphs show data points if the spot fleet is active. metrics are grouped first by namespace, and then by the various combinations of dimensions within each namespace. for example, you can view all spot fleet metrics or spot fleet metrics groups by spot fleet request id, instance type, or availability zone. to view spot fleet metrics open the cloudwatch console at . in the navigation pane, choose metrics. choose the ec2 spot namespace. noteif the ec2 spot namespace is not displayed, there are two reasons for this. either you've not yet used spot fleet—only the aws services that you're using send metrics to amazon cloudwatch. or, if you’ve not used spot fleet for the past two weeks, the namespace does not appear. (optional) to filter the metrics by dimension, select one of the following: fleet request metrics – group by spot fleet requestby availability zone – group by spot fleet request and availability zoneby instance type – group by spot fleet request and instance typeby availability zone/instance type – group by spot fleet request, availability zone, and instance typeto view the data for a metric, select the check box next to the metric. 
the following procedures are for creating an instance store-backed ami from an instance store-backed instance. before you begin, ensure that you've read the . topics this section describes the creation of an ami from an amazon linux instance. the following procedures may not work for instances running other linux distributions. for ubuntu-specific procedures, see . to prepare to use the ami tools (hvm instances only) the ami tools require grub legacy to boot properly. use the following command to install grub: install the partition management packages with the following command: to create an ami from an instance store-backed amazon linux instance this procedure assumes that you have satisfied the prerequisites in . upload your credentials to your instance. we use these credentials to ensure that only you and amazon ec2 can access your ami. create a temporary directory on your instance for your credentials as follows: this enables you to exclude your credentials from the created image. copy your x.509 certificate and corresponding private key from your computer to the  directory on your instance using a secure copy tool such as . the  option in the following scp command is the private key you use to connect to your instance with ssh, not the x.509 private key. for example: alternatively, because these are plain text files, you can open the certificate and key in a text editor and copy their contents into new files in . prepare the bundle to upload to amazon s3 by running the  command from inside your instance. be sure to specify the  option to exclude the directory where your credentials are stored. by default, the bundle process excludes files that might contain sensitive information. these files include , , , , , ,  , , , and . to include all of these files, use the  option. to include some of these files, use the  option. importantby default, the ami bundling process creates a compressed, encrypted collection of files in the  directory that represents your root volume. if you do not have enough free disk space in  to store the bundle, you need to specify a different location for the bundle to be stored with the  option. some instances have ephemeral storage mounted at  or  that you can use, or you can also , , and  a new amazon ebs volume to store the bundle. you must run the ec2-bundle-vol command as root. for most commands, you can use sudo to gain elevated permissions, but in this case, you should run sudo -e su to keep your environment variables. note that bash prompt now identifies you as the root user, and that the dollar sign has been replaced by a hash tag, signalling that you are in a root shell: to create the ami bundle, run the  command as follows: notefor the china (beijing) and aws govcloud (us-west) regions, use the  parameter and specify the certificates as per the . it can take a few minutes to create the image. when this command completes, your  (or non-default) directory contains the bundle (, plus multiple xx files). exit from the root shell. (optional) to add more instance store volumes, edit the block device mappings in the  file for your ami. for more information, see . create a backup of your  file. reformat the  file so that it is easier to read and edit. edit the block device mappings in  with a text editor. the example below shows a new entry for the  instance store volume.  notefor a list of excluded files, see .  save the  file and exit your text editor. to upload your bundle to amazon s3, run the  command as follows. importantto register your ami in a region other than us east (n. virginia), you must specify both the target region with the  option and a bucket path that already exists in the target region or a unique bucket path that can be created in the target region. (optional) after the bundle is uploaded to amazon s3, you can remove the bundle from the  directory on the instance using the following rm command: importantif you specified a path with the  option in , use that path instead of . to register your ami, run the  command as follows. importantif you previously specified a region for the  command, specify that region again for this command. this section describes the creation of an ami from an ubuntu linux instance with an instance store volume as the root volume. the following procedures may not work for instances running other linux distributions. for procedures specific to amazon linux, see . to prepare to use the ami tools (hvm instances only) the ami tools require grub legacy to boot properly. however, ubuntu is configured to use grub 2. you must check to see that your instance uses grub legacy, and if not, you need to install and configure it. hvm instances also require partitioning tools to be installed for the ami tools to work properly. grub legacy (version 0.9x or less) must be installed on your instance. check to see if grub legacy is present and install it if necessary. check the version of your grub installation. in this example, the grub version is greater than 0.9x, so you must install grub legacy. proceed to . if grub legacy is already present, you can skip to . install the  package using the following command. install the following partition management packages using the package manager for your distribution.   (some distributions may call this package  instead) use the following command. check the kernel parameters for your instance. note the options following the kernel and root device parameters: , , and . your options may differ. check the kernel entries in . note that the  parameter is pointing to  instead of  and that the  parameter is missing. again, your options may differ. edit the  file with your favorite text editor (such as vim or nano) to change the console and add the parameters you identified earlier to the boot entries. verify that your kernel entries now contain the correct parameters. [for ubuntu 14.04 and later only] starting with ubuntu 14.04, instance store backed ubuntu amis use a gpt partition table and a separate efi partition mounted at . the ec2-bundle-vol command will not bundle this boot partition, so you need to comment out the  entry for the efi partition as shown in the following example. to create an ami from an instance store-backed ubuntu instance this procedure assumes that you have satisfied the prerequisites in . upload your credentials to your instance. we use these credentials to ensure that only you and amazon ec2 can access your ami. create a temporary directory on your instance for your credentials as follows: this enables you to exclude your credentials from the created image. copy your x.509 certificate and private key from your computer to the  directory on your instance, using a secure copy tool such as . the  option in the following scp command is the private key you use to connect to your instance with ssh, not the x.509 private key. for example: alternatively, because these are plain text files, you can open the certificate and key in a text editor and copy their contents into new files in . prepare the bundle to upload to amazon s3 by running the  command from your instance. be sure to specify the  option to exclude the directory where your credentials are stored. by default, the bundle process excludes files that might contain sensitive information. these files include , , , , , ,  , , , and . to include all of these files, use the  option. to include some of these files, use the  option. importantby default, the ami bundling process creates a compressed, encrypted collection of files in the  directory that represents your root volume. if you do not have enough free disk space in  to store the bundle, you need to specify a different location for the bundle to be stored with the  option. some instances have ephemeral storage mounted at  or  that you can use, or you can also , , and  a new amazon ebs volume to store the bundle. you must run the ec2-bundle-vol command needs as root. for most commands, you can use sudo to gain elevated permissions, but in this case, you should run sudo -e su to keep your environment variables. note that bash prompt now identifies you as the root user, and that the dollar sign has been replaced by a hash tag, signalling that you are in a root shell: to create the ami bundle, run the  command as follows. importantfor ubuntu 14.04 and later hvm instances, add the  flag to bundle the boot instructions properly; otherwise, your newly-created ami will not boot. it can take a few minutes to create the image. when this command completes, your  directory contains the bundle (, plus multiple xx files). exit from the root shell. (optional) to add more instance store volumes, edit the block device mappings in the  file for your ami. for more information, see . create a backup of your  file. reformat the  file so that it is easier to read and edit. edit the block device mappings in  with a text editor. the example below shows a new entry for the ephemeral1 instance store volume. save the  file and exit your text editor. to upload your bundle to amazon s3, run the  command as follows. importantif you intend to register your ami in a region other than us east (n. virginia), you must specify both the target region with the  option and a bucket path that already exists in the target region or a unique bucket path that can be created in the target region. (optional) after the bundle is uploaded to amazon s3, you can remove the bundle from the  directory on the instance using the following rm command: importantif you specified a path with the  option in , use that same path below, instead of . to register your ami, run the  aws cli command as follows. importantif you previously specified a region for the  command, specify that region again for this command. [ubuntu 14.04 and later] uncomment the efi entry in ; otherwise, your running instance will not be able to reboot. 
an ec2 fleet is a group of on-demand instances and spot instances.  the ec2 fleet attempts to launch the number of instances that are required to meet the target capacity that you specify in the fleet request. the fleet can comprise only on-demand instances, only spot instances, or a combination of both on-demand instances and spot instances. the request for spot instances is fulfilled if there is available capacity and the maximum price per hour for your request exceeds the spot price. the fleet also attempts to maintain its target capacity if your spot instances are interrupted. you can also set a maximum amount per hour that you’re willing to pay for your fleet, and ec2 fleet launches instances until it reaches the maximum amount. when the maximum amount you're willing to pay is reached, the fleet stops launching instances even if it hasn’t met the target capacity. a spot instance pool is a set of unused ec2 instances with the same instance type, operating system, availability zone, and network platform. when you create an ec2 fleet, you can include multiple launch specifications, which vary by instance type, availability zone, subnet, and maximum price. the fleet selects the spot instance pools that are used to fulfill the request, based on the launch specifications included in your request, and the configuration of the request. the spot instances come from the selected pools. an ec2 fleet enables you to provision large amounts of ec2 capacity that makes sense for your application based on number of cores or instances, or amount of memory. for example, you can specify an ec2 fleet to launch a target capacity of 200 instances, of which 130 are on-demand instances and the rest are spot instances. or you can request 1000 cores with a minimum of 2 gb of ram per core. the fleet determines the combination of amazon ec2 options to launch that capacity at the absolute lowest cost. use the appropriate configuration strategies to create an ec2 fleet that meets your needs. topics when planning your ec2 fleet, we recommend that you do the following: determine whether you want to create an ec2 fleet that submits a synchronous or asynchronous one-time request for the desired target capacity, or one that maintains a target capacity over time. for more information, see .determine the instance types that meet your application requirements.if you plan to include spot instances in your ec2 fleet, review  before you create the fleet. use these best practices when you plan your fleet so that you can provision the instances at the lowest possible price.determine the target capacity for your ec2 fleet. you can set target capacity in instances or in custom units. for more information, see .determine what portion of the ec2 fleet target capacity must be on-demand capacity and spot capacity. you can specify 0 for on-demand capacity or spot capacity, or both.determine your price per unit, if you are using instance weighting. to calculate the price per unit, divide the price per instance hour by the number of units (or weight) that this instance represents. if you are not using instance weighting, the default price per unit is the price per instance hour.determine the maximum amount per hour that you’re willing to pay for your fleet. for more information, see .review the possible options for your ec2 fleet. for more information, see the . for ec2 fleet configuration examples, see .there are three types of ec2 fleet requests:  if you configure the request type as , ec2 fleet places a synchronous one-time request for your desired capacity. in the api response, it returns the instances that launched, along with errors for those instances that could not be launched. if you configure the request type as , ec2 fleet places an asynchronous one-time request for your desired capacity. thereafter, if capacity is diminished because of spot interruptions, the fleet does not attempt to replenish spot instances, nor does it submit requests in alternative spot instance pools if capacity is unavailable. (default) if you configure the request type as , ec2 fleet places an asynchronous request for your desired capacity, and maintains capacity by automatically replenishing any interrupted spot instances. all three types of requests benefit from an allocation strategy. for more information, see . the allocation strategy for your ec2 fleet determines how it fulfills your request for spot instances from the possible spot instance pools represented by its launch specifications. the following are the allocation strategies that you can specify in your fleet: the spot instances come from the pool with the lowest price. this is the default strategy. the spot instances are distributed across all pools. the spot instances come from the pool with optimal capacity for the number of instances that are launching. the spot instances are distributed across the number of spot pools that you specify. this parameter is valid only when used in combination with . after spot instances are terminated due to a change in the spot price or available capacity of a spot instance pool, an ec2 fleet of type  launches replacement spot instances. if the allocation strategy is , the fleet launches replacement instances in the pool where the spot price is currently the lowest. if the allocation strategy is  in combination with , the fleet selects the spot pools with the lowest price and launches spot instances across the number of spot pools that you specify. if the allocation strategy is , the fleet launches replacement instances in the pool that has the most available spot instance capacity. if the allocation strategy is , the fleet distributes the replacement spot instances across the remaining pools. to optimize the costs for your use of spot instances, specify the  allocation strategy so that ec2 fleet automatically deploys the least expensive combination of instance types and availability zones based on the current spot price. for on-demand instance target capacity, ec2 fleet always selects the cheapest instance type based on the public on-demand price, while continuing to follow the allocation strategy (either , , or ) for spot instances. to create a fleet of spot instances that is both cheap and diversified, use the  allocation strategy in combination with . ec2 fleet automatically deploys the least expensive combination of instance types and availability zones based on the current spot price across the number of spot pools that you specify. this combination can be used to avoid the most expensive spot instances. with spot instances, pricing changes slowly over time based on long-term trends in supply and demand, but capacity fluctuates in real time. the  strategy automatically launches spot instances into the most available pools by looking at real-time capacity data and predicting which are the most available. this works well for workloads such as big data and analytics, image and media rendering, machine learning, and high performance computing that may have a higher cost of interruption associated with restarting work and checkpointing. by offering the possibility of fewer interruptions, the  strategy can lower the overall cost of your workload. you can optimize your fleet based on your use case. if your fleet is small or runs for a short time, the probability that your spot instances will be interrupted is low, even with all of the instances in a single spot instance pool. therefore, the  strategy is likely to meet your needs while providing the lowest cost. if your fleet is large or runs for a long time, you can improve the availability of your fleet by distributing the spot instances across multiple pools. for example, if your ec2 fleet specifies 10 pools and a target capacity of 100 instances, the fleet launches 10 spot instances in each pool. if the spot price for one pool exceeds your maximum price for this pool, only 10% of your fleet is affected. using this strategy also makes your fleet less sensitive to increases in the spot price in any one pool over time. with the  strategy, the ec2 fleet does not launch spot instances into any pools with a spot price that is equal to or higher than the . to create a cheap and diversified fleet, use the  strategy in combination with . you can use a low or high number of spot pools across which to allocate your spot instances. for example, if you run batch processing, we recommend specifying a low number of spot pools (for example, ) to ensure that your queue always has compute capacity while maximizing savings. if you run a web service, we recommend specifying a high number of spot pools (for example, ) to minimize the impact if a spot instance pool becomes temporarily unavailable. if your fleet runs workloads that may have a higher cost of interruption associated with restarting work and checkpointing, then use the  strategy. this strategy offers the possibility of fewer interruptions, which can lower the overall cost of your workload. if you have urgent, unpredictable scaling needs, such as a news website that must scale during a major news event or game launch, we recommend that you specify alternative instance types for your on-demand instances, in the event that your preferred option does not have sufficient available capacity. for example, you might prefer  on-demand instances, but if there is insufficient available capacity, you'd be willing to use some  instances during peak load. in this case, ec2 fleet attempts to fulfill all of your target capacity using  instances, but if there is insufficient capacity, it automatically launches  instances to fulfill the target capacity. when ec2 fleet attempts to fulfill your on-demand capacity, it defaults to launching the lowest-priced instance type first. if  is set to , ec2 fleet uses priority to determine which instance type to use first in fulfilling on-demand capacity. the priority is assigned to the launch template override, and the highest priority is launched first.  for example, you have configured three launch template overrides, each with a different instance type: , , and . the on-demand price for  is less than the price for .  is the cheapest. if you do not use priority to determine the order, the fleet fulfills on-demand capacity by starting with , and then . because you often have unused reserved instances for , you can set the launch template override priority so that the order is , , and then . you can configure a fleet to use on-demand capacity reservations first when launching on-demand instances by setting the usage strategy for capacity reservations to . you can use this setting in conjunction with the allocation strategy for on-demand instances ( or ). when unused capacity reservations are used to fulfil on-demand capacity: the fleet uses unused capacity reservations to fulfill on-demand capacity up to the target on-demand capacity.if multiple instance pools have unused capacity reservations, the on-demand allocation strategy ( or ) is applied.if the number of unused capacity reservations is less than the on-demand target capacity, the remaining on-demand target capacity is launched according to the on-demand allocation strategy ( or ).you can only use unused on-demand capacity reservations for fleets of type . for examples of how to configure a fleet to use capacity reservations to fulfil on-demand capacity, see . for more information, see  and the . each ec2 fleet can either include a global maximum price, or use the default (the on-demand price). the fleet uses this as the default maximum price for each of its launch specifications. you can optionally specify a maximum price in one or more launch specifications. this price is specific to the launch specification. if a launch specification includes a specific price, the ec2 fleet uses this maximum price, overriding the global maximum price. any other launch specifications that do not include a specific maximum price still use the global maximum price. ec2 fleet stops launching instances when it has met one of the following parameters: the  or the  (the maximum amount you’re willing to pay). to control the amount you pay per hour for your fleet, you can specify the . when the maximum total price is reached, ec2 fleet stops launching instances even if it hasn’t met the target capacity. the following examples show two different scenarios. in the first, ec2 fleet stops launching instances when it has met the target capacity. in the second, ec2 fleet stops launching instances when it has reached the maximum amount you’re willing to pay (). example: stop launching instances when target capacity is reached given a request for  on-demand instances, where: on-demand price: $0.10 per hour: 10: $1.50ec2 fleet launches 10 on-demand instances because the total of $1.00 (10 instances x $0.10) does not exceed the  of $1.50 for on-demand instances. example: stop launching instances when maximum total price is reached given a request for  on-demand instances, where: on-demand price: $0.10 per hour: 10: $0.80if ec2 fleet launches the on-demand target capacity (10 on-demand instances), the total cost per hour would be $1.00. this is more than the amount ($0.80) specified for  for on-demand instances. to prevent spending more than you're willing to pay, ec2 fleet launches only 8 on-demand instances (below the on-demand target capacity) because launching more would exceed the  for on-demand instances. when you create an ec2 fleet, you can define the capacity units that each instance type would contribute to your application's performance. you can then adjust your maximum price for each launch specification by using instance weighting. by default, the price that you specify is per instance hour. when you use the instance weighting feature, the price that you specify is per unit hour. you can calculate your price per unit hour by dividing your price for an instance type by the number of units that it represents. ec2 fleet calculates the number of instances to launch by dividing the target capacity by the instance weight. if the result isn't an integer, the fleet rounds it up to the next integer, so that the size of your fleet is not below its target capacity. the fleet can select any pool that you specify in your launch specification, even if the capacity of the instances launched exceeds the requested target capacity. the following table includes examples of calculations to determine the price per unit for an ec2 fleet with a target capacity of 10. use ec2 fleet instance weighting as follows to provision the target capacity that you want in the pools with the lowest price per unit at the time of fulfillment: set the target capacity for your ec2 fleet either in instances (the default) or in the units of your choice, such as virtual cpus, memory, storage, or throughput. set the price per unit. for each launch specification, specify the weight, which is the number of units that the instance type represents toward the target capacity. instance weighting exampleconsider an ec2 fleet request with the following configuration: a target capacity of 24a launch specification with an instance type  and a weight of 6a launch specification with an instance type  and a weight of 5the weights represent the number of units that instance type represents toward the target capacity. if the first launch specification provides the lowest price per unit (price for  per instance hour divided by 6), the ec2 fleet would launch four of these instances (24 divided by 6). if the second launch specification provides the lowest price per unit (price for  per instance hour divided by 5), the ec2 fleet would launch five of these instances (24 divided by 5, result rounded up). instance weighting and allocation strategyconsider an ec2 fleet request with the following configuration: a target capacity of 30 spot instancesa launch specification with an instance type  and a weight of 8a launch specification with an instance type  and a weight of 8a launch specification with an instance type  and a weight of 8the ec2 fleet would launch four instances (30 divided by 8, result rounded up). with the  strategy, all four instances come from the pool that provides the lowest price per unit. with the  strategy, the fleet launches one instance in each of the three pools, and the fourth instance in whichever of the three pools provides the lowest price per unit. this tutorial uses a fictitious company called example corp to illustrate the process of requesting an ec2 fleet using instance weighting. example corp, a pharmaceutical company, wants to use the computational power of amazon ec2 for screening chemical compounds that might be used to fight cancer. example corp first reviews . next, example corp determines the requirements for their ec2 fleet. instance typesexample corp has a compute- and memory-intensive application that performs best with at least 60 gb of memory and eight virtual cpus (vcpus). they want to maximize these resources for the application at the lowest possible price. example corp decides that any of the following ec2 instance types would meet their needs: target capacity in unitswith instance weighting, target capacity can equal a number of instances (the default) or a combination of factors such as cores (vcpus), memory (gibs), and storage (gbs). by considering the base for their application (60 gb of ram and eight vcpus) as one unit, example corp decides that 20 times this amount would meet their needs. so the company sets the target capacity of their ec2 fleet request to 20. instance weightsafter determining the target capacity, example corp calculates instance weights. to calculate the instance weight for each instance type, they determine the units of each instance type that are required to reach the target capacity as follows: r3.2xlarge (61.0 gb, 8 vcpus) = 1 unit of 20r3.4xlarge (122.0 gb, 16 vcpus) = 2 units of 20r3.8xlarge (244.0 gb, 32 vcpus) = 4 units of 20therefore, example corp assigns instance weights of 1, 2, and 4 to the respective launch configurations in their ec2 fleet request. price per unit hourexample corp uses the  per instance hour as a starting point for their price. they could also use recent spot prices, or a combination of the two. to calculate the price per unit hour, they divide their starting price per instance hour by the weight. for example: example corp could use a global price per unit hour of $0.7 and be competitive for all three instance types. they could also use a global price per unit hour of $0.7 and a specific price per unit hour of $0.9 in the  launch specification. before creating an ec2 fleet, example corp verifies that it has an iam role with the required permissions. for more information, see . example corp creates a file, , with the following configuration for its ec2 fleet. example corp creates the ec2 fleet using the following  command. for more information, see . the allocation strategy determines which spot instance pools your spot instances come from. with the  strategy (which is the default strategy), the spot instances come from the pool with the lowest price per unit at the time of fulfillment. to provide 20 units of capacity, the ec2 fleet launches either 20  instances (20 divided by 1), 10  instances (20 divided by 2), or 5  instances (20 divided by 4). if example corp used the  strategy, the spot instances would come from all three pools. the ec2 fleet would launch 6  instances (which provide 6 units), 3  instances (which provide 6 units), and 2  instances (which provide 8 units), for a total of 20 units. this tutorial uses a fictitious company called abc online to illustrate the process of requesting an ec2 fleet with on-demand as the primary capacity, and spot capacity if available. abc online, a restaurant delivery company, wants to be able to provision amazon ec2 capacity across ec2 instance types and purchasing options to achieve their desired scale, performance, and cost. abc online requires a fixed capacity to operate during peak periods, but would like to benefit from increased capacity at a lower price. abc online determines the following requirements for their ec2 fleet: on-demand instance capacity – abc online requires 15 on-demand instances to ensure that they can accommodate traffic at peak periods.spot instance capacity – abc online would like to improve performance, but at a lower price, by provisioning 5 spot instances.before creating an ec2 fleet, abc online verifies that it has an iam role with the required permissions. for more information, see . abc online creates a file, , with the following configuration for its ec2 fleet. abc online creates the ec2 fleet using the following  command. for more information, see . the allocation strategy determines that the on-demand capacity is always fulfilled, while the balance of the target capacity is fulfilled as spot if there is capacity and availability. 
a spot fleet is a collection, or fleet, of spot instances, and optionally on-demand instances.  the spot fleet attempts to launch the number of spot instances and on-demand instances to meet the target capacity that you specified in the spot fleet request. the request for spot instances is fulfilled if there is available capacity and the maximum price you specified in the request exceeds the current spot price. the spot fleet also attempts to maintain its target capacity fleet if your spot instances are interrupted. you can also set a maximum amount per hour that you’re willing to pay for your fleet, and spot fleet launches instances until it reaches the maximum amount. when the maximum amount you're willing to pay is reached, the fleet stops launching instances even if it hasn’t met the target capacity. a spot instance pool is a set of unused ec2 instances with the same instance type (for example, ), operating system, availability zone, and network platform. when you make a spot fleet request, you can include multiple launch specifications, that vary by instance type, ami, availability zone, or subnet. the spot fleet selects the spot instance pools that are used to fulfill the request, based on the launch specifications included in your spot fleet request, and the configuration of the spot fleet request. the spot instances come from the selected pools. topics to ensure that you always have instance capacity, you can include a request for on-demand capacity in your spot fleet request. in your spot fleet request, you specify your desired target capacity and how much of that capacity must be on-demand. the balance comprises spot capacity, which is launched if there is available amazon ec2 capacity and availability. for example, if in your spot fleet request you specify target capacity as 10 and on-demand capacity as 8, amazon ec2 launches 8 capacity units as on-demand, and 2 capacity units (10-8=2) as spot. when spot fleet attempts to fulfill your on-demand capacity, it defaults to launching the lowest-priced instance type first. if  is set to , spot fleet uses priority to determine which instance type to use first in fulfilling on-demand capacity. the priority is assigned to the launch template override, and the highest priority is launched first. for example, you have configured three launch template overrides, each with a different instance type: , , and . the on-demand price for  is less than for .  is the cheapest. if you do not use priority to determine the order, the fleet fulfills on-demand capacity by starting with , and then . because you often have unused reserved instances for , you can set the launch template override priority so that the order is , , and then . the allocation strategy for the spot instances in your spot fleet determines how it fulfills your spot fleet request from the possible spot instance pools represented by its launch specifications. the following are the allocation strategies that you can specify in your spot fleet request: the spot instances come from the pool with the lowest price. this is the default strategy. the spot instances are distributed across all pools. the spot instances come from the pool with optimal capacity for the number of instances that are launching. the spot instances are distributed across the number of spot pools that you specify. this parameter is valid only when used in combination with . after spot instances are terminated due to a change in the spot price or available capacity of a spot instance pool, a spot fleet of type  launches replacement spot instances. if the allocation strategy is , the fleet launches replacement instances in the pool where the spot price is currently the lowest. if the allocation strategy is , the fleet distributes the replacement spot instances across the remaining pools. if the allocation strategy is  in combination with , the fleet selects the spot pools with the lowest price and launches spot instances across the number of spot pools that you specify. to optimize the costs for your use of spot instances, specify the  allocation strategy so that spot fleet automatically deploys the least expensive combination of instance types and availability zones based on the current spot price. for on-demand instance target capacity, spot fleet always selects the least expensive instance type based on the public on-demand price, while continuing to follow the allocation strategy (either , , or ) for spot instances. to create a fleet of spot instances that is both cheap and diversified, use the  allocation strategy in combination with . spot fleet automatically deploys the cheapest combination of instance types and availability zones based on the current spot price across the number of spot pools that you specify. this combination can be used to avoid the most expensive spot instances. with spot instances, pricing changes slowly over time based on long-term trends in supply and demand, but capacity fluctuates in real time. the  strategy automatically launches spot instances into the most available pools by looking at real-time capacity data and predicting which are the most available. this works well for workloads such as big data and analytics, image and media rendering, machine learning, and high performance computing that may have a higher cost of interruption associated with restarting work and checkpointing. by offering the possibility of fewer interruptions, the  strategy can lower the overall cost of your workload. you can optimize your spot fleets based on your use case. if your fleet is small or runs for a short time, the probability that your spot instances may be interrupted is low, even with all the instances in a single spot instance pool. therefore, the  strategy is likely to meet your needs while providing the lowest cost. if your fleet is large or runs for a long time, you can improve the availability of your fleet by distributing the spot instances across multiple pools. for example, if your spot fleet request specifies 10 pools and a target capacity of 100 instances, the fleet launches 10 spot instances in each pool. if the spot price for one pool exceeds your maximum price for this pool, only 10% of your fleet is affected. using this strategy also makes your fleet less sensitive to increases in the spot price in any one pool over time. with the  strategy, the spot fleet does not launch spot instances into any pools with a spot price that is equal to or higher than the . to create a cheap and diversified fleet, use the  strategy in combination with . you can use a low or high number of spot pools across which to allocate your spot instances. for example, if you run batch processing, we recommend specifying a low number of spot pools (for example, ) to ensure that your queue always has compute capacity while maximizing savings. if you run a web service, we recommend specifying a high number of spot pools (for example, ) to minimize the impact if a spot instance pool becomes temporarily unavailable. if your fleet runs workloads that may have a higher cost of interruption associated with restarting work and checkpointing, then use the  strategy. this strategy offers the possibility of fewer interruptions, which can lower the overall cost of your workload. each spot fleet request can include a global maximum price, or use the default (the on-demand price). spot fleet uses this as the default maximum price for each of its launch specifications. you can optionally specify a maximum price in one or more launch specifications. this price is specific to the launch specification. if a launch specification includes a specific price, the spot fleet uses this maximum price, overriding the global maximum price. any other launch specifications that do not include a specific maximum price still use the global maximum price. spot fleet stops launching instances when it has either reached the target capacity or the maximum amount you’re willing to pay. to control the amount you pay per hour for your fleet, you can specify the  for spot instances and the  for on-demand instances. when the maximum total price is reached, spot fleet stops launching instances even if it hasn’t met the target capacity. the following examples show two different scenarios. in the first, spot fleet stops launching instances when it has met the target capacity. in the second, spot fleet stops launching instances when it has reached the maximum amount you’re willing to pay. example: stop launching instances when target capacity is reached given a request for  on-demand instances, where: on-demand price: $0.10 per hour: 10: $1.50spot fleet launches 10 on-demand instances because the total of $1.00 (10 instances x $0.10) does not exceed the  of $1.50. example: stop launching instances when maximum total price is reached given a request for  on-demand instances, where: on-demand price: $0.10 per hour: 10: $0.80if spot fleet launches the on-demand target capacity (10 on-demand instances), the total cost per hour would be $1.00. this is more than the amount ($0.80) specified for . to prevent spending more than you're willing to pay, spot fleet launches only 8 on-demand instances (below the on-demand target capacity) because launching more would exceed the . when you request a fleet of spot instances, you can define the capacity units that each instance type would contribute to your application's performance, and adjust your maximum price for each spot instance pool accordingly using instance weighting. by default, the price that you specify is per instance hour. when you use the instance weighting feature, the price that you specify is per unit hour. you can calculate your price per unit hour by dividing your price for an instance type by the number of units that it represents. spot fleet calculates the number of spot instances to launch by dividing the target capacity by the instance weight. if the result isn't an integer, the spot fleet rounds it up to the next integer, so that the size of your fleet is not below its target capacity. spot fleet can select any pool that you specify in your launch specification, even if the capacity of the instances launched exceeds the requested target capacity. the following tables provide examples of calculations to determine the price per unit for a spot fleet request with a target capacity of 10. use spot fleet instance weighting as follows to provision the target capacity that you want in the pools with the lowest price per unit at the time of fulfillment: set the target capacity for your spot fleet either in instances (the default) or in the units of your choice, such as virtual cpus, memory, storage, or throughput. set the price per unit. for each launch configuration, specify the weight, which is the number of units that the instance type represents toward the target capacity. instance weighting exampleconsider a spot fleet request with the following configuration: a target capacity of 24a launch specification with an instance type  and a weight of 6a launch specification with an instance type  and a weight of 5the weights represent the number of units that instance type represents toward the target capacity. if the first launch specification provides the lowest price per unit (price for  per instance hour divided by 6), the spot fleet would launch four of these instances (24 divided by 6). if the second launch specification provides the lowest price per unit (price for  per instance hour divided by 5), the spot fleet would launch five of these instances (24 divided by 5, result rounded up). instance weighting and allocation strategyconsider a spot fleet request with the following configuration: a target capacity of 30a launch specification with an instance type  and a weight of 8a launch specification with an instance type  and a weight of 8a launch specification with an instance type  and a weight of 8the spot fleet would launch four instances (30 divided by 8, result rounded up). with the  strategy, all four instances come from the pool that provides the lowest price per unit. with the  strategy, the spot fleet launches one instance in each of the three pools, and the fourth instance in whichever pool provides the lowest price per unit. this walkthrough uses a fictitious company called example corp to illustrate the process of requesting a spot fleet using instance weighting. example corp, a pharmaceutical company, wants to leverage the computational power of amazon ec2 for screening chemical compounds that might be used to fight cancer. example corp first reviews . next, example corp determines the following requirements for their spot fleet. instance typesexample corp has a compute- and memory-intensive application that performs best with at least 60 gb of memory and eight virtual cpus (vcpus). they want to maximize these resources for the application at the lowest possible price. example corp decides that any of the following ec2 instance types would meet their needs: target capacity in unitswith instance weighting, target capacity can equal a number of instances (the default) or a combination of factors such as cores (vcpus), memory (gibs), and storage (gbs). by considering the base for their application (60 gb of ram and eight vcpus) as 1 unit, example corp decides that 20 times this amount would meet their needs. so the company sets the target capacity of their spot fleet request to 20. instance weightsafter determining the target capacity, example corp calculates instance weights. to calculate the instance weight for each instance type, they determine the units of each instance type that are required to reach the target capacity as follows: r3.2xlarge (61.0 gb, 8 vcpus) = 1 unit of 20r3.4xlarge (122.0 gb, 16 vcpus) = 2 units of 20r3.8xlarge (244.0 gb, 32 vcpus) = 4 units of 20therefore, example corp assigns instance weights of 1, 2, and 4 to the respective launch configurations in their spot fleet request. price per unit hourexample corp uses the  per instance hour as a starting point for their price. they could also use recent spot prices, or a combination of the two. to calculate the price per unit hour, they divide their starting price per instance hour by the weight. for example: example corp could use a global price per unit hour of $0.7 and be competitive for all three instance types. they could also use a global price per unit hour of $0.7 and a specific price per unit hour of $0.9 in the  launch specification. before creating a spot fleet request, example corp verifies that it has an iam role with the required permissions. for more information, see . example corp creates a file, , with the following configuration for its spot fleet request: example corp creates the spot fleet request using the  command. for more information, see . the allocation strategy determines which spot instance pools your spot instances come from. with the  strategy (which is the default strategy), the spot instances come from the pool with the lowest price per unit at the time of fulfillment. to provide 20 units of capacity, the spot fleet launches either 20  instances (20 divided by 1), 10  instances (20 divided by 2), or 5  instances (20 divided by 4). if example corp used the  strategy, the spot instances would come from all three pools. the spot fleet would launch 6  instances (which provide 6 units), 3  instances (which provide 6 units), and 2  instances (which provide 8 units), for a total of 20 units. 
amazon ec2 provides you with flexible, cost effective, and easy-to-use data storage options for your instances. each option has a unique combination of performance and durability. these storage options can be used independently or in combination to suit your requirements. after reading this section, you should have a good understanding about how you can use the data storage options supported by amazon ec2 to meet your specific requirements. these storage options include the following: the following figure shows the relationship between these storage options and your instance.  amazon ebsamazon ebs provides durable, block-level storage volumes that you can attach to a running instance. you can use amazon ebs as a primary storage device for data that requires frequent and granular updates. for example, amazon ebs is the recommended storage option when you run a database on an instance. an ebs volume behaves like a raw, unformatted, external block device that you can attach to a single instance. the volume persists independently from the running life of an instance. after an ebs volume is attached to an instance, you can use it like any other physical hard drive. as illustrated in the previous figure, multiple volumes can be attached to an instance. you can also detach an ebs volume from one instance and attach it to another instance. you can dynamically change the configuration of a volume attached to an instance. ebs volumes can also be created as encrypted volumes using the amazon ebs encryption feature. for more information, see . to keep a backup copy of your data, you can create a snapshot of an ebs volume, which is stored in amazon s3. you can create an ebs volume from a snapshot, and attach it to another instance. for more information, see . amazon ec2 instance storemany instances can access storage from disks that are physically attached to the host computer. this disk storage is referred to as instance store. instance store provides temporary block-level storage for instances. the data on an instance store volume persists only during the life of the associated instance; if you stop or terminate an instance, any data on instance store volumes is lost. for more information, see . amazon efs file systemamazon efs provides scalable file storage for use with amazon ec2. you can create an efs file system and configure your instances to mount the file system. you can use an efs file system as a common data source for workloads and applications running on multiple instances. for more information, see . amazon s3amazon s3 provides access to reliable and inexpensive data storage infrastructure. it is designed to make web-scale computing easier by enabling you to store and retrieve any amount of data, at any time, from within amazon ec2 or anywhere on the web. for example, you can use amazon s3 to store backup copies of your data and applications. amazon ec2 uses amazon s3 to store ebs snapshots and instance store-backed amis. for more information, see . adding storageevery time you launch an instance from an ami, a root storage device is created for that instance. the root storage device contains all the information necessary to boot the instance. you can specify storage volumes in addition to the root device volume when you create an ami or launch an instance using block device mapping. for more information, see . you can also attach ebs volumes to a running instance. for more information, see . storage pricing for information about storage pricing, open , scroll down to services pricing, choose storage, and then choose the storage option to open that storage option's pricing page. for information about estimating the cost of storage, see the . 
the nvidia collective communications library (nccl) is a library of standard collective communication routines for multiple gpus across a single node or multiple nodes. nccl can be used together with efa, libfabric, and mpi to support various machine learning workloads. for more information, see the  website. notenccl with efa is supported with p3dn.24xlarge instances only. only nccl 2.4.2 and later is supported with efa. the following tutorials help you to launch an efa and nccl-enabled instance cluster for machine learning workloads. 
the following examples show launch configurations that you can use with the  command to create a spot instance request. for more information, see .     the following example does not include an availability zone or subnet. amazon ec2 selects an availability zone for you. amazon ec2 launches the instances in the default subnet of the selected availability zone. the following example includes an availability zone. amazon ec2 launches the instances in the default subnet of the specified availability zone. the following example includes a subnet. amazon ec2 launches the instances in the specified subnet. if the vpc is a nondefault vpc, the instance does not receive a public ipv4 address by default. to assign a public ipv4 address to an instance in a nondefault vpc, specify the  field as shown in the following example. when you specify a network interface, you must include the subnet id and security group id using the network interface, rather than using the  and  fields shown in example 3. the following example requests spot instance with a tenancy of . a dedicated spot instance must be launched in a vpc. 
because your instance metadata is available from your running instance, you do not need to use the amazon ec2 console or the aws cli. this can be helpful when you're writing scripts to run from your instance. for example, you can access the local ip address of your instance from instance metadata to manage a connection to an external application. instance metadata is divided into categories. for a description of each instance metadata category, see . to view all categories of instance metadata from within a running instance, use the following uri. the ip address  is a link-local address and is valid only from the instance. for more information, see  on wikipedia. note that you are not billed for http requests used to retrieve instance metadata and user data. the command format is different, depending on whether you use imdsv1 or imdsv2. by default, you can use both instance metadata services. to require the use of imdsv2, see . you can use a tool such as curl, as shown in the following example. you can also download the , which allows you to query the instance metadata using instance metadata service version 1 without having to enter the full uri or category names. all instance metadata is returned as text (http content type ). a request for a specific metadata resource returns the appropriate value, or a  http error code if the resource is not available.  a request for a general metadata resource (the uri ends with a /) returns a list of available resources, or a  http error code if there is no such resource. the list items are on separate lines, terminated by line feeds (ascii 10). for requests made using instance metadata service version 2, the following http error codes can be returned:  – the  request is not valid. – the  request uses an invalid token. the recommended action is to generate a new token. – the request is not allowed or the instance metadata service is turned off.topics this example gets the available versions of the instance metadata. these versions do not necessarily correlate with an amazon ec2 api version. the earlier versions are available to you in case you have scripts that rely on the structure and information present in a previous version.  this example gets the top-level metadata items. for more information, see . the following examples get the values of some of the top-level metadata items that were obtained in the preceding example. the imdsv2 requests use the stored token that was created in the preceding example command, assuming it has not expired.       this example gets the list of available public keys. this example shows the formats in which public key 0 is available. this example gets public key 0 (in the openssh key format). this example gets the subnet id for an instance. we throttle queries to the instance metadata service on a per-instance basis, and we place limits on the number of simultaneous connections from an instance to the instance metadata service.  if you're using the instance metadata service to retrieve aws security credentials, avoid querying for credentials during every transaction or concurrently from a high number of threads or processes, as this might lead to throttling. instead, we recommend that you cache the credentials until they start approaching their expiry time.  if you are throttled while accessing the instance metadata service, retry your query with an exponential backoff strategy. you can consider using local firewall rules to disable access from some or all processes to the instance metadata service. using iptables to limit access the following example uses linux iptables and its  module to prevent the apache webserver (based on its default installation user id of ) from accessing 169.254.169.254. it uses a deny rule to reject all instance metadata requests (whether imdsv1 or imdsv2) from any process running as that user. or, you can consider only allowing access to particular users or groups, by using allow rules. allow rules might be easier to manage from a security perspective, because they require you to make a decision about what software needs access to instance metadata. if you use allow rules, it's less likely you will accidentally allow software to access the metadata service (that you did not intend to have access) if you later change the software or configuration on an instance. you can also combine group usage with allow rules, so that you can add and remove users from a permitted group without needing to change the firewall rule. the following example prevents access to the instance metadata service by all processes, except for processes running in the user account . noteto use local firewall rules, you need to adapt the preceding example commands to suit your needs.  by default, iptables rules are not persistent across system reboots. they can be made to be persistent by using os features, not described here. the iptables  module only matches group membership if the group is the primary group of a given local user. other groups are not matched. using pf or ipfw to limit access if you are using freebsd or openbsd, you can also consider using pf or ipfw. the following examples limit access to the instance metadata service to just the root user. pf ipfw notethe order of the pf and ipfw commands matter. pf defaults to last matching rule and ipfw defaults to first matching rule. 
cloud file storage is a method for storing data in the cloud that provides servers and applications access to data through shared file systems. this compatibility makes cloud file storage ideal for workloads that rely on shared file systems and provides simple integration without code changes. there are many file storage solutions that exist, ranging from a single node file server on a compute instance using block storage as the underpinnings with no scalability or few redundancies to protect the data, to a do-it-yourself clustered solution, to a fully-managed solution, such as  or . 
to help you manage your instances, images, and other amazon ec2 resources, you can assign your own metadata to each resource in the form of tags. tags enable you to categorize your aws resources in different ways, for example, by purpose, owner, or environment. this is useful when you have many resources of the same type—you can quickly identify a specific resource based on the tags that you've assigned to it. this topic describes tags and shows you how to create them. warningtag keys and their values are returned by many different api calls. denying access to  doesn’t automatically deny access to tags returned by other apis. as a best practice, we recommend that you do not include sensitive data in your tags. topics a tag is a label that you assign to an aws resource. each tag consists of a key and an optional value, both of which you define. tags enable you to categorize your aws resources in different ways, for example, by purpose, owner, or environment. for example, you could define a set of tags for your account's amazon ec2 instances that helps you track each instance's owner and stack level. the following diagram illustrates how tagging works. in this example, you've assigned two tags to each of your instances—one tag with the key  and another with the key . each tag also has an associated value.  we recommend that you devise a set of tag keys that meets your needs for each resource type. using a consistent set of tag keys makes it easier for you to manage your resources. you can search and filter the resources based on the tags you add. for more information about how to implement an effective resource tagging strategy, see the aws whitepaper . tags don't have any semantic meaning to amazon ec2 and are interpreted strictly as a string of characters. also, tags are not automatically assigned to your resources. you can edit tag keys and values, and you can remove tags from a resource at any time. you can set the value of a tag to an empty string, but you can't set the value of a tag to null. if you add a tag that has the same key as an existing tag on that resource, the new value overwrites the old value. if you delete a resource, any tags for the resource are also deleted. you can work with tags using the aws management console, the aws cli, and the amazon ec2 api. if you're using aws identity and access management (iam), you can control which users in your aws account have permission to create, edit, or delete tags. for more information, see . you can tag most amazon ec2 resources that already exist in your account. the  below lists the resources that support tagging. if you're using the amazon ec2 console, you can apply tags to resources by using the tags tab on the relevant resource screen, or you can use the tags screen. some resource screens enable you to specify tags for a resource when you create the resource; for example, a tag with a key of  and a value that you specify. in most cases, the console applies the tags immediately after the resource is created (rather than during resource creation). the console may organize resources according to the  tag, but this tag doesn't have any semantic meaning to the amazon ec2 service. if you're using the amazon ec2 api, the aws cli, or an aws sdk, you can use the  ec2 api action to apply tags to existing resources. additionally, some resource-creating actions enable you to specify tags for a resource when the resource is created. if tags cannot be applied during resource creation, we roll back the resource creation process. this ensures that resources are either created with tags or not created at all, and that no resources are left untagged at any time. by tagging resources at the time of creation, you can eliminate the need to run custom tagging scripts after resource creation. the following table describes the amazon ec2 resources that can be tagged, and the resources that can be tagged on creation using the amazon ec2 api, the aws cli, or an aws sdk. tagging support for amazon ec2 resources   you can tag instances and volumes on creation using the amazon ec2 launch instances wizard in the amazon ec2 console. you can tag your ebs volumes on creation using the volumes screen, or ebs snapshots using the snapshots screen. alternatively, use the resource-creating amazon ec2 apis (for example, ) to apply tags when creating your resource. you can apply tag-based resource-level permissions in your iam policies to the amazon ec2 api actions that support tagging on creation to implement granular control over the users and groups that can tag resources on creation. your resources are properly secured from creation—tags are applied immediately to your resources, therefore any tag-based resource-level permissions controlling the use of resources are immediately effective. your resources can be tracked and reported on more accurately. you can enforce the use of tagging on new resources, and control which tag keys and values are set on your resources.  you can also apply resource-level permissions to the  and  amazon ec2 api actions in your iam policies to control which tag keys and values are set on your existing resources. for more information, see .  for more information about tagging your resources for billing, see  in the aws billing and cost management user guide. the following basic restrictions apply to tags: maximum number of tags per resource – 50for each resource, each tag key must be unique, and each tag key can have only one value.maximum key length – 128 unicode characters in utf-8maximum value length – 256 unicode characters in utf-8although ec2 allows for any character in its tags, other services are more restrictive. the allowed characters across services are: letters, numbers, and spaces representable in utf-8, and the following characters: + - = . _ : / @.tag keys and values are case-sensitive.the  prefix is reserved for aws use. if a tag has a tag key with this prefix, then you can't edit or delete the tag's key or value. tags with the  prefix do not count against your tags per resource limit.you can't terminate, stop, or delete a resource based solely on its tags; you must specify the resource identifier. for example, to delete snapshots that you tagged with a tag key called , you must use the  action with the resource identifiers of the snapshots, such as .  you can tag public or shared resources, but the tags you assign are available only to your aws account and not to the other accounts sharing the resource. you can't tag all resources. for more information, see . you can use tags to organize your aws bill to reflect your own cost structure. to do this, sign up to get your aws account bill with tag key values included. for more information about setting up a cost allocation report with tags, see  in aws billing and cost management user guide. to see the cost of your combined resources, you can organize your billing information based on resources that have the same tag key values. for example, you can tag several resources with a specific application name, and then organize your billing information to see the total cost of that application across several services. for more information, see  in the aws billing and cost management user guide. noteif you've just enabled reporting, data for the current month is available for viewing after 24 hours. cost allocation tags can indicate which resources are contributing to costs, but deleting or deactivating resources doesn't always reduce costs. for example, snapshot data that is referenced by another snapshot is preserved, even if the snapshot that contains the original data is deleted. for more information, see  in the aws billing and cost management user guide. noteelastic ip addresses that are tagged do not appear on your cost allocation report. using the amazon ec2 console, you can see which tags are in use across all of your amazon ec2 resources in the same region. you can view tags by resource and by resource type, and you can also view how many items of each resource type are associated with a specified tag. you can also use the amazon ec2 console to apply or remove tags from one or more resources at a time. for more information about using filters when listing your resources, see . for ease of use and best results, use tag editor in the aws management console, which provides a central, unified way to create and manage your tags. for more information, see  in getting started with the aws management console. topics you can display tags in two different ways in the amazon ec2 console. you can display the tags for an individual resource or for all resources. displaying tags for individual resourceswhen you select a resource-specific page in the amazon ec2 console, it displays a list of those resources. for example, if you select instances from the navigation pane, the console displays a list of amazon ec2 instances. when you select a resource from one of these lists (for example, an instance), if the resource supports tags, you can view and manage its tags. on most resource pages, you can view the tags in the tags tab on the details pane. you can add a column to the resource list that displays all values for tags with the same key. this column enables you to sort and filter the resource list by the tag. there are two ways to add a new column to the resource list to display your tags. on the tags tab, select show column. a new column is added to the console.choose the show/hide columns gear-shaped icon, and in the show/hide columns dialog box, select the tag key under your tag keys.displaying tags for all resourcesyou can display tags across all resources by selecting tags from the navigation pane in the amazon ec2 console. the following image shows the tags pane, which lists all tags in use by resource type.  you can manage tags for an individual resource directly from the resource's page.  to add a tag to an individual resource open the amazon ec2 console at . from the navigation bar, select the region that meets your needs. this choice is important because some amazon ec2 resources can be shared between regions, while others can't. for more information, see . in the navigation pane, select a resource type (for example, instances). select the resource from the resource list and choose tags, add/edit tags. in the add/edit tags dialog box, specify the key and value for each tag, and then choose save. to delete a tag from an individual resource open the amazon ec2 console at . from the navigation bar, select the region that meets your needs. this choice is important because some amazon ec2 resources can be shared between regions, while others can't. for more information, see . in the navigation pane, choose a resource type (for example, instances). select the resource from the resource list and choose tags. choose add/edit tags, select the delete icon for the tag, and choose save. to add a tag to a group of resources open the amazon ec2 console at . from the navigation bar, select the region that meets your needs. this choice is important because some amazon ec2 resources can be shared between regions, while others can't. for more information, see . in the navigation pane, choose tags. at the top of the content pane, choose manage tags. for filter, select the type of resource (for example, instances) to which to add tags. in the resources list, select the check box next to each resource to which to add tags. under add tag, for key and value, type the tag key and values, and then choose add tag. noteif you add a new tag with the same tag key as an existing tag, the new tag overwrites the existing tag. to remove a tag from a group of resources open the amazon ec2 console at . from the navigation bar, select the region that meets your needs. this choice is important because some amazon ec2 resources can be shared between regions, while others can't. for more information, see . in the navigation pane, choose tags, manage tags. to view the tags in use, select the show/hide columns gear-shaped icon, and in the show/hide columns dialog box, select the tag keys to view and choose close. for filter, select the type of resource (for example, instances) from which to remove tags. in the resource list, select the check box next to each resource from which to remove tags. under remove tag, for key, type the tag's name and choose remove tag. to add a tag using the launch wizard from the navigation bar, select the region for the instance. this choice is important because some amazon ec2 resources can be shared between regions, while others can't. select the region that meets your needs. for more information, see . choose launch instance. the choose an amazon machine image (ami) page displays a list of basic configurations called amazon machine images (amis). select the ami to use and choose select. for more information about selecting an ami, see . on the configure instance details page, configure the instance settings as necessary, and then choose next: add storage. on the add storage page, you can specify additional storage volumes for your instance. choose next: add tags when done. on the add tags page, specify tags for the instance, the volumes, or both. choose add another tag to add more than one tag to your instance. choose next: configure security group when you are done.  on the configure security group page, you can choose from an existing security group that you own, or let the wizard create a new security group for you. choose review and launch when you are done. review your settings. when you're satisfied with your selections, choose launch. select an existing key pair or create a new one, select the acknowledgment check box, and then choose launch instances. you can filter your list of resources based on one or more tag keys and tag values. to filter a list of resources by tag display a column for the tag as follows: select a resource. in the details pane, choose tags. locate the tag in the list and choose show column. choose the filter icon in the top right corner of the column for the tag to display the filter list.  select the tag values, and then choose apply filter to filter the results list. notefor more information about filters, see . use the following to add, update, list, and delete the tags for your resources. the corresponding documentation provides examples. you can also filter a list of resources according to their tags. the following examples demonstrate how to filter your instances using tags with the  command. notethe way you enter json-formatted parameters on the command line differs depending on your operating system. linux, macos, or unix and windows powershell use the single quote (') to enclose the json data structure. omit the single quotes when using the commands with the windows command line. for more information, see . example 1: describe instances with the specified tag keythe following command describes the instances with a stack tag, regardless of the value of the tag. example 2: describe instances with the specified tagthe following command describes the instances with the tag stack=production. example 3: describe instances with the specified tag valuethe following command describes the instances with a tag with the value production, regardless of the tag key. some resource-creating actions enable you to specify tags when you create the resource. the following actions support tagging on creation. the following examples demonstrate how to apply tags when you create resources. example 4: launch an instance and apply tags to the instance and volumethe following command launches an instance and applies a tag with a key of  and value of  to the instance. the command also applies a tag with a key of  and a value of  to any ebs volume that's created (in this case, the root volume). you can apply the same tag keys and values to both instances and volumes during launch. the following command launches an instance and applies a tag with a key of  and a value of  to both the instance and any ebs volume that's created. example 5: create a volume and apply a tagthe following command creates a volume and applies two tags:  = , and  = . example 6: add a tag to a resourcethis example adds the tag  to the specified image, or overwrites an existing tag for the ami where the tag key is . if the command succeeds, no output is returned. example 7: add tags to multiple resourcesthis example adds (or overwrites) two tags for an ami and an instance. one of the tags contains just a key (), with no value (we set the value to an empty string). the other tag consists of a key () and value (). if the command succeeds, no output is returned. example 8: add tags with special charactersthis example adds the tag  to an instance. the square brackets ( and ) are special characters, which must be escaped. if you are using linux or os x, to escape the special characters, enclose the element with the special character with double quotes (), and then enclose the entire key and value structure with single quotes (). if you are using windows, to escape the special characters, enclose the element that has special characters with double quotes ("), and then precede each double quote character with a backslash () as follows: if you are using windows powershell, to escape the special characters, enclose the value that has special characters with double quotes (), precede each double quote character with a backslash (), and then enclose the entire key and value structure with single quotes () as follows: 
a consistent and accurate time reference is crucial for many server tasks and processes. most system logs include a time stamp that you can use to determine when problems occur and in what order the events take place. if you use the aws cli or an aws sdk to make requests from your instance, these tools sign requests on your behalf. if your instance's date and time are not set correctly, the date in the signature may not match the date of the request, and aws rejects the request.  amazon provides the amazon time sync service, which is accessible from all ec2 instances, and is also used by other aws services. this service uses a fleet of satellite-connected and atomic reference clocks in each region to deliver accurate current time readings of the coordinated universal time (utc) global standard through network time protocol (ntp). the amazon time sync service automatically smooths any leap seconds that are added to utc. the amazon time sync service is available through ntp at the  ip address for any instance running in a vpc. your instance does not require access to the internet, and you do not have to configure your security group rules or your network acl rules to allow access. the latest versions of amazon linux 2 and amazon linux amis synchronize with the amazon time sync service by default. use the following procedures to configure the amazon time sync service on your instance using the  client. alternatively, you can use external ntp sources. for more information about ntp and public time sources, see . an instance needs access to the internet for the external ntp time sources to work.  noteon amazon linux 2, the default  configuration is already set up to use the amazon time sync service ip address.  with the amazon linux ami, you must edit the  configuration file to add a server entry for the amazon time sync service. to configure your instance to use the amazon time sync service connect to your instance and uninstall the ntp service. install the  package. open the  file using a text editor (such as vim or nano). verify that the file includes the following line: if the line is present, then the amazon time sync service is already configured and you can go to the next step. if not, add the line after any other  or  statements that are already present in the file, and save your changes. restart the  daemon (). noteon rhel and centos (up to version 6), the service name is  instead of . use the  command to configure  to start at each system boot. verify that  is using the  ip address to synchronize the time. in the output that's returned,  indicates the preferred time source. verify the time synchronization metrics that are reported by . you must edit the  configuration file to add a server entry for the amazon time sync service. to configure your instance to use the amazon time sync service connect to your instance and use  to install the  package. noteif necessary, update your instance first by running . open the  file using a text editor (such as vim or nano). add the following line before any other  or  statements that are already present in the file, and save your changes: restart the  service. verify that  is using the  ip address to synchronize the time. in the output that's returned,  indicates the preferred time source. verify the time synchronization metrics that are reported by . install chrony from . open the  file using a text editor (such as vim or nano). verify that the file contains the following line: if this line is not present, add it. comment out any other server or pool lines. open yast and enable the chrony service. amazon linux instances are set to the utc (coordinated universal time) time zone by default, but you may wish to change the time on an instance to the local time or to another time zone in your network. importantthis information applies to amazon linux. for information about other distributions, see their specific documentation. to change the time zone on an instance identify the time zone to use on the instance. the  directory contains a hierarchy of time zone data files. browse the directory structure at that location to find a file for your time zone. some of the entries at this location are directories (such as ), and these directories contain time zone files for specific cities. find your city (or a city in your time zone) to use for the instance. in this example, you can use the time zone file for los angeles, . update the  file with the new time zone. open the  file with your favorite text editor (such as vim or nano). you need to use sudo with your editor command because  is owned by . locate the  entry, and change it to the time zone file (omitting the  section of the path). for example, to change to the los angeles time zone, change the  entry to the following: notedo not change the  entry to another value. this entry is for the hardware clock, and does not need to be adjusted when you're setting a different time zone on your instance. save the file and exit the text editor. create a symbolic link between  and your time zone file so that the instance finds the time zone file when it references local time information. reboot the system to pick up the new time zone information in all services and applications. 
an ec2 fleet contains the configuration information to launch a fleet—or group—of instances. in a single api call, a fleet can launch multiple instance types across multiple availability zones, using the on-demand instance, reserved instance, and spot instance purchasing options together. using ec2 fleet, you can: define separate on-demand and spot capacity targets and the maximum amount you’re willing to pay per hourspecify the instance types that work best for your applicationsspecify how amazon ec2 should distribute your fleet capacity within each purchasing optionyou can also set a maximum amount per hour that you’re willing to pay for your fleet, and ec2 fleet launches instances until it reaches the maximum amount. when the maximum amount you're willing to pay is reached, the fleet stops launching instances even if it hasn’t met the target capacity. the ec2 fleet attempts to launch the number of instances that are required to meet the target capacity specified in your request. if you specified a total maximum price per hour, it fulfills the capacity until it reaches the maximum amount that you’re willing to pay. the fleet can also attempt to maintain its target spot capacity if your spot instances are interrupted. for more information, see .  you can specify an unlimited number of instance types per ec2 fleet. those instance types can be provisioned using both on-demand and spot purchasing options. you can also specify multiple availability zones, specify different maximum spot prices for each instance, and choose additional spot options for each fleet. amazon ec2 uses the specified options to provision capacity when the fleet launches. while the fleet is running, if amazon ec2 reclaims a spot instance because of a price increase or instance failure, ec2 fleet can try to replace the instances with any of the instance types that you specify. this makes it easier to regain capacity during a spike in spot pricing. you can develop a flexible and elastic resourcing strategy for each fleet. for example, within specific fleets, your primary capacity can be on-demand supplemented with less-expensive spot capacity if available.  if you have reserved instances and you specify on-demand instances in your fleet, ec2 fleet uses your reserved instances. for example, if your fleet specifies an on-demand instance as , and you have reserved instances for , you receive the reserved instance pricing. there is no additional charge for using ec2 fleet. you pay only for the ec2 instances that the fleet launches for you. topics the following limitations apply to ec2 fleet: ec2 fleet is available only through the api or aws cli.an ec2 fleet request can't span aws regions. you need to create a separate ec2 fleet for each region.an ec2 fleet request can't span different subnets from the same availability zone.the usual amazon ec2 limits apply to instances launched by an ec2 fleet, such as spot request price limits, instance limits, and volume limits. in addition, the following limits apply: the number of active ec2 fleets per aws region: 1,000 * †the number of launch specifications per fleet: 50 †the size of the user data in a launch specification: 16 kb †the target capacity per ec2 fleet: 10,000the target capacity across all ec2 fleets in a region: 100,000 *if you need more than the default limits for target capacity, complete the aws support center  form to request a limit increase. for limit type, choose ec2 fleet, choose a region, and then choose target fleet capacity per fleet (in units) or target fleet capacity per region (in units), or both. * these limits apply to both your ec2 fleets and your spot fleets. † these are hard limits. you cannot request a limit increase for these limits. if you plan to use your t3 spot instances immediately and for a short duration, with no idle time for accruing cpu credits, we recommend that you launch your t3 spot instances in  mode to avoid paying higher costs. if you launch your t3 spot instances in  mode and burst cpu immediately, you'll spend surplus credits for bursting. if you use the instance for a short duration, your instance doesn't have time to accrue cpu credits to pay down the surplus credits, and you are charged for the surplus credits when you terminate your instance.  mode for t3 spot instances is suitable only if the instance runs for long enough to accrue cpu credits for bursting. otherwise, paying for surplus credits makes t3 spot instances more expensive than m5 or c5 instances. for more information, see . launch credits are meant to provide a productive initial launch experience for t2 instances by providing sufficient compute resources to configure the instance. repeated launches of t2 instances to access new launch credits is not permitted. if you require sustained cpu, you can earn credits (by idling over some period), use , or use an instance type with dedicated cpu (for example, ). 
the  mode is a configuration option for burstable performance instances. it can be enabled or disabled at any time for a running or stopped instance. you can set  as the default credit option at the account level per aws region, per burstable performance instance family, so that all new burstable performance instances in the account launch using the default credit option. notet3 and t3a instances are launched as  by default. t2 instances are launched as  by default. you can change the default at the account level per aws region. for more information, see . when a burstable performance instance configured as  is in a running state, it continuously earns (at a millisecond-level resolution) a set rate of earned credits per hour. for t2 standard, when the instance is stopped, it loses all its accrued credits, and its credit balance is reset to zero. when it is restarted, it receives a new set of launch credits, and begins to accrue earned credits. for t3 and t3a standard, the cpu credit balance persists for seven days after the instance stops and the credits are lost thereafter. if you start the instance within seven days, no credits are lost. a t2 standard instance receives two types of cpu credits: earned credits and launch credits. when a t2 standard instance is in a running state, it continuously earns (at a millisecond-level resolution) a set rate of earned credits per hour. at start, it has not yet earned credits for a good startup experience; therefore, to provide a good startup experience, it receives launch credits at start, which it spends first while it accrues earned credits. t3 and t3a standard instances do not receive launch credits. t2 standard instances get 30 launch credits per vcpu at launch or start. for example, a  instance has one vcpu and gets 30 launch credits, while a  instance has four vcpus and gets 120 launch credits. launch credits are designed to provide a good startup experience to allow instances to burst immediately after launch before they have accrued earned credits. launch credits are spent first, before earned credits. unspent launch credits are accrued in the cpu credit balance, but do not count towards the cpu credit balance limit. for example, a  instance has a cpu credit balance limit of 144 earned credits. if it is launched and remains idle for 24 hours, its cpu credit balance reaches 174 (30 launch credits + 144 earned credits), which is over the limit. however, after the instance spends the 30 launch credits, the credit balance cannot exceed 144. for more information about the cpu credit balance limit for each instance size, see the . the following table lists the initial cpu credit allocation received at launch or start, and the number of vcpus. there is a limit to the number of times t2 standard instances can receive launch credits. the default limit is 100 launches or starts of all t2 standard instances combined per account, per region, per rolling 24-hour period. for example, the limit is reached when one instance is stopped and started 100 times within a 24-hour period, or when 100 instances are launched within a 24-hour period, or other combinations that equate to 100 starts. new accounts may have a lower limit, which increases over time based on your usage. tipto ensure that your workloads always get the performance they need, switch to  or consider using a larger instance size. the following table lists the differences between launch credits and earned credits. the number of accrued launch credits and accrued earned credits is tracked by the cloudwatch metric . for more information, see  in the . 
to use spot instances, create a spot instance request or a spot fleet request. the request can include the maximum price that you are willing to pay per hour per instance (the default is the on-demand price), and other constraints such as the instance type and availability zone. if your maximum price exceeds the current spot price for the specified instance, and capacity is available, your request is fulfilled immediately. otherwise, the request is fulfilled whenever the maximum price exceeds the spot price and the capacity is available. spot instances run until you stop or terminate them, or until amazon ec2 must interrupt them (known as a spot instance interruption). when you use spot instances, you must be prepared for interruptions. amazon ec2 can interrupt your spot instance when the spot price exceeds your maximum price, when the demand for spot instances rises, or when the supply of spot instances decreases. when amazon ec2 interrupts a spot instance, it provides a spot instance interruption notice, which gives the instance a two-minute warning before amazon ec2 interrupts it. you can't enable termination protection for spot instances. for more information, see . you can stop, start, reboot, or terminate an amazon ebs-backed instance. the spot service can stop, terminate, or hibernate a spot instance when it interrupts it. topics specify a launch group in your spot instance request to tell amazon ec2 to launch a set of spot instances only if it can launch them all. in addition, if the spot service must terminate one of the instances in a launch group (for example, if the spot price exceeds your maximum price), it must terminate them all. however, if you terminate one or more of the instances in a launch group, amazon ec2 does not terminate the remaining instances in the launch group. although this option can be useful, adding this constraint can decrease the chances that your spot instance request is fulfilled and increase the chances that your spot instances are terminated. for example, your launch group includes instances in multiple availability zones. if capacity in one of these availability zones decreases and is no longer available, then amazon ec2 terminates all instances for the launch group. if you create another successful spot instance request that specifies the same (existing) launch group as an earlier successful request, then the new instances are added to the launch group. subsequently, if an instance in this launch group is terminated, all instances in the launch group are terminated, which includes instances launched by the first and second requests. specify an availability zone group in your spot instance request to tell the spot service to launch a set of spot instances in the same availability zone. amazon ec2 need not interrupt all instances in an availability zone group at the same time. if amazon ec2 must interrupt one of the instances in an availability zone group, the others remain running. although this option can be useful, adding this constraint can lower the chances that your spot instance request is fulfilled. if you specify an availability zone group but don't specify an availability zone in the spot instance request, the result depends on the network you specified. default vpcamazon ec2 uses the availability zone for the specified subnet. if you don't specify a subnet, it selects an availability zone and its default subnet, but not necessarily the lowest-priced zone. if you deleted the default subnet for an availability zone, then you must specify a different subnet. nondefault vpcamazon ec2 uses the availability zone for the specified subnet. you specify a subnet for your spot instances the same way that you specify a subnet for your on-demand instances. you should use the default maximum price (the on-demand price), or base your maximum price on the spot price history of spot instances in a vpc.[default vpc] if you want your spot instance launched in a specific low-priced availability zone, you must specify the corresponding subnet in your spot instance request. if you do not specify a subnet, amazon ec2 selects one for you, and the availability zone for this subnet might not have the lowest spot price.[nondefault vpc] you must specify the subnet for your spot instance.
a shared ami is an ami that a developer created and made available for other developers to use. one of the easiest ways to get started with amazon ec2 is to use a shared ami that has the components you need and then add custom content. you can also create your own amis and share them with others.  you use a shared ami at your own risk. amazon can't vouch for the integrity or security of amis shared by other amazon ec2 users. therefore, you should treat shared amis as you would any foreign code that you might consider deploying in your own data center and perform the appropriate due diligence. we recommend that you get an ami from a trusted source. amazon's public images have an aliased owner, which appears as  in the account field. this enables you to find amis from amazon easily. other users can't alias their amis.  for information about creating an ami, see  or  . for more information about building, delivering, and maintaining your applications on the aws marketplace, see the . topics 
amazon ec2 instance connect provides a simple and secure way to connect to your instances using secure shell (ssh). with ec2 instance connect, you use aws identity and access management (iam) policies and principals to control ssh access to your instances, removing the need to share and manage ssh keys. all connection requests using ec2 instance connect are . you can use instance connect to connect to your linux instances using a browser-based client, the amazon ec2 instance connect cli, or the ssh client of your choice. when you connect to an instance using ec2 instance connect, the instance connect api pushes a one-time-use ssh public key to the  where it remains for 60 seconds. an iam policy attached to your iam user authorizes your iam user to push the public key to the instance metadata. the ssh daemon uses  and , which are configured when instance connect is installed, to look up the public key from the instance metadata for authentication, and connects you to the instance. tip if you are connecting to a linux instance from a local computer running windows, see the following documentation instead: topics 
to use spot instances, you create a spot instance request that includes the desired number of instances, the instance type, the availability zone, and the maximum price that you are willing to pay per instance hour. if your maximum price exceeds the current spot price, amazon ec2 fulfills your request immediately if capacity is available. otherwise, amazon ec2 waits until your request can be fulfilled or until you cancel the request. the following illustration shows how spot requests work. notice that the request type (one-time or persistent) determines whether the request is opened again when amazon ec2 interrupts a spot instance or if you stop a spot instance. if the request is persistent, the request is opened again after your spot instance is interrupted. if the request is persistent and you stop your spot instance, the request only opens after you start your spot instance.  topics a spot instance request can be in one of the following states:  – the request is waiting to be fulfilled. – the request is fulfilled and has an associated spot instance. – the request has one or more bad parameters. – the spot instance was interrupted or terminated. – you stopped the spot instance. – you canceled the request, or the request expired.the following illustration represents the transitions between the request states. notice that the transitions depend on the request type (one-time or persistent).  a one-time spot instance request remains active until amazon ec2 launches the spot instance, the request expires, or you cancel the request. if the spot price exceeds your maximum price or capacity is not available, your spot instance is terminated and the spot instance request is closed. a persistent spot instance request remains active until it expires or you cancel it, even if the request is fulfilled. if the spot price exceeds your maximum price or capacity is not available, your spot instance is interrupted. after your instance is interrupted, when your maximum price exceeds the spot price or capacity becomes available again, the spot instance is started if stopped or resumed if hibernated. you can stop a spot instance and start it again if capacity is available and your maximum price exceeds the current spot price. if the spot instance is terminated (irrespective of whether the spot instance is in a stopped or running state), the spot instance request is opened again and amazon ec2 launches a new spot instance. for more information, see , , and . you can track the status of your spot instance requests, as well as the status of the spot instances launched, through the status. for more information, see . spot instances with a defined duration (also known as spot blocks) are designed not to be interrupted and will run continuously for the duration you select. this makes them ideal for jobs that take a finite time to complete, such as batch processing, encoding and rendering, modeling and analysis, and continuous integration. you can use a duration of 1, 2, 3, 4, 5, or 6 hours. the price that you pay depends on the specified duration. to view the current prices for a 1-hour duration or a 6-hour duration, see . you can use these prices to estimate the cost of the 2, 3, 4, and 5-hour durations. when a request with a duration is fulfilled, the price for your spot instance is fixed, and this price remains in effect until the instance terminates. you are billed at this price for each hour or partial hour that the instance is running. a partial instance hour is billed to the nearest second. when you define a duration in your spot request, the duration period for each spot instance starts as soon as the instance receives its instance id. the spot instance runs until you terminate it or the duration period ends. at the end of the duration period, amazon ec2 marks the spot instance for termination and provides a spot instance termination notice, which gives the instance a two-minute warning before it terminates. in rare situations, spot blocks may be interrupted due to amazon ec2 capacity needs. in these cases, we provide a two-minute warning before we terminate an instance, and you are not charged for the terminated instances even if you used them. to launch spot instances with a defined duration (console)follow the  procedure. to launch spot instances with a defined duration, for tell us your application or task need, choose defined duration workloads. to launch spot instances with a defined duration (aws cli)to specify a duration for your spot instances, include the  option with the  command. for example, the following command creates a spot request that launches spot instances that run for two hours. to retrieve the cost for spot instances with a defined duration (aws cli)use the  command to retrieve the fixed cost for your spot instances with a specified duration. the information is in the  field. you can run a spot instance on single-tenant hardware. dedicated spot instances are physically isolated from instances that belong to other aws accounts. for more information, see  and the  product page. to run a dedicated spot instance, do one of the following: specify a tenancy of  when you create the spot instance request. for more information, see .request a spot instance in a vpc with an instance tenancy of . for more information, see . you cannot request a spot instance with a tenancy of  if you request it in a vpc with an instance tenancy of .the following instance types support dedicated spot instances. current generation previous generation amazon ec2 uses service-linked roles for the permissions that it requires to call other aws services on your behalf. a service-linked role is a unique type of iam role that is linked directly to an aws service. service-linked roles provide a secure way to delegate permissions to aws services because only the linked service can assume a service-linked role. for more information, see  in the iam user guide. amazon ec2 uses the service-linked role named awsserviceroleforec2spot to launch and manage spot instances on your behalf. amazon ec2 uses awsserviceroleforec2spot to complete the following actions:  – describe spot instances – stop spot instances – start spot instancesunder most circumstances, you don't need to manually create a service-linked role. amazon ec2 creates the awsserviceroleforec2spot service-linked role the first time you request a spot instance using the console. if you had an active spot instance request before october 2017, when amazon ec2 began supporting this service-linked role, amazon ec2 created the awsserviceroleforec2spot role in your aws account. for more information, see  in the iam user guide. ensure that this role exists before you use the aws cli or an api to request a spot instance. to create the role, use the iam console as follows. to manually create the awsserviceroleforec2spot service-linked role open the iam console at . in the navigation pane, choose roles. choose create role. on the select type of trusted entity page, choose ec2, ec2 - spot instances, next: permissions. on the next page, choose next:review. on the review page, choose create role. if you no longer need to use spot instances, we recommend that you delete the awsserviceroleforec2spot role. after this role is deleted from your account, amazon ec2 will create the role again if you request spot instances. if you specify an  or an  for your spot instances and you use a customer managed customer master key (cmk) for encryption, you must grant the awsserviceroleforec2spot role permission to use the cmk so that amazon ec2 can launch spot instances on your behalf. to do this, you must add a grant to the cmk, as shown in the following procedure. when providing permissions, grants are an alternative to key policies. for more information, see  and  in the aws key management service developer guide. to grant the awsserviceroleforec2spot role permissions to use the cmk use the  command to add a grant to the cmk and to specify the principal (the awsserviceroleforec2spot service-linked role) that is given permission to perform the operations that the grant permits. the cmk is specified by the  parameter and the arn of the cmk. the principal is specified by the  parameter and the arn of the awsserviceroleforec2spot service-linked role. the procedure for requesting a spot instance is similar to the procedure for launching an on-demand instance. you can request a spot instance in the following ways: to request a spot instance using the console, use the launch instance wizard. for more information, see .to request a spot instance using the cli, use the  command or the  command. for more information, see  and .to request a spot instance with a defined duration using the console, follow the  procedure. for tell us your application or task need, choose defined duration workloads. for more information, see .to request a spot instance with a defined duration using the cli, use the  command and specify the  parameter. for more information, see .after you've submitted your spot instance request, you can't change the parameters of the request. this means that you can't make changes to the maximum price that you're willing to pay. if you request multiple spot instances at one time, amazon ec2 creates separate spot instance requests so that you can track the status of each request separately. for more information about tracking spot instance requests, see . to launch a fleet that includes spot instances and on-demand instances, see . noteyou can't launch a spot instance and an on-demand instance in the same call using the launch instance wizard or the  command. prerequisitesbefore you begin, decide on your maximum price, how many spot instances you'd like, and what instance type to use. to review spot price trends, see . to create a spot instance request (console) open the amazon ec2 console at . in the navigation bar at the top of the screen, select a region. from the amazon ec2 console dashboard, choose launch instance. on the choose an amazon machine image (ami) page, choose an ami. for more information, see . on the choose an instance type page, select the hardware configuration and size of the instance to launch, and then choose next: configure instance details. for more information, see . on the configure instance details page, configure the spot instance request as follows: number of instances: enter the number of instances to launch. noteamazon ec2 creates a separate request for each spot instance.(optional) to help ensure that you maintain the correct number of instances to handle demand on your application, you can choose launch into auto scaling group to create a launch configuration and an auto scaling group. auto scaling scales the number of instances in the group according to your specifications. for more information, see the .purchasing option: choose request spot instances to launch a spot instance. when you choose this option, the following fields appear.current price: the current spot price in each availability zone is displayed for the instance type that you selected.(optional) maximum price: you can leave the field empty, or you can specify the maximum amount you're willing to pay.if you leave the field empty, then the maximum price defaults to the current on-demand price. your spot instance launches at the current spot price, not exceeding the on-demand price.if you specify a maximum price that is more than the current spot price, your spot instance launches and is charged at the current spot price.if you specify a maximum price that is lower than the spot price, your spot instance is not launched.persistent request: choose persistent request to resubmit the spot instance request if your spot instance is interrupted.interruption behavior: by default, the spot service terminates a spot instance when it is interrupted. if you choose persistent request, you can then specify that the spot service stops or hibernates your spot instance when it's interrupted. for more information, see .(optional) request valid to: choose edit to specify when the spot instance request expires.for more information about configuring your spot instance, see . the ami you selected includes one or more volumes of storage, including the root device volume. on the add storage page, you can specify additional volumes to attach to the instance by choosing add new volume. for more information, see . on the add tags page, specify  by providing key and value combinations. for more information, see . on the configure security group page, use a security group to define firewall rules for your instance. these rules specify which incoming network traffic is delivered to your instance. all other traffic is ignored. (for more information about security groups, see .) select or create a security group, and then choose review and launch. for more information, see . on the review instance launch page, check the details of your instance, and make any necessary changes by choosing the appropriate edit link. when you are ready, choose launch. for more information, see . in the select an existing key pair or create a new key pair dialog box, you can choose an existing key pair, or create a new one. for example, choose choose an existing key pair, then select the key pair that you created when getting set up. for more information, see . importantif you choose the proceed without key pair option, you won't be able to connect to the instance unless you choose an ami that is configured to allow users another way to log in. to launch your instance, select the acknowledgment check box, then choose launch instances. if the instance fails to launch or the state immediately goes to  instead of , see . to create a spot instance request using  (aws cli)use the  command to create a one-time request. use the  command to create a persistent request. for example launch specification files to use with these commands, see . if you download a launch specification file from the console, you must use the  command instead (the console specifies a spot request using a spot fleet). to create a spot instance request using  (aws cli)use the  command and specify the spot instance options in the  parameter. the following is the data structure to specify in the json file for . you can also specify , , and . if you do not specify a field in the data structure, the default value is used. this example creates a  request and specifies  as the maximum price you're willing to pay for the spot instance.  amazon ec2 launches a spot instance when the maximum price exceeds the spot price and capacity is available. a spot instance runs until it is interrupted or you terminate it yourself. if your maximum price is exactly equal to the spot price, there is a chance that your spot instance remains running, depending on demand. to find running spot instances (console) open the amazon ec2 console at . in the navigation pane, choose spot requests. you can see both spot instance requests and spot fleet requests. if a spot instance request has been fulfilled, capacity is the id of the spot instance. for a spot fleet, capacity indicates how much of the requested capacity has been fulfilled. to view the ids of the instances in a spot fleet, choose the expand arrow, or select the fleet and choose instances. notefor spot instance requests that are created by a spot fleet, the requests are not tagged instantly with the system tag that indicates the spot fleet to which they belong, and for a period of time may appear separate from spot fleet request. alternatively, in the navigation pane, choose instances. in the top right corner, choose the show/hide icon, and then under instance attributes, select lifecycle. for each instance, lifecycle is either , , or . to find running spot instances (aws cli)to enumerate your spot instances, use the  command with the  option. the following is example output: alternatively, you can enumerate your spot instances using the  command with the  option. to describe a single spot instance instance, use the  command with the  option. to help categorize and manage your spot instance requests, you can tag them with custom metadata. you can assign a tag to a spot instance request when you create it, or afterward. you can assign tags using the amazon ec2 console or a command line tool. when you tag a spot instance request, the instances and volumes that are launched by the spot instance request are not automatically tagged. you need to explicitly tag the instances and volumes launched by the spot instance request. you can assign a tag to a spot instance and volumes during launch, or afterward. for more information about how tags work, see . topics grant the iam user the permission to tag resources. for more information about iam policies and example policies, see . the iam policy you create is determined by which method you use for creating a spot instance request. if you use the launch instance wizard or  to request spot instances, see .if you use the spot console to request spot instances with a defined duration or use the  command to request spot instances, see .to grant an iam user the permission to tag resources when using the launch instance wizard or run-instancescreate a iam policy that includes the following: the  action. this grants the iam user permission to launch an instance.for , specify . this allows users to create spot instance requests, which request spot instances.the  action. this grants the iam user permission to create tags.for , specify . this allows users to tag all resources that are created during instance launch.notewhen you use the runinstances action to create spot instance requests and tag the spot instance requests on create, you need to be aware of how amazon ec2 evaluates the  resource in the runinstances statement.the  resource is evaluated in the iam policy as follows:if you don't tag a spot instance request on create, amazon ec2 does not evaluate the  resource in the runinstances statement. if you tag a spot instance request on create, amazon ec2 evaluates the  resource in the runinstances statement. therefore, for the  resource, the following rules apply to the iam policy:if you use runinstances to create a spot instance request and you don't intend to tag the spot instance request on create, you don’t need to explicitly allow the  resource; the call will succeed. if you use runinstances to create a spot instance request and intend to tag the spot instance request on create, you must include the  resource in the runinstances allow statement, otherwise the call will fail. if you use runinstances to create a spot instance request and intend to tag the spot instance request on create, you must specify the  resource or include a  wildcard in the createtags allow statement, otherwise the call will fail. for example iam policies, including policies that are not supported for spot instance requests, see . to grant an iam user the permission to tag resources when using request-spot-instancescreate a iam policy that includes the following: the  action. this grants the iam user permission to create a spot instance request.the  action. this grants the iam user permission to create tags.for , specify . this allows users to tag only the spot instance request.to tag a new spot instance request using the console follow the  procedure. to add a tag, on the add tags page, choose add tag, and enter the key and value for the tag. choose add another tag for each additional tag. for each tag, you can tag the spot instance request, the spot instances, and the volumes with the same tag. to tag all three, ensure that instances, volumes, and spot instance requests are selected. to tag only one or two, ensure that the resources you want to tag are selected, and the other resources are cleared. complete the required fields to create a spot instance request, and then choose launch. for more information, see . to tag a new spot instance request using the aws clito tag a spot instance request when you create it, configure the spot instance request configuration as follows: specify the tags for the spot instance request using the  parameter.for , specify . if you specify another value, the spot instance request will fail.for , specify the key-value pair. you can specify more than one key-value pair.in the following example, the spot instance request is tagged with two tags: key=environment and value=production, and key=cost-center and value=123. to tag an existing spot instance request using the console after you have created a spot instance request, you can add tags to the spot instance request using the console. open the spot console at . select your spot instance request. choose the tags tab and choose create tag. to tag an existing spot instance using the consoleafter your spot instance request has launched your spot instance, you can add tags to the instance using the console. for more information, see . to tag an existing spot instance request or spot instance using the aws cliuse the  command to tag existing resources. in the following example, the existing spot instance request and the spot instance are tagged with key=purpose and value=test.  to view spot instance request tags using the console open the spot console at . select your spot instance request and choose the tags tab. to describe spot instance request tagsuse the  command to view the tags for the specified resource. in the following example, you describe the tags for the specified request. you can also view the tags of a spot instance request by describing the spot instance request. use the  command to view the configuration of the specified spot instance request, which includes any tags that were specified for the request. if you no longer want your spot instance request, you can cancel it. you can only cancel spot instance requests that are , , or . your spot instance request is  when your request has not yet been fulfilled and no instances have been launched.your spot instance request is  when your request has been fulfilled and spot instances have launched as a result. your spot instance request is  when you stop your spot instance.if your spot instance request is  and has an associated running spot instance, canceling the request does not terminate the instance. for more information about terminating a spot instance, see . to cancel a spot instance request (console) open the amazon ec2 console at . in the navigation pane, choose spot requests and select the spot request. choose actions, cancel request. (optional) if you are finished with the associated spot instances, you can terminate them. in the cancel spot request dialog box, select terminate instances, and then choose confirm. to cancel a spot instance request (aws cli) use the  command to cancel the specified spot request. if you don’t need your spot instances now, but you want to restart them later without losing the data persisted in the amazon ebs volume, you can stop them. the steps for stopping a spot instance are similar to the steps for stopping an on-demand instance. you can only stop a spot instance if the spot instance was launched from a  spot instance request. notewhile a spot instance is stopped, you can modify some of its instance attributes, but not the instance type.we don't charge usage for a stopped spot instance, or data transfer fees, but we do charge for the storage for any amazon ebs volumes. limitations you can't stop a spot instance if it is part of a fleet or launch group, availability zone group, or spot block.to stop a spot instance (console) open the amazon ec2 console at . in the navigation pane, choose instances and select the spot instance. choose actions, instance state, stop. to stop a spot instance (aws cli) use the  command to manually stop one or more spot instances. you can start a spot instance that you previously stopped. the steps for starting a spot instance are similar to the steps for starting an on-demand instance. prerequisites you can only start a spot instance if: you manually stopped the spot instance.the spot instance is an ebs-backed instance.spot instance capacity is available.the spot price is lower than your maximum price.limitations you can't start a spot instance if it is part of fleet or launch group, availability zone group, or spot block.to start a spot instance (console) open the amazon ec2 console at . in the navigation pane, choose instances and select the spot instance. choose actions, instance state, start. to start a spot instance (aws cli) use the  command to manually start one or more spot instances. if your spot instance request is  and has an associated running spot instance, or your spot instance request is  and has an associated stopped spot instance, canceling the request does not terminate the instance; you must terminate the running spot instance manually. if you terminate a running or stopped spot instance that was launched by a persistent spot request, the spot request returns to the  state so that a new spot instance can be launched. to cancel a persistent spot request and terminate its spot instances, you must cancel the spot request first and then terminate the spot instances. otherwise, the persistent spot request can launch a new instance. for more information about canceling a spot instance request, see . to manually terminate a spot instance (aws cli) use the  command to manually terminate spot instances. 
before you can launch an instance, you must select an instance type to use. the instance type that you choose might depend on your requirements for the instances that you'll launch. for example, you might choose an instance type based on the following requirements: availability zone or regioncomputememorynetworkingpricingstorageyou can find an instance type that meets your needs using the amazon ec2 console. to find an instance type using the console open the amazon ec2 console at . from the navigation bar, select the region in which to launch your instances. you can select any region that's available to you, regardless of your location. in the navigation pane, choose instance types.  (optional) choose the preferences (gear) icon to select which instance type attributes to display, such as on-demand linux pricing, and then choose confirm. alternatively, select an instance type and view all attributes using the details pane. use the instance type attributes to filter the list of displayed instance types to only the instance types that meet your needs. for example, you can list all instance types that have more than eight vcpus and also support hibernation. (optional) select multiple instance types to see a side-by-side comparison across all attributes in the details pane. (optional) to save the list of instance types to a comma-separated values (.csv) file for further review, choose download list csv. the file includes all instance types that match the filters you set. after locating instance types that meet your needs, you can use them to launch instances. for more information, see . you can use aws cli commands for amazon ec2 to find an instance type that meet your needs. to find an instance type using the aws cli if you have not done so already, install the aws cli for more information, see the . use the  command to filter instance types based on instance attributes. for example, you can use the following command to display only instance types with 48 vcpus. use the  command to filter instance types offered by location (region or availability zone). for example, you can use the following command to display the instance types offered in the specified availability zone.  after locating instance types that meet your needs, make note of them so that you can use these instance types when you launch instances. for more information, see  in the aws command line interface user guide. 
the following examples show you how to use the aws management console or the aws cli to determine the maximum cpu utilization of a specific ec2 instance. requirements you must have the id of the instance. you can get the instance id using the aws management console or the  command.by default, basic monitoring is enabled, but you can enable detailed monitoring. for more information, see .to display the cpu utilization for a specific instance (console) open the cloudwatch console at . in the navigation pane, choose metrics. choose the ec2 metric namespace. choose the per-instance metrics dimension. in the search field, enter cpuutilization and press enter. choose the row for the specific instance, which displays a graph for the cpuutilization metric for the instance. to name the graph, choose the pencil icon. to change the time range, select one of the predefined values or choose custom. to change the statistic or the period for the metric, choose the graphed metrics tab. choose the column heading or an individual value, and then choose a different value. to get the cpu utilization for a specific instance (aws cli)use the following  command to get the cpuutilization metric for the specified instance, using the specified period and time interval: the following is example output. each value represents the maximum cpu utilization percentage for a single ec2 instance. 
this topic explains how to verify the instance identity document using the pkcs7 signature and the aws dsa public certificate. to verify the instance identity document using the pkcs7 signature and the aws dsa public certificate connect to the instance. retrieve the pkcs7 signature from the instance metadata and add it to a file named . add the  header to the  file. retrieve the pkcs7 signature from the instance metadata and append it to the  file. use one of the following commands depending on the imds version used by the instance. append the  footer to a new line in the  file. add the contents of the instance identity document from the instance metadata to a file named . use one of the following commands depending on the imds version used by the instance.add the aws dsa public certificate to a file named . create the  file. open the  file using your preferred text editor and add the contents of the aws dsa public certificate. choose the correct certificate for the aws region that your instance is in. importantif the aws dsa public certificate for your region is not listed below, contact . save and close the file.use the openssl smime command to verify the signature. include the  option to indicate that the signature needs to be verified, and the  option to indicate that the certificate does not need to be verified. if the signature is valid, the  message appears. if the signature cannot be verified, contact aws support. 
applications must sign their api requests with aws credentials. therefore, if you are an application developer, you need a strategy for managing credentials for your applications that run on ec2 instances. for example, you can securely distribute your aws credentials to the instances, enabling the applications on those instances to use your credentials to sign requests, while protecting your credentials from other users. however, it's challenging to securely distribute credentials to each instance, especially those that aws creates on your behalf, such as spot instances or instances in auto scaling groups. you must also be able to update the credentials on each instance when you rotate your aws credentials. we designed iam roles so that your applications can securely make api requests from your instances, without requiring you to manage the security credentials that the applications use. instead of creating and distributing your aws credentials, you can delegate permission to make api requests using iam roles as follows: create an iam role. define which accounts or aws services can assume the role. define which api actions and resources the application can use after assuming the role. specify the role when you launch your instance, or attach the role to an existing instance. have the application retrieve a set of temporary credentials and use them. for example, you can use iam roles to grant permissions to applications running on your instances that need to use a bucket in amazon s3. you can specify permissions for iam roles by creating a policy in json format. these are similar to the policies that you create for iam users. if you change a role, the change is propagated to all instances. when creating iam roles, associate least privilege iam policies that restrict access to the specific api calls the application requires. you cannot attach multiple iam roles to a single instance, but you can attach a single iam role to multiple instances. for more information about creating and using iam roles, see  in the iam user guide. you can apply resource-level permissions to your iam policies to control the users' ability to attach, replace, or detach iam roles for an instance. for more information, see  and the following example: . topics amazon ec2 uses an instance profile as a container for an iam role. when you create an iam role using the iam console, the console creates an instance profile automatically and gives it the same name as the role to which it corresponds. if you use the amazon ec2 console to launch an instance with an iam role or to attach an iam role to an instance, you choose the role based on a list of instance profile names.  if you use the aws cli, api, or an aws sdk to create a role, you create the role and instance profile as separate actions, with potentially different names. if you then use the aws cli, api, or an aws sdk to launch an instance with an iam role or to attach an iam role to an instance, specify the instance profile name.  an instance profile can contain only one iam role. this limit cannot be increased. for more information, see  in the iam user guide. an application on the instance retrieves the security credentials provided by the role from the instance metadata item role-name. the application is granted the permissions for the actions and resources that you've defined for the role through the security credentials associated with the role. these security credentials are temporary and we rotate them automatically. we make new credentials available at least five minutes before the expiration of the old credentials. warningif you use services that use instance metadata with iam roles, ensure that you don't expose your credentials when the services make http calls on your behalf. the types of services that could expose your credentials include http proxies, html/css validator services, and xml processors that support xml inclusion. the following command retrieves the security credentials for an iam role named . the following is example output. for applications, aws cli, and tools for windows powershell commands that run on the instance, you do not have to explicitly get the temporary security credentials—the aws sdks, aws cli, and tools for windows powershell automatically get the credentials from the ec2 instance metadata service and use them. to make a call outside of the instance using temporary security credentials (for example, to test iam policies), you must provide the access key, secret key, and the session token. for more information, see  in the iam user guide. for more information about instance metadata, see .  to enable an iam user to launch an instance with an iam role or to attach or replace an iam role for an existing instance, you must grant the user permission to pass the role to the instance. the following iam policy grants users permission to launch instances () with an iam role, or to attach or replace an iam role for an existing instance ( and ). this policy grants iam users access to all your roles by specifying the resource as "*" in the policy. however, consider whether users who launch instances with your roles (ones that exist or that you create later on) might be granted permissions that they don't need or shouldn't have. you can create an iam role and attach it to an instance during or after launch. you can also replace or detach an iam role for an instance. topics you must create an iam role before you can launch an instance with that role or attach it to an instance. to create an iam role using the iam console open the iam console at . in the navigation pane, choose roles, create role. on the select role type page, choose ec2 and the ec2 use case. choose next: permissions. on the attach permissions policy page, select an aws managed policy that grants your instances access to the resources that they need. on the review page, enter a name for the role and choose create role. alternatively, you can use the aws cli to create an iam role. the following example creates an iam role with a policy that allows the role to use an amazon s3 bucket. to create an iam role and instance profile (aws cli) create the following trust policy and save it in a text file named . create the  role and specify the trust policy that you created using the  command. create an access policy and save it in a text file named . for example, this policy grants administrative permissions for amazon s3 to applications running on the instance. attach the access policy to the role using the  command. create an instance profile named  using the  command. add the  role to the  instance profile. alternatively, you can use the following aws tools for windows powershell commands: after you've created an iam role, you can launch an instance, and associate that role with the instance during launch. importantafter you create an iam role, it may take several seconds for the permissions to propagate. if your first attempt to launch an instance with a role fails, wait a few seconds before trying again. for more information, see  in the iam user guide. to launch an instance with an iam role (console) open the amazon ec2 console at . on the dashboard, choose launch instance. select an ami and instance type and then choose next: configure instance details. on the configure instance details page, for iam role, select the iam role that you created.  notethe iam role list displays the name of the instance profile that you created when you created your iam role. if you created your iam role using the console, the instance profile was created for you and given the same name as the role. if you created your iam role using the aws cli, api, or an aws sdk, you may have named your instance profile differently.  configure any other details, then follow the instructions through the rest of the wizard, or choose review and launch to accept default settings and go directly to the review instance launch page. review your settings, then choose launch to choose a key pair and launch your instance. if you are using the amazon ec2 api actions in your application, retrieve the aws security credentials made available on the instance and use them to sign the requests. the aws sdk does this for you. alternatively, you can use the aws cli to associate a role with an instance during launch. you must specify the instance profile in the command. to launch an instance with an iam role (aws cli) use the  command to launch an instance using the instance profile. the following example shows how to launch an instance with the instance profile.  alternatively, use the  tools for windows powershell command. if you are using the amazon ec2 api actions in your application, retrieve the aws security credentials made available on the instance and use them to sign the requests. the aws sdk does this for you. to attach an iam role to an instance that has no role, the instance can be in the  or  state. to attach an iam role to an instance (console) open the amazon ec2 console at . in the navigation pane, choose instances. select the instance, choose actions, instance settings, attach/replace iam role. select the iam role to attach to your instance, and choose apply. to attach an iam role to an instance (aws cli) if required, describe your instances to get the id of the instance to which to attach the role. use the  command to attach the iam role to the instance by specifying the instance profile. you can use the amazon resource name (arn) of the instance profile, or you can use its name. alternatively, use the following tools for windows powershell commands: to replace the iam role on an instance that already has an attached iam role, the instance must be in the  state. you can do this if you want to change the iam role for an instance without detaching the existing one first. for example, you can do this to ensure that api actions performed by applications running on the instance are not interrupted. to replace an iam role for an instance (console) open the amazon ec2 console at . in the navigation pane, choose instances. select the instance, choose actions, instance settings, attach/replace iam role. select the iam role to attach to your instance, and choose apply. to replace an iam role for an instance (aws cli) if required, describe your iam instance profile associations to get the association id for the iam instance profile to replace. use the  command to replace the iam instance profile by specifying the association id for the existing instance profile and the arn or name of the instance profile that should replace it. alternatively, use the following tools for windows powershell commands: you can detach an iam role from a running or stopped instance.  to detach an iam role from an instance (console) open the amazon ec2 console at . in the navigation pane, choose instances. select the instance, choose actions, instance settings, attach/replace iam role. for iam role, choose no role. choose apply. in the confirmation dialog box, choose yes, detach. to detach an iam role from an instance (aws cli) if required, use  to describe your iam instance profile associations and get the association id for the iam instance profile to detach. use the  command to detach the iam instance profile using its association id. alternatively, use the following tools for windows powershell commands: 
ebs volumes are exposed as nvme block devices on instances built on the . the device names are , , and so on. the device names that you specify in a block device mapping are renamed using nvme device names (). the block device driver can assign nvme device names in a different order than you specified for the volumes in the block device mapping. the ebs performance guarantees stated in  are valid regardless of the block-device interface. topics to access nvme volumes, the nvme drivers must be installed. instances can support nvme ebs volumes, nvme instance store volumes, both types of nvme volumes, or no nvme volumes. for more information, see . the following amis include the required nvme drivers: amazon linux 2amazon linux ami 2018.03ubuntu 14.04 (with  kernel) or laterred hat enterprise linux 7.4 or latersuse linux enterprise server 12 sp2 or latercentos 7.4.1708 or laterfreebsd 11.1 or laterdebian gnu/linux 9 or laterfor more information about nvme drivers on windows instances, see  in the amazon ec2 user guide for windows instances. to confirm that your instance has the nvme driveryou can confirm that your instance has the nvme driver and check the driver version using the following command. if the instance has the nvme driver, the command returns information about the driver. to update the nvme driver if your instance has the nvme driver, you can update the driver to the latest version using the following procedure. connect to your instance. update your package cache to get necessary package updates as follows. for amazon linux 2, amazon linux, centos, and red hat enterprise linux: for ubuntu and debian: ubuntu 16.04 and later include the  package, which contains the nvme and ena drivers required by nitro-based instances. upgrade the  package to receive the latest version as follows: for ubuntu 14.04, you can install the latest  package as follows: reboot your instance to load the latest kernel version. reconnect to your instance after it has rebooted. ebs uses single-root i/o virtualization (sr-iov) to provide volume attachments on nitro-based instances using the nvme specification. these devices rely on standard nvme drivers on the operating system. these drivers typically discover attached devices by scanning the pci bus during instance boot, and create device nodes based on the order in which the devices respond, not on how the devices are specified in the block device mapping. in linux, nvme device names follow the pattern , where  is the enumeration order, and, for ebs,  is 1. occasionally, devices can respond to discovery in a different order in subsequent instance starts, which causes the device name to change. we recommend that you use stable identifiers for your ebs volumes within your instance, such as one of the following: for nitro-based instances, the block device mappings that are specified in the amazon ec2 console when you are attaching an ebs volume or during  or  api calls are captured in the vendor-specific data field of the nvme controller identification. with amazon linux amis later than version 2017.09.01, we provide a  rule that reads this data and creates a symbolic link to the block-device mapping.nvme ebs volumes have the ebs volume id set as the serial number in the device identification. use the  command to list the serial number. when a device is formatted, a uuid is generated that persists for the life of the filesystem. a device label can be specified at the same time. for more information, see  and .amazon linux amiswith amazon linux ami 2017.09.01 or later (including amazon linux 2), you can run the ebsnvme-id command as follows to map the nvme device name to a volume id and device name: amazon linux also creates a symbolic link from the device name in the block device mapping (for example, ), to the nvme device name. freebsd amisstarting with freebsd 12.2-release, you can run the ebsnvme-id command as shown above. pass either the name of the nvme device (for example, ) or the disk device (for example,  or ). freebsd also creates symbolic links to the disk devices (for example, volume_id). other linux amiswith a kernel version of 4.2 or later, you can run the nvme id-ctrl command as follows to map an nvme device to a volume id. first, install the nvme command line package, , using the package management tools for your linux distribution. for download and installation instructions for other distributions, refer to the documentation specific to your distribution. the following example gets the volume id and device name. the device name is available through the nvme controller vendor-specific extension (bytes 384:4095 of the controller identification): the lsblk command lists available devices and their mount points (if applicable). this helps you determine the correct device name to use. in this example,  is mounted as the root device and  is attached but not mounted. to format and mount an nvme ebs volume, see . if you are using linux kernel 4.2 or later, any change you make to the volume size of an nvme ebs volume is automatically reflected in the instance. for older linux kernels, you might need to detach and attach the ebs volume or reboot the instance for the size change to be reflected. with linux kernel 3.19 or later, you can use the hdparm command as follows to force a rescan of the nvme device: when you detach an nvme ebs volume, the instance does not have an opportunity to flush the file system caches or metadata before detaching the volume. therefore, before you detach an nvme ebs volume, you should first sync and unmount it. if the volume fails to detach, you can attempt a  command as described in . ebs volumes attached to nitro-based instances use the default nvme driver provided by the operating system. most operating systems specify a timeout for i/o operations submitted to nvme devices. the default timeout is 30 seconds and can be changed using the  boot parameter. with linux kernels earlier than version 4.6, this parameter is . if i/o latency exceeds the value of this timeout parameter, the linux nvme driver fails the i/o and returns an error to the filesystem or application. depending on the i/o operation, your filesystem or application can retry the error. in some cases, your filesystem might be remounted as read-only. for an experience similar to ebs volumes attached to xen instances, we recommend setting  to the highest value possible. for current kernels, the maximum is 4294967295, while for earlier kernels the maximum is 255. depending on the version of linux, the timeout might already be set to the supported maximum value. for example, the timeout is set to 4294967295 by default for amazon linux ami 2017.09.01 and later. you can verify the maximum value for your linux distribution by writing a value higher than the suggested maximum to  and checking for the numerical result out of range error when attempting to save the file. 
you can view descriptive information about your ebs volumes. for example, you can view information about all volumes in a specific region or view detailed information about a single volume, including its size, volume type, whether the volume is encrypted, which master key was used to encrypt the volume, and the specific instance to which the volume is attached. you can get additional information about your ebs volumes, such as how much disk space is available, from the operating system on the instance. to view information about an ebs volume open the amazon ec2 console at . in the navigation pane, choose volumes.  (optional) use the filter options in the search bar to display only the volumes that interest you. for example, if you know the instance id you want to see volumes for, go to the search bar and choose instance id from the filter menu, then choose the instance id you want from the list provided. to remove a filter, choose it again. to view more information about a volume, select it. in the details pane, you can inspect the information provided about the volume. attachment information shows the instance id this volume is attached to and the device name under which it is attached. (optional) choose the attachment information link to view instance details. to view the ebs volumes that are attached to an instance open the amazon ec2 console at . in the navigation pane, choose instances. to view more information about an instance, select it. in the details pane, you can inspect the information provided about root and block devices. choose the device name link to show information on the volume attached under that device name. (optional) choose the ebs id link to view details for the volume attached to this instance under the device name chosen in the prior step. to view information about an ebs volume using the command line you can use one of the following commands to view volume attributes. for more information, see .  (aws cli) (aws tools for windows powershell)you can get additional information about your ebs volumes from amazon cloudwatch. for more information, see . you can get additional information about your ebs volumes, such as how much disk space is available, from the linux operating system on the instance. for example, use the following command: 
all reserved instances provide you with a discount compared to on-demand pricing. with reserved instances, you pay for the entire term regardless of actual use. you can choose to pay for your reserved instance upfront, partially upfront, or monthly, depending on the  specified for the reserved instance.  when reserved instances expire, you are charged on-demand rates for ec2 instance usage. you can queue a reserved instance for purchase up to three years in advance. this can help you ensure that you have uninterrupted coverage. for more information, see . the aws free tier is available for new aws accounts. if you are using the aws free tier to run amazon ec2 instances, and you purchase a reserved instance, you are charged under standard pricing guidelines. for information, see . topics reserved instances are billed for every clock-hour during the term that you select, regardless of whether an instance is running. each clock-hour starts on the hour (zero minutes and zero seconds past the hour) of a standard 24-hour clock. for example, 1:00:00 to 1:59:59 is one clock-hour. for more information about instance states, see . a reserved instance billing benefit can be applied to a running instance on a per-second basis. per-second billing is available for instances using an open-source linux distribution, such as amazon linux and ubuntu. per-hour billing is used for commercial linux distributions, such as red hat enterprise linux and suse linux enterprise server. a reserved instance billing benefit can apply to a maximum of 3600 seconds (one hour) of instance usage per clock-hour. you can run multiple instances concurrently, but can only receive the benefit of the reserved instance discount for a total of 3600 seconds per clock-hour; instance usage that exceeds 3600 seconds in a clock-hour is billed at the on-demand rate. for example, if you purchase one  reserved instance and run four  instances concurrently for one hour, one instance is charged at one hour of reserved instance usage and the other three instances are charged at three hours of on-demand usage. however, if you purchase one  reserved instance and run four  instances for 15 minutes (900 seconds) each within the same hour, the total running time for the instances is one hour, which results in one hour of reserved instance usage and 0 hours of on-demand usage.  if multiple eligible instances are running concurrently, the reserved instance billing benefit is applied to all the instances at the same time up to a maximum of 3600 seconds in a clock-hour; thereafter, on-demand rates apply.  cost explorer on the  console enables you to analyze the savings against running on-demand instances. the  includes an example of a list value calculation. if you close your aws account, on-demand billing for your resources stops. however, if you have any reserved instances in your account, you continue to receive a bill for these until they expire. you can find out about the charges and fees to your account by viewing the  console. the dashboard displays a spend summary for your account.on the bills page, under details expand the elastic compute cloud section and the region to get billing information about your reserved instances.you can view the charges online, or you can download a csv file. you can also track your reserved instance utilization using the aws cost and usage report. for more information, see  under cost and usage report in the aws billing and cost management user guide. the pricing benefits of reserved instances are shared when the purchasing account is part of a set of accounts billed under one consolidated billing payer account. the instance usage across all member accounts is aggregated in the payer account every month. this is typically useful for companies in which there are different functional teams or groups; then, the normal reserved instance logic is applied to calculate the bill. for more information, see  in the aws organizations user guide. if you close the account that purchased the reserved instance, the payer account will continue being charged for the reserved instance until either the reserved instance expires or the closed account is permanently deleted. the closed account is permanently deleted after 90 days. after it is deleted, the member accounts will stop benefitting from the reserved instance billing discount. for more information about closing an account, see  in the aws organizations user guide. if your account qualifies for a discount pricing tier, it automatically receives discounts on upfront and instance usage fees for reserved instance purchases that you make within that tier level from that point on. to qualify for a discount, the list value of your reserved instances in the region must be $500,000 usd or more. the following rules apply: pricing tiers and related discounts apply only to purchases of amazon ec2 standard reserved instances.pricing tiers do not apply to reserved instances for windows with sql server standard, sql server web, and sql server enterprise. pricing tiers do not apply to reserved instances for linux with sql server standard, sql server web, and sql server enterprise. pricing tier discounts only apply to purchases made from aws. they do not apply to purchases of third-party reserved instances. discount pricing tiers are currently not applicable to convertible reserved instance purchases. topics you can determine the pricing tier for your account by calculating the list value for all of your reserved instances in a region. multiply the hourly recurring price for each reservation by the total number of hours for the term and add the undiscounted upfront price (also known as the fixed price) at the time of purchase. because the list value is based on undiscounted (public) pricing, it is not affected if you qualify for a volume discount or if the price drops after you buy your reserved instances. for example, for a 1-year partial upfront  reserved instance, assume the upfront price is $60.00 and the hourly rate is $0.007. this provides a list value of $121.32. to view the fixed price values for reserved instances using the amazon ec2 console open the amazon ec2 console at . in the navigation pane, choose reserved instances. display the upfront price column by choosing show/hide columns (the gear-shaped icon) in the top right corner. to view the fixed price values for reserved instances using the command line  (aws cli) (aws tools for windows powershell) (amazon ec2 api)when you buy reserved instances, amazon ec2 automatically applies any discounts to the part of your purchase that falls within a discount pricing tier. you don't need to do anything differently, and you can buy reserved instances using any of the amazon ec2 tools. for more information, see . after the list value of your active reserved instances in a region crosses into a discount pricing tier, any future purchase of reserved instances in that region are charged at a discounted rate. if a single purchase of reserved instances in a region takes you over the threshold of a discount tier, then the portion of the purchase that is above the price threshold is charged at the discounted rate. for more information about the temporary reserved instance ids that are created during the purchase process, see . if your list value falls below the price point for that discount pricing tier—for example, if some of your reserved instances expire—future purchases of reserved instances in the region are not discounted. however, you continue to get the discount applied against any reserved instances that were originally purchased within the discount pricing tier. when you buy reserved instances, one of four possible scenarios occurs: no discount—your purchase within a region is still below the discount threshold.partial discount—your purchase within a region crosses the threshold of the first discount tier. no discount is applied to one or more reservations and the discounted rate is applied to the remaining reservations.full discount—your entire purchase within a region falls within one discount tier and is discounted appropriately.two discount rates—your purchase within a region crosses from a lower discount tier to a higher discount tier. you are charged two different rates: one or more reservations at the lower discounted rate, and the remaining reservations at the higher discounted rate.if your purchase crosses into a discounted pricing tier, you see multiple entries for that purchase: one for that part of the purchase charged at the regular price, and another for that part of the purchase charged at the applicable discounted rate. the reserved instance service generates several reserved instance ids because your purchase crossed from an undiscounted tier, or from one discounted tier to another. there is an id for each set of reservations in a tier. consequently, the id returned by your purchase cli command or api action is different from the actual id of the new reserved instances. a consolidated billing account aggregates the list value of member accounts within a region. when the list value of all active reserved instances for the consolidated billing account reaches a discount pricing tier, any reserved instances purchased after this point by any member of the consolidated billing account are charged at the discounted rate (as long as the list value for that consolidated account stays above the discount pricing tier threshold). for more information, see .  
the price for a capacity reservation varies by payment option. when the capacity reservation is active, you are charged the equivalent on-demand rate whether you run the instances or not. if you do not use the reservation, this shows up as unused reservation on your ec2 bill. when you run an instance that matches the attributes of a reservation, you just pay for the instance and nothing for the reservation. there are no upfront or additional charges.  for example, if you create a capacity reservation for 20  linux instances and run 15  linux instances in the same availability zone, you will be charged for 15 active instances and for 5 unused instances in the reservation. billing discounts for savings plans and regional reserved instances apply to capacity reservations. for more information, see . for more information, see . capacity reservations are billed at per-second granularity. this means that you are charged for partial hours. for example, if a reservation remains active in your account for 24 hours and 15 minutes, you will be billed for 24.25 reservation hours. the following example shows how a capacity reservation is billed. the capacity reservation is created for one  linux instance, which has an on-demand rate of $0.10 per usage hour. in this example, the capacity reservation is active in the account for five hours. the capacity reservation is unused for the first hour, so it is billed for one unused hour at the  instance type's standard on-demand rate. in hours two through five, the capacity reservation is occupied by an  instance. during this time, the capacity reservation accrues no charges, and the account is instead billed for the  instance occupying it. in the sixth hour, the capacity reservation is canceled and the  instance runs normally outside of the reserved capacity. for that hour, it is charged at the on-demand rate of the  instance type.  billing discounts for savings plans and regional reserved instances apply to capacity reservations. aws automatically applies these discounts to capacity reservations that have matching attributes. when a capacity reservation is used by an instance, the discount is applied to the instance. discounts are preferentially applied to instance usage before covering unused capacity reservations. billing discounts for zonal reserved instances do not apply to capacity reservations. for more information, see the following: you can review the charges and fees to your account on the aws billing and cost management console. the dashboard displays a spend summary for your account.on the bills page, under details, expand the elastic compute cloud section and the region to get billing information about your capacity reservations.you can view the charges online, or you can download a csv file. for more information, see  in the aws billing and cost management user guide. 
an amazon ebs volume is a durable, block-level storage device that you can attach to your instances. after you attach a volume to an instance, you can use it as you would use a physical hard drive. ebs volumes are flexible. for current-generation volumes attached to current-generation instance types, you can dynamically increase size, modify the provisioned iops capacity, and change volume type on live production volumes. you can use ebs volumes as primary storage for data that requires frequent updates, such as the system drive for an instance or storage for a database application. you can also use them for throughput-intensive applications that perform continuous disk scans. ebs volumes persist independently from the running life of an ec2 instance. you can attach multiple ebs volumes to a single instance. the volume and instance must be in the same availability zone. depending on the volume and instance types, you can use  to mount a volume to multiple instances at the same time. amazon ebs provides the following volume types: general purpose ssd (), provisioned iops ssd (), throughput optimized hdd (), cold hdd (), and magnetic (, a previous-generation type). they differ in performance characteristics and price, allowing you to tailor your storage performance and cost to the needs of your applications. for more information, see . your account has a limit on the number of ebs volumes that you can use, and the total storage available to you. for more information about these limits, and how to request an increase in your limits, see . topics ebs volumes provide benefits that are not provided by instance store volumes. when you create an ebs volume, it is automatically replicated within its availability zone to prevent data loss due to failure of any single hardware component. you can attach an ebs volume to any ec2 instance in the same availability zone. after you attach a volume, it appears as a native block device similar to a hard drive or other physical device. at that point, the instance can interact with the volume just as it would with a local drive. you can connect to the instance and format the ebs volume with a file system, such as ext3, and then install applications.  if you attach multiple volumes to a device that you have named, you can stripe data across the volumes for increased i/o and throughput performance. you can attach an  ebs volume to up to 16 nitro-based instances. for more information, see . otherwise, you can attach an ebs volume to a single instance. you can get monitoring data for your ebs volumes, including root device volumes for ebs-backed instances, at no additional charge. for more information about monitoring metrics, see . for information about tracking the status of your volumes, see . an ebs volume is off-instance storage that can persist independently from the life of an instance. you continue to pay for the volume usage as long as the data persists.  ebs volumes that are attached to a running instance can automatically detach from the instance with their data intact when the instance is terminated if you uncheck the delete on termination checkbox when you configure ebs volumes for your instance on the ec2 console. the volume can then be reattached to a new instance, enabling quick recovery. if the checkbox for delete on termination is checked, the volume(s) will delete upon termination of the ec2 instance. if you are using an ebs-backed instance, you can stop and restart that instance without affecting the data stored in the attached volume. the volume remains attached throughout the stop-start cycle. this enables you to process and store the data on your volume indefinitely, only using the processing and storage resources when required. the data persists on the volume until the volume is deleted explicitly. the physical block storage used by deleted ebs volumes is overwritten with zeroes before it is allocated to another account. if you are dealing with sensitive data, you should consider encrypting your data manually or storing the data on a volume protected by amazon ebs encryption. for more information, see . by default, the root ebs volume that is created and attached to an instance at launch is deleted when that instance is terminated. you can modify this behavior by changing the value of the flag  to  when you launch the instance. this modified value causes the volume to persist even after the instance is terminated, and enables you to attach the volume to another instance.  by default, additional ebs volumes that are created and attached to an instance at launch are not deleted when that instance is terminated. you can modify this behavior by changing the value of the flag  to  when you launch the instance. this modified value causes the volumes to be deleted when the instance is terminated.  for simplified data encryption, you can create encrypted ebs volumes with the amazon ebs encryption feature. all ebs volume types support encryption. you can use encrypted ebs volumes to meet a wide range of data-at-rest encryption requirements for regulated/audited data and applications. amazon ebs encryption uses 256-bit advanced encryption standard algorithms (aes-256) and an amazon-managed key infrastructure. the encryption occurs on the server that hosts the ec2 instance, providing encryption of data-in-transit from the ec2 instance to amazon ebs storage. for more information, see .   amazon ebs encryption uses aws key management service (aws kms) master keys when creating encrypted volumes and any snapshots created from your encrypted volumes. the first time you create an encrypted ebs volume in a region, a default master key is created for you automatically. this key is used for amazon ebs encryption unless you select a customer master key (cmk) that you created separately using aws kms. creating your own cmk gives you more flexibility, including the ability to create, rotate, disable, define access controls, and audit the encryption keys used to protect your data. for more information, see the .  amazon ebs provides the ability to create snapshots (backups) of any ebs volume and write a copy of the data in the volume to amazon s3, where it is stored redundantly in multiple availability zones. the volume does not need to be attached to a running instance in order to take a snapshot. as you continue to write data to a volume, you can periodically create a snapshot of the volume to use as a baseline for new volumes. these snapshots can be used to create multiple new ebs volumes or move volumes across availability zones. snapshots of encrypted ebs volumes are automatically encrypted.  when you create a new volume from a snapshot, it's an exact copy of the original volume at the time the snapshot was taken. ebs volumes that are created from encrypted snapshots are automatically encrypted. by optionally specifying a different availability zone, you can use this functionality to create a duplicate volume in that zone. the snapshots can be shared with specific aws accounts or made public. when you create snapshots, you incur charges in amazon s3 based on the volume's total size. for a successive snapshot of the volume, you are only charged for any additional data beyond the volume's original size.  snapshots are incremental backups, meaning that only the blocks on the volume that have changed after your most recent snapshot are saved. if you have a volume with 100 gib of data, but only 5 gib of data have changed since your last snapshot, only the 5 gib of modified data is written to amazon s3. even though snapshots are saved incrementally, the snapshot deletion process is designed so that you need to retain only the most recent snapshot. to help categorize and manage your volumes and snapshots, you can tag them with metadata of your choice. for more information, see . to back up your volumes automatically, you can use  or . ebs volumes support live configuration changes while in production. you can modify volume type, volume size, and iops capacity without service interruptions. for more information, see . 
some resources can be used in all regions (global), and some resources are specific to the region or availability zone in which they reside.  
with amazon ebs elastic volumes, you can increase the volume size, change the volume type, or adjust the performance of your ebs volumes. if your instance supports elastic volumes, you can do so without detaching the volume or restarting the instance. this enables you to continue using your application while the changes take effect. there is no charge to modify the configuration of a volume. you are charged for the new volume configuration after volume modification starts. for more information, see the  page. topics 
the following examples show policy statements that you could use to control the permissions that iam users have to amazon ec2. these policies are designed for requests that are made with the aws cli or an aws sdk. for example policies for working in the amazon ec2 console, see . for examples of iam policies specific to amazon vpc, see . topics the following policy grants users permissions to use all amazon ec2 api actions whose names begin with . the  element uses a wildcard to indicate that users can specify all resources with these api actions. the * wildcard is also necessary in cases where the api action does not support resource-level permissions. for more information about which arns you can use with which amazon ec2 api actions, see  in the iam user guide. users don't have permission to perform any actions on the resources (unless another statement grants them permission to do so) because they're denied permission to use api actions by default. the following policy denies users permission to use all amazon ec2 api actions unless the region is europe (frankfurt). it uses the global condition key , which is supported by all amazon ec2 api actions. alternatively, you can use the condition key , which is specific to amazon ec2 and is supported by all amazon ec2 api actions. topics the following policy grants users permissions to use the api actions specified in the  element. the  element uses a * wildcard to indicate that users can specify all resources with these api actions. the * wildcard is also necessary in cases where the api action does not support resource-level permissions. for more information about which arns you can use with which amazon ec2 api actions, see  in the iam user guide. the users don't have permission to use any other api actions (unless another statement grants them permission to do so) because users are denied permission to use api actions by default. the following policy allows users to describe all instances, to start and stop only instances i-1234567890abcdef0 and i-0598c7d356eba48d7, and to terminate only instances in the us east (n. virginia) region () with the resource tag "".  the first statement uses a * wildcard for the  element to indicate that users can specify all resources with the action; in this case, they can list all instances. the * wildcard is also necessary in cases where the api action does not support resource-level permissions (in this case, ). for more information about which arns you can use with which amazon ec2 api actions, see  in the iam user guide. the second statement uses resource-level permissions for the  and  actions. the specific instances are indicated by their arns in the  element. the third statement allows users to terminate all instances in the us east (n. virginia) region () that belong to the specified aws account, but only where the instance has the tag . the  element qualifies when the policy statement is in effect.  topics when an api action requires a caller to specify multiple resources, you must create a policy statement that allows users to access all required resources. if you need to use a  element with one or more of these resources, you must create multiple statements as shown in this example. the following policy allows users to attach volumes with the tag "=iam-user-name" to instances with the tag "", and to detach those volumes from those instances. if you attach this policy to an iam group, the  policy variable gives each iam user in the group permission to attach or detach volumes from the instances with a tag named  that has his or her iam user name as a value. the following policy allows users to use the  api action. the user is allowed to create a volume only if the volume is encrypted and only if the volume size is less than 20 gib. the following policy includes the  condition key that requires users to tag any volumes they create with the tags  and . the  condition key uses the  modifier to indicate that only the keys  and  are allowed in the request (no other tags can be specified). if users don't pass these specific tags, or if they don't specify tags at all, the request fails.  for resource-creating actions that apply tags, users must also have permissions to use the  action. the second statement uses the  condition key to allow users to create tags only in the context of . users cannot tag existing volumes or any other resources. for more information, see . the following policy allows users to create a volume without having to specify tags. the  action is only evaluated if tags are specified in the  request. if users do specify tags, the tag must be . no other tags are allowed in the request. the following are example policies for both  (point-in-time snapshot of an ebs volume) and  (multi-volume snapshots). topics the following policy allows customers to use the  api action. the customer can create snapshots only if the volume is encrypted and only if the volume size is less than 20 gib. the following policy allows customers to use the  api action. the customer can create snapshots only if all of the volumes on the instance are type gp2. the following policy includes the  condition key that requires the customer to apply the tags  and  to any new snapshot. the  condition key uses the  modifier to indicate that only the keys  and  can be specified in the request. the request fails if either of these conditions is not met. for resource-creating actions that apply tags, customers must also have permissions to use the  action. the third statement uses the  condition key to allow customers to create tags only in the context of . customers cannot tag existing volumes or any other resources. for more information, see . the following policy includes the  condition key that requires the customer to apply the tags  and  to any new snapshot. the  condition key uses the  modifier to indicate that only the keys  and  can be specified in the request. the request fails if either of these conditions is not met. the following policy allows customers to create a snapshot without having to specify tags. the  action is evaluated only if tags are specified in the  or  request. if a tag is specified, the tag must be . no other tags are allowed in the request. the following policy allows snapshots to be created only if the source volume is tagged with  for the customer, and the snapshot itself is tagged with  and . the customer can add additional tags to the snapshot. the following policy for  allows snapshots to be created only if the source volume is tagged with  for the customer, and the snapshot itself is tagged with  and .  the following policy allows deletion of a snapshot only if the snapshot is tagged with user:username for the customer. the following policy allows a customer to create a snapshot but denies the action if the snapshot being created has a tag key . the following policy allows a customer to create snapshots but denies the action if the snapshots being created have a tag key . the following policy allows you to combine multiple actions into a single policy. you can only create a snapshot (in the context of ) when the snapshot is created in region . you can only create snapshots (in the context of ) when the snapshots are being created in the region  and when the instance type is . the following policy allows modification of a snapshot only if the snapshot is tagged with , where username is the customer's aws account user name. the request fails if this condition is not met. the  api action launches one or more on-demand instances or one or more spot instances.  requires an ami and creates an instance. users can specify a key pair and security group in the request. launching into a vpc requires a subnet, and creates a network interface. launching from an amazon ebs-backed ami creates a volume. therefore, the user must have permissions to use these amazon ec2 resources. you can create a policy statement that requires users to specify an optional parameter on , or restricts users to particular values for a parameter. for more information about the resource-level permissions that are required to launch an instance, see  in the iam user guide. by default, users don't have permissions to describe, start, stop, or terminate the resulting instances. one way to grant the users permission to manage the resulting instances is to create a specific tag for each instance, and then create a statement that enables them to manage instances with that tag. for more information, see . topics the following policy allows users to launch instances using only the specified amis,  and . the users can't launch an instance using other amis (unless another statement grants the users permission to do so). alternatively, the following policy allows users to launch instances from all amis owned by amazon. the  element of the first statement tests whether  is . the users can't launch an instance using other amis (unless another statement grants the users permission to do so). the following policy allows users to launch instances using only the  or  instance type, which you might do to control costs. the users can't launch larger instances because the  element of the first statement tests whether  is either  or .  alternatively, you can create a policy that denies users permissions to launch any instances except  and  instance types. the following policy allows users to launch instances using only the specified subnet, . the group can't launch instances into any another subnet (unless another statement grants the users permission to do so). alternatively, you could create a policy that denies users permissions to launch an instance into any other subnet. the statement does this by denying permission to create a network interface, except where subnet  is specified. this denial overrides any other policies that are created to allow launching instances into other subnets. the following policy allows users to launch instances only if the ebs volumes for the instance are encrypted. the user must launch an instance from an ami that was created with encrypted snapshots, to ensure that the root volume is encrypted. any additional volume that the user attaches to the instance during launch must also be encrypted. tag instances on creation the following policy allows users to launch instances and tag the instances during creation. for resource-creating actions that apply tags, users must have permissions to use the  action. the second statement uses the  condition key to allow users to create tags only in the context of , and only for instances. users cannot tag existing resources, and users cannot tag volumes using the  request.  for more information, see . tag instances and volumes on creation with specific tags the following policy includes the  condition key that requires users to tag any instances and volumes that are created by  with the tags  and . the  condition key uses the  modifier to indicate that only the keys  and  are allowed in the request (no other tags can be specified). if no tags are specified in the request, the request fails.  tag instances and volumes on creation with at least one specific tag the following policy uses the  modifier on the  condition to indicate that at least one tag must be specified in the request, and it must contain the key  or . the tag must be applied to both instances and volumes. any tag values can be specified in the request.  if instances are tagged on creation, they must be tagged with a specific tag in the following policy, users do not have to specify tags in the request, but if they do, the tag must be . no other tags are allowed. users can apply the tags to any taggable resource in the  request. to disallow anyone called tag on create for runinstances only allow specific tags for spot-instances-request. surprise inconsistency number 2 comes into play here. under normal circumstances, specifying no tags will result in unauthenticated. in the case of spot-instances-request, this policy will not be evaluated if there are no spot-instances-request tags, so a non-tag spot on run request will succeed.  in the following example, users can launch instances, but only if they use a specific launch template (). the  condition key prevents users from overriding any of the resources specified in the launch template. the second part of the statement allows users to tag instances on creation—this part of the statement is necessary if tags are specified for the instance in the launch template. in the following policy, users can launch an instance and specify an elastic gpu to attach to the instance. users can launch instances in any region, but they can only attach an elastic gpu during a launch in the  region.  the  condition key uses the  modifier to indicate that only the elastic gpu types  and  are allowed in the request.  in the following example, users can launch instances, but only if they use a specific launch template (). users can override any parameters in the launch template by specifying the parameters in the  action. in this example, users can launch instances only if they use a launch template. the policy uses the  condition key to prevent users from overriding any pre-existing arns in the launch template. the following example policy allows user to launch instances, but only if they use a launch template. users cannot override the subnet and network interface parameters in the request; these parameters can only be specified in the launch template. the first part of the statement uses the  element to allow all other resources except subnets and network interfaces. the second part of the statement allows the subnet and network interface resources, but only if they are sourced from the launch template. the following example allows users to launch instances only if they use a launch template, and only if the launch template has the tag . users cannot override any of the launch template parameters in the  action. you can use the runinstances action to create spot instance requests, and tag the spot instance requests on create. the resource to specify for runinstances is . the  resource is evaluated in the iam policy as follows: if you don't tag a spot instance request on create, amazon ec2 does not evaluate the  resource in the runinstances statement.if you tag a spot instance request on create, amazon ec2 evaluates the  resource in the runinstances statement.therefore, for the  resource, the following rules apply to the iam policy: if you use runinstances to create a spot instance request and you don't intend to tag the spot instance request on create, you don’t need to explicitly allow the  resource; the call will succeed.if you use runinstances to create a spot instance request and intend to tag the spot instance request on create, you must include the  resource in the runinstances allow statement, otherwise the call will fail.if you use runinstances to create a spot instance request and intend to tag the spot instance request on create, you must specify the  resource or  wildcard in the createtags allow statement, otherwise the call will fail. you can request spot instances using runinstances or requestspotinstances. the following example iam policies apply only when requesting spot instances using runinstances. example: request spot instances using runinstances the following policy allows users to request spot instances by using the runinstances action. the  resource, which is created by runinstances, requests spot instances. noteto use runinstances to create spot instance requests, you can omit  from the  list if you do not intend to tag the spot instance requests on create. this is because amazon ec2 does not evaluate the  resource in the runinstances statement if the spot instance request is not tagged on create. warningnot supported – example: deny users permission to request spot instances using runinstancesthe following policy is not supported for the  resource.the following policy is meant to give users the permission to launch on-demand instances, but deny users the permission to request spot instances. the  resource, which is created by runinstances, is the resource that requests spot instances. the second statement is meant to deny the runinstances action for the  resource. however, this condition is not supported because amazon ec2 does not evaluate the  resource in the runinstances statement if the spot instance request is not tagged on create.   example: tag spot instance requests on create the following policy allows users to tag all resources that are created during instance launch. the first statement allows runinstances to create the listed resources. the  resource, which is created by runinstances, is the resource that requests spot instances. the second statement provides a  wildcard to allow all resources to be tagged when they are created at instance launch. noteif you tag a spot instance request on create, amazon ec2 evaluates the  resource in the runinstances statement. therefore, you must explicitly allow the  resource for the runinstances action, otherwise the call will fail. example: deny tag on create for spot instance requests the following policy denies users the permission to tag the resources that are created during instance launch. the first statement allows runinstances to create the listed resources. the  resource, which is created by runinstances, is the resource that requests spot instances. the second statement provides a  wildcard to deny all resources being tagged when they are created at instance launch. if  or any other resource is tagged on create, the runinstances call will fail. warningnot supported – example: allow creating a spot instance request only if it is assigned a specific tagthe following policy is not supported for the  resource.the following policy is meant to grant runinstances the permission to create a spot instance request only if the request is tagged with a specific tag.the first statement allows runinstances to create the listed resources.the second statement is meant to grant users the permission to create a spot instance request only if the request has the tag . if this condition is applied to other resources created by runinstances, specifying no tags results in an  error. however, if no tags are specified for the spot instance request, amazon ec2 does not evaluate the  resource in the runinstances statement, which results in non-tagged spot instance requests being created by runinstances.note that specifying another tag other than  results in an  error, because if a user tags a spot instance request, amazon ec2 evaluates the  resource in the runinstances statement.   example: deny creating a spot instance request if it is assigned a specific tag the following policy denies runinstances the permission to create a spot instance request if the request is tagged with .  the first statement allows runinstances to create the listed resources. the second statement denies users the permission to create a spot instance request if the request has the tag . specifying  as a tag results in an  error. specifying other tags or specifying no tags will result in the creation of a spot instance request. the following policy gives users permission to view, modify, and purchase reserved instances in your account. it is not possible to set resource-level permissions for individual reserved instances. this policy means that users have access to all the reserved instances in the account. the  element uses a * wildcard to indicate that users can specify all resources with the action; in this case, they can list and modify all reserved instances in the account. they can also purchase reserved instances using the account credentials. the * wildcard is also necessary in cases where the api action does not support resource-level permissions. to allow users to view and modify the reserved instances in your account, but not purchase new reserved instances. the following policy allows users to use the  action to apply tags to an instance only if the tag contains the key  and the value . the  modifier is used with the  condition key to indicate that only the key  is allowed in the request (no other tags are allowed). the user cannot tag any other resource types. the following policy allows users to tag any taggable resource that already has a tag with a key of  and a value of the iam username. in addition, users must specify a tag with a key of  and a value of either  or  in the request. users can specify additional tags in the request. you can create an iam policy that allows users to delete specific tags for a resource. for example, the following policy allows users to delete tags for a volume if the tag keys specified in the request are  or . any value can be specified for the tag but the tag key must match either of the specified keys. noteif you delete a resource, all tags associated with the resource are also deleted. users do not need permissions to use the  action to delete a resource that has tags; they only need permissions to perform the deleting action. this policy allows users to delete only the  tag on any resource, and only if the resource is already tagged with a key of  and a value of the iam username. users cannot delete any other tags for a resource. the following policy allows users to attach, replace, and detach an iam role to instances that have the tag . replacing or detaching an iam role requires an association id, therefore the policy also grants users permission to use the  action.  iam users must have permission to use the  action in order to pass the role to the instance.  the following policy allows users to attach or replace an iam role for any instance. users can only attach or replace iam roles with names that begin with . for the  action, ensure that you specify the name of the iam role and not the instance profile (if the names are different). for more information, see . the following policy allows users to add, remove, and replace routes for route tables that are associated with vpc  only. to specify a vpc for the  condition key, you must specify the full arn of the vpc. the following is an example of a policy that you might attach to an iam role. the policy allows an instance to view resources in various aws services. it uses the  condition key to specify that the instance from which the request is made must be instance . if the same iam role is associated with another instance, the other instance cannot perform any of these actions. the  key is an aws-wide condition key, therefore it can be used for other service actions, not just amazon ec2. the following policy allows users to create a launch template version and modify a launch template, but only for a specific launch template (). users cannot work with other launch templates. the following policy allows users to delete any launch template and launch template version, provided that the launch template has the tag =. the following policies ensure that users can only retrieve  using instance metadata service version 2 (imdsv2). you can combine the following four policies into one policy with four statements. when combined as one policy, you can use the policy as a service control policy (scp). it can work equally well as a deny policy that you apply to an existing iam policy (taking away and limiting existing permission), or as an scp that is applied globally across an account, an organizational unit (ou), or an entire organization. notethe following runinstances metadata options policies must be used in conjunction with a policy that gives the principal permissions to launch an instance with runinstances. if the principal does not also have runinstances permissions, it will not be able to launch an instance. for more information, see the policies in  and . importantif you use auto scaling groups and you need to require the use of imdsv2 on all new instances, your auto scaling groups must use launch templates.when an auto scaling group uses a launch template, the  permissions of the iam principal are checked when a new auto scaling group is created. they are also checked when an existing auto scaling group is updated to use a new launch template or a new version of a launch template.restrictions on the use of imdsv1 on iam principals for  are only checked when an auto scaling group that is using a launch template, is created or updated. for an auto scaling group that is configured to use the  or  launch template, the permissions are not checked when a new version of the launch template is created. for permissions to be checked, you must configure the auto scaling group to use a specific version of the launch template.disable the use of launch configurations for all accounts in your organization by using either service control policies (scps) or iam permissions boundaries for new principals that are created. for existing iam principals with auto scaling group permissions, update their associated policies with this condition key. to disable the use of launch configurations, create or modify the relevant scp, permissions boundary, or iam policy with the  condition key with the value specified as . for new launch templates, configure the instance metadata options in the launch template. for existing launch templates, create a new version of the launch template and configure the instance metadata options in the new version. in the policy that gives any principal the permission to use a launch template, restrict association of  and  by specifying . by restricting the use to a specific version of a launch template, you can ensure that new instances will be launched using the version in which the instance metadata options are configured. for more information, see  in the amazon ec2 auto scaling api reference, specifically the  parameter. for an auto scaling group that uses a launch configuration, replace the launch configuration with a launch template. for more information, see  in the amazon ec2 auto scaling user guide. for an auto scaling group that uses a launch template, make sure that it uses a new launch template with the instance metadata options configured, or uses a new version of the current launch template with the instance metadata options configured. for more information, see  in the aws cli command reference. topics the following policy specifies that you can’t call the runinstances api unless the instance is also opted in to require the use of imdsv2 (indicated by ). if you do not specify that the instance requires imdsv2, you get an  error when you call the runinstances api. the following policy specifies that you can’t call the runinstances api unless you also specify a hop limit, and the hop limit can’t be more than 3. if you fail to do that, you get an  error when you call the runinstances api. notewhen the following policy and the preceding one are applied to an account via an scp, you can’t use the ec2 console to launch instances because the console doesn’t yet support the  and  parameters. the following policy removes the ability for the general population of administrators to modify instance metadata options, and permits only users with the role  to make changes. if any principal other than the  role tries to call the modifyinstancemetadataoptions api, it will get an  error. this statement could be used to control the use of the modifyinstancemetadataoptions api; there are currently no fine-grained access controls (conditions) for the modifyinstancemetadataoptions api. the following policy specifies that if this policy is applied to a role, and the role is assumed by the ec2 service and the resulting credentials are used to sign a request, then the request must be signed by ec2 role credentials retrieved from imdsv2. otherwise, all of its api calls will get an  error. this statement/policy can be applied generally because, if the request is not signed by ec2 role credentials, it has no effect. 
you can aggregate statistics for your instances that have detailed monitoring enabled. instances that use basic monitoring are not included. note that amazon cloudwatch cannot aggregate data across regions. metrics are completely separate between regions. before you can get statistics aggregated across instances, you must enable detailed monitoring (at an additional charge), which provides data in 1-minute periods. for more information, see . this example shows you how to determine average cpu utilization for all instances that use a specific amazon machine image (ami). the average is over 60-second time intervals for a one-day period. to display the average cpu utilization by ami (console) open the cloudwatch console at . in the navigation pane, choose metrics. choose the ec2 namespace and then choose by image (ami) id. choose the row for the cpuutilization metric and the specific ami, which displays a graph for the metric for the specified ami. to name the graph, choose the pencil icon. to change the time range, select one of the predefined values or choose custom. to change the statistic or the period for the metric, choose the graphed metrics tab. choose the column heading or an individual value, and then choose a different value. to get the average cpu utilization for an image id (aws cli)use the  command as follows. the following is example output. each value represents an average cpu utilization percentage for the ec2 instances running the specified ami. 
to help you track your spot instance requests and plan your use of spot instances, use the request status provided by amazon ec2. for example, the request status can provide the reason why your spot request isn't fulfilled yet, or list the constraints that are preventing the fulfillment of your spot request. at each step of the process—also called the spot request lifecycle—specific events determine successive request states. topics the following diagram shows you the paths that your spot request can follow throughout its lifecycle, from submission to termination. each step is depicted as a node, and the status code for each node describes the status of the spot request and spot instance.  pending evaluationas soon as you create a spot instance request, it goes into the  state unless one or more request parameters are not valid (). holdingif one or more request constraints are valid but can't be met yet, or if there is not enough capacity, the request goes into a holding state waiting for the constraints to be met. the request options affect the likelihood of the request being fulfilled. for example, if you specify a maximum price below the current spot price, your request stays in a holding state until the spot price goes below your maximum price. if you specify an availability zone group, the request stays in a holding state until the availability zone constraint is met. in the event of an outage of one of the availability zones, there is a chance that the spare ec2 capacity available for spot instance requests in other availability zones can be affected. pending evaluation/fulfillment-terminalyour spot instance request can go to a  state if you create a request that is valid only during a specific time period and this time period expires before your request reaches the pending fulfillment phase. it might also happen if you cancel the request, or if a system error occurs. * if you cancel the request. pending fulfillmentwhen the constraints you specified (if any) are met and your maximum price is equal to or higher than the current spot price, your spot request goes into the  state. at this point, amazon ec2 is getting ready to provision the instances that you requested. if the process stops at this point, it is likely to be because it was canceled by the user before a spot instance was launched. it may also be because an unexpected system error occurred. fulfilledwhen all the specifications for your spot instances are met, your spot request is fulfilled. amazon ec2 launches the spot instances, which can take a few minutes. if a spot instance is hibernated or stopped when interrupted, it remains in this state until the request can be fulfilled again or the request is canceled. if you stop a spot instance, your spot request goes into the  or  state until the spot instance can be started again or the request is cancelled.  * a spot instance goes into the  state if you stop the instance or run the shutdown command from the instance. after you've stopped the instance, you can start it again. on restart, the spot instance request returns to the  state and then amazon ec2 launches a new spot instance when the constraints are met.  ** the spot request state is  if you stop the spot instance but do not cancel the request. the request state is  if your spot instance is stopped and the request expires.  fulfilled-terminalyour spot instances continue to run as long as your maximum price is at or above the spot price, there is available capacity for your instance type, and you don't terminate the instance. if a change in the spot price or available capacity requires amazon ec2 to terminate your spot instances, the spot request goes into a terminal state. a request also goes into the terminal state if you cancel the spot request or terminate the spot instances. * the request state is  if you terminate the instance but do not cancel the request. the request state is  if you terminate the instance and cancel the request. even if you terminate a spot instance before you cancel its request, there might be a delay before amazon ec2 detects that your spot instance was terminated. in this case, the request state can either be  or . persistent requestswhen your spot instances are terminated (either by you or amazon ec2), if the spot request is a persistent request, it returns to the  state and then amazon ec2 can launch a new spot instance when the constraints are met. you can get request status information using the aws management console or a command line tool. to get request status information (console) open the amazon ec2 console at . in the navigation pane, choose spot requests and select the spot request. to check the status, on the description tab, check the status field. to get request status information using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)spot request status information is composed of a status code, the update time, and a status message. together, these help you determine the disposition of your spot request. the following are the spot request status codes: amazon ec2 cannot launch all the instances you requested in the same availability zone. one or more parameters for your spot request are not valid (for example, the ami you specified does not exist). the status message indicates which parameter is not valid. the user canceled the spot request before it was fulfilled. there is not enough capacity available for the instances that you requested. the spot request can't be fulfilled because one or more constraints are not valid (for example, the availability zone does not exist). the status message indicates which constraint is not valid. the spot request is , and amazon ec2 is launching your spot instances. your instance was stopped because the spot price exceeded your maximum price. your instance was stopped because a user stopped the instance or ran the shutdown command from the instance. your instance was stopped because there was no longer enough spot capacity available for the instance. your instance was terminated because the spot price exceeded your maximum price. if your request is persistent, the process restarts, so your request is pending evaluation. your spot instance was terminated at the end of its scheduled duration. your instance was terminated from a stopped state.  or you terminated a spot instance that had been fulfilled, so the request state is  (unless it's a persistent request) and the instance state is . one or more of the instances in your launch group was terminated, so the launch group constraint is no longer fulfilled. your instance was terminated because there is no longer enough spot capacity available for the instance. amazon ec2 cannot launch all the instances that you requested at the same time. all instances in a launch group are started and terminated together. the limit on the number of ebs volumes or total volume storage was exceeded. for more information about these limits and how to request an increase, see  in the amazon web services general reference. the spot instance is marked for stopping. the spot instance is marked for termination. the spot request is not evaluated until the scheduled date. after you make a spot instance request, it goes into the  state while the system evaluates the parameters of your request. amazon ec2 is trying to provision your spot instances. the spot request can't be fulfilled yet because a spot instance can't be added to the placement group at this time. the request can't be fulfilled yet because your maximum price is below the spot price. in this case, no instance is launched and your request remains . you canceled the spot request while the spot instances are still running. the request is , but the instances remain . the spot request expired because it was not fulfilled before the specified date. there was an unexpected system error. if this is a recurring issue, please contact aws support for assistance. 
the  provide the tools for developing, testing, and building machine learning applications using aws inferentia. you can use the deep learning amis for aws inferentia development on any amazon ec2 inf1 instance. for more information, see the documentation for . 
amazon ebs provides the following data services. topics 
classiclink allows you to link ec2-classic instances to a vpc in your account, within the same region. if you associate the vpc security groups with a ec2-classic instance, this enables communication between your ec2-classic instance and instances in your vpc using private ipv4 addresses. classiclink removes the need to make use of public ipv4 addresses or elastic ip addresses to enable communication between instances in these platforms. classiclink is available to all users with accounts that support the ec2-classic platform, and can be used with any ec2-classic instance. for more information about migrating your resources to a vpc, see . there is no additional charge for using classiclink. standard charges for data transfer and instance usage apply. topics there are two steps to linking an ec2-classic instance to a vpc using classiclink. first, you must enable the vpc for classiclink. by default, all vpcs in your account are not enabled for classiclink, to maintain their isolation. after you've enabled the vpc for classiclink, you can then link any running ec2-classic instance in the same region in your account to that vpc. linking your instance includes selecting security groups from the vpc to associate with your ec2-classic instance. after you've linked the instance, it can communicate with instances in your vpc using their private ip addresses, provided the vpc security groups allow it. your ec2-classic instance does not lose its private ip address when linked to the vpc. notelinking your instance to a vpc is sometimes referred to as attaching your instance. a linked ec2-classic instance can communicate with instances in a vpc, but it does not form part of the vpc. if you list your instances and filter by vpc, for example, through the  api request, or by using the instances screen in the amazon ec2 console, the results do not return any ec2-classic instances that are linked to the vpc. for more information about viewing your linked ec2-classic instances, see . by default, if you use a public dns hostname to address an instance in a vpc from a linked ec2-classic instance, the hostname resolves to the instance's public ip address. the same occurs if you use a public dns hostname to address a linked ec2-classic instance from an instance in the vpc. if you want the public dns hostname to resolve to the private ip address, you can enable classiclink dns support for the vpc. for more information, see . if you no longer require a classiclink connection between your instance and the vpc, you can unlink the ec2-classic instance from the vpc. this disassociates the vpc security groups from the ec2-classic instance. a linked ec2-classic instance is automatically unlinked from a vpc when it's stopped. after you've unlinked all linked ec2-classic instances from the vpc, you can disable classiclink for the vpc. linked ec2-classic instances can access the following aws services in the vpc: amazon redshift, amazon elasticache, elastic load balancing, and amazon rds. however, instances in the vpc cannot access the aws services provisioned by the ec2-classic platform using classiclink. if you use elastic load balancing, you can register your linked ec2-classic instances with the load balancer. you must create your load balancer in the classiclink-enabled vpc and enable the availability zone in which the instance runs. if you terminate the linked ec2-classic instance, the load balancer deregisters the instance. if you use amazon ec2 auto scaling, you can create an amazon ec2 auto scaling group with instances that are automatically linked to a specified classiclink-enabled vpc at launch. for more information, see  in the amazon ec2 auto scaling user guide. if you use amazon rds instances or amazon redshift clusters in your vpc, and they are publicly accessible (accessible from the internet), the endpoint you use to address those resources from a linked ec2-classic instance by default resolves to a public ip address. if those resources are not publicly accessible, the endpoint resolves to a private ip address. to address a publicly accessible rds instance or redshift cluster over private ip using classiclink, you must use their private ip address or private dns hostname, or you must enable classiclink dns support for the vpc. if you use a private dns hostname or a private ip address to address an rds instance, the linked ec2-classic instance cannot use the failover support available for multi-az deployments. you can use the amazon ec2 console to find the private ip addresses of your amazon redshift, amazon elasticache, or amazon rds resources. to locate the private ip addresses of aws resources in your vpc open the amazon ec2 console at . in the navigation pane, choose network interfaces. check the descriptions of the network interfaces in the description column. a network interface that's used by amazon redshift, amazon elasticache, or amazon rds will have the name of the service in the description. for example, a network interface that's attached to an amazon rds instance will have the following description: . select the required network interface.  in the details pane, get the private ip address from the primary private ipv4 ip field.  by default, iam users do not have permission to work with classiclink. you can create an iam policy that grants users permissions to enable or disable a vpc for classiclink, link or unlink an instance to a classiclink-enabled vpc, and to view classiclink-enabled vpcs and linked ec2-classic instances. for more information about iam policies for amazon ec2, see .  for more information about policies for working with classiclink, see the following example: . linking your ec2-classic instance to a vpc does not affect your ec2-classic security groups. they continue to control all traffic to and from the instance. this excludes traffic to and from instances in the vpc, which is controlled by the vpc security groups that you associated with the ec2-classic instance. ec2-classic instances that are linked to the same vpc cannot communicate with each other through the vpc; regardless of whether they are associated with the same vpc security group. communication between ec2-classic instances is controlled by the ec2-classic security groups associated with those instances. for an example of a security group configuration, see . after you've linked your instance to a vpc, you cannot change which vpc security groups are associated with the instance. to associate different security groups with your instance, you must first unlink the instance, and then link it to the vpc again, choosing the required security groups. when you enable a vpc for classiclink, a static route is added to all of the vpc route tables with a destination of  and a target of . this allows communication between instances in the vpc and any ec2-classic instances that are then linked to the vpc. if you add a custom route table to a classiclink-enabled vpc, a static route is automatically added with a destination of  and a target of . when you disable classiclink for a vpc, this route is automatically deleted in all of the vpc route tables. vpcs that are in the  and  ip address ranges can be enabled for classiclink only if they do not have any existing static routes in route tables in the  ip address range, excluding the local routes that were automatically added when the vpc was created. similarly, if you've enabled a vpc for classiclink, you may not be able to add any more specific routes to your route tables within the  ip address range.  importantif your vpc cidr block is a publicly routable ip address range, consider the security implications before you link an ec2-classic instance to your vpc. for example, if your linked ec2-classic instance receives an incoming denial of service (dos) request flood attack from a source ip address that falls within the vpc’s ip address range, the response traffic is sent into your vpc. we strongly recommend that you create your vpc using a private ip address range as specified in . for more information about route tables and routing in your vpc, see  in the amazon vpc user guide. if you have a vpc peering connection between two vpcs, and there are one or more ec2-classic instances that are linked to one or both of the vpcs via classiclink, you can extend the vpc peering connection to enable communication between the ec2-classic instances and the instances in the vpc on the other side of the vpc peering connection. this enables the ec2-classic instances and the instances in the vpc to communicate using private ip addresses. to do this, you can enable a local vpc to communicate with a linked ec2-classic instance in a peer vpc, or you can enable a local linked ec2-classic instance to communicate with instances in a peer vpc. if you enable a local vpc to communicate with a linked ec2-classic instance in a peer vpc, a static route is automatically added to your route tables with a destination of  and a target of .  for more information and examples, see  in the amazon vpc peering guide. to use the classiclink feature, you need to be aware of the following limitations: you can link an ec2-classic instance to only one vpc at a time.if you stop your linked ec2-classic instance, it's automatically unlinked from the vpc and the vpc security groups are no longer associated with the instance. you can link your instance to the vpc again after you've restarted it.you cannot link an ec2-classic instance to a vpc that's in a different region or a different aws account.you cannot use classiclink to link a vpc instance to a different vpc, or to a ec2-classic resource. to establish a private connection between vpcs, you can use a vpc peering connection. for more information, see the .you cannot associate a vpc elastic ip address with a linked ec2-classic instance.you cannot enable ec2-classic instances for ipv6 communication. you can associate an ipv6 cidr block with your vpc and assign ipv6 address to resources in your vpc, however, communication between a classiclinked instance and resources in the vpc is over ipv4 only.vpcs with routes that conflict with the ec2-classic private ip address range of  cannot be enabled for classiclink. this does not include vpcs with  and  ip address ranges that already have local routes in their route tables. for more information, see .vpcs configured for dedicated hardware tenancy cannot be enabled for classiclink. contact aws support to request that your dedicated tenancy vpc be allowed to be enabled for classiclink. importantec2-classic instances are run on shared hardware. if you've set the tenancy of your vpc to  because of regulatory or security requirements, then linking an ec2-classic instance to your vpc might not conform to those requirements, as this allows a shared tenancy resource to address your isolated resources directly using private ip addresses. if you need to enable your dedicated vpc for classiclink, provide a detailed reason in your request to aws support.if you link your ec2-classic instance to a vpc in the  range, and you have a dns server running on the  ip address within the vpc, then your linked ec2-classic instance can't access the vpc dns server. to work around this issue, run your dns server on a different ip address within the vpc.classiclink doesn't support transitive relationships out of the vpc. your linked ec2-classic instance doesn't have access to any vpn connection, vpc gateway endpoint, nat gateway, or internet gateway associated with the vpc. similarly, resources on the other side of a vpn connection or an internet gateway don't have access to a linked ec2-classic instance.you can use the amazon ec2 and amazon vpc consoles to work with the classiclink feature. you can enable or disable a vpc for classiclink, and link and unlink ec2-classic instances to a vpc. notethe classiclink features are only visible in the consoles for accounts and regions that support ec2-classic.  topics to link an ec2-classic instance to a vpc, you must first enable the vpc for classiclink. you cannot enable a vpc for classiclink if the vpc has routing that conflicts with the ec2-classic private ip address range. for more information, see . to enable a vpc for classiclink open the amazon vpc console at . in the navigation pane, choose your vpcs. choose a vpc, and then choose actions, enable classiclink. in the confirmation dialog box, choose yes, enable. (optional) if you want the public dns hostname to resolve to the private ip address, enable classiclink dns support for the vpc before you link any instances. for more information, see . you can create a new vpc and immediately enable it for classiclink by using the vpc wizard in the amazon vpc console. to create a vpc with classiclink enabled open the amazon vpc console at . from the amazon vpc dashboard, choose start vpc wizard. select one of the vpc configuration options and choose select.  on the next page of the wizard, choose yes for enable classiclink. complete the rest of the steps in the wizard to create your vpc. for more information about using the vpc wizard, see  in the amazon vpc user guide. (optional) if you want the public dns hostname to resolve to the private ip address, enable classiclink dns support for the vpc before you link any instances. for more information, see . after you've enabled a vpc for classiclink, you can link an ec2-classic instance to it. noteyou can only link a running ec2-classic instance to a vpc. you cannot link an instance that's in the  state. if you want the public dns hostname to resolve to the private ip address, enable classiclink dns support for the vpc before you link the instance. for more information, see . to link an instance to a vpc open the amazon ec2 console at . in the navigation pane, choose instances.  select the running ec2-classic instance, choose actions, classiclink, link to vpc. you can select more than one instance to link to the same vpc. in the dialog box that displays, select a vpc from the list. only vpcs that have been enabled for classiclink are displayed. select one or more of the vpc security groups to associate with your instance. when you are done, choose link to vpc. you can use the launch wizard in the amazon ec2 console to launch an ec2-classic instance and immediately link it to a classiclink-enabled vpc. to link an instance to a vpc at launch open the amazon ec2 console at . from the amazon ec2 dashboard, choose launch instance. select an ami, and then choose an instance type. on the configure instance details page, ensure that you select launch into ec2-classic from the network list. notesome instance types, such as t2 instance types, can only be launched into a vpc. ensure that you select an instance type that can be launched into ec2-classic. in the link to vpc (classiclink) section, select a vpc from link to vpc. only classiclink-enabled vpcs are displayed. select the security groups from the vpc to associate with the instance. complete the other configuration options on the page, and then complete the rest of the steps in the wizard to launch your instance. for more information about using the launch wizard, see . you can view all of your classiclink-enabled vpcs in the amazon vpc console, and your linked ec2-classic instances in the amazon ec2 console. to view your classiclink-enabled vpcs open the amazon vpc console at . in the navigation pane, choose your vpcs. select a vpc, and in the summary tab, look for the classiclink field. a value of enabled indicates that the vpc is enabled for classiclink. alternatively, look for the classiclink column, and view the value that's displayed for each vpc (enabled or disabled). if the column is not visible, choose edit table columns (the gear-shaped icon), select the classiclink attribute, and then choose close.  to view your linked ec2-classic instances open the amazon ec2 console at . in the navigation pane, choose instances. select an ec2-classic instance, and in the description tab, look for the classiclink field. if the instance is linked to a vpc, the field displays the id of the vpc to which the instance is linked. if the instance is not linked to any vpc, the field displays unlinked. alternatively, you can filter your instances to display only linked ec2-classic instances for a specific vpc or security group. in the search bar, start typing , select the relevant classiclink resource attribute, and then select the security group id or the vpc id. you can enable classiclink dns support for your vpc so that dns hostnames that are addressed between linked ec2-classic instances and instances in the vpc resolve to private ip addresses and not public ip addresses. for this feature to work, your vpc must be enabled for dns hostnames and dns resolution. noteif you enable classiclink dns support for your vpc, your linked ec2-classic instance can access any private hosted zone associated with the vpc. for more information, see  in the amazon route 53 developer guide.  to enable classiclink dns support open the amazon vpc console at . in the navigation pane, choose your vpcs. select your vpc, and choose actions, edit classiclink dns support. for classiclink dns support, choose enable, and then choose save. you can disable classiclink dns support for your vpc so that dns hostnames that are addressed between linked ec2-classic instances and instances in the vpc resolve to public ip addresses and not private ip addresses. to disable classiclink dns support open the amazon vpc console at . in the navigation pane, choose your vpcs. select your vpc, and choose actions, edit classiclink dns support. for classiclink dns support, clear the enable check box, and then choose save. if you no longer require a classiclink connection between your ec2-classic instance and your vpc, you can unlink the instance from the vpc. unlinking the instance disassociates the vpc security groups from the instance. notea stopped instance is automatically unlinked from a vpc. to unlink an instance from a vpc open the amazon ec2 console at . in the navigation pane, choose instances, and select your instance. in the actions list, select classiclink, unlink instance. you can select more than one instance to unlink from the same vpc.  choose yes in the confirmation dialog box. if you no longer require a connection between ec2-classic instances and your vpc, you can disable classiclink on the vpc. you must first unlink all linked ec2-classic instances that are linked to the vpc. to disable classiclink for a vpc open the amazon vpc console at . in the navigation pane, choose your vpcs. select your vpc, then choose actions, disable classiclink. in the confirmation dialog box, choose yes, disable. you can enable a vpc for classiclink and then link an ec2-classic instance to the vpc. you can also view your classiclink-enabled vpcs, and all of your ec2-classic instances that are linked to a vpc. you can create policies with resource-level permission for the , , , and  actions to control how users are able to use those actions. resource-level permissions are not supported for  actions. topics the following policy grants users permissions to view classiclink-enabled vpcs and linked ec2-classic instances, to enable and disable a vpc for classiclink, and to link and unlink instances from a classiclink-enabled vpc.  the following policy allows user to enable and disable vpcs for classiclink that have the specific tag ''. users cannot enable or disable any other vpcs for classiclink.  the following policy grants users permissions to link instances to a vpc only if the instance is an  instance type. the second statement allows users to use the vpc and security group resources, which are required to link an instance to a vpc. the following policy grants users permissions to link instances to a specific vpc () only, and to associate only specific security groups from the vpc to the instance ( and ). users cannot link an instance to any other vpc, and they cannot specify any other of the vpc security groups to associate with the instance in the request. the following grants users permission to unlink any linked ec2-classic instance from a vpc, but only if the instance has the tag "". the second statement grants users permissions to use the vpc resource, which is required to unlink an instance from a vpc. in this example, you have an application with three instances: a public-facing web server, an application server, and a database server. your web server accepts https traffic from the internet, and then communicates with your application server over tcp port 6001. your application server then communicates with your database server over tcp port 6004. you're in the process of migrating your entire application to a vpc in your account. you've already migrated your application server and your database server to your vpc. your web server is still in ec2-classic and linked to your vpc via classiclink.  you want a security group configuration that allows traffic to flow only between these instances. you have four security groups: two for your web server ( and ), one for your application server (), and one for your database server (). the following diagram displays the architecture of your instances, and their security group configuration.  security groups for your web server ( and )you have one security group in ec2-classic, and the other in your vpc. you associated the vpc security group with your web server instance when you linked the instance to your vpc via classiclink. the vpc security group enables you to control the outbound traffic from your web server to your application server. the following are the security group rules for the ec2-classic security group (). |  |  | inbound |  | --- | |  source  |  type  |  port range  |  comments  |  |  0.0.0.0/0  |  https  |  443  |  allows internet traffic to reach your web server.  |  the following are the security group rules for the vpc security group (). |  |  | outbound |  | --- | |  destination  |  type  |  port range  |  comments  |  |  sg-3c3c3c3c  |  tcp  |  6001  |  allows outbound traffic from your web server to your application server in your vpc (or to any other instance associated with ).  |  security group for your application server ()the following are the security group rules for the vpc security group that's associated with your application server. |  |  | inbound |  | --- | |  source  |  type  |  port range  |  comments  |  |  sg-2b2b2b2b  |  tcp  |  6001  |  allows the specified type of traffic from your web server (or any other instance associated with ) to reach your application server.  |  | outbound |  | --- | |  destination  |  type  |  port range  |  comments  |  | sg-4d4d4d4d | tcp | 6004 | allows outbound traffic from the application server to the database server (or to any other instance associated with sg-4d4d4d4d). |  security group for your database server ()the following are the security group rules for the vpc security group that's associated with your database server. |  |  | inbound |  | --- | |  source  |  type  |  port range  |  comments  |  |  sg-3c3c3c3c  |  tcp  |  6004  |  allows the specified type of traffic from your application server (or any other instance associated with ) to reach your database server.  |  
after you attach an amazon ebs volume to your instance, it is exposed as a block device. you can format the volume with any file system and then mount it. after you make the ebs volume available for use, you can access it in the same ways that you access any other volume. any data written to this file system is written to the ebs volume and is transparent to applications using the device. you can take snapshots of your ebs volume for backup purposes or to use as a baseline when you create another volume. for more information, see . you can get directions for volumes on a windows instance from  in the amazon ec2 user guide for windows instances. suppose that you have an ec2 instance with an ebs volume for the root device, , and that you have just attached an empty ebs volume to the instance using . use the following procedure to make the newly attached volume available for use. to format and mount an ebs volume on linux connect to your instance using ssh. for more information, see . the device could be attached to the instance with a different device name than you specified in the block device mapping. for more information, see . use the lsblk command to view your available disk devices and their mount points (if applicable) to help you determine the correct device name to use. the output of lsblk removes the  prefix from full device paths. the following is example output for an instance built on the , which exposes ebs volumes as nvme block devices. the root device is . the attached volume is , which is not yet mounted. the following is example output for a t2 instance. the root device is . the attached volume is , which is not yet mounted. determine whether there is a file system on the volume. new volumes are raw block devices, and you must create a file system on them before you can mount and use them. volumes that were created from snapshots likely have a file system on them already; if you create a new file system on top of an existing file system, the operation overwrites your data. use the file -s command to get information about a device, such as its file system type. if the output shows simply , as in the following example output, there is no file system on the device and you must create one. if the device has a file system, the command shows information about the file system type. for example, the following output shows a root device with the xfs file system. (conditional) if you discovered that there is a file system on the device in the previous step, skip this step. if you have an empty volume, use the mkfs -t command to create a file system on the volume. warningdo not use this command if you're mounting a volume that already has data on it (for example, a volume that was created from a snapshot). otherwise, you'll format the volume and delete the existing data. if you get an error that  is not found, use the following command to install the xfs tools and then repeat the previous command: use the mkdir command to create a mount point directory for the volume. the mount point is where the volume is located in the file system tree and where you read and write files to after you mount the volume. the following example creates a directory named . use the following command to mount the volume at the directory you created in the previous step. review the file permissions of your new volume mount to make sure that your users and applications can write to the volume. for more information about file permissions, see  at the linux documentation project. the mount point is not automatically preserved after rebooting your instance. to automatically mount this ebs volume after reboot, see . to mount an attached ebs volume on every system reboot, add an entry for the device to the  file. you can use the device name, such as , in , but we recommend using the device's 128-bit universally unique identifier (uuid) instead. device names can change, but the uuid persists throughout the life of the partition. by using the uuid, you reduce the chances that the system becomes unbootable after a hardware reconfiguration. for more information, see . to mount an attached volume automatically after reboot (optional) create a backup of your  file that you can use if you accidentally destroy or delete this file while editing it. use the blkid command to find the uuid of the device. for ubuntu 18.04 use the lsblk command. open the  file using any text editor, such as nano or vim. add the following entry to  to mount the device at the specified mount point. the fields are the uuid value returned by blkid (or lsblk for ubuntu 18.04), the mount point, the file system, and the recommended file system mount options. for more information, see the manual page for fstab (run man fstab). noteif you ever boot your instance without this volume attached (for example, after moving the volume to another instance), the  mount option enables the instance to boot even if there are errors mounting the volume. debian derivatives, including ubuntu versions earlier than 16.04, must also add the  mount option. to verify that your entry works, run the following commands to unmount the device and then mount all file systems in . if there are no errors, the  file is ok and your file system will mount automatically after it is rebooted. if you receive an error message, address the errors in the file. warningerrors in the  file can render a system unbootable. do not shut down a system that has errors in the  file. if you are unsure how to correct errors in  and you created a backup file in the first step of this procedure, you can restore from your backup file using the following command. 
with on-demand instances, you pay for compute capacity by the second with no long-term commitments. you have full control over its lifecycle—you decide when to launch, stop, hibernate, start, reboot, or terminate it. there is no long-term commitment required when you purchase on-demand instances. you pay only for the seconds that your on-demand instances are in the  state. the price per second for a running on-demand instance is fixed, and is listed on the . we recommend that you use on-demand instances for applications with short-term, irregular workloads that cannot be interrupted. for significant savings over on-demand instances, use , , or . contents you can work with on-demand instances in the following ways: if you're new to amazon ec2, see . there is a limit on the number of running on-demand instances per aws account per region. on-demand instance limits are managed in terms of the number of virtual central processing units (vcpus) that your running on-demand instances are using, regardless of the instance type. there are five on-demand instance limits, listed in the following table. each limit specifies the vcpu limit for one or more instance families. for information about the different instance families, generations, and sizes, see . notenew aws accounts may start with limits that are lower than the limits described here. with vcpu limits, you can use your limit in terms of the number of vcpus required to launch any combination of instance types that meet your changing application needs. for example, with a standard instance limit of 256 vcpus, you could launch 32  instances (32 x 8 vcpus) or 16  instances (16 x 16 vcpus), or a combination of any standard instance types and sizes that total 256 vcpus. for more information, see . you can use the vcpu limits calculator to determine the number of vcpus that you require for your application needs. when using the calculator, keep the following in mind: the calculator assumes that you have reached your current limit. the value that you enter for instance count is the number of instances that you need to launch in addition to what is permitted by your current limit. the calculator adds your current limit to the instance count to arrive at a new limit. the following screenshot shows the vcpu limits calculator.  you can view and use the following controls and information: instance type – the instance types that you add to the vcpu limits calculator.instance count – the number of instances that you require for the selected instance type.vcpu count – the number of vcpus that corresponds to the instance count.current limit – your current limit for the limit type to which the instance type belongs. the limit applies to all instance types of the same limit type. for example, in the preceding screenshot, the current limit for  and  is 1,920 vcpus, which is the limit for all the instance types that belong to the all standard instances limit.new limit – the new limit, in number of vcpus, which is calculated by adding vcpu count and current limit.x – choose the x to remove the row.add instance type – choose add instance type to add another instance type to the calculator.limits calculation – displays the current limit, vcpus needed, and new limit for the limit types.instance limit name – the limit type for the instance types that you selected.current limit – the current limit for the limit type.vcpus needed – the number of vcpus that corresponds to the number of instances that you specified in instance count. for the all standard instances limit type, the vcpus needed is calculated by adding the values for vcpu count for all the instance types of this limit type.new limit – the new limit is calculated by adding current limit and vcpus needed.options – choose request limit increase to request a limit increase for the corresponding limit type.to calculate the number of required vcpus open the amazon ec2 console at . from the navigation bar, select a region. from the left navigator, choose limits. choose calculate vcpu limit. choose add instance type, choose the required instance type, and specify the required number of instances. to add more instance types, choose add instance type again. view limits calculation for the required new limit. when you've finished using the calculator, choose close. you can request a limit increase for each on-demand instance limit type from the  or the vcpu limits calculator in the amazon ec2 console. complete the required fields on the aws support center  with your use case. for primary instance type, select the limit type that corresponds to the instance limit name in the vcpu limits calculator. for the new limit value, use the value that appears in the new limit column in the vcpu limits calculator. for more information about requesting a limit increase, see . you can view and manage your on-demand instance limits from the  in the amazon ec2 console, from the amazon ec2  in the service quotas console, and from the  in the aws trusted advisor console. for more information, see  in the amazon ec2 user guide for linux instances,  in the service quotas user guide, and . with amazon cloudwatch metrics integration, you can monitor ec2 usage against limits. you can also configure alarms to warn about approaching limits. for more information, see  in the service quotas user guide. you can use the price list service api or the aws price list api to query the prices of on-demand instances. for more information, see  in the aws billing and cost management user guide. 
the steps for launching, monitoring, and modifying these instances are similar. the key difference is the default credit specification when they launch. if you do not change the default credit specification, the defaults are: t3 and t3a instances launch as  by default.t2 instances launch as  by default.topics t3 and t3a instances launch as  by default. t2 instances launch as  by default. for more information about ami and driver requirements for these instances, see . you must launch your instances using an amazon ebs volume as the root device. for more information, see . you can launch your instances as  or  using the amazon ec2 console, an aws sdk, a command line tool, or with an auto scaling group. for more information, see . to launch a burstable performance instance as unlimited or standard (console) follow the  procedure. on the choose an instance type page, select an instance type, and choose next: configure instance details. choose a credit specification. the default for t3 and t3a is , and for t2 it is . to launch a t3 or t3a instance as , on the configure instance details page, for t2/t3 unlimited, clear enable. to launch a t2 instance as , on the configure instance details page, for t2/t3 unlimited, select enable. continue as prompted by the wizard. when you've finished reviewing your options on the review instance launch page, choose launch. for more information, see . to launch a burstable performance instance as unlimited or standard (aws cli)use the  command to launch your instances. specify the credit specification using the  parameter. valid credit specifications are  and . for t3 and t3a, if you do not include the  parameter, the instance launches as  by default.for t2, if you do not include the  parameter, the instance launches as  by default.when burstable performance instances are launched or started, they require cpu credits for a good bootstrapping experience. if you use an auto scaling group to launch your instances, we recommend that you configure your instances as . if you do, the instances use surplus credits when they are automatically launched or restarted by the auto scaling group. using surplus credits prevents performance restrictions. you must use a launch template for launching instances as  in an auto scaling group. a launch configuration does not support launching instances as . to create a launch template that launches instances as unlimited (console) follow the  procedure. in launch template contents, for instance type, choose a t3, t3a, or t2 instance size. to launch instances as  in an auto scaling group, in advanced details, for t2/t3 unlimited, choose enable. when you've finished defining the launch template parameters, choose create launch template. for more information, see  in the amazon ec2 auto scaling user guide. to create a launch template that launches instances as unlimited (aws cli)use the  command and specify  as the credit specification. for t3 and t3a, if you do not include the  value, the instance launches as  by default.for t2, if you do not include the  value, the instance launches as  by default.to associate the launch template with an auto scaling group, create the auto scaling group using the launch template, or add the launch template to an existing auto scaling group. to create an auto scaling group using a launch template (console) open the amazon ec2 console at . on the navigation bar at the top of the screen, select the same region that you used when you created the launch template. in the navigation pane, choose auto scaling groups, create auto scaling group. choose launch template, select your launch template, and then choose next step. complete the fields for the auto scaling group. when you've finished reviewing your configuration settings on the review page, choose create auto scaling group. for more information, see  in the amazon ec2 auto scaling user guide. to create an auto scaling group using a launch template (aws cli)use the  aws cli command and specify the  parameter.  to add a launch template to an existing auto scaling group (console) open the amazon ec2 console at . on the navigation bar at the top of the screen, select the same region that you used when you created the launch template. in the navigation pane, choose auto scaling groups. from the auto scaling group list, select an auto scaling group, and choose actions, edit. on the details tab, for launch template, choose a launch template, and then choose save. to add a launch template to an existing auto scaling group (aws cli)use the  aws cli command and specify the  parameter.  you can view the credit specification ( or ) of a running or stopped instance. to view the credit specification of a burstable instance (console) open the amazon ec2 console at . in the left navigation pane, choose instances and select the instance. choose description and view the t2/t3 unlimited field. if the value is , then your instance is configured as .if the value is , then your instance is configured as  .to describe the credit specification of a burstable performance instance (aws cli)use the  command. if you do not specify one or more instance ids, all instances with the credit specification of  are returned, as well as instances that were previously configured with the  credit specification. for example, if you resize a t3 instance to an m4 instance, while it is configured as , amazon ec2 returns the m4 instance. example   the following is example output:   you can switch the credit specification of a running or stopped instance at any time between  and . to modify the credit specification of a burstable performance instance (console) open the amazon ec2 console at . in the left navigation pane, choose instances and select the instance. to modify the credit specification for several instances at one time, select all applicable instances. choose actions, instance settings, change t2/t3 unlimited. notethe change t2/t3 unlimited option is enabled only if you select a t3, t3a, or t2 instance. to change the credit specification to , choose enable. to change the credit specification to , choose disable. the current credit specification for the instance appears in parentheses after the instance id. to modify the credit specification of a burstable performance instance (aws cli)use the  command. specify the instance and its credit specification using the  parameter. valid credit specifications are  and . example   the following is example output:   you can set the default credit specification at the account level per aws region. you specify the default credit specification per instance family (t2, t3, or t3a). if you use the launch instance wizard in the aws management console to launch instances, the value for t2/t3 unlimited overrides the account-level default credit specification. if you use the aws cli to launch instances, all new burstable performance instances in the account launch using the default credit option. the credit specification for existing running or stopped instances is not affected. the  api is an asynchronous operation, which works at an aws region level and modifies the credit option for each availability zone. all zones in a region are updated within five minutes. but if instances are launched during this operation, they might not get the new credit option until the zone is updated. to verify whether the update has occurred, you can call  and check the default credit specification for updates. for more information, see . notethe default credit specification for an instance family can be modified only once in a rolling 5-minute period, and up to four times in a rolling 24-hour period. to set the default credit specification at the account level (aws cli)use the  command. specify the aws region, instance family, and the default credit specification using the  parameter. valid default credit specifications are  and . you can view the default credit specification of a burstable performance instance family at the account level per aws region. to view the default credit specification at the account level (aws cli)use the  command. specify the aws region and instance family. 
you can share an ami with specific aws accounts without making the ami public. all you need is the aws account ids. you can only share amis that have unencrypted volumes and volumes that are encrypted with a customer managed cmk. if you share an ami with encrypted volumes, you must also share any cmks used to encrypt them. for more information, see . you cannot share an ami that has volumes that are encrypted with a aws managed cmk. amis are a regional resource. therefore, sharing an ami makes it available in that region. to make an ami available in a different region, copy the ami to the region and then share it. for more information, see . there is no limit to the number of aws accounts with which an ami can be shared. to grant explicit launch permissions using the console open the amazon ec2 console at . in the navigation pane, choose amis. select your ami in the list, and then choose actions, modify image permissions. specify the aws account number of the user with whom you want to share the ami in the aws account number field, then choose add permission. to share this ami with multiple users, repeat this step until you have added all the required users.  to allow create volume permissions for snapshots, select  add "create volume" permissions to the following associated snapshots when creating permissions.  noteyou do not need to share the amazon ebs snapshots that an ami references in order to share the ami. only the ami itself needs to be shared; the system automatically provides the instance access to the referenced amazon ebs snapshots for the launch. however, you do need to share any cmks used to encrypt snapshots that the ami references. for more information, see . choose save when you are done. (optional) to view the aws account ids with which you have shared the ami, select the ami in the list, and choose the permissions tab. to find amis that are shared with you, see . use the  command (aws cli) to share an ami as shown in the following examples. to grant explicit launch permissionsthe following command grants launch permissions for the specified ami to the specified aws account. the following command grants create volume permission for a snapshot. noteyou do not need to share the amazon ebs snapshots that an ami references in order to share the ami. only the ami itself needs to be shared; the system automatically provides the instance access to the referenced amazon ebs snapshots for the launch. however, you do need to share any cmks used to encrypt snapshots that the ami references. for more information, see . to remove launch permissions for an accountthe following command removes launch permissions for the specified ami from the specified aws account: the following command removes create volume permission for a snapshot. to remove all launch permissionsthe following command removes all public and explicit launch permissions from the specified ami. note that the owner of the ami always has launch permissions and is therefore unaffected by this command. 
use this tutorial to get started with amazon elastic compute cloud (amazon ec2). you'll learn how to launch, connect to, and use a linux instance. an instance is a virtual server in the aws cloud. with amazon ec2, you can set up and configure the operating system and applications that run on your instance. to get started with a windows instance, see . when you sign up for aws, you can get started with amazon ec2 using the . if you created your aws account less than 12 months ago, and have not already exceeded the free tier benefits for amazon ec2, it will not cost you anything to complete this tutorial, because we help you select options that are within the free tier benefits. otherwise, you'll incur the standard amazon ec2 usage fees from the time that you launch the instance until you terminate the instance (which is the final task of this tutorial), even if it remains idle. topics the instance is an amazon ebs-backed instance (meaning that the root volume is an ebs volume). you can either specify the availability zone in which your instance runs, or let amazon ec2 select an availability zone for you. when you launch your instance, you secure it by specifying a key pair and security group. when you connect to your instance, you must specify the private key of the key pair that you specified when launching your instance.  tasks to complete this tutorial, perform the following tasks:    related tutorials if you'd prefer to launch a windows instance, see this tutorial in the amazon ec2 user guide for windows instances: .if you'd prefer to use the command line, see this tutorial in the aws command line interface user guide: .before you begin, be sure that you've completed the steps in . you can launch a linux instance using the aws management console as described in the following procedure. this tutorial is intended to help you launch your first instance quickly, so it doesn't cover all possible options. for more information about the advanced options, see . to launch an instance open the amazon ec2 console at . from the console dashboard, choose launch instance. the choose an amazon machine image (ami) page displays a list of basic configurations, called amazon machine images (amis), that serve as templates for your instance. select an hvm version of amazon linux 2. notice that these amis are marked "free tier eligible." on the choose an instance type page, you can select the hardware configuration of your instance. select the  instance type, which is selected by default. the  instance type is eligible for the free tier. in regions where  is unavailable, you can use a  instance under the free tier. for more information, see . choose review and launch to let the wizard complete the other configuration settings for you. on the review instance launch page, under security groups, you'll see that the wizard created and selected a security group for you. you can use this security group, or alternatively you can select the security group that you created when getting set up using the following steps: choose edit security groups. on the configure security group page, ensure that select an existing security group is selected. select your security group from the list of existing security groups, and then choose review and launch. on the review instance launch page, choose launch. when prompted for a key pair, select choose an existing key pair, then select the key pair that you created when getting set up. warningdon't select proceed without a key pair. if you launch your instance without a key pair, then you can't connect to it. when you are ready, select the acknowledgement check box, and then choose launch instances. a confirmation page lets you know that your instance is launching. choose view instances to close the confirmation page and return to the console. on the instances screen, you can view the status of the launch. it takes a short time for an instance to launch. when you launch an instance, its initial state is . after the instance starts, its state changes to  and it receives a public dns name. (if the public dns (ipv4) column is hidden, choose show/hide columns (the gear-shaped icon) in the top right corner of the page and then select public dns (ipv4).) it can take a few minutes for the instance to be ready so that you can connect to it. check that your instance has passed its status checks; you can view this information in the status checks column. there are several ways to connect to your linux instance. for more information, see . importantyou can't connect to your instance unless you launched it with a key pair for which you have the  file and you launched it with a security group that allows ssh access from your computer. if you can't connect to your instance, see  for assistance. after you've finished with the instance that you created for this tutorial, you should clean up by terminating the instance. if you want to do more with this instance before you clean up, see . importantterminating an instance effectively deletes it; you can't reconnect to an instance after you've terminated it. if you launched an instance that is not within the , you'll stop incurring charges for that instance as soon as the instance status changes to  or . if you'd like to keep your instance for later, but not incur charges, you can stop the instance now and then start it again later. for more information, see . to terminate your instance in the navigation pane, choose instances. in the list of instances, select the instance. choose actions, instance state, terminate. choose yes, terminate when prompted for confirmation. amazon ec2 shuts down and terminates your instance. after your instance is terminated, it remains visible on the console for a short while, and then the entry is deleted. after you start your instance, you might want to try some of the following exercises: learn how to remotely manage your ec2 instance using run command. for more information, see  in the aws systems manager user guide.configure a cloudwatch alarm to notify you if your usage exceeds the free tier. for more information, see  in the aws billing and cost management user guide.add an ebs volume. for more information, see  and .install the lamp stack. for more information, see .
if you require high processing capability, you'll benefit from using accelerated computing instances, which provide access to hardware-based compute accelerators such as graphics processing units (gpus) or field programmable gate arrays (fpgas). accelerated computing instances enable more parallelism for higher throughput on compute-intensive workloads. gpu-based instances provide access to nvidia gpus with thousands of compute cores. you can use gpu-based accelerated computing instances to accelerate scientific, engineering, and rendering applications by leveraging the cuda or open computing language (opencl) parallel computing frameworks. you can also use them for graphics applications, including game streaming, 3-d application streaming, and other graphics workloads. fpga-based instances provide access to large fpgas with millions of parallel system logic cells. you can use fpga-based accelerated computing instances to accelerate workloads such as genomics, financial analysis, real-time video processing, big data analysis, and security workloads by leveraging custom hardware accelerations. you can develop these accelerations using hardware description languages such as verilog or vhdl, or by using higher-level languages such as opencl parallel computing frameworks. you can either develop your own hardware acceleration code or purchase hardware accelerations through the . importantfpga-based instances do not support microsoft windows. you can cluster accelerated computing instances into a cluster placement group. cluster placement groups provide low latency and high-bandwidth connectivity between the instances within a single availability zone. for more information, see . topics for information about windows accelerated computing instances, see  in the amazon ec2 user guide for windows instances. accelerated computing instance families use hardware accelerators, or co-processors, to perform some functions, such as floating point number calculations, graphics processing, or data pattern matching, more efficiently than is possible in software running on cpus. the following accelerated computing instance families are available for you to launch in amazon ec2. f1 instancesf1 instances use xilinx ultrascale+ vu9p fpgas and are designed to accelerate computationally intensive algorithms, such as data-flow or highly parallel operations not suited to general purpose cpus. each fpga in an f1 instance contains approximately 2.5 million logic elements and approximately 6,800 digital signal processing (dsp) engines, along with 64 gib of local ddr ecc protected memory, connected to the instance by a dedicated pcie gen3 x16 connection. f1 instances provide local nvme ssd volumes. developers can use the fpga developer ami and aws hardware developer kit to create custom hardware accelerations for use on f1 instances. the fpga developer ami includes development tools for full-cycle fpga development in the cloud. using these tools, developers can create and share amazon fpga images (afis) that can be loaded onto the fpga of an f1 instance. for more information, see . inf1 instancesinf1 instances use aws inferentia machine learning inference chips. inferentia was developed to enable highly cost-effective low latency inference performance at any scale. developers can use the aws deep learning ami which comes with prepackaged and optimized software for aws inferentia. for more information, see .  p3 instancesp3 instances use nvidia tesla v100 gpus and are designed for general purpose gpu computing using the cuda or opencl programming models or through a machine learning framework. p3 instances provide high-bandwidth networking, powerful half, single, and double-precision floating-point capabilities, and up to 32 gib of memory per gpu, which makes them ideal for deep learning, computational fluid dynamics, computational finance, seismic analysis, molecular modeling, genomics, rendering, and other server-side gpu compute workloads. tesla v100 gpus do not support graphics mode. for more information, see . p3 instances support nvidia nvlink peer to peer transfers. to view topology information about the system, run the following command: for more information, see .  p2 instancesp2 instances use nvidia tesla k80 gpus and are designed for general purpose gpu computing using the cuda or opencl programming models. p2 instances provide high-bandwidth networking, powerful single and double precision floating-point capabilities, and 12 gib of memory per gpu, which makes them ideal for deep learning, graph databases, high-performance databases, computational fluid dynamics, computational finance, seismic analysis, molecular modeling, genomics, rendering, and other server-side gpu compute workloads. p2 instances support nvidia gpudirect peer to peer transfers. to view topology information about the system, run the following command: for more information, see .  g4 instancesg4 instances use nvidia tesla gpus and provide a cost-effective, high-performance platform for general purpose gpu computing using the cuda or machine learning frameworks along with graphics applications using directx or opengl. g4 instances provide high- bandwidth networking, powerful half and single-precision floating-point capabilities, along with int8 and int4 precisions. each gpu has 16 gib of gddr6 memory, making g4 instances well-suited for machine learning inference, video transcoding, and graphics applications like remote graphics workstations and game streaming in the cloud. g4 instances support nvidia grid virtual workstation. for more information, see .  g3 instancesg3 instances use nvidia tesla m60 gpus and provide a cost-effective, high-performance platform for graphics applications using directx or opengl. g3 instances also provide nvidia grid virtual workstation features, such as support for four monitors with resolutions up to 4096x2160, and nvidia grid virtual applications. g3 instances are well-suited for applications such as 3d visualizations, graphics-intensive remote workstations, 3d rendering, video encoding, virtual reality, and other server-side graphics workloads requiring massively parallel processing power.  g3 instances support nvidia grid virtual workstation and nvidia grid virtual applications. to activate either of these features, see .  g2 instancesg2 instances use nvidia grid k520 gpus and provide a cost-effective, high-performance platform for graphics applications using directx or opengl. nvidia grid gpus also support nvidia’s fast capture and encode api operations. example applications include video creation services, 3d visualizations, streaming graphics-intensive applications, and other server-side graphics workloads. the following is a summary of the hardware specifications for accelerated computing instances. for more information about the hardware specifications for each amazon ec2 instance type, see . for more information about specifying cpu options, see . there are several gpu setting optimizations that you can perform to achieve the best performance on your instances. for more information, see . ebs-optimized instances enable you to get consistently high performance for your ebs volumes by eliminating contention between amazon ebs i/o and other network traffic from your instance. some accelerated computing instances are ebs-optimized by default at no additional cost. for more information, see . some accelerated computing instance types provide the ability to control processor c-states and p-states on linux. c-states control the sleep levels that a core can enter when it is inactive, while p-states control the desired performance (in cpu frequency) from a core. for more information, see . you can enable enhanced networking on supported instance types to provide lower latencies, lower network jitter, and higher packet-per-second (pps) performance. most applications do not consistently need a high level of network performance, but can benefit from access to increased bandwidth when they send or receive data. for more information, see . the following is a summary of network performance for accelerated computing instances that support enhanced networking. † these instances use a network i/o credit mechanism to allocate network bandwidth to instances based on average bandwidth utilization. they accrue credits when their bandwidth is below their baseline bandwidth, and can use these credits when they perform network data transfers. for more information, open a support case and ask about baseline bandwidth for the specific instance types that you are interested in. the following is a summary of features for accelerated computing instances. * the root device volume must be an amazon ebs volume. for more information, see the following: you must launch the instance using an hvm ami.instances built on the  have the following requirements:  must be installed must be installedthe following linux amis meet these requirements: amazon linux 2amazon linux ami 2018.03ubuntu 14.04 (with  kernel) or laterred hat enterprise linux 7.4 or latersuse linux enterprise server 12 sp2 or latercentos 7.4.1708 or laterfreebsd 11.1 or laterdebian gnu/linux 9 or latergpu-based instances can't access the gpu unless the nvidia drivers are installed. for more information, see .launching a bare metal instance boots the underlying server, which includes verifying all hardware and firmware components. this means that it can take 20 minutes from the time the instance enters the running state until it becomes available over the network.to attach or detach ebs volumes or secondary network interfaces from a bare metal instance requires pcie native hotplug support. amazon linux 2 and the latest versions of the amazon linux ami support pcie native hotplug, but earlier versions do not. you must enable the following linux kernel configuration options: bare metal instances use a pci-based serial device rather than an i/o port-based serial device. the upstream linux kernel and the latest amazon linux amis support this device. bare metal instances also provide an acpi spcr table to enable the system to automatically use the pci-based serial device. the latest windows amis automatically use the pci-based serial device.there is a limit of 100 afis per region.there is a limit on the total number of instances that you can launch in a region, and there are additional limits on some instance types. for more information, see  in the amazon ec2 faq.
you can use amazon data lifecycle manager to automate the creation, retention, and deletion of snapshots taken to back up your amazon ebs volumes. automating snapshot management helps you to: protect valuable data by enforcing a regular backup schedule.retain backups as required by auditors or internal compliance.reduce storage costs by deleting outdated backups.combined with the monitoring features of amazon cloudwatch events and aws cloudtrail, amazon data lifecycle manager provides a complete backup solution for ebs volumes at no additional cost. topics the following are the key elements of amazon data lifecycle manager. topics snapshots are the primary means to back up data from your ebs volumes. to save storage costs, successive snapshots are incremental, containing only the volume data that changed since the previous snapshot. when you delete one snapshot in a series of snapshots for a volume, only the data unique to that snapshot is removed. the rest of the captured history of the volume is preserved. for more information, see . amazon data lifecycle manager uses resource tags to identify the ebs volumes to back up. tags are customizable metadata that you can assign to your aws resources (including ebs volumes and snapshots). an amazon data lifecycle manager policy (described below) targets a volume for backup using a single tag. multiple tags can be assigned to a volume if you want to run multiple policies on it. you can't use a '\' or '=' character in a tag key. for more information, see . amazon data lifecycle manager applies the following tags to all snapshots created by a policy, to distinguish them from snapshots created by any other means: you can also specify custom tags to be applied to snapshots on creation. you can't use a '\' or '=' character in a tag key. the target tags that amazon data lifecycle manager uses to associate volumes with a policy can optionally be applied to snapshots created by the policy. a lifecycle policy consists of these core settings: resource type—the type of aws resource managed by the policy. use  to create snapshots of individual volumes or use  to create multi-volume snapshots from the volumes for an instance. for more information, see .target tags—the tags that must be associated with an ebs volume or an ec2 instance for it to be managed by the policy.schedule—the start time and interval for creating snapshots.retention—you can retain snapshots based on either the total count of snapshots or the age of each snapshot.for example, you could create a policy that manages all ebs volumes with the tag , creates snapshots every 24 hours at 0900, and retains the five most recent snapshots. snapshot creation would start by 0959 each day. your aws account has the following quotas related to amazon data lifecycle manager: you can create up to 100 lifecycle policies per region.you can add up to 45 tags per resource.you can create one schedule per lifecycle policy.the following considerations apply to lifecycle policies: a policy does not begin creating snapshots until you set its activation status to enabled. you can configure a policy to be enabled upon creation.the first snapshot is created by a policy within one hour after the specified start time. subsequent snapshots are created within one hour of their scheduled time.if you modify a policy by removing or changing its target tag, the ebs volumes with that tag are no longer affected by the policy.if you modify the schedule name for a policy, the snapshots created under the old schedule name are no longer affected by the policy.if you modify a retention schedule based on time to use a new time interval, the new interval is used only for new snapshots. the new schedule does not affect the retention schedule of existing snapshots created by this policy.you cannot change the retention schedule of a policy from the count of snapshots to the age of each snapshot. to make this change, you must create a new policy.if you disable a policy with a retention schedule based on the age of each snapshot, the snapshots whose retention periods expire while the policy is disabled are retained indefinitely. you must delete these snapshots manually. when you enable the policy again, amazon data lifecycle manager resumes deleting snapshots as their retention periods expire.if you delete the resource to which a policy with count-based retention applies, the policy no longer manages the previously created snapshots. you must manually delete the snapshots if they are no longer needed.if you delete the resource to which a policy with age-based retention applies, the policy continues to delete snapshots on the defined schedule, up to the last snapshot. you must manually delete the last snapshot if it is no longer needed.you can create multiple policies to back up an ebs volume or an ec2 instance. for example, if an ebs volume has two tags, where tag a is the target for policy a to create a snapshot every 12 hours, and tag b is the target for policy b to create a snapshot every 24 hours, amazon data lifecycle manager creates snapshots according to the schedules for both policies.the following considerations apply to lifecycle policies and : a snapshot that is enabled for fast snapshot restore remains enabled even if you delete or disable the lifecycle policy, disable fast snapshot restore for the lifecycle policy, or disable fast snapshot restore for the availability zone. you can disable fast snapshot restore for these snapshots manually.if you enable fast snapshot restore and you exceed the maximum number of snapshots that can be enabled for fast snapshot restore, amazon data lifecycle manager creates snapshots as scheduled but does not enable them for fast snapshot restore. after a snapshot that is enabled for fast snapshot restore is deleted, the next snapshot amazon data lifecycle manager creates is enabled for fast snapshot restore.when you enable fast snapshot restore for a snapshot, it takes 60 minutes per tib to optimize the snapshot. we recommend that you create a schedule that ensures that each snapshot is fully optimized before amazon data lifecycle manager creates the next snapshot.you are billed for each minute that fast snapshot restore is enabled for a snapshot in a particular availability zone. charges are pro-rated with a minimum of one hour. for more information, see . notedepending on the configuration of your lifecycle policies, you could have multiple snapshots enabled for fast snapshot restore simultaneously.the following considerations apply to lifecycle policies and  enabled volumes: when creating a lifecycle policy based on instance tags for multi-volume snapshots, amazon data lifecycle manager initiates a snapshot of the volume for each attached instance. use the timestamp tag to identify the set of time-consistent snapshots created from the attached instances.the following prerequisites are required by amazon data lifecycle manager. topics amazon data lifecycle manager uses an iam role to get the permissions that are required to manage snapshots on your behalf. amazon data lifecycle manager creates the awsdatalifecyclemanagerdefaultrole role the first time that you create a lifecycle policy using the aws management console. you can also create this role using the following  command. alternatively, you can create a custom iam role with the required permissions and select it when you create a lifecycle policy.  to create a custom iam role create a role with the following permissions. for more information, see  in the iam user guide. add a trust relationship to the role. in the iam console, choose roles. select the role you created and then choose trust relationships. choose edit trust relationship, add the following policy, and then choose update trust policy. an iam user must have the following permissions to use amazon data lifecycle manager. for more information, see  in the iam user guide. to copy an encrypted snapshot between regions, you must have access to both the source and destination customer master key (cmk) from aws key management service (aws kms).  if the source volume is encrypted, ensure that the awsdatalifecyclemanagerdefaultrole role has permission to use the cmk used to encrypt the volume. if you enable cross region copy and choose to encrypt the copied snapshot, ensure that the awsdatalifecyclemanagerdefaultrole role has permission to use the cmk needed to encrypt the snapshot in the destination region. for more information, see  in the aws key management service developer guide. the following examples show how to use amazon data lifecycle manager to manage the backups of your ebs volumes using the aws management console. topics use the following procedure to create a lifecycle policy. to create a lifecycle policy open the amazon ec2 console at . in the navigation pane, choose elastic block store, lifecycle manager, then choose create snapshot lifecycle policy. provide the following information for your policy as needed: description–a description of the policy.resource type–the type of resource to back up. use  to create snapshots of individual volumes or use  to create multi-volume snapshots from the volumes for an instance.target with these tags–the resource tags that identify the volumes or instances to back up.lifecycle policy tags–the tags for the lifecycle policy.schedule name–a name for the schedule.frequency–the interval between policy runs. you can configure policy runs on a daily, weekly, monthly, or yearly schedule. alternatively, choose custom cron expression to specify an interval of up to 1 year. for more information, see  in the amazon cloudwatch events user guide.starting at *hh:mm* utc–the time at which the policy runs are scheduled to start. the first policy run starts within an hour after the scheduled time.retention type–you can retain snapshots based on either the total count of snapshots or the age of each snapshot. for retention based on the count, the range is 1 to 1000. after the maximum count is reached, the oldest snapshot is deleted when a new one is created. for age-based retention, the range is 1 day to 100 years. after the retention period of each snapshot expires, it is deleted. the retention period should be greater than or equal to the creation interval.tagging information–choose whether to copy all user-defined tags on a source volume to the snapshots created by this policy. you can also specify additional tags for the snapshots in addition to the tags applied by amazon data lifecycle manager. if the resource type is , you can choose to automatically tag your snapshots with the following variable tags:  and . the values of the variable tags are determined when the tags are added.fast snapshot restore–choose whether to enable fast snapshot restore for all snapshots created by this policy. if you enable fast snapshot restore, you must choose the availability zones in which to enable it. you are billed for each minute that fast snapshot restore is enabled for a snapshot in a particular availability zone. charges are pro-rated with a minimum of one hour. you can also specify the maximum number of snapshots that can be enabled for fast snapshot restore.enable cross region copy–you can copy each snapshot to up at three additional regions. you must ensure that you do not exceed the number of concurrent snapshot copies per region. for each region, you can choose different retention policies and whether to copy all tags or no tags. if the source snapshot is encrypted or if encryption by default is enabled, the snapshots copies are encrypted. if the source snapshot is unencrypted, you can enable encryption. if you do not specify a cmk, the snapshots are encrypted using the default key for ebs encryption in each destination region. if you specify a cmk for the destination region, you must have access to the cmk.iam role–an iam role that has permissions to create, delete, and describe snapshots, and to describe volumes. aws provides a default role, awsdatalifecyclemanagerdefaultrole, or you can create a custom iam role.policy status after creation–choose enable policy to start the policy runs at the next scheduled time or disable policy to prevent the policy from running.choose create policy. use the following procedure to view a lifecycle policy. to view a lifecycle policy open the amazon ec2 console at . in the navigation pane, choose elastic block store, lifecycle manager. select a lifecycle policy from the list. the details tab displays information about the policy. use the following procedure to modify a lifecycle policy. to modify a lifecycle policy open the amazon ec2 console at . in the navigation pane, choose elastic block store, lifecycle manager. select a lifecycle policy from the list. choose actions, modify snapshot lifecycle policy. modify the policy settings as needed. for example, you can modify the schedule, add or remove tags, or enable or disable the policy. choose update policy. use the following procedure to delete a lifecycle policy. to delete a lifecycle policy open the amazon ec2 console at . in the navigation pane, choose elastic block store, lifecycle manager. select a lifecycle policy from the list. choose actions, delete snapshot lifecycle policy. when prompted for confirmation, choose delete snapshot lifecycle policy. the following examples show how to use amazon data lifecycle manager to manage the backups of your ebs volumes using the aws cli. topics use the  command to create a lifecycle policy. to simplify the syntax, the example uses a json file, , that includes the policy details. this example uses a resource type of  to create snapshots of all volumes with the specified target tags. to create snapshots of all volumes for all instances with the specified target tags, use a resource type of  instead. the following is an example of the  file. upon success, the command returns the id of the newly created policy. the following is example output. use the  command to display information about a lifecycle policy. the following is example output. it includes the information that you specified, plus metadata inserted by aws. use the  command to modify the information in a lifecycle policy. to simplify the syntax, this example references a json file, , that includes the policy details. the following is an example of the  file. to view the updated policy, use the  command. you can see that the state, the value of the tag, the snapshot interval, and the snapshot start time were changed. use the  command to delete a lifecycle policy and free up the target tags specified in the policy for reuse. the  provides descriptions and syntax for each of the actions and data types for the amazon data lifecycle manager query api. alternatively, you can use one of the aws sdks to access the api in a way that's tailored to the programming language or platform that you're using. for more information, see . you can use the following features to monitor the lifecycle of your snapshots. topics you can view your lifecycle policies using the amazon ec2 console or the aws cli. each snapshot created by a policy has a timestamp and policy-related tags. you can filter snapshots using tags to verify that your backups are being created as you intend. for information about viewing lifecycle policies using the console, see . for information about displaying information about lifecycle policies using the cli, see . amazon ebs and amazon data lifecycle manager emit events related to lifecycle policy actions. you can use aws lambda and amazon cloudwatch events to handle event notifications programmatically. for more information, see the . the following events are available: —an amazon ebs event emitted when a  action succeeds or fails. for more information, see .—a amazon data lifecycle manager event emitted when a lifecycle policy enters an error state. the event contains a description of what caused the error. the following is an example of an event when the permissions granted by the iam role are insufficient: the following is an example of an event when a limit is exceeded: with aws cloudtrail, you can track user activity and api usage to demonstrate compliance with internal policies and regulatory standards. for more information, see the . 
when you request spot instances, we recommend that you use the default maximum price (the on-demand price). if you want to specify a maximum price, we recommend that you review the spot price history before you do so. you can view the spot price history for the last 90 days, filtering by instance type, operating system, and availability zone. spot instance prices are set by amazon ec2 and adjust gradually based on long-term trends in supply and demand for spot instance capacity. for the current spot instance prices see . to view the spot price history (console) open the amazon ec2 console at . in the navigation pane, choose spot requests. if you are new to spot instances, you see a welcome page. choose get started, scroll to the bottom of the screen, and then choose cancel. choose pricing history.  choose the operating system (product), instance type, and date range for which to view the price history. move your pointer over the graph to display the prices at specific times in the selected date range. (optional) to review the spot price history for a specific availability zone, you can filter the availability zones by removing availability zones from the graph. to remove an availability zone from the graph, select the zone to remove it. you can also select a different product, instance type, or date range. to view the spot price history using the command line you can use one of the following commands. for more information, see .  (aws cli) (aws tools for windows powershell)
you can use the ami tools commands to create and manage instance store-backed linux amis. to set up the tools, see . for information about your access keys, see . topics describes the version of the ami tools. ec2-ami-tools-version the version information. this example command displays the version information for the ami tools that you're using. creates an instance store-backed linux ami from an operating system image created in a loopback file. ec2-bundle-image -c path -k path -u account -i path [-d path] [--ec2cert path] [-r architecture] [--productcodes code1,code2,...] [-b mapping] [-p prefix]   paththe user's pem encoded rsa public key certificate file.required: yes  paththe path to a pem-encoded rsa key file. you'll need to specify this key to unbundle this bundle, so keep it in a safe place. note that the key doesn't have to be registered to your aws account.required: yes  accountthe user's aws account id, without dashes.required: yes  paththe path to the image to bundle.required: yes  paththe directory in which to create the bundle.default: required: no  paththe path to the amazon ec2 x.509 public key certificate used to encrypt the image manifest.the  and  regions use a non-default public key certificate and the path to that certificate must be specified with this option. the path to the certificate varies based on the installation method of the ami tools. for amazon linux, the certificates are located at . if you installed the ami tools from the rpm or zip file in , the certificates are located at .required: only for the  and  regions.  architectureimage architecture. if you don't provide the architecture on the command line, you'll be prompted for it when bundling starts.valid values:  | required: no  code1,code2,...product codes to attach to the image at registration time, separated by commas.required: no  mappingdefines how block devices are exposed to an instance of this ami if its instance type supports the specified device.specify a comma-separated list of key-value pairs, where each key is a virtual name and each value is the corresponding device name. virtual names include the following:   —the root file system device, as seen by the instance—the root file system device, as seen by the kernel—the swap device, as seen by the instance—the nth instance store volume required: no prefixthe filename prefix for bundled ami files.default: the name of the image file. for example, if the image path is , then the default prefix is .required: no  kernel_iddeprecated. use  to set the kernel.required: no  ramdisk_iddeprecated. use  to set the ram disk if required.required: no status messages describing the stages and status of the bundling process. this example creates a bundled ami from an operating system image that was created in a loopback file. creates an instance store-backed linux ami by compressing, encrypting, and signing a copy of the root device volume for the instance. amazon ec2 attempts to inherit product codes, kernel settings, ram disk settings, and block device mappings from the instance. by default, the bundle process excludes files that might contain sensitive information. these files include , , , , , ,  , , , and . to include all of these files, use the  option. to include some of these files, use the  option. for more information, see . ec2-bundle-vol -c path -k path -u account [-d path] [--ec2cert path] [-r architecture] [--productcodes code1,code2,...] [-b mapping] [--all] [-e directory1,directory2,...] [-i file1,file2,...] [--no-filter] [-p prefix] [-s size] [--[no-]inherit] [-v volume] [-p type] [-s script] [--fstab path] [--generate-fstab] [--grub-config path]   paththe user's pem encoded rsa public key certificate file.required: yes  paththe path to the user's pem-encoded rsa key file.required: yes  accountthe user's aws account id, without dashes.required: yes  destinationthe directory in which to create the bundle.default: required: no  paththe path to the amazon ec2 x.509 public key certificate used to encrypt the image manifest.the  and  regions use a non-default public key certificate and the path to that certificate must be specified with this option. the path to the certificate varies based on the installation method of the ami tools. for amazon linux, the certificates are located at . if you installed the ami tools from the rpm or zip file in , the certificates are located at .required: only for the  and  regions.  architecturethe image architecture. if you don't provide this on the command line, you'll be prompted to provide it when the bundling starts.valid values:  | required: no  code1,code2,...product codes to attach to the image at registration time, separated by commas.required: no  mappingdefines how block devices are exposed to an instance of this ami if its instance type supports the specified device.specify a comma-separated list of key-value pairs, where each key is a virtual name and each value is the corresponding device name. virtual names include the following:   —the root file system device, as seen by the instance—the root file system device, as seen by the kernel—the swap device, as seen by the instance—the nth instance store volume required: nobundle all directories, including those on remotely mounted file systems.required: no  directory1,directory2,...a list of absolute directory paths and files to exclude from the bundle operation. this parameter overrides the  option. when exclude is specified, the directories and subdirectories listed with the parameter will not be bundled with the volume.required: no  file1,file2,...a list of files to include in the bundle operation. the specified files would otherwise be excluded from the ami because they might contain sensitive information.required: no if specified, we won't exclude files from the ami because they might contain sensitive information.required: no  prefixthe file name prefix for bundled ami files.default: required: no  sizethe size, in mb (1024 * 1024 bytes), of the image file to create. the maximum size is 10240 mb.default: 10240required: no indicates whether the image should inherit the instance's metadata (the default is to inherit). bundling fails if you enable  but the instance metadata is not accessible.required: no  volumethe absolute path to the mounted volume from which to create the bundle.default: the root directory (/)required: no  typeindicates whether the disk image should use a partition table. if you don't specify a partition table type, the default is the type used on the parent block device of the volume, if applicable, otherwise the default is .valid values:  |  | required: no  scripta customization script to be run right before bundling. the script must expect a single argument, the mount point of the volume.required: no  paththe path to the fstab to bundle into the image. if this is not specified, amazon ec2 bundles /etc/fstab.required: no bundles the volume using an amazon ec2-provided fstab.required: no the path to an alternate grub configuration file to bundle into the image. by default,  expects either  or  to exist on the cloned image. this option allows you to specify a path to an alternative grub configuration file, which will then be copied over the defaults (if present).required: no  kernel_iddeprecated. use  to set the kernel.required: no ramdisk_iddeprecated. use  to set the ram disk if required.required: no status messages describing the stages and status of the bundling. this example creates a bundled ami by compressing, encrypting and signing a snapshot of the local machine's root file system.  deletes the specified bundle from amazon s3 storage. after you delete a bundle, you can't launch instances from the corresponding ami. ec2-delete-bundle -b bucket -a access_key_id -s secret_access_key [-t token] [--url url] [--region region] [--sigv version] [-m path] [-p prefix] [--clear] [--retry] [-y]  bucketthe name of the amazon s3 bucket containing the bundled ami, followed by an optional '/'-delimited path prefixrequired: yes  access_key_idthe aws access key id.required: yes  secret_access_keythe aws secret access key.required: yes  tokenthe delegation token to pass along to the aws request. for more information, see the .required: only when you are using temporary security credentials.default: the value of the  environment variable (if set). regionthe region to use in the request signature.default: required: required if using signature version 4 versionthe signature version to use when signing the request.valid values:  | default: required: no paththe path to the manifest file.required: you must specify  or .  prefixthe bundled ami filename prefix. provide the entire prefix. for example, if the prefix is image.img, use  and not .required: you must specify  or . deletes the amazon s3 bucket if it's empty after deleting the specified bundle.required: no automatically retries on all amazon s3 errors, up to five times per operation.required: no automatically assumes the answer to all prompts is yes.required: no amazon ec2 displays status messages indicating the stages and status of the delete process. this example deletes a bundle from amazon s3. downloads the specified instance store-backed linux amis from amazon s3 storage. ec2-download-bundle -b bucket -a access_key_id -s secret_access_key -k path [--url url] [--region region] [--sigv version] [-m file] [-p prefix] [-d directory] [--retry]   bucketthe name of the amazon s3 bucket where the bundle is located, followed by an optional '/'-delimited path prefix.required: yes  access_key_idthe aws access key id.required: yes  secret_access_keythe aws secret access key.required: yes  paththe private key used to decrypt the manifest.required: yes  urlthe amazon s3 service url.default: required: no  regionthe region to use in the request signature.default: required: required if using signature version 4  versionthe signature version to use when signing the request.valid values:  | default: required: no  filethe name of the manifest file (without the path). we recommend that you specify either the manifest () or a prefix ().required: no  prefixthe filename prefix for the bundled ami files.default: required: no  directorythe directory where the downloaded bundle is saved. the directory must exist.default: the current working directory.required: no  automatically retries on all amazon s3 errors, up to five times per operation.required: no status messages indicating the various stages of the download process are displayed. this example creates the  directory (using the linux mkdir command) and downloads the bundle from the  amazon s3 bucket. modifies an instance store-backed linux ami (for example, its certificate, kernel, and ram disk) so that it supports a different region. ec2-migrate-manifest -c path -k path -m path {(-a access_key_id -s secret_access_key --region region) | (--no-mapping)} [--ec2cert ec2_cert_path] [--kernel kernel-id] [--ramdisk ramdisk_id]   paththe user's pem encoded rsa public key certificate file.required: yes  paththe path to the user's pem-encoded rsa key file.required: yes  paththe path to the manifest file.required: yes  access_key_idthe aws access key id.required: required if using automatic mapping.  secret_access_keythe aws secret access key.required: required if using automatic mapping.  regionthe region to look up in the mapping file.required: required if using automatic mapping. disables automatic mapping of kernels and ram disks. during migration, amazon ec2 replaces the kernel and ram disk in the manifest file with a kernel and ram disk designed for the destination region. unless the  parameter is given,  might use the  and  operations to perform automated mappings.required: required if you're not providing the , , and  options used for automatic mapping.  paththe path to the amazon ec2 x.509 public key certificate used to encrypt the image manifest.the  and  regions use a non-default public key certificate and the path to that certificate must be specified with this option. the path to the certificate varies based on the installation method of the ami tools. for amazon linux, the certificates are located at . if you installed the ami tools from the zip file in , the certificates are located at .required: only for the  and  regions.  kernel_idthe id of the kernel to select.we recommend that you use pv-grub instead of kernels and ram disks. for more information, see . required: no  ramdisk_idthe id of the ram disk to select.we recommend that you use pv-grub instead of kernels and ram disks. for more information, see . required: no status messages describing the stages and status of the bundling process. this example copies the ami specified in the  manifest from the us to the eu. re-creates the bundle from an instance store-backed linux ami. ec2-unbundle -k path -m path [-s source_directory] [-d destination_directory]   paththe path to your pem-encoded rsa key file.required: yes  paththe path to the manifest file.required: yes  source_directorythe directory containing the bundle.default: the current directory.required: no  destination_directorythe directory in which to unbundle the ami. the destination directory must exist.default: the current directory.required: no this linux and unix example unbundles the ami specified in the  file. status messages indicating the various stages of the unbundling process are displayed. uploads the bundle for an instance store-backed linux ami to amazon s3 and sets the appropriate acls on the uploaded objects. for more information, see . ec2-upload-bundle -b bucket -a access_key_id -s secret_access_key [-t token] -m path [--url url] [--region region] [--sigv version] [--acl acl] [-d directory] [--part part] [--retry] [--skipmanifest]   bucketthe name of the amazon s3 bucket in which to store the bundle, followed by an optional '/'-delimited path prefix. if the bucket doesn't exist, it's created if the bucket name is available.required: yes  access_key_idyour aws access key id.required: yes  secret_access_keyyour aws secret access key.required: yes  tokenthe delegation token to pass along to the aws request. for more information, see the .required: only when you are using temporary security credentials.default: the value of the  environment variable (if set).  paththe path to the manifest file. the manifest file is created during the bundling process and can be found in the directory containing the bundle.required: yes  urldeprecated. use the  option instead unless your bucket is constrained to the  location (and not ). the  flag is the only way to target that specific location restraint.the amazon s3 endpoint service url.default: required: no  regionthe region to use in the request signature for the destination s3 bucket.   if the bucket doesn't exist and you don't specify a region, the tool creates the bucket without a location constraint (in ).if the bucket doesn't exist and you specify a region, the tool creates the bucket in the specified region.if the bucket exists and you don't specify a region, the tool uses the bucket's location.if the bucket exists and you specify  as the region, the tool uses the bucket's actual location without any error message, any existing matching files are over-written.if the bucket exists and you specify a region (other than ) that doesn't match the bucket's actual location, the tool exits with an error. if your bucket is constrained to the  location (and not ), use the  flag instead. the  flag is the only way to target that specific location restraint.default: required: required if using signature version 4 versionthe signature version to use when signing the request.valid values:  | default: required: no  aclthe access control list policy of the bundled image.valid values:  | default: required: no  directorythe directory containing the bundled ami parts.default: the directory containing the manifest file (see the  option).required: no  partstarts uploading the specified part and all subsequent parts. for example, .required: no automatically retries on all amazon s3 errors, up to five times per operation.required: no does not upload the manifest.required: no  locationdeprecated. use the  option instead, unless your bucket is constrained to the  location (and not ). the  flag is the only way to target that specific location restraint.the location constraint of the destination amazon s3 bucket. if the bucket exists and you specify a location that doesn't match the bucket's actual location, the tool exits with an error. if the bucket exists and you don't specify a location, the tool uses the bucket's location. if the bucket doesn't exist and you specify a location, the tool creates the bucket in the specified location. if the bucket doesn't exist and you don't specify a location, the tool creates the bucket without a location constraint (in ).default: if  is specified, the location is set to that specified region. if  is not specified, the location defaults to .required: no amazon ec2 displays status messages that indicate the stages and status of the upload process. this example uploads the bundle specified by the  manifest. most of the ami tools accept the following optional parameters. displays the help message. displays the version and copyright notice. displays the manual entry. runs in batch mode, suppressing interactive prompts. displays information that can be useful when troubleshooting problems. 
if you have stopped your amazon ebs-backed instance and it appears stuck in the  state, there may be an issue with the underlying host computer. there is no cost for any instance usage while an instance is not in the  state. force the instance to stop using either the console or the aws cli. to force the instance to stop using the console, select the stuck instance, and choose actions, instance state, stop, and yes, forcefully stop.to force the instance to stop using the aws cli, use the  command and the  option as follows: if, after 10 minutes, the instance has not stopped, post a request for help in the . to help expedite a resolution, include the instance id, and describe the steps that you've already taken. alternatively, if you have a support plan, create a technical support case in the . to attempt to resolve the problem while you are waiting for assistance from the  or the , create a replacement instance. create an ami of the stuck instance, and launch a new instance using the new ami.  to create a replacement instance using the console open the amazon ec2 console at . in the navigation pane, choose instances and select the stuck instance. choose actions, image, create image. in the create image dialog box, fill in the following fields, and then choose create image: specify a name and description for the ami. choose no reboot. for more information, see . launch a new instance from the ami and verify that the new instance is working. select the stuck instance, and choose actions, instance state, terminate. if the instance also gets stuck terminating, amazon ec2 automatically forces it to terminate within a few hours. to create a replacement instance using the cli create an ami from the stuck instance using the  (aws cli) command and the  option as follows:. launch a new instance from the ami using the  (aws cli) command as follows: verify that the new instance is working. terminate the stuck instance using the  (aws cli) command as follows: if you are unable to create an ami from the instance as described in the previous procedures, you can set up a replacement instance as follows: (alternate) to create a replacement instance using the console select the instance and choose description, block devices. select each volume and write down its volume id. be sure to note which volume is the root volume. in the navigation pane, choose volumes. select each volume for the instance, and choose actions, create snapshot. in the navigation pane, choose snapshots. select the snapshot that you just created, and choose actions, create volume. launch an instance with the same operating system as the stuck instance. note the volume id and device name of its root volume. in the navigation pane, choose instances, select the instance that you just launched, choose actions, instance state, and then choose stop. in the navigation pane, choose volumes, select the root volume of the stopped instance, and choose actions, detach volume. select the root volume that you created from the stuck instance, choose actions, attach volume, and attach it to the new instance as its root volume (using the device name that you wrote down). attach any additional non-root volumes to the instance. in the navigation pane, choose instances and select the replacement instance. choose actions, instance state, start. verify that the instance is working. select the stuck instance, choose actions, instance state, terminate. if the instance also gets stuck terminating, amazon ec2 automatically forces it to terminate within a few hours. 
use the following best practices for monitoring to help you with your amazon ec2 monitoring tasks. make monitoring a priority to head off small problems before they become big ones.create and implement a monitoring plan that collects monitoring data from all of the parts in your aws solution so that you can more easily debug a multi-point failure if one occurs. your monitoring plan should address, at a minimum, the following questions:what are your goals for monitoring?what resources you will monitor?how often you will monitor these resources?what monitoring tools will you use?who will perform the monitoring tasks?who should be notified when something goes wrong?automate monitoring tasks as much as possible.check the log files on your ec2 instances.
if you lose the private key for an ebs-backed instance, you can regain access to your instance. you must stop the instance, detach its root volume and attach it to another instance as a data volume, modify the  file with a new public key, move the volume back to the original instance, and restart the instance. for more information about launching, connecting to, and stopping instances, see . this procedure is not supported for instances with instance-store backed root volumes. to determine the root device type of your instance, open the amazon ec2 console, choose instances, select the instance, and check the value of root device type in the details pane. the value is either  or . if the root device is an instance store volume, you cannot use this procedure to regain access to your instance; you must have the private key to connect to the instance. topics create a new key pair using either the amazon ec2 console or a third-party tool. if you want to name your new key pair exactly the same as the lost private key, you must first delete the existing key pair. for information about creating a new key pair, see  or . open the amazon ec2 console at . choose instances in the navigation pane, and then select the instance that you'd like to connect to. (we'll refer to this as the original instance.) from the description tab, save the following information that you need to complete this procedure. write down the instance id, ami id, and availability zone of the original instance.in the root device field, take note of the device name for the root volume (for example,  or ). choose the link and write down the volume id in the ebs id field (vol-xxxxxxxxxxxxxxxxx).choose actions, select instance state, and then select stop. if stop is disabled, either the instance is already stopped or its root device is an instance store volume. warningwhen you stop an instance, the data on any instance store volumes is erased. to keep data from instance store volumes, be sure to back it up to persistent storage. choose launch instance, and then use the launch wizard to launch a temporary instance with the following options: on the choose an ami page, select the same ami that you used to launch the original instance. if this ami is unavailable, you can create an ami that you can use from the stopped instance. for more information, see .on the choose an instance type page, leave the default instance type that the wizard selects for you.on the configure instance details page, specify the same availability zone as the original instance. if you're launching an instance in a vpc, select a subnet in this availability zone.on the add tags page, add the tag  to the instance to indicate that this is a temporary instance.on the review page, choose launch. create a new key pair, download it to a safe location on your computer, and then choose launch instances.in the navigation pane, choose volumes and select the root device volume for the original instance (you wrote down its volume id in a previous step). choose actions, detach volume, and then select yes, detach. wait for the state of the volume to become . (you might need to choose the refresh icon.) with the volume still selected, choose actions, and then select attach volume. select the instance id of the temporary instance, write down the device name specified under device (for example, ), and then choose attach. noteif you launched your original instance from an aws marketplace ami and your volume contains aws marketplace codes, you must first stop the temporary instance before you can attach the volume. connect to the temporary instance. from the temporary instance, mount the volume that you attached to the instance so that you can access its file system. for example, if the device name is , use the following commands to mount the volume as . notethe device name might appear differently on your instance. for example, devices mounted as  might show up as  on the instance. some versions of red hat (or its variants, such as centos) might even increment the trailing letter by 4 characters, where  becomes . use the lsblk command to determine if the volume is partitioned. in the preceding example,  and  are partitioned volumes, and  is not. if your volume is partitioned, you mount the partition ( instead of the raw device () in the next steps. create a temporary directory to mount the volume. mount the volume (or partition) at the temporary mount point, using the volume name or device name that you identified earlier. the required command depends on your operating system's file system. amazon linux, ubuntu, and debian amazon linux 2, centos, suse linux 12, and rhel 7.x noteif you get an error stating that the file system is corrupt, run the following command to use the fsck utility to check the file system and repair any issues:   from the temporary instance, use the following command to update  on the mounted volume with the new public key from the  for the temporary instance. importantthe following examples use the amazon linux user name . you might need to substitute a different user name, such as  for ubuntu instances. if this copy succeeded, you can go to the next step. (optional) otherwise, if you don't have permission to edit files in , you must update the file using sudo and then check the permissions on the file to verify that you are able to log into the original instance. use the following command to check the permissions on the file. in this example output, 222 is the user id and 500 is the group id. next, use sudo to re-run the copy command that failed. run the following command again to determine whether the permissions changed. if the user id and group id have changed, use the following command to restore them. from the temporary instance, unmount the volume that you attached so that you can reattach it to the original instance. for example, use the following command to unmount the volume at . detach the volume from the temporary instance (you unmounted it in the previous step): from the amazon ec2 console, select the root device volume for the original instance (you wrote down volume id in a previous step), choose actions, detach volume, and then select yes, detach. wait for the state of the volume to become . (you might need to choose the refresh icon.) reattach the volume to the original instance: with the volume still selected, choose actions, attach volume. select the instance id of the original instance, specify the device name that you noted earlier in  for the original root device attachment ( or ), and then choose attach. importantif you don't specify the same device name as the original attachment, you cannot start the original instance. amazon ec2 expects the root device volume at  or . select the original instance, choose actions, select instance state, and then choose start. after the instance enters the  state, you can connect to it using the private key file for your new key pair. noteif the name of your new key pair and corresponding private key file is different from the name of the original key pair, ensure that you specify the name of the new private key file when you connect to your instance. (optional) you can terminate the temporary instance if you have no further use for it. select the temporary instance, choose actions, select instance state, and then choose terminate. 
it is important to keep software up-to-date. many packages in a linux distribution are updated frequently to fix bugs, add features, and protect against security exploits. when you first launch and connect to an amazon linux instance, you may see a message asking you to update software packages for security purposes. this section shows how to update an entire system, or just a single package. importantthis information applies to amazon linux. for information about other distributions, see their specific documentation. to update all packages on an amazon linux instance (optional) start a screen session in your shell window. sometimes you may experience a network interruption that can disconnect the ssh connection to your instance. if this happens during a long software update, it can leave the instance in a recoverable, although confused state. a screen session allows you to continue running the update even if your connection is interrupted, and you can reconnect to the session later without problems. execute the screen command to begin the session. if your session is disconnected, log back into your instance and list the available screens. reconnect to the screen using the screen -r command and the process id from the previous command. when you are finished using screen, use the exit command to close the session. run the yum update command. optionally, you can add the  flag to apply only security updates. review the packages listed, type y, and press enter to accept the updates. updating all of the packages on a system can take several minutes. the yum output shows the status of the update while it is running. (optional) reboot your instance to ensure that you are using the latest packages and libraries from your update; kernel updates are not loaded until a reboot occurs. updates to any  libraries should also be followed by a reboot. for updates to packages that control services, it may be sufficient to restart the services to pick up the updates, but a system reboot ensures that all previous package and library updates are complete. to update a single package on an amazon linux instance use this procedure to update a single package (and its dependencies) and not the entire system. run the yum update command with the name of the package you would like to update. review the package information listed, type y, and press enter to accept the update or updates. sometimes there will be more than one package listed if there are package dependencies that must be resolved. the yum output shows the status of the update while it is running. (optional) reboot your instance to ensure that you are using the latest packages and libraries from your update; kernel updates are not loaded until a reboot occurs. updates to any  libraries should also be followed by a reboot. for updates to packages that control services, it may be sufficient to restart the services to pick up the updates, but a system reboot ensures that all previous package and library updates are complete. 
you can use the ami tools to create and manage instance store-backed linux amis. to use the tools, you must install them on your linux instance. the ami tools are available as both an rpm and as a .zip file for linux distributions that don't support rpm. to set up the ami tools using the rpm install ruby using the package manager for your linux distribution, such as yum. for example: download the rpm file using a tool such as wget or curl. for example: verify the rpm file's signature using the following command: the command above should indicate that the file's sha1 and md5 hashes are  if the command indicates that the hashes are , use the following command to view the file's header sha1 and md5 hashes: then, compare your file's header sha1 and md5 hashes with the following verified ami tools hashes to confirm the file's authenticity: header sha1: a1f662d6f25f69871104e6a62187fa4df508f880md5: 9faff05258064e2f7909b66142de6782if your file's header sha1 and md5 hashes match the verified ami tools hashes, continue to the next step. install the rpm using the following command: verify your ami tools installation using the  command. noteif you receive a load error such as "cannot load such file -- ec2/amitools/version (loaderror)", complete the next step to add the location of your ami tools installation to your  path. (optional) if you received an error in the previous step, add the location of your ami tools installation to your  path. run the following command to determine the paths to add. in the above example, the missing file from the previous load error is located at  and . add the locations from the previous step to your  path. verify your ami tools installation using the  command. to set up the ami tools using the .zip file install ruby and unzip using the package manager for your linux distribution, such as apt-get. for example: download the .zip file using a tool such as wget or curl. for example: unzip the files into a suitable installation directory, such as . notice that the .zip file contains a folder ec2-ami-tools-x.x.x, where x.x.x is the version number of the tools (for example, ). set the  environment variable to the installation directory for the tools. for example: add the tools to your  environment variable. for example: you can verify your ami tools installation using the  command. certain commands in the ami tools require a signing certificate (also known as x.509 certificate). you must create the certificate and then upload it to aws. for example, you can use a third-party tool such as openssl to create the certificate. to create a signing certificate install and configure openssl. create a private key using the  command and save the output to a  file. we recommend that you create a 2048- or 4096-bit rsa key. generate a certificate using the  command. to upload the certificate to aws, use the  command. to list the certificates for a user, use the  command: to disable or re-enable a signing certificate for a user, use the  command. the following command disables the certificate: to delete a certificate, use the  command: 
monitoring is an important part of maintaining the reliability, availability, and performance of your amazon elastic compute cloud (amazon ec2) instances and your aws solutions. you should collect monitoring data from all of the parts in your aws solutions so that you can more easily debug a multi-point failure if one occurs. before you start monitoring amazon ec2, however, you should create a monitoring plan that should include: what are your goals for monitoring?what resources will you monitor?how often will you monitor these resources?what monitoring tools will you use?who will perform the monitoring tasks?who should be notified when something goes wrong? after you have defined your monitoring goals and have created your monitoring plan, the next step is to establish a baseline for normal amazon ec2 performance in your environment. you should measure amazon ec2 performance at various times and under different load conditions. as you monitor amazon ec2, you should store a history of monitoring data that you've collected. you can compare current amazon ec2 performance to this historical data to help you to identify normal performance patterns and performance anomalies, and devise methods to address them. for example, you can monitor cpu utilization, disk i/o, and network utilization for your ec2 instances. when performance falls outside your established baseline, you might need to reconfigure or optimize the instance to reduce cpu utilization, improve disk i/o, or reduce network traffic. to establish a baseline you should, at a minimum, monitor the following items: 
an amazon ebs–optimized instance uses an optimized configuration stack and provides additional, dedicated capacity for amazon ebs i/o. this optimization provides the best performance for your ebs volumes by minimizing contention between amazon ebs i/o and other traffic from your instance. ebs–optimized instances deliver dedicated bandwidth to amazon ebs. when attached to an ebs–optimized instance, general purpose ssd () volumes are designed to deliver their baseline and burst performance 99% of the time, and provisioned iops ssd () volumes are designed to deliver their provisioned performance 99.9% of the time. both throughput optimized hdd () and cold hdd () guarantee performance consistency of 90% of burst throughput 99% of the time. non-compliant periods are approximately uniformly distributed, targeting 99% of expected total throughput each hour. for more information, see . topics the following tables show which instance types support ebs optimization. they include the dedicated bandwidth to amazon ebs, the typical maximum aggregate throughput that can be achieved on that connection with a streaming read workload and 128 kib i/o size, and the maximum iops the instance can support if you are using a 16 kib i/o size. choose an ebs–optimized instance that provides more dedicated amazon ebs throughput than your application needs; otherwise, the connection between amazon ebs and amazon ec2 can become a performance bottleneck. the following table lists the instance types that support ebs optimization and ebs optimization is enabled by default. there is no need to enable ebs optimization and no effect if you disable ebs optimization. * these instance types can support maximum performance for 30 minutes at least once every 24 hours. if you have a workload that requires sustained maximum performance for longer than 30 minutes, select an instance type according to baseline performance as shown in the following table. the following table lists the instance types that support ebs optimization but ebs optimization is not enabled by default. you can enable ebs optimization when you launch these instances or after they are running. instances must have ebs optimization enabled to achieve the level of performance described. when you enable ebs optimization for an instance that is not ebs-optimized by default, you pay an additional low, hourly fee for the dedicated capacity. for pricing information, see ebs-optimized instances on the . the , , and  instances do not have dedicated ebs bandwidth and therefore do not offer ebs optimization. on these instances, network traffic and amazon ebs traffic share the same 10-gigabit network interface. you can use the  and  metrics to help you determine whether your instances are sized correctly. you can view these metrics in the cloudwatch console and set an alarm that is triggered based on a threshold you specify. these metrics are expressed as a percentage. instances with a consistently low balance percentage are candidates to size up. instances where the balance percentage never drops below 100% are candidates for downsizing. for more information, see . the high memory instances are designed to run large in-memory databases, including production deployments of the sap hana in-memory database, in the cloud. to maximize ebs performance, use high memory instances with an even number of  volumes with identical provisioned performance. for example, for iops heavy workloads, use four  volumes with 40,000 provisioned iops to get the maximum 160,000 instance iops. similarly, for throughput heavy workloads, use six  volumes with 48,000 provisioned iops to get the maximum 4,750 mb/s throughput. for additional recommendations, see . considerations g4, i3en, inf1, m5a, m5ad, r5a, r5ad, t3, t3a, and z1d instances launched after february 26, 2020 provide the maximum performance listed in the table above. to get the maximum performance from an instance launched before february 26, 2020, stop and start it.c5, c5d, c5n, m5, m5d, m5n, m5dn, r5, r5d, r5n, r5dn, and p3dn instances launched after december 3, 2019 provide the maximum performance listed in the table above. to get the maximum performance from an instance launched before december 3, 2019, stop and start it., , and  instances launched after march 12, 2020 provide the performance in the table above. instances of these types launched before march 12, 2020 might provide lower performance. to get the maximum performance from an instance launched before march 12, 2020, contact your account team to upgrade the instance at no additional cost.you can enable optimization for an instance by setting its attribute for ebs optimization. to enable amazon ebs optimization when launching an instance using the console open the amazon ec2 console at . choose launch instance. in step 1: choose an amazon machine image (ami), select an ami. in step 2: choose an instance type, select an instance type that is listed as supporting amazon ebs optimization. in step 3: configure instance details, complete the fields that you need and choose launch as ebs-optimized instance. if the instance type that you selected in the previous step doesn't support amazon ebs optimization, this option is not present. if the instance type that you selected is amazon ebs–optimized by default, this option is selected and you can't deselect it. follow the directions to complete the wizard and launch your instance. to enable ebs optimization when launching an instance using the command line you can use one of the following commands with the corresponding option. for more information about these command line interfaces, see .  with  (aws cli) with  (aws tools for windows powershell)you can enable or disable optimization for an existing instance by modifying its amazon ebs–optimized instance attribute. if the instance is running, you must stop it first. warningwhen you stop an instance, the data on any instance store volumes is erased. to keep data from instance store volumes, be sure to back it up to persistent storage. to enable ebs optimization for an existing instance using the console open the amazon ec2 console at . in the navigation pane, choose instances, and select the instance. to stop the instance, choose actions, instance state, stop. in the confirmation dialog box, choose yes, stop. it can take a few minutes for the instance to stop. with the instance still selected, choose actions, instance settings, change instance type. in the change instance type dialog box, do one of the following: if the instance type of your instance is amazon ebs–optimized by default, ebs-optimized is selected and you can't change it. you can choose cancel, because amazon ebs optimization is already enabled for the instance.if the instance type of your instance supports amazon ebs optimization, choose ebs-optimized, apply.if the instance type of your instance does not support amazon ebs optimization, you can't choose ebs-optimized. you can select an instance type from instance type that supports amazon ebs optimization, and then choose ebs-optimized, apply.choose actions, instance state, start. to enable ebs optimization for an existing instance using the command line if the instance is running, use one of the following commands to stop it:  (aws cli) (aws tools for windows powershell)to enable ebs optimization, use one of the following commands with the corresponding option:  with  (aws cli) with  (aws tools for windows powershell)
the following topics explain the structure of an iam policy. topics an iam policy is a json document that consists of one or more statements. each statement is structured as follows. there are various elements that make up a statement: effect: the effect can be  or . by default, iam users don't have permission to use resources and api actions, so all requests are denied. an explicit allow overrides the default. an explicit deny overrides any allows.action: the action is the specific api action for which you are granting or denying permission. to learn about specifying action, see . resource: the resource that's affected by the action. some amazon ec2 api actions allow you to include specific resources in your policy that can be created or modified by the action. you specify a resource using an amazon resource name (arn) or using the wildcard (*) to indicate that the statement applies to all resources. for more information, see . condition: conditions are optional. they can be used to control when your policy is in effect. for more information about specifying conditions for amazon ec2, see .for more information about example iam policy statements for amazon ec2, see .  in an iam policy statement, you can specify any api action from any service that supports iam. for amazon ec2, use the following prefix with the name of the api action: . for example:  and . to specify multiple actions in a single statement, separate them with commas as follows: you can also specify multiple actions using wildcards. for example, you can specify all actions whose name begins with the word "describe" as follows: to specify all amazon ec2 api actions, use the * wildcard as follows: for a list of amazon ec2 actions, see  in the amazon ec2 api reference. resource-level permissions refers to the ability to specify which resources users are allowed to perform actions on. amazon ec2 has partial support for resource-level permissions. this means that for certain amazon ec2 actions, you can control when users are allowed to use those actions based on conditions that have to be fulfilled, or specific resources that users are allowed to use. for example, you can grant users permissions to launch instances, but only of a specific type, and only using a specific ami. to specify a resource in an iam policy statement, use its amazon resource name (arn). for more information about specifying the arn value, see . if an api action does not support individual arns, you must use a wildcard (*) to specify that all resources can be affected by the action.  to see tables that identify which amazon ec2 api actions support resource-level permissions, and the arns and condition keys that you can use in a policy, see  in the iam user guide. condition keys for amazon ec2 are also further explained in a later section. keep in mind that you can apply tag-based resource-level permissions in the iam policies you use for amazon ec2 api actions. this gives you better control over which resources a user can create, modify, or use. for more information, see .  each iam policy statement applies to the resources that you specify using their arns.  an arn has the following general syntax: servicethe service (for example, ). regionthe region for the resource (for example, ). accountthe aws account id, with no hyphens (for example, ). resourcetypethe type of resource (for example, ). resourcepatha path that identifies the resource. you can use the * wildcard in your paths. for example, you can indicate a specific instance () in your statement using its arn as follows.  you can specify all instances that belong to a specific account by using the * wildcard as follows. you can also specify all amazon ec2 resources that belong to a specific account by using the * wildcard as follows. to specify all resources, or if a specific api action does not support arns, use the * wildcard in the  element as follows. many amazon ec2 api actions involve multiple resources. for example,  attaches an amazon ebs volume to an instance, so an iam user must have permissions to use the volume and the instance. to specify multiple resources in a single statement, separate their arns with commas, as follows. for a list of arns for amazon ec2 resources, see  in the iam user guide. in a policy statement, you can optionally specify conditions that control when it is in effect. each condition contains one or more key-value pairs. condition keys are not case-sensitive. we've defined aws-wide condition keys, plus additional service-specific condition keys. for a list of service-specific condition keys for amazon ec2, see  in the iam user guide. amazon ec2 also implements the aws-wide condition keys. for more information, see  in the iam user guide.  to use a condition key in your iam policy, use the  statement. for example, the following policy grants users permission to add and remove inbound and outbound rules for any security group. it uses the  condition key to specify that these actions can only be performed on security groups in a specific vpc. if you specify multiple conditions, or multiple keys in a single condition, we evaluate them using a logical and operation. if you specify a single condition with multiple values for one key, we evaluate the condition using a logical or operation. for permissions to be granted, all conditions must be met. you can also use placeholders when you specify conditions. for example, you can grant an iam user permission to use resources with a tag that specifies his or her iam user name. for more information, see  in the iam user guide. importantmany condition keys are specific to a resource, and some api actions use multiple resources. if you write a policy with a condition key, use the  element of the statement to specify the resource to which the condition key applies. if not, the policy may prevent users from performing the action at all, because the condition check fails for the resources to which the condition key does not apply. if you do not want to specify a resource, or if you've written the  element of your policy to include multiple api actions, then you must use the  condition type to ensure that the condition key is ignored for resources that do not use it. for more information, see  in the iam user guide. all amazon ec2 actions support the  and  condition keys. for more information, see . the  key can be used for conditions that specify the arn of the instance from which a request is made. this condition key is available aws-wide and is not service-specific. for policy examples, see  and . the  key cannot be used as a variable to populate the arn for the  element in a statement. for example policy statements for amazon ec2, see . after you've created an iam policy, we recommend that you check whether it grants users the permissions to use the particular api actions and resources they need before you put the policy into production. first, create an iam user for testing purposes, and then attach the iam policy that you created to the test user. then, make a request as the test user. if the amazon ec2 action that you are testing creates or modifies a resource, you should make the request using the  parameter (or run the aws cli command with the  option). in this case, the call completes the authorization check, but does not complete the operation. for example, you can check whether the user can terminate a particular instance without actually terminating it. if the test user has the required permissions, the request returns ; otherwise, it returns . if the policy doesn't grant the user the permissions that you expected, or is overly permissive, you can adjust the policy as needed and retest until you get the desired results.  importantit can take several minutes for policy changes to propagate before they take effect. therefore, we recommend that you allow five minutes to pass before you test your policy updates. if an authorization check fails, the request returns an encoded message with diagnostic information. you can decode the message using the  action. for more information, see  in the aws security token service api reference, and  in the aws cli command reference. 
an elastic fabric adapter (efa) is a network device that you can attach to your amazon ec2 instance to accelerate high performance computing (hpc) and machine learning applications. efa enables you to achieve the application performance of an on-premises hpc cluster, with the scalability, flexibility, and elasticity provided by the aws cloud. efa provides lower and more consistent latency and higher throughput than the tcp transport traditionally used in cloud-based hpc systems. it enhances the performance of inter-instance communication that is critical for scaling hpc and machine learning applications. it is optimized to work on the existing aws network infrastructure and it can scale depending on application requirements. efa integrates with libfabric 1.10 and it supports open mpi 4.0.3 and intel mpi 2019 update 7 for hpc applications, and nvidia collective communications library (nccl) for machine learning applications. notethe os-bypass capabilities of efas are not supported on windows instances. if you attach an efa to a windows instance, the instance functions as an elastic network adapter, without the added efa capabilities. topics an efa is an elastic network adapter (ena) with added capabilities. it provides all of the functionality of an ena, with an additional os-bypass functionality. os-bypass is an access model that allows hpc and machine learning applications to communicate directly with the network interface hardware to provide low-latency, reliable transport functionality.  traditionally, hpc applications use the message passing interface (mpi) to interface with the system’s network transport. in the aws cloud, this has meant that applications interface with mpi, which then uses the operating system's tcp/ip stack and the ena device driver to enable network communication between instances. with an efa, hpc applications use mpi or nccl to interface with the libfabric api. the libfabric api bypasses the operating system kernel and communicates directly with the efa device to put packets on the network. this reduces overhead and enables the hpc application to run more efficiently. notelibfabric is a core component of the openfabrics interfaces (ofi) framework, which defines and exports the user-space api of ofi. for more information, see the  website. elastic network adapters (enas) provide traditional ip networking features that are required to support vpc networking. efas provide all of the same traditional ip networking features as enas, and they also support os-bypass capabilities. os-bypass enables hpc and machine learning applications to bypass the operating system kernel and to communicate directly with the efa device. efa supports the following interfaces and libraries: open mpi 4.0.3intel mpi 2019 update 7nvidia collective communications library (nccl) 2.4.2 and laterthe following instance types support efas: , , , , , , , , , , and . the available instance types vary by region. to see the available instance types that support efa in a region, use the following command and, for , specify a region. the following is example output. the following amis support efas: amazon linux, amazon linux 2, rhel 7.6, rhel 7.7, rhel 7.8, centos 7, ubuntu 16.04, and ubuntu 18.04. efa has the following limitations: you can attach only one efa per instance.efa os-bypass traffic is limited to a single subnet. in other words, efa traffic cannot be sent from one subnet to another. normal ip traffic from the efa can be sent from one subnet to another.efa os-bypass traffic is not routable. normal ip traffic from the efa remains routable.the efa must be a member of a security group that allows all inbound and outbound traffic to and from the security group itself.
an instance store provides temporary block-level storage for your instance. this storage is located on disks that are physically attached to the host computer. instance store is ideal for temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers. an instance store consists of one or more instance store volumes exposed as block devices. the size of an instance store as well as the number of devices available varies by instance type. the virtual devices for instance store volumes are . instance types that support one instance store volume have . instance types that support two instance store volumes have  and , and so on.  topics you can specify instance store volumes for an instance only when you launch it. you can't detach an instance store volume from one instance and attach it to a different instance. the data in an instance store persists only during the lifetime of its associated instance. if an instance reboots (intentionally or unintentionally), data in the instance store persists. however, data in the instance store is lost under any of the following circumstances: the underlying disk drive failsthe instance stopsthe instance terminatestherefore, do not rely on instance store for valuable, long-term data. instead, use more durable data storage, such as amazon s3, amazon ebs, or amazon efs. when you stop or terminate an instance, every block of storage in the instance store is reset. therefore, your data cannot be accessed through the instance store of another instance. if you create an ami from an instance, the data on its instance store volumes isn't preserved and isn't present on the instance store volumes of the instances that you launch from the ami. if you change the instance type, an instance store will not be attached to the new instance type. for more information, see . the instance type determines the size of the instance store available and the type of hardware used for the instance store volumes. instance store volumes are included as part of the instance's usage cost. you must specify the instance store volumes that you'd like to use when you launch the instance (except for nvme instance store volumes, which are available by default). then format and mount the instance store volumes before using them. you can't make an instance store volume available after you launch the instance. for more information, see . some instance types use nvme or sata-based solid state drives (ssd) to deliver high random i/o performance. this is a good option when you need storage with very low latency, but you don't need the data to persist when the instance terminates or you can take advantage of fault-tolerant architectures. for more information, see . the following table provides the quantity, size, type, and performance optimizations of instance store volumes available on each supported instance type. for a complete list of instance types, including ebs-only types, see . * volumes attached to certain instances suffer a first-write penalty unless initialized. for more information, see . ** for more information, see . † the  and  instance types also include a 900 mb instance store swap volume, which may not be automatically enabled at boot time. for more information, see . 
the following information can help you troubleshoot issues if your instance fails a status check. first determine whether your applications are exhibiting any problems. if you verify that the instance is not running your applications as expected, review the status check information and the system logs. topics to investigate impaired instances using the amazon ec2 console open the amazon ec2 console at . in the navigation pane, choose instances, and then select your instance. in the details pane, choose status checks to see the individual results for all system status checks and instance status checks. if a system status check has failed, you can try one of the following options: create an instance recovery alarm. for more information, see .if you changed the instance type to an instance built on the , status checks fail if you migrated from an instance that does not have the required ena and nvme drivers. for more information, see .for an instance using an amazon ebs-backed ami, stop and restart the instance.for an instance using an instance-store backed ami, terminate the instance and launch a replacement.wait for amazon ec2 to resolve the issue.post your issue to the .if your instance is in an auto scaling group, the amazon ec2 auto scaling service automatically launches a replacement instance. for more information, see  in the amazon ec2 auto scaling user guide.retrieve the system log and look for errors.if an instance status check fails, you can reboot the instance and retrieve the system logs. the logs may reveal an error that can help you troubleshoot the issue. rebooting clears unnecessary information from the logs. to reboot an instance and retrieve the system log open the amazon ec2 console at . in the navigation pane, choose instances, and select your instance. choose actions, instance state, reboot. it may take a few minutes for your instance to reboot. verify that the problem still exists; in some cases, rebooting may resolve the problem. when the instance is in the  state, choose actions, instance settings, get system log.  review the log that appears on the screen, and use the list of known system log error statements below to troubleshoot your issue. if your experience differs from our check results, or if you are having an issue with your instance that our checks did not detect, choose submit feedback on the status checks tab to help us improve our detection tests. if your issue is not resolved, you can post your issue to the . for linux-based instances that have failed an instance status check, such as the instance reachability check, verify that you followed the steps above to retrieve the system log. the following list contains some common system log errors and suggested actions you can take to resolve the issue for each error. memory errors device errors kernel errors file system errors operating system errors an out-of-memory error is indicated by a system log entry similar to the one shown below. exhausted memory memory management update failures are indicated by a system log entry similar to the following: issue with amazon linux post your issue to the  or contact . an input/output error is indicated by a system log entry similar to the following example: an input/output error on the device is indicated by a system log entry similar to the following example: terminate the instance and launch a new instance.  for an amazon ebs-backed instance you can recover data from a recent snapshot by creating an image from it. any data added after the snapshot cannot be recovered. this condition is indicated by a system log similar to the one shown below. using an unstable or old linux kernel (for example, 2.6.16-xenu) can cause an interminable loop condition at startup. this condition is indicated by a system log similar to the one shown below. incompatible kernel and userland this condition is indicated by a system log similar to the one shown below. one or more of the following conditions can cause this problem: missing ramdisk missing correct modules from ramdiskamazon ebs root volume not correctly attached as this condition is indicated by a system log similar to the one shown below. one or both of the following conditions can cause this problem: supplied kernel is not supported by grub fallback kernel does not exist this condition is indicated by a system log similar to the one shown below. a bug exists in ramdisk filesystem definitions /etc/fstabmisconfigured filesystem definitions in /etc/fstabmissing/failed drivethis condition is indicated by a system log similar to the one shown below. this condition is indicated by a system log similar to the one shown below. this condition is indicated by a system log similar to the one shown below. missing or incorrectly configured virtual block device driverdevice enumeration clash (sda versus xvda or sda instead of sda1)incorrect choice of instance kernelthis condition is indicated by a system log similar to the one shown below. missing or incorrectly configured virtual block device driverdevice enumeration clash (sda versus xvda)incorrect choice of instance kernelthis condition is indicated by a system log similar to the one shown below. filesystem check time passed; a filesystem check is being forced. wait until the filesystem check completes. a filesystem check can take a long time depending on the size of the root filesystem. modify your filesystems to remove the filesystem check (fsck) enforcement using tune2fs or tools appropriate for your filesystem. this condition is indicated by a system log similar to the one shown below. ramdisk looking for missing drivefilesystem consistency check forceddrive failed or detachedthis condition is indicated by a system log similar to the one shown below.  this condition is indicated by a system log similar to the one shown below. there is a hardcoded interface mac in the ami configuration this condition is indicated by a system log similar to the one shown below. selinux has been enabled in error: supplied kernel is not supported by grubfallback kernel does not existthis condition is indicated by a system log similar to the one shown below. the block device is not connected to the instancethis instance is using an old instance kernel
host recovery automatically restarts your instances on to a new replacement host if failures are detected on your dedicated host. host recovery reduces the need for manual intervention and lowers the operational burden if there is an unexpected dedicated host failure. additionally, built-in integration with aws license manager automates the tracking and management of your licenses if a host recovery occurs.  noteaws license manager integration is supported only in regions in which aws license manager is available.  topics host recovery uses host-level health checks to assess dedicated host availability and to detect underlying system failures. examples of problems that can cause host-level health checks to fail include: loss of network connectivityloss of system powerhardware or software issues on the physical hostwhen a system failure is detected on your dedicated host, host recovery is initiated and amazon ec2 automatically allocates a replacement dedicated host. the replacement dedicated host receives a new host id, but retains the same attributes as the original dedicated host, including: availability zoneinstance typetagsauto placement settingsafter the replacement dedicated host is allocated, the instances are recovered on to the replacement dedicated host. the recovered instances retain the same attributes as the original instances, including: instance idprivate ip addresseselastic ip addressesebs volume attachmentsall instance metadataif instances have a host affinity relationship with the impaired dedicated host, the recovered instances establish host affinity with the replacement dedicated host. when all of the instances have been recovered on to the replacement dedicated host, the impaired dedicated host is released, and the replacement dedicated host becomes available for use. when host recovery is initiated, the aws account owner is notified by email and by an aws personal health dashboard event. a second notification is sent after the host recovery has been successfully completed.  stopped instances are not recovered on to the replacement dedicated host. if you attempt to start a stopped instance that targets the impaired dedicated host, the instance start fails. we recommend that you modify the stopped instance to either target a different dedicated host, or to launch on any available dedicated host with matching configurations and auto-placement enabled. instances with instance storage are not recovered on to the replacement dedicated host. as a remedial measure, the impaired dedicated host is marked for retirement and you receive a retirement notification after the host recovery is complete. follow the remedial steps described in the retirement notification within the specified time period to manually recover the remaining instances on the impaired dedicated host. if you are using aws license manager to track your licenses, aws license manager allocates new licenses for the replacement dedicated host based on the license configuration limits. if the license configuration has hard limits that will be breached as a result of the host recovery, the recovery process is not allowed and you are notified of the host recovery failure through an amazon sns notification. if the license configuration has soft limits that will be breached as a result of the host recovery, the recovery is allowed to continue and you are notified of the limit breach through an amazon sns notification. for more information, see  in the aws license manager user guide. host recovery is supported for the following instance families: a1, c3, c4, c5, c5n, c6g, inf1, m3, m4, m5, m5n, m6g, p3, r3, r4, r5, r5n, r6g, x1, x1e, u-6tb1, u-9tb1, u-12tb1, u-18tb1, and u-24tb1. to recover instances that are not supported, see . you can configure host recovery at the time of dedicated host allocation, or after allocation using the amazon ec2 console or aws command line interface (cli). topics you can enable host recovery at the time of dedicated host allocation or after allocation. for more information about enabling host recovery at the time of dedicated host allocation, see . to enable host recovery after allocation using the console open the amazon ec2 console at . in the navigation pane, choose dedicated hosts. select the dedicated host for which to enable host recovery, and then choose actions, modify host recovery. for host recovery, choose enable, and then choose save. to enable host recovery after allocation using the aws cliuse the  command and specify the  parameter. you can disable host recovery at any time after the dedicated host has been allocated. to disable host recovery after allocation using the console open the amazon ec2 console at . in the navigation pane, choose dedicated hosts. select the dedicated host for which to disable host recovery, and then choose actions, modify host recovery. for host recovery, choose disable, and then choose save. to disable host recovery after allocation using the aws cliuse the  command and specify the  parameter. you can view the host recovery configuration for a dedicated host at any time. to view the host recovery configuration for a dedicated host using the console open the amazon ec2 console at . in the navigation pane, choose dedicated hosts. select the dedicated host, and in the description tab, review the host recovery field. to view the host recovery configuration for a dedicated host using the aws cliuse the  command. the  response element indicates whether host recovery is enabled or disabled. when a dedicated host failure is detected, the impaired dedicated host enters the  state, and all of the instances enter the  state. you can't launch instances on to the impaired dedicated host while it is in the  state. after the replacement dedicated host is allocated, it enters the  state. it remains in this state until the host recovery process is complete. you can't launch instances on to the replacement dedicated host while it is in the  state. recovered instances on the replacement dedicated host remain in the  state during the recovery process. after the host recovery is complete, the replacement dedicated host enters the  state, and the recovered instances return to the  state. you can launch instances on to the replacement dedicated host after it enters the  state. the original impaired dedicated host is permanently released and it enters the  state. if the impaired dedicated host has instances that do not support host recovery, such as instances with instance store-backed volumes, the dedicated host is not released. instead, it is marked for retirement and enters the  state. host recovery does not support recovering instances that use instance store volumes. follow the instructions below to manually recover any of your instances that could not be automatically recovered. warningdata on instance store volumes is lost when an instance is stopped or terminated. this includes instance store volumes that are attached to an instance that has an ebs volume as the root device. to protect data from instance store volumes, back it up to persistent storage before the instance is stopped or terminated. for ebs-backed instances that could not be automatically recovered, we recommend that you manually stop and start the instances to recover them onto a new dedicated host. for more information about stopping your instance, and about the changes that occur in your instance configuration when it's stopped, see . for instance store-backed instances that could not be automatically recovered, we recommend that you do the following: launch a replacement instance on a new dedicated host from your most recent ami. migrate all of the necessary data to the replacement instance. terminate the original instance on the impaired dedicated host. dedicated host integrates with the following aws services: aws license manager—tracks licenses across your amazon ec2 dedicated hosts (supported only in regions in which aws license manager is available). for more information, see the .there are no additional charges for using host recovery, but the usual dedicated host charges apply. for more information, see . as soon as host recovery is initiated, you are no longer billed for the impaired dedicated host. billing for the replacement dedicated host begins only after it enters the  state. if the impaired dedicated host was billed using the on-demand rate, the replacement dedicated host is also billed using the on-demand rate. if the impaired dedicated host had an active dedicated host reservation, it is transferred to the replacement dedicated host. 
you can view the usage and savings information for spot instances at the per-fleet level, or for all running spot instances. at the per-fleet level, the usage and savings information includes all instances launched and terminated by the fleet. you can view this information from the last hour or the last three days. the following screenshot from the spot requests page shows the spot usage and savings information for a spot fleet.  you can view the following usage and savings information: spot instances – the number of spot instances launched and terminated by the spot fleet. when viewing the savings summary, the number represents all your running spot instances.vcpu-hours – the number of vcpu hours used across all the spot instances for the selected time frame.mem(gib)-hours – the number of gib hours used across all the spot instances for the selected time frame.on-demand total – the total amount you would've paid for the selected time frame had you launched these instances as on-demand instances.spot total – the total amount to pay for the selected time frame.savings – the percentage that you are saving by not paying the on-demand price.average cost per vcpu-hour – the average hourly cost of using the vcpus across all the spot instances for the selected time frame, calculated as follows: average cost per vcpu-hour = spot total / vcpu-hours.average cost per mem(gib)-hour – the average hourly cost of using the gibs across all the spot instances for the selected time frame, calculated as follows: average cost per mem(gib)-hour = spot total / mem(gib)-hours.details table – the different instance types (the number of instances per instance type is in parentheses) that comprise the spot fleet. when viewing the savings summary, these comprise all your running spot instances.savings information can only be viewed using the amazon ec2 console. to view the savings information for a spot fleet (console) open the amazon ec2 console at . on the navigation pane, choose spot requests. select a spot fleet request and choose savings. by default, the page displays usage and savings information for the last three days. you can choose last hour or the last three days. for spot fleets that were launched less than an hour ago, the page shows the estimated savings for the hour. to view the savings information for all running spot instances (console) open the amazon ec2 console at . on the navigation pane, choose spot requests. choose savings summary. 
swap space in linux can be used when a system requires more memory than it has been physically allocated. when swap space is enabled, linux systems can swap infrequently used memory pages from physical memory to swap space (either a dedicated partition or a swap file in an existing file system) and free up that space for memory pages that require high-speed access. noteusing swap space for memory paging is not as fast or efficient as using ram. if your workload is regularly paging memory into swap space, you should consider migrating to a larger instance type with more ram. for more information, see . the  and  instance types have a limited amount of physical memory to work with, and they are given a 900 mib swap volume at launch time to act as virtual memory for linux amis. although the linux kernel sees this swap space as a partition on the root device, it is actually a separate instance store volume, regardless of your root device type. amazon linux automatically enables and uses this swap space, but your ami may require some additional steps to recognize and use this swap space. to see if your instance is using swap space, you can use the swapon -s command. the above instance has a 900 mib swap volume attached and enabled. if you don't see a swap volume listed with this command, you may need to enable swap space for the device. check your available disks using the lsblk command. here, the swap volume  is available to the instance, but it is not enabled (notice that the  field is empty). you can enable the swap volume with the swapon command. noteyou must prepend  to the device name listed by lsblk. your device may be named differently, such as , , or . use the device name for your system in the command below. now the swap space should show up in lsblk and swapon -s output. you also need to edit your  file so that this swap space is automatically enabled at every system boot. append the following line to your  file (using the swap device name for your system): to use an instance store volume as swap space any instance store volume can be used as swap space. for example, the  instance type includes a 4 gb ssd instance store volume that is appropriate for swap space. if your instance store volume is much larger (for example, 350 gb), you may consider partitioning the volume with a smaller swap partition of 4-8 gb and the rest for a data volume. notethis procedure applies only to instance types that support instance storage. for a list of supported instance types, see . list the block devices attached to your instance to get the device name for your instance store volume. in this example, the instance store volume is . because this is an amazon linux instance, the instance store volume is formatted and mounted at ; not all linux operating systems do this automatically. (optional) if your instance store volume is mounted (it lists a  in the lsblk command output), unmount it with the following command. set up a linux swap area on the device with the mkswap command. enable the new swap space. verify that the new swap space is being used. edit your  file so that this swap space is automatically enabled at every system boot. if your  file has an entry for  (or ) change it to match the line below; if it does not have an entry for this device, append the following line to your  file (using the swap device name for your system): importantinstance store volume data is lost when an instance is stopped; this includes the instance store swap space formatting created in . if you stop and restart an instance that has been configured to use instance store swap space, you must repeat  through  on the new instance store volume. 
if you have created a public ami, or shared an ami with another aws user, you can create a bookmark that allows a user to access your ami and launch an instance in their own account immediately. this is an easy way to share ami references, so users don't have to spend time finding your ami in order to use it. note that your ami must be public, or you must have shared it with the user to whom you want to send the bookmark. to create a bookmark for your ami type a url with the following information, where region is the region in which your ami resides: for example, this url launches an instance from the ami-0abcdef1234567890 ami in the us-east-1 region: distribute the link to users who want to use your ami. to use a bookmark, choose the link or copy and paste it into your browser. the launch wizard opens, with the ami already selected. 
amazon ebs emits notifications based on amazon cloudwatch events for a variety of volume, snapshot, and encryption status changes. with cloudwatch events, you can establish rules that trigger programmatic actions in response to a change in volume, snapshot, or encryption key state. for example, when a snapshot is created, you can trigger an aws lambda function to share the completed snapshot with another account or copy it to another region for disaster-recovery purposes. events in cloudwatch are represented as json objects. the fields that are unique to the event are contained in the "detail" section of the json object. the "event" field contains the event name. the "result" field contains the completed status of the action that triggered the event. for more information, see  in the amazon cloudwatch events user guide. for more information, see  in the amazon cloudwatch user guide. topics amazon ebs sends events to cloudwatch events when the following volume events occur. topics the  event is sent to your aws account when an action to create a volume completes. however it is not saved, logged, or archived. this event can have a result of either  or . creation will fail if an invalid kms key was provided, as shown in the examples below. event datathe listing below is an example of a json object emitted by ebs for a successful  event.  the listing below is an example of a json object emitted by ebs after a failed  event. the cause for the failure was a disabled kms key. the following is an example of a json object that is emitted by ebs after a failed  event. the cause for the failure was a kms key pending import. the  event is sent to your aws account when an action to delete a volume completes. however it is not saved, logged, or archived. this event has the result . if the deletion does not complete, the event is never sent. event datathe listing below is an example of a json object emitted by ebs for a successful  event.  the  or  event is sent to your aws account if a volume fails to attach or reattach to an instance. however it is not saved, logged, or archived. if you use a kms key to encrypt an ebs volume and the key becomes invalid, ebs will emit an event if that key is later used to attach or reattach to an instance, as shown in the examples below. event datathe listing below is an example of a json object emitted by ebs after a failed  event. the cause for the failure was a kms key pending deletion. noteaws may attempt to reattach to a volume following routine server maintenance. the listing below is an example of a json object emitted by ebs after a failed  event. the cause for the failure was a kms key pending deletion. amazon ebs sends events to cloudwatch events when the following volume events occur. topics the  event is sent to your aws account when an action to create a snapshot completes. however it is not saved, logged, or archived. this event can have a result of either  or . event datathe listing below is an example of a json object emitted by ebs for a successful  event. in the  section, the  field contains the arn of the source volume. the  and  fields indicate when creation of the snapshot started and completed. the  event is sent to your aws account when an action to create a multi-volume snapshot completes. this event can have a result of either  or . event datathe listing below is an example of a json object emitted by ebs for a successful  event. in the  section, the  field contains the arns of the source volumes of the multi-volume snapshot set. the  and  fields indicate when creation of the snapshot started and completed. the listing below is an example of a json object emitted by ebs after a failed  event. the cause for the failure was one or more snapshots failed to complete. the values of  are the arns of the failed snapshots.  and  represent when the create-snapshots action started and ended.  the  event is sent to your aws account when an action to copy a snapshot completes. however it is not saved, logged, or archived. this event can have a result of either  or . event datathe listing below is an example of a json object emitted by ebs after a successful  event. the value of  is the arn of the newly created snapshot. in the  section, the value of  is the arn of the source snapshot.  and  represent when the copy-snapshot action started and ended. the listing below is an example of a json object emitted by ebs after a failed  event. the cause for the failure was an invalid source snapshot id. the value of  is the arn of the failed snapshot. in the  section, the value of  is the arn of the source snapshot.  and  represent when the copy-snapshot action started and ended.  the  event is sent to your aws account when another account shares a snapshot with it. however it is not saved, logged, or archived. the result is always . event datathe following is an example of a json object emitted by ebs after a completed  event. in the  section, the value of  is the aws account number of the user that shared the snapshot with you.  and  represent when the share-snapshot action started and ended. the  event is emitted only when a private snapshot is shared with another user. sharing a public snapshot does not trigger the event. amazon ebs sends  events to cloudwatch events when a volume is modified. however it is not saved, logged, or archived. amazon ebs sends events to cloudwatch events when the state of fast snapshot restore for a snapshot changes. the following is example data for this event. the possible values for  are , , , , and . the possible values for  are as follows: a request to enable fast snapshot restore failed and the state transitioned to  or . fast snapshot restore cannot be enabled for this snapshot. the state successfully transitioned to  or . the state successfully transitioned to , , or . a request to enable fast snapshot restore failed due to insufficient capacity, and the state transitioned to  or . wait and then try again. a request to enable fast snapshot restore failed due to an internal error, and the state transitioned to  or . wait and then try again. you can use amazon ebs and cloudwatch events to automate your data-backup workflow. this requires you to create an iam policy, a aws lambda function to handle the event, and an amazon cloudwatch events rule that matches incoming events and routes them to the lambda function. the following procedure uses the  event to automatically copy a completed snapshot to another region for disaster recovery.  to copy a completed snapshot to another region create an iam policy, such as the one shown in the following example, to provide permissions to execute a  action and write to the cloudwatch events log. assign the policy to the iam user that will handle the cloudwatch event. define a function in lambda that will be available from the cloudwatch console. the sample lambda function below, written in node.js, is invoked by cloudwatch when a matching  event is emitted by amazon ebs (signifying that a snapshot was completed). when invoked, the function copies the snapshot from  to . to ensure that your lambda function is available from the cloudwatch console, create it in the region where the cloudwatch event will occur. for more information, see the . open the cloudwatch console at . choose events, create rule, select event source, and amazon ebs snapshots. for specific event(s), choose createsnapshot and for specific result(s), choose succeeded. for rule target, find and choose the sample function that you previously created. choose target, add target. for lambda function, select the lambda function that you previously created and choose configure details. on the configure rule details page, type values for name and description. select the state check box to activate the function (setting it to enabled).  choose create rule. your rule should now appear on the rules tab. in the example shown, the event that you configured should be emitted by ebs the next time you copy a snapshot. 
c-states control the sleep levels that a core can enter when it is idle. c-states are numbered starting with c0 (the shallowest state where the core is totally awake and executing instructions) and go to c6 (the deepest idle state where a core is powered off). p-states control the desired performance (in cpu frequency) from a core. p-states are numbered starting from p0 (the highest performance setting where the core is allowed to use intel turbo boost technology to increase frequency if possible), and they go from p1 (the p-state that requests the maximum baseline frequency) to p15 (the lowest possible frequency). the following instance types provide the ability for an operating system to control processor c-states and p-states: general purpose:  |  |  | compute optimized:  |  | memory optimized:  |  |  |  |  |  |  |  |  |  |  |  | storage optimized:  |  |  |  |  |  | accelerated computing:  |  |  |  | the following instance types provide the ability for an operating system to control processor c-states: general purpose:  |  |  |  |  |  |  | compute optimized:  |  |  |  |  |   |  |  |  |  | memory optimized:  |  |  |  |  |  |  |  |  | storage optimized:  | accelerated computing:  | aws graviton processors have built-in power saving modes and operate at a fixed frequency. therefore, they do not provide the ability for the operating system to control c-states and p-states. you might want to change the c-state or p-state settings to increase processor performance consistency, reduce latency, or tune your instance for a specific workload. the default c-state and p-state settings provide maximum performance, which is optimal for most workloads. however, if your application would benefit from reduced latency at the cost of higher single- or dual-core frequencies, or from consistent performance at lower frequencies as opposed to bursty turbo boost frequencies, consider experimenting with the c-state or p-state settings that are available to these instances. the following sections describe the different processor state configurations and how to monitor the effects of your configuration. these procedures were written for, and apply to amazon linux; however, they may also work for other linux distributions with a linux kernel version of 3.9 or newer. for more information about other linux distributions and processor state control, see your system-specific documentation. notethe examples on this page use the turbostat utility (which is available on amazon linux by default) to display processor frequency and c-state information, and the stress command (which can be installed by running sudo yum install -y stress) to simulate a workload.if the output does not display the c-state information, include the --debug option in the command (sudo turbostat --debug stress ). topics this is the default processor state control configuration for the amazon linux ami, and it is recommended for most workloads. this configuration provides the highest performance with lower variability. allowing inactive cores to enter deeper sleep states provides the thermal headroom required for single or dual core processes to reach their maximum turbo boost potential. the following example shows a  instance with two cores actively performing work reaching their maximum processor turbo boost frequency. in this example, vcpus 21 and 28 are running at their maximum turbo boost frequency because the other cores have entered the  sleep state to save power and provide both power and thermal headroom for the working cores. vcpus 3 and 10 (each sharing a processor core with vcpus 21 and 28) are in the  state, waiting for instruction. in the following example, all 18 cores are actively performing work, so there is no headroom for maximum turbo boost, but they are all running at the "all core turbo boost" speed of 3.2 ghz. c-states control the sleep levels that a core may enter when it is inactive. you may want to control c-states to tune your system for latency versus performance. putting cores to sleep takes time, and although a sleeping core allows more headroom for another core to boost to a higher frequency, it takes time for that sleeping core to wake back up and perform work. for example, if a core that is assigned to handle network packet interrupts is asleep, there may be a delay in servicing that interrupt. you can configure the system to not use deeper c-states, which reduces the processor reaction latency, but that in turn also reduces the headroom available to other cores for turbo boost. a common scenario for disabling deeper sleep states is a redis database application, which stores the database in system memory for the fastest possible query response time. to limit deeper sleep states on amazon linux 2 open the  file with your editor of choice. edit the  line and add the  option to set  as the deepest c-state for idle cores. save the file and exit your editor. run the following command to rebuild the boot configuration. reboot your instance to enable the new kernel option. to limit deeper sleep states on amazon linux ami open the  file with your editor of choice. edit the  line of the first entry and add the  option to set  as the deepest c-state for idle cores. save the file and exit your editor. reboot your instance to enable the new kernel option. the following example shows a  instance with two cores actively performing work at the "all core turbo boost" core frequency. in this example, the cores for vcpus 19 and 28 are running at 3.2 ghz, and the other cores are in the  c-state, awaiting instruction. although the working cores are not reaching their maximum turbo boost frequency, the inactive cores will be much faster to respond to new requests than they would be in the deeper  c-state. you can reduce the variability of processor frequency with p-states. p-states control the desired performance (in cpu frequency) from a core. most workloads perform better in p0, which requests turbo boost. but you may want to tune your system for consistent performance rather than bursty performance that can happen when turbo boost frequencies are enabled.  intel advanced vector extensions (avx or avx2) workloads can perform well at lower frequencies, and avx instructions can use more power. running the processor at a lower frequency, by disabling turbo boost, can reduce the amount of power used and keep the speed more consistent. for more information about optimizing your instance configuration and workload for avx, see . this section describes how to limit deeper sleep states and disable turbo boost (by requesting the  p-state) to provide low-latency and the lowest processor speed variability for these types of workloads. to limit deeper sleep states and disable turbo boost on amazon linux 2 open the  file with your editor of choice. edit the  line and add the  option to set  as the deepest c-state for idle cores. save the file and exit your editor. run the following command to rebuild the boot configuration. reboot your instance to enable the new kernel option. when you need the low processor speed variability that the  p-state provides, execute the following command to disable turbo boost. when your workload is finished, you can re-enable turbo boost with the following command. to limit deeper sleep states and disable turbo boost on amazon linux ami open the  file with your editor of choice. edit the  line of the first entry and add the  option to set  as the deepest c-state for idle cores. save the file and exit your editor. reboot your instance to enable the new kernel option. when you need the low processor speed variability that the  p-state provides, execute the following command to disable turbo boost. when your workload is finished, you can re-enable turbo boost with the following command. the following example shows a  instance with two vcpus actively performing work at the baseline core frequency, with no turbo boost. the cores for vcpus 21 and 28 are actively performing work at the baseline processor speed of 2.9 ghz, and all inactive cores are also running at the baseline speed in the  c-state, ready to accept instructions. 
you can bring part or all of your public ipv4 address range or ipv6 address range from your on-premises network to your aws account. you continue to own the address range, but aws advertises it on the internet by default. after you bring the address range to aws, it appears in your account as an address pool. importantbyoip is not available in all regions. for a list of supported regions, see the . topics the address range must be registered with your regional internet registry (rir), such as the american registry for internet numbers (arin), réseaux ip européens network coordination centre (ripe), or asia-pacific network information centre (apnic). it must be registered to a business or institutional entity and cannot be registered to an individual person.the most specific ipv4 address range that you can bring is /24.the most specific ipv6 address range that you can bring is /48 for cidrs that are publicly advertised, and /56 for cidrs that are .you can bring each address range to one region at a time.you can bring a total of five ipv4 and ipv6 address ranges per region to your aws account.the addresses in the ip address range must have a clean history. we might investigate the reputation of the ip address range and reserve the right to reject an ip address range if it contains an ip address that has a poor reputation or is associated with malicious behavior.you must own the ip address that you use. this means that only the following are supported:arin - "direct allocation" and "direct assignment" network typesripe - "allocated pa", "legacy", "assigned pi", and "allocated-by-rir" allocation statusesapnic – "allocated portable" and "assigned portable" allocation statusesto ensure that only you can bring your address range to your aws account, you must authorize amazon to advertise the address range. you must also provide proof that you own the address range through a signed authorization message. a route origin authorization (roa) is a cryptographic statement about your route announcements that you can create through your rir. it contains the address range, the autonomous system numbers (asn) that are allowed to advertise the address range, and an expiration date. an roa authorizes amazon to advertise an address range under a specific as number. however, it does not authorize your aws account to bring the address range to aws. to authorize your aws account to bring an address range to aws, you must publish a self-signed x509 certificate in the registry data access protocol (rdap) remarks for the address range. the certificate contains a public key, which aws uses to verify the authorization-context signature that you provide. keep your private key secure and use it to sign the authorization-context message. the commands in these tasks are supported on linux. on windows, you can use the  to run linux commands. topics create a roa object to authorize amazon asns 16509 and 14618 to advertise your address range, plus the asns that are currently authorized to advertise the address range. you must set the maximum length to the size of the smallest prefix that you want to bring (for example, /24). it might take up to 24 hours for the roa to become available to amazon. for more information, see the following: arin — ripe — apnic — use the following procedure to create a self-signed x509 certificate and add it to the rdap record for your rir. the openssl commands require openssl version 1.0.2 or later. copy the commands below and replace only the placeholder values (in colored italic text).  to create a self-signed x509 certificate and add it to the rdap record generate an rsa 2048-bit key pair as shown in the following. create a public x509 certificate from the key pair using the following command. in this example, the certificate expires in 365 days, after which time it cannot be trusted. be sure to set the expiration appropriately. when prompted for information, you can accept the default values. update the rdap record for your rir with the x509 certificate. be sure to copy the  and  from the certificate. be sure that you have removed newline characters, if you haven't already done so using the tr -d "\n" commands in the previous steps. to view your certificate, run the following command. for arin, add the certificate in the "public comments" section for your address range. do not add it to the comments section for your organization. for ripe, add the certificate as a new "descr" field for your address range. do not add it to the comments section for your organization. for apnic, email the public key to  to manually add it to the "remarks" field for your address range. send the email using the apnic authorized contact for the ip addresses. the format of the signed authorization message is as follows, where the date is the expiry date of the message. to create a signed authorization message create a plaintext authorization message and store it in a variable named  as shown in the following example. copy the following example and replace only the example account number, address range, and expiry date with your own values. sign the authorization message in  using the key pair that you created, and store it in a variable named . importantwe recommend that you copy and paste this command. do not modify or replace any of the values. when you provision an address range for use with aws, you are confirming that you own the address range and are authorizing amazon to advertise it. we also verify that you own the address range through a signed authorization message. this message is signed with the self-signed x509 key pair that you used when updating the rdap record with the x509 certificate. to provision the address range, use the following  command. replace the example address range with your own address range. the  option uses the variables that you created previously, not the roa message. provisioning an address range is an asynchronous operation, so the call returns immediately, but the address range is not ready to use until its status changes from  to . it can take up to three weeks to complete the provisioning process. to monitor the status of the address ranges that you've provisioned, use the following  command. if there are issues during provisioning and the status goes to , you must run the  command again after the issues have been resolved. by default, an address range is provisioned to be publicly advertised to the internet. you can provision an ipv6 address range that will not be publicly advertised. when you associate an ipv6 cidr block from a non-public address range with a vpc, the ipv6 cidr can only be accessed through an aws direct connect connection.  an roa is not required to provision a non-public address range. to provision an ipv6 address range that will not be publicly advertised, use the following  command. importantyou can only set the  or  flag during provisioning. you cannot change the advertisable status of an address range later. after the address range is provisioned, it is ready to be advertised. you must advertise the exact address range that you provisioned. you can't advertise only a portion of the provisioned address range. if you provisioned an ipv6 address range that will not be publicly advertised, you do not need to complete this step. we recommend that you stop advertising the address range from other locations before you advertise it through aws. if you keep advertising your ip address range from other locations, we can't reliably support it or troubleshoot issues. specifically, we can't guarantee that traffic to the address range will enter our network. to minimize down time, you can configure your aws resources to use an address from your address pool before it is advertised, and then simultaneously stop advertising it from the current location and start advertising it through aws. for more information about allocating an elastic ip address from your address pool, see . to advertise the address range, use the following  command. importantyou can run the advertise-byoip-cidr command at most once every 10 seconds, even if you specify different address ranges each time. to stop advertising the address range, use the following  command. importantyou can run the withdraw-byoip-cidr command at most once every 10 seconds, even if you specify different address ranges each time. you can view and work with the ipv4 and ipv6 address ranges that you've provisioned in your account. you can create an elastic ip address from your ipv4 address pool and use it with your aws resources, such as ec2 instances, nat gateways, and network load balancers. to view information about the ipv4 address pools that you've provisioned in your account, use the following  command. to create an elastic ip address from your ipv4 address pool, use the  command. you can use the  option to specify the id of the address pool returned by . or you can use the  option to specify an address from the address range that you provisioned. to view information about the ipv6 address pools that you've provisioned in your account, use the following  command. to create a vpc and specify an ipv6 cidr from your ipv6 address pool, use the following  command. to let amazon choose the ipv6 cidr from your ipv6 address pool, omit the  option. to associate an ipv6 cidr block from your ipv6 address pool with a vpc, use the following  command. to let amazon choose the ipv6 cidr from your ipv6 address pool, omit the  option. to view your vpcs and the associated ipv6 address pool information, use the  command. to view information about associated ipv6 cidr blocks from a specific ipv6 address pool, use the following  command. if you disassociate the ipv6 cidr block from your vpc, it's released back into your ipv6 address pool. for more information about working with ipv6 cidr blocks in the vpc console, see  in the amazon vpc user guide. to stop using your address range with aws, first release any elastic ip addresses and disassociate any ipv6 cidr blocks that are still allocated from the address pool. then stop advertising the address range, and finally, deprovision the address range. you cannot deprovision a portion of the address range. if you want to use a more specific address range with aws, deprovision the entire address range and provision a more specific address range. (ipv4) to release each elastic ip address, use the following  command. (ipv6) to disassociate an ipv6 cidr block, use the following  command. to stop advertising the address range, use the following  command. to deprovision the address range, use the following  command. it can take up to a day to deprovision an address range. 
a requester-managed network interface is a network interface that an aws service creates in your vpc. this network interface can represent an instance for another service, such as an amazon rds instance, or it can enable you to access another service or resource, such as an aws privatelink service, or an amazon ecs task. you cannot modify or detach a requester-managed network interface. if you delete the resource that the network interface represents, the aws service detaches and deletes the network interface for you. to change the security groups for a requester-managed network interface, you might have to use the console or command line tools for that service. for more information, see the service-specific documentation. you can tag a requester-managed network interface. for more information, see . you can view the requester-managed network interfaces that are in your account. to view requester-managed network interfaces using the console open the amazon ec2 console at . in the navigation pane, choose network interfaces. select the network interface and view the following information on the details pane: attachment owner: if you created the network interface, this field displays your aws account id. otherwise, it displays an alias or id for the principal or service that created the network interface.description: provides information about the purpose of the network interface; for example, "vpc endpoint interface".to view requester-managed network interfaces using the command line use the  aws cli command to describe the network interfaces in your account.  in the output, the  field displays  if the network interface is managed by another aws service. alternatively, use the  tools for windows powershell command. 
the following examples explain credit use when instances are configured as . topics in this example, you see how a  instance launched as  earns, accrues, and spends earned credits. you see how the credit balance reflects the accrued earned credits. notet3 and t3a instances configured as  do not receive launch credits. a running  instance earns 144 credits every 24 hours. its credit balance limit is 144 earned credits. after the limit is reached, new credits that are earned are discarded. for more information about the number of credits that can be earned and accrued, see the . you might launch a t3 standard instance and use it immediately. or, you might launch a t3 standard instance and leave it idle for a few days before running applications on it. whether an instance is used or remains idle determines if credits are spent or accrued. if an instance remains idle for 24 hours from the time it is launched, the credit balance reaches it limit, which is the maximum number of earned credits that can be accrued.  this example describes an instance that remains idle for 24 hours from the time it is launched, and walks you through seven periods of time over a 96-hour period, showing the rate at which credits are earned, accrued, spent, and discarded, and the value of the credit balance at the end of each period. the following workflow references the numbered points on the graph: p1 – at 0 hours on the graph, the instance is launched as  and immediately begins to earn credits. the instance remains idle from the time it is launched—cpu utilization is 0%—and no credits are spent. all unspent credits are accrued in the credit balance. for the first 24 hours,  is at 0, and the  value reaches its maximum of 144. p2 – for the next 12 hours, cpu utilization is at 2.5%, which is below the 5% baseline. the instance earns more credits than it spends, but the  value cannot exceed its maximum of 144 credits. any credits that are earned in excess of the limit are discarded. p3 – for the next 24 hours, cpu utilization is at 7% (above the baseline), which requires a spend of 57.6 credits. the instance spends more credits than it earns, and the  value reduces to 86.4 credits. p4 – for the next 12 hours, cpu utilization decreases to 2.5% (below the baseline), which requires a spend of 36 credits. in the same time, the instance earns 72 credits. the instance earns more credits than it spends, and the  value increases to 122 credits. p5 – for the next two hours, the instance bursts at 100% cpu utilization, and depletes its entire  value of 122 credits. at the end of this period, with the  at zero, cpu utilization is forced to drop to the baseline utilization level of 5%. at the baseline, the instance earns as many credits as it spends. p6 – for the next 14 hours, cpu utilization is at 5% (the baseline). the instance earns as many credits as it spends. the  value remains at 0. p7 – for the last 24 hours in this example, the instance is idle and cpu utilization is 0%. during this time, the instance earns 144 credits, which it accrues in its .  in this example, you see how a  instance launched as  earns, accrues, and spends launch and earned credits. you see how the credit balance reflects not only accrued earned credits, but also accrued launch credits. a  instance gets 30 launch credits when it is launched, and earns 72 credits every 24 hours. its credit balance limit is 72 earned credits; launch credits do not count towards the limit. after the limit is reached, new credits that are earned are discarded. for more information about the number of credits that can be earned and accrued, see the . for more information about limits, see . you might launch a t2 standard instance and use it immediately. or, you might launch a t2 standard instance and leave it idle for a few days before running applications on it. whether an instance is used or remains idle determines if credits are spent or accrued. if an instance remains idle for 24 hours from the time it is launched, the credit balance appears to exceed its limit because the balance reflects both accrued earned credits and accrued launch credits. however, after cpu is used, the launch credits are spent first. thereafter, the limit always reflects the maximum number of earned credits that can be accrued.  this example describes an instance that remains idle for 24 hours from the time it is launched, and walks you through seven periods of time over a 96-hour period, showing the rate at which credits are earned, accrued, spent, and discarded, and the value of the credit balance at the end of each period. at 0 hours on the graph, the t2 instance is launched as  and immediately gets 30 launch credits. it earns credits while in the running state. the instance remains idle from the time it is launched—cpu utilization is 0%—and no credits are spent. all unspent credits are accrued in the credit balance. at approximately 14 hours after launch, the credit balance is 72 (30 launch credits + 42 earned credits), which is equivalent to what the instance can earn in 24 hours. at 24 hours after launch, the credit balance exceeds 72 credits because the unspent launch credits are accrued in the credit balance—the credit balance is 102 credits: 30 launch credits + 72 earned credits.   conclusionif there is no cpu utilization after launch, the instance accrues more credits than what it can earn in 24 hours (30 launch credits + 72 earned credits = 102 credits). in a real-world scenario, an ec2 instance consumes a small number of credits while launching and running, which prevents the balance from reaching the maximum theoretical value in this example. for the next 12 hours, the instance continues to remain idle and earn credits, but the credit balance does not increase. it plateaus at 102 credits (30 launch credits + 72 earned credits). the credit balance has reached its limit of 72 accrued earned credits, so newly earned credits are discarded.  conclusionan instance constantly earns credits, but it cannot accrue more earned credits if the credit balance has reached its limit. after the limit is reached, newly earned credits are discarded. launch credits do not count towards the credit balance limit. if the balance includes accrued launch credits, the balance appears to be over the limit. for the next 25 hours, the instance uses 2% cpu, which requires 30 credits. in the same period, it earns 75 credits, but the credit balance decreases. the balance decreases because the accrued launch credits are spent first, while newly earned credits are discarded because the credit balance is already at its limit of 72 earned credits.  conclusionan instance spends launch credits first, before spending earned credits. launch credits do not count towards the credit limit. after the launch credits are spent, the balance can never go higher than what can be earned in 24 hours. furthermore, while an instance is running, it cannot get more launch credits. for the next 11 hours, the instance uses 2% cpu, which requires 13.2 credits. this is the same cpu utilization as in the previous period, but the balance does not decrease. it stays at 72 credits. the balance does not decrease because the credit earn rate is higher than the credit spend rate. in the time that the instance spends 13.2 credits, it also earns 33 credits. however, the balance limit is 72 credits, so any earned credits that exceed the limit are discarded. the balance plateaus at 72 credits, which is different from the plateau of 102 credits during period 2, because there are no accrued launch credits.  conclusionafter launch credits are spent, the credit balance limit is determined by the number of credits that an instance can earn in 24 hours. if the instance earns more credits than it spends, newly earned credits over the limit are discarded. for the next three hours, the instance bursts at 20% cpu utilization, which requires 36 credits. the instance earns nine credits in the same three hours, which results in a net balance decrease of 27 credits. at the end of three hours, the credit balance is 45 accrued earned credits.  conclusionif an instance spends more credits than it earns, its credit balance decreases. for the next 15 hours, the instance uses 2% cpu, which requires 18 credits. this is the same cpu utilization as in periods 3 and 4. however, the balance increases in this period, whereas it decreased in period 3 and plateaued in period 4. in period 3, the accrued launch credits were spent, and any earned credits that exceeded the credit limit were discarded, resulting in a decrease in the credit balance. in period 4, the instance spent fewer credits than it earned. any earned credits that exceeded the limit were discarded, so the balance plateaued at its maximum of 72 credits. in this period, there are no accrued launch credits, and the number of accrued earned credits in the balance is below the limit. no earned credits are discarded. furthermore, the instance earns more credits than it spends, resulting in an increase in the credit balance.  conclusionif an instance spends fewer credits than it earns, its credit balance increases. for the next six hours, the instance remains idle—cpu utilization is 0%—and no credits are spent. this is the same cpu utilization as in period 2, but the balance does not plateau at 102 credits—it plateaus at 72 credits, which is the credit balance limit for the instance. in period 2, the credit balance included 30 accrued launch credits. the launch credits were spent in period 3. a running instance cannot get more launch credits. after its credit balance limit is reached, any earned credits that exceed the limit are discarded.  conclusionan instance constantly earns credits, but cannot accrue more earned credits if the credit balance limit has been reached. after the limit is reached, newly earned credits are discarded. the credit balance limit is determined by the number of credits that an instance can earn in 24 hours. for more information about credit balance limits, see the . 
there is a wealth of open-source software available on the internet that has not been pre-compiled and made available for download from a package repository. you may eventually discover a software package that you need to compile yourself, from its source code. for your system to be able to compile software, you need to install several development tools, such as make, gcc, and autoconf. importantthis information applies to amazon linux. for information about other distributions, see their specific documentation. because software compilation is not a task that every amazon ec2 instance requires, these tools are not installed by default, but they are available in a package group called "development tools" that is easily added to an instance with the yum groupinstall command. software source code packages are often available for download (from web sites such as  and ) as a compressed archive file, called a tarball. these tarballs will usually have the  file extension. you can decompress these archives with the tar command. after you have decompressed and unarchived the source code package, you should look for a  or  file in the source code directory that can provide you with further instructions for compiling and installing the source code.  to retrieve source code for amazon linux packages amazon web services provides the source code for maintained packages. you can download the source code for any installed packages with the yumdownloader --source command. run the yumdownloader --source package command to download the source code for package. for example, to download the source code for the  package, enter the following command. the location of the source rpm is in the directory from which you ran the command. 
with an amazon ebs-backed linux instance, you can back up the root device volume of the instance by creating a snapshot. when you have a snapshot of the root device volume of an instance, you can terminate that instance and then later launch a new instance from the snapshot. this can be useful if you don't have the original ami that you launched an instance from, but you need to be able to launch an instance using the same image. use the following procedure to create an ami from the root volume of your instance using the console. if you prefer, you can use one of the following commands instead:  (aws cli) or  (aws tools for windows powershell). you specify the snapshot using the block device mapping. to create an ami from your root volume using the console open the amazon ec2 console at . in the navigation pane, choose elastic block store, snapshots. choose create snapshot. for volumes, start typing the name or id of the root volume, and then select it from the list of options. choose the snapshot that you just created, and then choose actions, create image. in the create image from ebs snapshot dialog box, provide the following information and then choose create. if you're re-creating a parent instance, then choose the same options as the parent instance. architecture: choose i386 for 32-bit or x86_64 for 64-bit.root device name: enter the appropriate name for the root volume. for more information, see .virtualization type: choose whether instances launched from this ami use paravirtual (pv) or hardware virtual machine (hvm) virtualization. for more information, see .(pv virtualization type only) kernel id and ram disk id: choose the aki and ari from the lists. if you choose the default aki or don't choose an aki, you are required to specify an aki every time you launch an instance using this ami. in addition, your instance may fail the health checks if the default aki is incompatible with the instance.(optional) block device mappings: add volumes or expand the default size of the root volume for the ami. for more information about resizing the file system on your instance for a larger volume, see .in the navigation pane, choose amis. choose the ami that you just created, and then choose launch. follow the wizard to launch your instance. for more information about how to configure each step in the wizard, see . 
amazon ec2 sends metrics to amazon cloudwatch. you can use the aws management console, the aws cli, or an api to list the metrics that amazon ec2 sends to cloudwatch. by default, each data point covers the 5 minutes that follow the start time of activity for the instance. if you've enabled detailed monitoring, each data point covers the next minute of activity from the start time. for information about getting the statistics for these metrics, see . topics the  namespace includes the following instance metrics. the  namespace includes the following cpu credit metrics for your . the  namespace includes the following amazon ebs metrics for the nitro-based instances that are not bare metal instances. for the list of nitro-based instance types, see . metric values for nitro-based instances will always be integers (whole numbers), whereas values for xen-based instances support decimals. therefore, low instance cpu utilization on nitro-based instances may appear to be rounded down to 0. for information about the metrics provided for your ebs volumes, see . for information about the metrics provided for your spot fleets, see . the  namespace includes the following status check metrics. by default, status check metrics are available at a 1-minute frequency at no charge. for a newly-launched instance, status check metric data is only available after the instance has completed the initialization state (within a few minutes of the instance entering the running state). for more information about ec2 status checks, see . the  namespace includes metrics for mirrored traffic. for more information, see  in the amazon vpc traffic mirroring guide. you can use the following dimensions to refine the metrics listed in the previous tables. you can use cloudwatch usage metrics to provide visibility into your account's usage of resources. use these metrics to visualize your current service usage on cloudwatch graphs and dashboards. amazon ec2 usage metrics correspond to aws service quotas. you can configure alarms that alert you when your usage approaches a service quota. for more information about cloudwatch integration with service quotas, see . amazon ec2 publishes the following metrics in the  namespace. the following dimensions are used to refine the usage metrics that are published by amazon ec2. metrics are grouped first by namespace, and then by the various dimension combinations within each namespace. for example, you can view all metrics provided by amazon ec2, or metrics grouped by instance id, instance type, image (ami) id, or auto scaling group. to view available metrics by category (console) open the cloudwatch console at . in the navigation pane, choose metrics. choose the ec2 metric namespace. select a metric dimension (for example, per-instance metrics). to sort the metrics, use the column heading. to graph a metric, select the check box next to the metric. to filter by resource, choose the resource id and then choose add to search. to filter by metric, choose the metric name and then choose add to search. use the  command to list the cloudwatch metrics for your instances. to list all the available metrics for amazon ec2 (aws cli)the following example specifies the  namespace to view all the metrics for amazon ec2. the following is example output: to list all the available metrics for an instance (aws cli)the following example specifies the  namespace and the  dimension to view the results for the specified instance only. to list a metric across all instances (aws cli)the following example specifies the  namespace and a metric name to view the results for the specified metric only. 
you can use the yum search command to search the descriptions of packages that are available in your configured repositories. this is especially helpful if you don't know the exact name of the package you want to install. simply append the keyword search to the command; for multiple word searches, wrap the search query with quotation marks. importantthis information applies to amazon linux. for information about other distributions, see their specific documentation. the following is example output for amazon linux 2. the following is example output for amazon linux. multiple word search queries in quotation marks only return results that match the exact query. if you don't see the expected package, simplify your search to one keyword and then scan the results. you can also try keyword synonyms to broaden your search. for more information about packages for amazon linux 2 and amazon linux, see the following: 
a security group acts as a virtual firewall for your ec2 instances to control incoming and outgoing traffic. inbound rules control the incoming traffic to your instance, and outbound rules control the outgoing traffic from your instance. when you launch an instance, you can specify one or more security groups. if you don't specify a security group, amazon ec2 uses the default security group. you can add rules to each security group that allow traffic to or from its associated instances. you can modify the rules for a security group at any time. new and modified rules are automatically applied to all instances that are associated with the security group. when amazon ec2 decides whether to allow traffic to reach an instance, it evaluates all of the rules from all of the security groups that are associated with the instance. when you launch an instance in a vpc, you must specify a security group that's created for that vpc. after you launch an instance, you can change its security groups. security groups are associated with network interfaces. changing an instance's security groups changes the security groups associated with the primary network interface (eth0). for more information, see  in the amazon vpc user guide. you can also change the security groups associated with any other network interface. for more information, see . if you have requirements that aren't fully met by security groups, you can maintain your own firewall on any of your instances in addition to using security groups. to allow traffic to a windows instance, see  in the amazon ec2 user guide for windows instances. contents the rules of a security group control the inbound traffic that's allowed to reach the instances that are associated with the security group. the rules also control the outbound traffic that's allowed to leave them. the following are the characteristics of security group rules: by default, security groups allow all outbound traffic.security group rules are always permissive; you can't create rules that deny access.security groups are stateful — if you send a request from your instance, the response traffic for that request is allowed to flow in regardless of inbound security group rules. for vpc security groups, this also means that responses to allowed inbound traffic are allowed to flow out, regardless of outbound rules. for more information, see .you can add and remove rules at any time. your changes are automatically applied to the instances that are associated with the security group. the effect of some rule changes can depend on how the traffic is tracked. for more information, see . when you associate multiple security groups with an instance, the rules from each security group are effectively aggregated to create one set of rules. amazon ec2 uses this set of rules to determine whether to allow access. you can assign multiple security groups to an instance. therefore, an instance can have hundreds of rules that apply. this might cause problems when you access the instance. we recommend that you condense your rules as much as possible.  for each rule, you specify the following: protocol: the protocol to allow. the most common protocols are 6 (tcp), 17 (udp), and 1 (icmp). port range: for tcp, udp, or a custom protocol, the range of ports to allow. you can specify a single port number (for example, ), or range of port numbers (for example, ).icmp type and code: for icmp, the icmp type and code.source or destination: the source (inbound rules) or destination (outbound rules) for the traffic. specify one of these options:an individual ipv4 address. you must use the  prefix length; for example, . an individual ipv6 address. you must use the  prefix length; for example, .a range of ipv4 addresses, in cidr block notation; for example, .a range of ipv6 addresses, in cidr block notation; for example, .a prefix list id, for example, . for more information, see  in the amazon vpc user guide.another security group. this allows instances that are associated with the specified security group to access instances associated with this security group. choosing this option does not add rules from the source security group to this security group. you can specify one of the following security groups:the current security groupa different security group for the same vpca different security group for a peer vpc in a vpc peering connection(optional) description: you can add a description for the rule, which can help you identify it later. a description can be up to 255 characters in length. allowed characters are a-z, a-z, 0-9, spaces, and ._-:/()#,@[]+=;{}!$*.when you specify a security group as the source or destination for a rule, the rule affects all instances that are associated with the security group. incoming traffic is allowed based on the private ip addresses of the instances that are associated with the source security group (and not the public ip or elastic ip addresses). for more information about ip addresses, see . if your security group rule references a security group in a peer vpc, and the referenced security group or vpc peering connection is deleted, the rule is marked as stale. for more information, see  in the amazon vpc peering guide. if there is more than one rule for a specific port, amazon ec2 applies the most permissive rule. for example, if you have a rule that allows access to tcp port 22 (ssh) from ip address , and another rule that allows access to tcp port 22 from everyone, everyone has access to tcp port 22. your security groups use connection tracking to track information about traffic to and from the instance. rules are applied based on the connection state of the traffic to determine if the traffic is allowed or denied. this approach allows security groups to be stateful. this means that responses to inbound traffic are allowed to flow out of the instance regardless of outbound security group rules, and vice versa. for example, if you initiate an icmp  command to your instance from your home computer, and your inbound security group rules allow icmp traffic, information about the connection (including the port information) is tracked. response traffic from the instance for the  command is not tracked as a new request, but rather as an established connection and is allowed to flow out of the instance, even if your outbound security group rules restrict outbound icmp traffic. not all flows of traffic are tracked. if a security group rule permits tcp or udp flows for all traffic () and there is a corresponding rule in the other direction that permits all response traffic () for all ports (0-65535), then that flow of traffic is not tracked. the response traffic is therefore allowed to flow based on the inbound or outbound rule that permits the response traffic, and not on tracking information.  in the following example, the security group has specific inbound rules for tcp and icmp traffic, and an outbound rule that allows all outbound traffic. |  |  | inbound rules |  | --- | | protocol type | port number | source ip |  | tcp  | 22 (ssh) | 203.0.113.1/32 |  | tcp  | 80 (http) | 0.0.0.0/0 |  | icmp | all | 0.0.0.0/0 |  | outbound rules |  | --- | | protocol type | port number | destination ip |  | all | all | 0.0.0.0/0 |  tcp traffic on port 22 (ssh) to and from the instance is tracked, because the inbound rule allows traffic from  only, and not all ip addresses (). tcp traffic on port 80 (http) to and from the instance is not tracked, because both the inbound and outbound rules allow all traffic (). icmp traffic is always tracked, regardless of rules. if you remove the outbound rule from the security group, all traffic to and from the instance is tracked, including traffic on port 80 (http). an untracked flow of traffic is immediately interrupted if the rule that enables the flow is removed or modified. for example, if you have an open (0.0.0.0/0) outbound rule, and you remove a rule that allows all (0.0.0.0/0) inbound ssh (tcp port 22) traffic to the instance (or modify it such that the connection would no longer be permitted), your existing ssh connections to the instance are immediately dropped. the connection was not previously being tracked, so the change will break the connection. on the other hand, if you have a narrower inbound rule that initially allows the ssh connection (meaning that the connection was tracked), but change that rule to no longer allow new connections from the address of the current ssh client, the existing connection will not be broken by changing the rule. for protocols other than tcp, udp, or icmp, only the ip address and protocol number is tracked. if your instance sends traffic to another host (host b), and host b initiates the same type of traffic to your instance in a separate request within 600 seconds of the original request or response, your instance accepts it regardless of inbound security group rules. your instance accepts it because it’s regarded as response traffic. to ensure that traffic is immediately interrupted when you remove a security group rule, or to ensure that all inbound traffic is subject to firewall rules, you can use a network acl for your subnet. network acls are stateless and therefore do not automatically allow response traffic. for more information, see  in the amazon vpc user guide. your aws account automatically has a default security group for the default vpc in each region. if you don't specify a security group when you launch an instance, the instance is automatically associated with the default security group for the vpc. a default security group is named , and it has an id assigned by aws. the following are the default rules for each default security group: allows all inbound traffic from other instances associated with the default security group. the security group specifies itself as a source security group in its inbound rules.allows all outbound traffic from the instance.you can add or remove inbound and outbound rules for any default security group. you can't delete a default security group. if you try to delete a default security group, you see the following error: .  if you don't want your instances to use the default security group, you can create your own security groups and specify them when you launch your instances. you can create multiple security groups to reflect the different roles that your instances play; for example, a web server or a database server.  when you create a security group, you must provide it with a name and a description. security group names and descriptions can be up to 255 characters in length, and are limited to the following characters: a-z, a-z, 0-9, spaces, and ._-:/()#,@[]+=&;{}!$* a security group name cannot start with . a security group name must be unique for the vpc. the following are the default rules for a security group that you create: allows no inbound trafficallows all outbound trafficafter you've created a security group, you can change its inbound rules to reflect the type of inbound traffic that you want to reach the associated instances. you can also change its outbound rules. for more information about the rules you can add to a security group, see . 
you can aggregate statistics for the ec2 instances in an auto scaling group. note that amazon cloudwatch cannot aggregate data across regions. metrics are completely separate between regions. this example shows you how to retrieve the total bytes written to disk for one auto scaling group. the total is computed for one-minute periods for a 24-hour interval across all ec2 instances in the specified auto scaling group. to display diskwritebytes for the instances in an auto scaling group (console) open the cloudwatch console at . in the navigation pane, choose metrics. choose the ec2 namespace and then choose by auto scaling group. choose the row for the diskwritebytes metric and the specific auto scaling group, which displays a graph for the metric for the instances in the auto scaling group. to name the graph, choose the pencil icon. to change the time range, select one of the predefined values or choose custom. to change the statistic or the period for the metric, choose the graphed metrics tab. choose the column heading or an individual value, and then choose a different value. to display diskwritebytes for the instances in an auto scaling group (aws cli)use the  command as follows. the following is example output: 
each instance that you launch has an instance identity document that provides information about the instance itself. you can use the instance identity document to validate the attributes of the instance. the instance identity document is generated when the instance is launched and it is exposed (in plaintext json format) through the instance metadata service. the ip address  is a link-local address and is valid only from the instance. for more information, see  on wikipedia. you can retrieve the instance identity document from a running instance at any time. the instance identity document includes the following information: to retrieve the plaintext instance identity documentconnect to the instance and run one of the following commands depending on the instance metadata service (imds) version used by the instance. the following is example output. if you intend to use the contents of the instance identity document for an important purpose, you should verify its contents and authenticity before using it. the plaintext instance identity document is accompanied by three hashed and encrypted signatures. you can use these signatures to verify the origin and authenticity of the instance identity document and the information that it includes. the following signatures are provided: base64-encoded signature—this is a base64-encoded sha256 hash of the instance identity document that is encrypted using an rsa key pair.pkcs7 signature—this is a sha1 hash of the instance identity document that is encrypted using a dsa key pair.rsa-2048 signature—this is a sha256 hash of the instance identity document that is encrypted using an rsa-2048 key pair.each signature is available at a different endpoint in the instance metadata. you can use any one of these signatures depending on your hashing and encryption requirements. to verify the signatures, you must use the corresponding aws public certificate. importantto validate the instance identity document using the base64-encoded signature or rsa2048 signature, you must request the corresponding aws public certificate from .  the following topics provide detailed steps for validating the instance identity document using each signature. 
empty ebs volumes receive their maximum performance the moment that they are created and do not require initialization (formerly known as pre-warming). for volumes that were created from snapshots, the storage blocks must be pulled down from amazon s3 and written to the volume before you can access them. this preliminary action takes time and can cause a significant increase in the latency of i/o operations the first time each block is accessed. volume performance is achieved after all blocks have been downloaded and written to the volume. importantwhile initializing  volumes that were created from snapshots, the performance of the volume may drop below 50 percent of its expected level, which causes the volume to display a  state in the i/o performance status check. this is expected, and you can ignore the  state on  volumes while you are initializing them. for more information, see . for most applications, amortizing the initialization cost over the lifetime of the volume is acceptable. to avoid this initial performance hit in a production environment, you can use one of the following options: force the immediate initialization of the entire volume. for more information, see .enable fast snapshot restore on a snapshot to ensure that the ebs volumes created from it are fully-initialized at creation and instantly deliver all of their provisioned performance. for more information, see .empty ebs volumes receive their maximum performance the moment that they are available and do not require initialization (formerly known as pre-warming). for volumes that have been created from snapshots, use the dd or fio utilities to read from all of the blocks on a volume. all existing data on the volume will be preserved. for information about initializing amazon ebs volumes on windows, see . to initialize a volume created from a snapshot on linux attach the newly-restored volume to your linux instance. use the lsblk command to list the block devices on your instance. here you can see that the new volume, , is attached, but not mounted (because there is no path listed under the  column). use the dd or fio utilities to read all of the blocks on the device. the dd command is installed by default on linux systems, but fio is considerably faster because it allows multi-threaded reads. notethis step may take several minutes up to several hours, depending on your ec2 instance bandwidth, the iops provisioned for the volume, and the size of the volume. [dd] the  (input file) parameter should be set to the drive you wish to initialize. the  (output file) parameter should be set to the linux null virtual device, . the  parameter sets the block size of the read operation; for optimal performance, this should be set to 1 mb. importantincorrect use of dd can easily destroy a volume's data. be sure to follow precisely the example command below. only the  parameter will vary depending on the name of the device you are reading. [fio] if you have fio installed on your system, use the following command to initialize your volume. the  (input file) parameter should be set to the drive you wish to initialize. to install fio on amazon linux, use the following command: to install fio on ubuntu, use the following command: when the operation is finished, you will see a report of the read operation. your volume is now ready for use. for more information, see . 
you can back up the data on your amazon ebs volumes to amazon s3 by taking point-in-time snapshots. snapshots are incremental backups, which means that only the blocks on the device that have changed after your most recent snapshot are saved. this minimizes the time required to create the snapshot and saves on storage costs by not duplicating data. when you delete a snapshot, only the data unique to that snapshot is removed. each snapshot contains all of the information that is needed to restore your data (from the moment when the snapshot was taken) to a new ebs volume. when you create an ebs volume based on a snapshot, the new volume begins as an exact replica of the original volume that was used to create the snapshot. the replicated volume loads data in the background so that you can begin using it immediately. if you access data that hasn't been loaded yet, the volume immediately downloads the requested data from amazon s3, and then continues loading the rest of the volume's data in the background. for more information, see . snapshot eventsyou can track the status of your ebs snapshots through cloudwatch events. for more information, see . multi-volume snapshotssnapshots can be used to create a backup of critical workloads, such as a large database or a file system that spans across multiple ebs volumes. multi-volume snapshots allow you to take exact point-in-time, data coordinated, and crash-consistent snapshots across multiple ebs volumes attached to an ec2 instance. you are no longer required to stop your instance or to coordinate between volumes to ensure crash consistency, because snapshots are automatically taken across multiple ebs volumes. for more information, see the steps for creating a multi-volume ebs snapshot under . snapshot pricingcharges for your snapshots are based on the amount of data stored. because snapshots are incremental, deleting a snapshot might not reduce your data storage costs. data referenced exclusively by a snapshot is removed when that snapshot is deleted, but data referenced by other snapshots is preserved. for more information, see  in the aws billing and cost management user guide. topics this section provides illustrations of how an ebs snapshot captures the state of a volume at a point in time, and also how successive snapshots of a changing volume create a history of those changes. in the diagram below, volume 1 is shown at three points in time. a snapshot is taken of each of these three volume states.  in state 1, the volume has 10 gib of data. because snap a is the first snapshot taken of the volume, the entire 10 gib of data must be copied.in state 2, the volume still contains 10 gib of data, but 4 gib have changed. snap b needs to copy and store only the 4 gib that changed after snap a was taken. the other 6 gib of unchanged data, which are already copied and stored in snap a, are referenced by snap b rather than (again) copied. this is indicated by the dashed arrow.in state 3, 2 gib of data have been added to the volume, for a total of 12 gib. snap c needs to copy the 2 gib that were added after snap b was taken. as shown by the dashed arrows, snap c also references 4 gib of data stored in snap b, and 6 gib of data stored in snap a. the total storage required for the three snapshots is 16 gib. relations among multiple snapshots of a volume   noteif you copy a snapshot and encrypt it to a new cmk, a complete (non-incremental) copy is always created, resulting in additional delay and storage costs. for more information about how data is managed when you delete a snapshot, see . you can share a snapshot across aws accounts by modifying its access permissions. you can make copies of your own snapshots as well as snapshots that have been shared with you. for more information, see . a snapshot is constrained to the aws region where it was created. after you create a snapshot of an ebs volume, you can use it to create new volumes in the same region. for more information, see . you can also copy snapshots across regions, making it possible to use multiple regions for geographical expansion, data center migration, and disaster recovery. you can copy any accessible snapshot that has a  status. for more information, see . ebs snapshots fully support ebs encryption. snapshots of encrypted volumes are automatically encrypted.volumes that you create from encrypted snapshots are automatically encrypted.volumes that you create from an unencrypted snapshot that you own or have access to can be encrypted on-the-fly.when you copy an unencrypted snapshot that you own, you can encrypt it during the copy process.when you copy an encrypted snapshot that you own or have access to, you can reencrypt it with a different key during the copy process.the first snapshot you take of an encrypted volume that has been created from an unencrypted snapshot is always a full snapshot.the first snapshot you take of a reencrypted volume, which has a different cmk compared to the source snapshot, is always a full snapshot.noteif you copy a snapshot and encrypt it to a new cmk, a complete (non-incremental) copy is always created, resulting in additional delay and storage costs. complete documentation of possible snapshot encryption scenarios is provided in  and in . for more information, see . 
to use a spot fleet, you create a spot fleet request that includes the target capacity, an optional on-demand portion, one or more launch specifications for the instances, and the maximum price that you are willing to pay. amazon ec2 attempts to maintain your spot fleet's target capacity as spot prices change. for more information, see . there are two types of spot fleet requests:  and . you can create a spot fleet to submit a one-time request for your desired capacity, or require it to maintain a target capacity over time. both types of requests benefit from spot fleet's allocation strategy. when you make a one-time request, spot fleet places the required requests but does not attempt to replenish spot instances if capacity is diminished. if capacity is not available, spot fleet does not submit requests in alternative spot pools. to maintain a target capacity, spot fleet places requests to meet the target capacity and automatically replenish any interrupted instances. it is not possible to modify the target capacity of a one-time request after it's been submitted. to change the target capacity, cancel the request and submit a new one. a spot fleet request remains active until it expires or you cancel it. when you cancel a spot fleet request, you may specify whether canceling your spot fleet request terminates the spot instances in your spot fleet. each launch specification includes the information that amazon ec2 needs to launch an instance, such as an ami, instance type, subnet or availability zone, and one or more security groups. topics a spot fleet request can be in one of the following states:  – the spot fleet request is being evaluated and amazon ec2 is preparing to launch the target number of instances. – the spot fleet has been validated and amazon ec2 is attempting to maintain the target number of running spot instances. the request remains in this state until it is modified or canceled. – the spot fleet request is being modified. the request remains in this state until the modification is fully processed or the spot fleet is canceled. a one-time  cannot be modified, and this state does not apply to such spot requests. – the spot fleet is canceled and does not launch additional spot instances. its existing spot instances continue to run until they are interrupted or terminated. the request remains in this state until all instances are interrupted or terminated. – the spot fleet is canceled and its spot instances are terminating. the request remains in this state until all instances are terminated. – the spot fleet is canceled and has no running spot instances. the spot fleet request is deleted two days after its instances were terminated.the following illustration represents the transitions between the request states. if you exceed your spot fleet limits, the request is canceled immediately.  spot fleet checks the health status of the spot instances in the fleet every two minutes. the health status of an instance is either  or . spot fleet determines the health status of an instance using the status checks provided by amazon ec2. if the status of either the instance status check or the system status check is  for three consecutive health checks, the health status of the instance is . otherwise, the health status is . for more information, see . you can configure your spot fleet to replace unhealthy instances. after enabling health check replacement, an instance is replaced after its health status is reported as . the spot fleet could go below its target capacity for up to a few minutes while an unhealthy instance is being replaced. requirements health check replacement is supported only with spot fleets that maintain a target capacity, not with one-time spot fleets.you can configure your spot fleet to replace unhealthy instances only when you create it.iam users can use health check replacement only if they have permission to call the  action.before you create a spot fleet request, review . use these best practices when you plan your spot fleet request so that you can provision the type of instances you want at the lowest possible price. we also recommend that you do the following: determine whether you want to create a spot fleet that submits a one-time request for the desired target capacity, or one that maintains a target capacity over time.determine the instance types that meet your application requirements.determine the target capacity for your spot fleet request. you can set the target capacity in instances or in custom units. for more information, see .determine what portion of the spot fleet target capacity must be on-demand capacity. you can specify 0 for on-demand capacity.determine your price per unit, if you are using instance weighting. to calculate the price per unit, divide the price per instance hour by the number of units (or weight) that this instance represents. if you are not using instance weighting, the default price per unit is the price per instance hour.review the possible options for your spot fleet request. for more information, see the  command in the aws cli command reference. for additional examples, see .if your iam users will create or manage a spot fleet, you need to grant them the required permissions. if you use the amazon ec2 console to create a spot fleet, it creates a service-linked role named  and a role named  that grant the spot fleet the permissions to request, launch, terminate, and tag resources on your behalf. if you use the aws cli or an api, you must ensure that these roles exist. use the following instructions to grant the required permissions and create the roles. topics if your iam users will create or manage a spot fleet, be sure to grant them the required permissions as follows. to grant an iam user permissions for spot fleet open the iam console at . in the navigation pane, choose policies, create policy. on the create policy page, choose json, and replace the text with the following. the preceding example policy grants an iam user the permissions required for most spot fleet use cases. to limit the user to specific api actions, specify only those api actions instead. required ec2 and iam apis the following apis must be included in the policy:  – required to launch instances in a spot fleet – required to tag the spot fleet request, instances, or volumes – required to specify the spot fleet role – required to create the service-linked role – required to enumerate existing iam roles – required to enumerate existing instance profiles importantif you specify a role for the iam instance profile in the launch specification or launch template, you must grant the iam user the permission to pass the role to the service. to do this, in the iam policy include  as a resource for the  action. for more information, see  in the iam user guide.spot fleet apis add the following spot fleet api actions to your policy, as needed: optional iam apis (optional) to enable an iam user to create roles or instance profiles using the iam console, you must add the following actions to the policy: choose review policy. on the review policy page, enter a policy name and description, and choose create policy. in the navigation pane, choose users and select the user. choose permissions, add permissions. choose attach existing policies directly. select the policy that you created earlier and choose next: review. choose add permissions. amazon ec2 uses service-linked roles for the permissions that it requires to call other aws services on your behalf. a service-linked role is a unique type of iam role that is linked directly to an aws service. service-linked roles provide a secure way to delegate permissions to aws services because only the linked service can assume a service-linked role. for more information, see  in the iam user guide. amazon ec2 uses the service-linked role named awsserviceroleforec2spotfleet to launch and manage instances on your behalf. importantif you specify an  or an  in your spot fleet, you must grant the awsserviceroleforec2spotfleet role permission to use the cmk so that amazon ec2 can launch instances on your behalf. for more information, see . amazon ec2 uses awsserviceroleforec2spotfleet to complete the following actions:  - request spot instances - launch instances - terminate instances - describe amazon machine images (amis) for the instances - describe the status of the instances - describe the subnets for the instances - add tags to the spot fleet request, instances, and volumes - add the specified instances to the specified load balancer - register the specified targets with the specified target groupunder most circumstances, you don't need to manually create a service-linked role. amazon ec2 creates the awsserviceroleforec2spotfleet service-linked role the first time you create a spot fleet using the console.  if you use the aws cli or an api, you must ensure that this role exists. if you had an active spot fleet request before october 2017, when amazon ec2 began supporting this service-linked role, amazon ec2 created the awsserviceroleforec2spotfleet role in your aws account. for more information, see  in the iam user guide. ensure that this role exists before you use the aws cli or an api to create a spot fleet. to create the role, use the iam console as follows. to manually create the awsserviceroleforec2spotfleet service-linked role open the iam console at . in the navigation pane, choose roles. choose create role. on the select type of trusted entity page, choose ec2, ec2 - spot fleet, next: permissions. on the next page, choose next:review. on the review page, choose create role. if you no longer need to use spot fleet, we recommend that you delete the awsserviceroleforec2spotfleet role. after this role is deleted from your account, amazon ec2 will create the role again if you request a spot fleet using the console. for more information, see  in the iam user guide. if you specify an  or an  in your spot fleet request and you use a customer managed customer master key (cmk) for encryption, you must grant the awsserviceroleforec2spotfleet role permission to use the cmk so that amazon ec2 can launch instances on your behalf. to do this, you must add a grant to the cmk, as shown in the following procedure. when providing permissions, grants are an alternative to key policies. for more information, see  and  in the aws key management service developer guide. to grant the awsserviceroleforec2spotfleet role permissions to use the cmk use the  command to add a grant to the cmk and to specify the principal (the awsserviceroleforec2spotfleet service-linked role) that is given permission to perform the operations that the grant permits. the cmk is specified by the  parameter and the arn of the cmk. the principal is specified by the  parameter and the arn of the awsserviceroleforec2spotfleet service-linked role. the  iam role grants the spot fleet permission to tag the spot fleet request, instances, and volumes. for more information, see . importantif you choose to tag instances in the fleet and you choose to maintain target capacity (the spot fleet request is of type ), the differences in permissions of the iam user and the  might lead to inconsistent tagging behavior of instances in the fleet. if the  does not include the  permission, some of the instances launched by the fleet might not be tagged. while we are working to fix this inconsistency, to ensure that all instances launched by the fleet are tagged, we recommend that you use the  role for the . alternatively, to use an existing role, attach the  aws managed policy to the existing role. otherwise, you need to manually add the  permission to your existing policy. to create the iam role for tagging a spot fleet open the iam console at . in the navigation pane, choose roles. on the select type of trusted entity page, choose aws service, ec2, ec2 - spot fleet tagging, next: permissions. on the attached permissions policy page, choose next:review. on the review page, type a name for the role (for example, aws-ec2-spot-fleet-tagging-role) and choose create role. using the aws management console, quickly create a spot fleet request by choosing only your application or task need and minimum compute specs. amazon ec2 configures a fleet that best meets your needs and follows spot best practice. for more information, see . otherwise, you can modify any of the default settings. for more information, see . follow these steps to quickly create a spot fleet request. to create a spot fleet request using the recommended settings (console) open the spot console at . if you are new to spot, you see a welcome page; choose get started. otherwise, choose request spot instances. for tell us your application or task need, choose load balancing workloads, flexible workloads, big data workloads, or defined duration workloads. under configure your instances, for minimum compute unit, choose the minimum hardware specifications (vcpus, memory, and storage) that you need for your application or task, either as specs or as an instance type. for as specs, specify the required number of vcpus and amount of memory.for as an instance type, accept the default instance type, or choose change instance type to choose a different instance type.under tell us how much capacity you need, for total target capacity, specify the number of units to request for target capacity. you can choose instances or vcpus. review the recommended fleet request settings based on your application or task selection, and choose launch. you can create a spot fleet using the parameters that you define. to create a spot fleet request using defined parameters (console) open the spot console at . if you are new to spot, you see a welcome page; choose get started. otherwise, choose request spot instances. for tell us your application or task need, choose load balancing workloads, flexible workloads, big data workloads, or defined duration workloads. for configure your instances, do the following: (optional) for launch template, choose a launch template. the launch template must specify an amazon machine image (ami), as you cannot override the ami using spot fleet if you specify a launch template. importantif you intend to specify optional on-demand portion, you must choose a launch template. for ami, choose one of the basic amis provided by aws, or choose search for ami to use an ami from our user community, the aws marketplace, or one of your own. for minimum compute unit, choose the minimum hardware specifications (vcpus, memory, and storage) that you need for your application or task, either as specs or as an instance type. for as specs, specify the required number of vcpus and amount of memory.for as an instance type, accept the default instance type, or choose change instance type to choose a different instance type.for network, choose an existing vpc or create a new one. [existing vpc] choose the vpc. [new vpc] choose create new vpc to go the amazon vpc console. when you are done, return to the wizard and refresh the list. (optional) for availability zone, let aws choose the availability zones for your spot instances, or specify one or more availability zones. if you have more than one subnet in an availability zone, choose the appropriate subnet from subnet. to add subnets, choose create new subnet to go to the amazon vpc console. when you are done, return to the wizard and refresh the list. (optional) for key pair name, choose an existing key pair or create a new one. [existing key pair] choose the key pair. [new key pair] choose create new key pair to go the amazon vpc console. when you are done, return to the wizard and refresh the list. (optional) for additional configurations, do the following: (optional) to add storage, specify additional instance store volumes or amazon ebs volumes, depending on the instance type. (optional) to enable amazon ebs optimization, for ebs-optimized, choose launch ebs-optimized instances. (optional) to add temporary block-level storage for your instances, for instance store, choose attach at launch. (optional) by default, basic monitoring is enabled for your instances. to enable detailed monitoring, for monitoring, choose enable cloudwatch detailed monitoring. (optional) to replace unhealthy instances, for health check, choose replace unhealthy instances. to enable this option, you must first choose maintain target capacity. (optional) to run a dedicated spot instance, for tenancy, choose dedicated - run a dedicated instance. (optional) for security groups, choose one or more security groups or create a new one. [existing security group] choose one or more security groups. [new security group] choose create new security group to go the amazon vpc console. when you are done, return to the wizard and refresh the list. (optional) to make your instances reachable from the internet, for auto-assign ipv4 public ip, choose enable. (optional) to launch your spot instances with an iam role, for iam instance profile, choose the role. (optional) to run a start-up script, copy it to user data. (optional) to add a tag, choose add new tag and enter the key and value for the tag. repeat for each tag. for each tag, to tag the instances and the spot fleet request with the same tag, ensure that both instance tags and fleet tags are selected. to tag only the instances launched by the fleet, clear fleet tags. to tag only the spot fleet request, clear instance tags.  for tell us how much capacity you need, do the following: for total target capacity, specify the number of units to request for target capacity. you can choose instances or vcpus. to specify a target capacity of 0 so that you can add capacity later, choose maintain target capacity. (optional) for optional on-demand portion, specify the number of on-demand units to request. the number must be less than the total target capacity. amazon ec2 calculates the difference, and allocates the difference to spot units to request. importantto specify an optional on-demand portion, you must first choose a launch template. (optional) by default, the spot service terminates spot instances when they are interrupted. to maintain the target capacity, choose maintain target capacity. you can then specify that the spot service terminates, stops, or hibernates spot instances when they are interrupted. to do so, choose the corresponding option from interruption behavior. for fleet request settings, do the following: review the fleet request and fleet allocation strategy based on your application or task selection. to change the instance types or allocation strategy, clear apply recommendations. (optional) to remove instance types, for fleet request, choose remove. to add instance types, choose select instance types. (optional) for fleet allocation strategy, choose the strategy that meets your needs. for more information, see . for additional request details, do the following: review the additional request details. to make changes, clear apply defaults. (optional) for iam fleet role, you can use the default role or choose a different role. to use the default role after changing the role, choose use default role. (optional) for maximum price, you can use the default maximum price (the on-demand price) or specify the maximum price you are willing to pay. if your maximum price is lower than the spot price for the instance types that you selected, your spot instances are not launched. (optional) to create a request that is valid only during a specific time period, edit request valid from and request valid until. (optional) by default, we terminate your spot instances when the request expires. to keep them running after your request expires, clear terminate the instances when the request expires. (optional) to register your spot instances with a load balancer, choose receive traffic from one or more load balancers and choose one or more classic load balancers or target groups. (optional) to download a copy of the launch configuration for use with the aws cli, choose json config. choose launch. the spot fleet request type is . when the request is fulfilled, requests of type  are added, where the state is  and the status is . to create a spot fleet request using the aws cli use the  command to create a spot fleet request.for example configuration files, see . the following is example output: to help categorize and manage your spot fleet requests, you can tag them with custom metadata. you can assign a tag to a spot fleet request when you create it, or afterward. you can assign tags using the amazon ec2 console or a command line tool. when you tag a spot fleet request, the instances and volumes that are launched by the spot fleet are not automatically tagged. you need to explicitly tag the instances and volumes launched by the spot fleet. you can choose to assign tags to only the spot fleet request, or to only the instances launched by the fleet, or to only the volumes attached to the instances launched by the fleet, or to all three. notevolume tags are only supported for volumes that are attached to on-demand instances. you can't tag volumes that are attached to spot instances. for more information about how tags work, see . topics grant the iam user the permission to tag resources. for more information, see . to grant an iam user the permission to tag resourcescreate a iam policy that includes the following: the  action. this grants the iam user permission to create tags.the  action. this grants the iam user permission to create a spot fleet request.for , you must specify . this allows users to tag all resource types.importantwe currently do not support resource-level permissions for the  resource. if you specify  as a resource, you will get an unauthorized exception when you try to tag the fleet. the following example illustrates how not to set the policy.    to tag a new spot fleet request using the console follow the  procedure. to add a tag, expand additional configurations, choose add new tag, and enter the key and value for the tag. repeat for each tag. for each tag, you can tag the spot fleet request and the instances with the same tag. to tag both, ensure that both instance tags and fleet tags are selected. to tag only the spot fleet request, clear instance tags. to tag only the instances launched by the fleet, clear fleet tags. complete the required fields to create a spot fleet request, and then choose launch. for more information, see . to tag a new spot fleet request using the aws clito tag a spot fleet request when you create it, configure the spot fleet request configuration as follows: specify the tags for the spot fleet request in .for , specify . if you specify another value, the fleet request will fail.for , specify the key-value pair. you can specify more than one key-value pair.in the following example, the spot fleet request is tagged with two tags: key=environment and value=production, and key=cost-center and value=123. to tag a new spot fleet request and the instances and volumes that it launches using the aws clito tag a spot fleet request when you create it, and to tag the instances and volumes when they are launched by the fleet, configure the spot fleet request configuration as follows: spot fleet request tags: specify the tags for the spot fleet request in .for , specify . if you specify another value, the fleet request will fail.for , specify the key-value pair. you can specify more than one key-value pair.instance tags: specify the tags for the instances in .for , specify . if you specify another value, the fleet request will fail.for , specify the key-value pair. you can specify more than one key-value pair. alternatively, you can specify the tags for the instance in the  that is referenced in the spot fleet request. volume tags: specify the tags for the volumes in the  that is referenced in the spot fleet request. volume tagging in  is not supported.in the following example, the spot fleet request is tagged with two tags: key=environment and value=production, and key=cost-center and value=123. the instances that are launched by the fleet are tagged with one tag (which is the same as one of the tags for the spot fleet request): key=cost-center and value=123. to tag instances launched by a spot fleet using the aws clito tag instances when they are launched by the fleet, you can either specify the tags in the  that is referenced in the spot fleet request, or you can specify the tags in the spot fleet request configuration as follows: specify the tags for the instances in .for , specify . if you specify another value, the fleet request will fail.for , specify the key-value pair. you can specify more than one key-value pair.in the following example, the instances that are launched by the fleet are tagged with one tag: key=cost-center and value=123. to tag volumes attached to on-demand instances launched by a spot fleet using the aws clito tag volumes when they are created by the fleet, you must specify the tags in the  that is referenced in the spot fleet request. notevolume tags are only supported for volumes that are attached to on-demand instances. you can't tag volumes that are attached to spot instances.volume tagging in  is not supported. to tag an existing spot fleet request using the console after you have created a spot fleet request, you can add tags to the fleet request using the console. open the spot console at . select your spot fleet request. choose the tags tab and choose create tag. to tag an existing spot fleet request using the aws cliyou can use the  command to tag existing resources. in the following example, the existing spot fleet request is tagged with key=purpose and value=test. to view spot fleet request tags using the console open the spot console at . select your spot fleet request and choose the tags tab. to describe spot fleet request tagsuse the  command to view the tags for the specified resource. in the following example, you describe the tags for the specified spot fleet request. you can also view the tags of a spot fleet request by describing the spot fleet request. use the  command to view the configuration of the specified spot fleet request, which includes any tags that were specified for the fleet request. the spot fleet launches spot instances when your maximum price exceeds the spot price and capacity is available. the spot instances run until they are interrupted or you terminate them. to monitor your spot fleet (console) open the amazon ec2 console at . in the navigation pane, choose spot requests. select your spot fleet request. to see the configuration details, choose description. to list the spot instances for the spot fleet, choose instances. to view the history for the spot fleet, choose history. to monitor your spot fleet (aws cli)use the  command to describe your spot fleet requests. use the  command to describe the spot instances for the specified spot fleet. use the  command to describe the history for the specified spot fleet request. you can modify an active spot fleet request to complete the following tasks: increase the target capacity and on-demand portiondecrease the target capacity and on-demand portionnoteyou can't modify a one-time spot fleet request. you can only modify a spot fleet request if you selected maintain target capacity when you created the spot fleet request. when you increase the target capacity, the spot fleet launches additional spot instances. when you increase the on-demand portion, the spot fleet launches additional on-demand instances. when you increase the target capacity, the spot fleet launches the additional spot instances according to the allocation strategy for its spot fleet request. if the allocation strategy is , the spot fleet launches the instances from the lowest-priced spot instance pool in the spot fleet request. if the allocation strategy is , the spot fleet distributes the instances across the pools in the spot fleet request. when you decrease the target capacity, the spot fleet cancels any open requests that exceed the new target capacity. you can request that the spot fleet terminate spot instances until the size of the fleet reaches the new target capacity. if the allocation strategy is , the spot fleet terminates the instances with the highest price per unit. if the allocation strategy is , the spot fleet terminates instances across the pools. alternatively, you can request that the spot fleet keep the fleet at its current size, but not replace any spot instances that are interrupted or that you terminate manually. when a spot fleet terminates an instance because the target capacity was decreased, the instance receives a spot instance interruption notice. to modify a spot fleet request (console) open the spot console at . select your spot fleet request. choose actions, modify target capacity. in modify target capacity, do the following: enter the new target capacity and on-demand portion. (optional) if you are decreasing the target capacity but want to keep the fleet at its current size, clear terminate instances. choose submit. to modify a spot fleet request using the aws cliuse the  command to update the target capacity of the specified spot fleet request. you can modify the previous command as follows to decrease the target capacity of the specified spot fleet without terminating any spot instances as a result. when you are finished using your spot fleet, you can cancel the spot fleet request. this cancels all spot requests associated with the spot fleet, so that no new spot instances are launched for your spot fleet. you must specify whether the spot fleet should terminate its spot instances. if you terminate the instances, the spot fleet request enters the  state. otherwise, the spot fleet request enters the  state and the instances continue to run until they are interrupted or you terminate them manually. to cancel a spot fleet request (console) open the spot console at . select your spot fleet request. choose actions, cancel spot request. in cancel spot request, verify that you want to cancel the spot fleet. to keep the fleet at its current size, clear terminate instances. when you are ready, choose confirm. to cancel a spot fleet request using the aws cliuse the  command to cancel the specified spot fleet request and terminate the instances. the following is example output: you can modify the previous command as follows to cancel the specified spot fleet request without terminating the instances. the following is example output: 
the following instructions explain how to connect to your linux instance using ec2 instance connect. topics limitations the following linux distributions are supported:amazon linux 2 (any version)ubuntu 16.04 or laterto connect using the browser-based client, the instance must have a public ipv4 address.to connect using the ec2 instance connect cli, your instance does not need to have a public ipv4 address because the private ip address can be used. if your instance has both a public and private ip address, the api first tries to connect using the public ip address.ec2 instance connect does not support connecting using an ipv6 address.the safari browser is currently not supported.prerequisites install instance connect on your instance. for more information, see . (optional) install an ssh client on your local computer. there is no need to install an ssh client if users only use the console or the ec2 instance connect cli to connect to an instance. your local computer most likely has an ssh client installed by default. you can check for an ssh client by typing ssh at the command line. if your local computer doesn't recognize the command, you can install an ssh client. for information about installing an ssh client on linux or macos x, see . for information about installing an ssh client on windows 10, see . (optional) install the ec2 instance connect cli on your local computer. there is no need to install the ec2 instance connect cli if users only use the console or an ssh client to connect to an instance. for more information, see . you can connect to an instance using the browser-based client by selecting the instance from the amazon ec2 console and choosing to connect using ec2 instance connect. instance connect handles the permissions and provides a successful connection. to connect to your instance using the browser-based client from the amazon ec2 console open the amazon ec2 console at . in the navigation pane, choose instances. select the instance and choose connect. choose ec2 instance connect (browser-based ssh connection), connect. a window opens, and you are connected to your instance. you can connect to an instance using the ec2 instance connect cli by providing only the instance id, while the instance connect cli performs the following three actions in one call: it generates a one-time-use ssh public key, pushes the key to the instance where it remains for 60 seconds, and connects the user to the instance. you can use basic ssh/sftp commands with the instance connect cli. note is not supported when using mssh. when using the mssh command to connect to your instance, you do not need to specify any kind of identity file because instance connect manages the key pair. to connect to an instance using the ec2 instance connect cliuse the mssh command with the instance id as follows. you do not need to specify the user name for the ami. to connect to an instance using the ec2 instance connect cliuse the mssh command with the instance id and the default user name for the ubuntu ami as follows. you must specify the user name for the ami or you get the following error: authentication failed. you can use your own ssh key and connect to your instance from the ssh client of your choice while using the ec2 instance connect api. this enables you to benefit from the instance connect capability to push a public key to the instance. requirementthe supported rsa key types are openssh and ssh2. the supported lengths are 2048 and 4096. for more information, see . to connect to your instance using your own key and any ssh client (optional) generate new ssh private and public keys you can generate new ssh private and public keys,  and , using the following command: push your ssh public key to the instance    use the  command to push your ssh public key to the instance. if you launched your instance using amazon linux 2, the default user name for the ami is . if you launched your instance using ubuntu, the default user name for the ami is .    the following example pushes the public key to the specified instance in the specified availability zone, to authenticate : connect to the instance using your private key use the ssh command to connect to the instance using the private key before the public key is removed from the instance metadata (you have 60 seconds before it is removed). specify the private key that corresponds to the public key, the default user name for the ami that you used to launch your instance, and the instance's public dns name. add the  option to ensure that only the files in the ssh config and the specified key are used for the connection.  
idempotency ensures that an api request completes only once. with an idempotent request, if the original request completes successfully. the subsequent retries return the result from the original successful request and they have no additional effect. the  api supports idempotency using a client token. a client token is a unique string that you specify when you make an api request. if you retry an api request with the same client token and the same request parameters after it has completed successfully, the result of the original request is returned. if you retry a request with the same client token, but change one or more of the request parameters, the  error is returned. if you do not specify your own client token, the aws sdks automatically generates a client token for the request to ensure that it is idempotent. a client token can be any string that includes up to up to 64 ascii characters. you should not reuse the same client tokens for different requests. to make an idempotent startsnapshot request with your own client token using the apispecify the  request parameter. to make an idempotent startsnapshot request with your own client token using the aws clispecify the  request parameter. 
with target tracking scaling policies, you select a metric and set a target value. spot fleet creates and manages the cloudwatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value. the scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value. in addition to keeping the metric close to the target value, a target tracking scaling policy also adjusts to the fluctuations in the metric due to a fluctuating load pattern and minimizes rapid fluctuations in the capacity of the fleet. you can create multiple target tracking scaling policies for a spot fleet, provided that each of them uses a different metric. the fleet scales based on the policy that provides the largest fleet capacity. this enables you to cover multiple scenarios and ensure that there is always enough capacity to process your application workloads. to ensure application availability, the fleet scales out proportionally to the metric as fast as it can, but scales in more gradually. when a spot fleet terminates an instance because the target capacity was decreased, the instance receives a spot instance interruption notice. do not edit or delete the cloudwatch alarms that spot fleet manages for a target tracking scaling policy. spot fleet deletes the alarms automatically when you delete the target tracking scaling policy. limitation the spot fleet request must have a request type of . automatic scaling is not supported for one-time requests or spot blocks.to configure a target tracking policy (console) open the amazon ec2 console at . in the navigation pane, choose spot requests. select your spot fleet request and choose auto scaling. if automatic scaling is not configured, choose configure. use scale capacity between to set the minimum and maximum capacity for your fleet. automatic scaling does not scale your fleet below the minimum capacity or above the maximum capacity. for policy name, type a name for the policy. choose a target metric. enter a target value for the metric. (optional) set cooldown period to modify the default cooldown period. (optional) select disable scale-in to omit creating a scale-in policy based on the current configuration. you can create a scale-in policy using a different configuration. choose save. to configure a target tracking policy using the aws cli register the spot fleet request as a scalable target using the  command. create a scaling policy using the  command. 
dedicated host sharing enables dedicated host owners to share their dedicated hosts with other aws accounts or within an aws organization. this enables you to create and manage dedicated hosts centrally, and share the dedicated host across multiple aws accounts or within your aws organization. in this model, the aws account that owns the dedicated host (owner) shares it with other aws accounts (consumers). consumers can launch instances onto dedicated hosts that are shared with them in the same way that they would launch instances onto dedicated hosts that they allocate in their own account. the owner is responsible for managing the dedicated host and the instances that they launch onto it. owners can't modify instances that consumers launch onto shared dedicated hosts. consumers are responsible for managing the instances that they launch onto dedicated hosts shared with them. consumers can't view or modify instances owned by other consumers or by the dedicated host owner, and they can't modify dedicated hosts that are shared with them. a dedicated host owner can share a dedicated host with: specific aws accounts inside or outside of its aws organizationan organizational unit inside its aws organizationits entire aws organizationtopics to share a dedicated host, you must own it in your aws account. you can't share a dedicated host that has been shared with you.to share a dedicated host with your aws organization or an organizational unit in your aws organization, you must enable sharing with aws organizations. for more information, see  in the aws ram user guide.you can't share dedicated hosts that have been allocated for the following instance types: , , , , and . dedicated host sharing integrates with aws resource access manager (aws ram). aws ram is a service that enables you to share your aws resources with any aws account or through aws organizations. with aws ram, you share resources that you own by creating a resource share. a resource share specifies the resources to share, and the consumers with whom to share them. consumers can be individual aws accounts, or organizational units or an entire organization from aws organizations. for more information about aws ram, see the . to ensure that resources are distributed across the availability zones for a region, we independently map availability zones to names for each account. this could lead to availability zone naming differences across accounts. for example, the availability zone  for your aws account might not have the same location as  for another aws account. to identify the location of your dedicated hosts relative to your accounts, you must use the availability zone id (az id). the availability zone id is a unique and consistent identifier for an availability zone across all aws accounts. for example,  is an availability zone id for the  region and it is the same location in every aws account. to view the availability zone ids for the availability zones in your account open the aws ram console at . the availability zone ids for the current region are displayed in the your az id panel on the right-hand side of the screen. when an owner shares a dedicated host, it enables consumers to launch instances on the host. consumers can launch as many instances onto the shared host as its available capacity allows. importantnote that you are responsible for ensuring that you have appropriate license rights to share any byol licenses on your dedicated hosts. if you share a dedicated host with auto-placement enabled, keep the following in mind as it could lead to unintended dedicated host usage: if consumers launch instances with dedicated host tenancy and they do not have capacity on a dedicated host that they own in their account, the instance is automatically launched onto the shared dedicated host.to share a dedicated host, you must add it to a resource share. a resource share is an aws ram resource that lets you share your resources across aws accounts. a resource share specifies the resources to share, and the consumers with whom they are shared. you can add the dedicated host to an existing resource, or you can add it to a new resource share. if you are part of an organization in aws organizations and sharing within your organization is enabled, consumers in your organization are automatically granted access to the shared dedicated host. otherwise, consumers receive an invitation to join the resource share and are granted access to the shared dedicated host after accepting the invitation. noteafter you share a dedicated host, it could take a few minutes for consumers to have access to it. you can share a dedicated host that you own by using one of the following methods. to share a dedicated host that you own using the amazon ec2 console open the amazon ec2 console at . in the navigation pane, choose dedicated hosts. choose the dedicated host to share and choose actions, share host. select the resource share to which to add the dedicated host and choose share host. it could take a few minutes for consumers to get access to the shared host. to share a dedicated host that you own using the aws ram consolesee  in the aws ram user guide. to share a dedicated host that you own using the aws cliuse the  command. the dedicated host owner can unshare a shared dedicated host at any time. when you unshare a shared dedicated host, the following rules apply: consumers with whom the dedicated host was shared can no longer launch new instances onto it.instances owned by consumers that were running on the dedicated host at the time of unsharing continue to run but are scheduled for . consumers receive retirement notifications for the instances and they have two weeks to take action on the notifications. however, if the dedicated host is reshared with the consumer within the retirement notice period, the instance retirements are cancelled.to unshare a shared dedicated host that you own, you must remove it from the resource share. you can do this by using one of the following methods. to unshare a shared dedicated host that you own using the amazon ec2 console open the amazon ec2 console at . in the navigation pane, choose dedicated hosts. choose the dedicated host to unshare and choose the sharing tab. the sharing tab lists the resource shares to which the dedicated host has been added. select the resource share from which to remove the dedicated host and choose remove host from resource share. to unshare a shared dedicated host that you own using the aws ram consolesee  in the aws ram user guide. to unshare a shared dedicated host that you own using the aws cliuse the  command. owners and consumers can identify shared dedicated hosts using one of the following methods. to identify a shared dedicated host using the amazon ec2 console open the amazon ec2 console at . in the navigation pane, choose dedicated hosts. the screen lists dedicated hosts that you own and dedicated hosts that are shared with you. the owner column shows the aws account id of the dedicated host owner.  to identify a shared dedicated host using the aws cliuse the  command. the command returns the dedicated hosts that you own and dedicated hosts that are shared with you. owners and consumers can view the instances running on a shared dedicated host at any time using one of the following methods. to view the instances running on a shared dedicated host using the amazon ec2 console open the amazon ec2 console at . in the navigation pane, choose dedicated hosts. select the dedicated host for which to view the instances and choose instances. the tab lists the instances that are running on the host. owners see all of the instances running on the host, including instances launched by consumers. consumers only see running instances that they launched onto the host. the owner column shows the aws account id of the account that launched the instance. to view the instances running on a shared dedicated host using the aws cliuse the  command. the command returns the instances running on each dedicated host. owners see all of the instances running on the host. consumers only see running instances that they launched on the shared hosts.  shows the aws account id of the instance owner. owners are responsible for managing their shared dedicated hosts and the instances that they launch onto them. owners can view all instances running on the shared dedicated host, including those launched by consumers. however, owners can't take any action on running instances that were launched by consumers. consumers are responsible for managing the instances that they launch onto a shared dedicated host. consumers can't modify the shared dedicated host in any way, and they can't view or modify instances that were launched by other consumers or the dedicated host owner. there are no additional charges for sharing dedicated hosts. owners are billed for dedicated hosts that they share. consumers are not billed for instances that they launch onto shared dedicated hosts. dedicated host reservations continue to provide billing discounts for shared dedicated hosts. only dedicated host owners can purchase dedicated host reservations for shared dedicated hosts that they own. shared dedicated hosts count towards the owner's dedicated hosts limits only. consumer's dedicated hosts limits are not affected by dedicated hosts that have been shared with them. similarly, instances that consumers launch onto shared dedicated hosts do not count towards their instance limits. host recovery recovers instances launched by the dedicated host owner and the consumers with whom it has been shared. the replacement dedicated host is allocated to the owner's account. it is added to the same resource shares as the original dedicated host, and it is shared with the same consumers. for more information, see . 
you can use the following features to monitor the performance of your elastic fabric adapters. you can create an amazon vpc flow log to capture information about the traffic going to and from an efa. flow log data can be published to amazon cloudwatch logs and amazon s3. after you create a flow log, you can retrieve and view its data in the chosen destination. for more information, see  in the amazon vpc user guide. you create a flow log for an efa in the same way that you create a flow log for an elastic network interface. for more information, see  in the amazon vpc user guide. in the flow log entries, efa traffic is identified by the  and , which are both formatted as mac addresses, as shown in the following example. amazon cloudwatch provides metrics that enable you to monitor your efas in real time. you can collect and track metrics, create customized dashboards, and set alarms that notify you or take actions when a specified metric reaches a threshold that you specify. for more information, see . 
the maximum number of volumes that your instance can have depends on the operating system and instance type. when considering how many volumes to add to your instance, you should consider whether you need increased i/o bandwidth or increased storage capacity. topics instances built on the  support a maximum number of attachments, which are shared between network interfaces, ebs volumes, and nvme instance store volumes. every instance has at least one network interface attachment. nvme instance store volumes are automatically attached. for more information, see  and . most of these instances support a maximum of 28 attachments. for example, if you have no additional network interface attachments on an ebs-only instance, you can attach up to 27 ebs volumes to it. if you have one additional network interface on an instance with 2 nvme instance store volumes, you can attach 24 ebs volumes to it. for other instances, the following limits apply:  and  instances support a maximum of 26 ebs volumes. instances support a maximum of 23 volumes. instances support a maximum of 11 ebs volumes.most bare metal instances support a maximum of 31 ebs volumes., , and  instances support a maximum of 19 ebs volumes if launched after march 12, 2020 and a maximum of 14 ebs volumes otherwise. to attach more than 14 ebs volumes to an instance launched before march 12, 2020, contact your account team to upgrade the instance at no additional cost. and  instances support a maximum of 19 ebs volumes.attaching more than 40 volumes can cause boot failures. this number includes the root volume, plus any attached instance store volumes and ebs volumes. if you experience boot problems on an instance with a large number of volumes, stop the instance, detach any volumes that are not essential to the boot process, and then reattach the volumes after the instance is running. importantattaching more than 40 volumes to a linux instance is supported on a best effort basis only and is not guaranteed. for consistent and predictable bandwidth use cases, use ebs-optimized or 10 gigabit network connectivity instances and general purpose ssd or provisioned iops ssd volumes. follow the guidance in  to match the iops you have provisioned for your volumes to the bandwidth available from your instances for maximum performance. for raid configurations, many administrators find that arrays larger than 8 volumes have diminished performance returns due to increased i/o overhead. test your individual application performance and tune it as required. 
you can copy an amazon machine image (ami) within or across aws regions using the aws management console, the aws command line interface or sdks, or the amazon ec2 api, all of which support the  action. you can copy both amazon ebs-backed amis and instance-store-backed amis. you can copy amis with encrypted snapshots and also change encryption status during the copy process. copying a source ami results in an identical but distinct target ami with its own unique identifier. in the case of an amazon ebs-backed ami, each of its backing snapshots is, by default, copied to an identical but distinct target snapshot. (the sole exceptions are when you choose to encrypt or re-encrypt the snapshot.) you can change or deregister the source ami with no effect on the target ami. the reverse is also true. there are no charges for copying an ami. however, standard storage and data transfer rates apply. if you copy an ebs-backed ami, you will incur charges for the storage of any additional ebs snapshots.  aws does not copy launch permissions, user-defined tags, or amazon s3 bucket permissions from the source ami to the new ami. after the copy operation is complete, you can apply launch permissions, user-defined tags, and amazon s3 bucket permissions to the new ami. you can't copy an ami that was obtained from the aws marketplace, regardless of whether you obtained it directly or it was shared with you. instead, launch an ec2 instance using the aws marketplace ami and then create an ami from the instance. for more information, see . if you use an iam user to copy an instance store-backed ami, the user must have the following amazon s3 permissions: , , , , , and . the following example policy allows the user to copy the ami source in the specified bucket to the specified region. to find the amazon resource name (arn) of the ami source bucket, open the amazon ec2 console at , in the navigation pane choose amis, and locate the bucket name in the source column. copying an ami across geographically diverse regions provides the following benefits: consistent global deployment: copying an ami from one region to another enables you to launch consistent instances in different regions based on the same ami.scalability: you can more easily design and build global applications that meet the needs of your users, regardless of their location.performance: you can increase performance by distributing your application, as well as locating critical components of your application in closer proximity to your users. you can also take advantage of region-specific features, such as instance types or other aws services.high availability: you can design and deploy applications across aws regions, to increase availability.the following diagram shows the relations among a source ami and two copied amis in different regions, as well as the ec2 instances launched from each. when you launch an instance from an ami, it resides in the same region where the ami resides. if you make changes to the source ami and want those changes to be reflected in the amis in the target regions, you must recopy the source ami to the target regions.  when you first copy an instance store-backed ami to a region, we create an amazon s3 bucket for the amis copied to that region. all instance store-backed amis that you copy to that region are stored in this bucket. the bucket names have the following format: amis-for-account-in-region-hash. for example: . prerequisiteprior to copying an ami, you must ensure that the contents of the source ami are updated to support running in a different region. for example, you should update any database connection strings or similar application configuration data to point to the appropriate resources. otherwise, instances launched from the new ami in the destination region may still use the resources from the source region, which can impact performance and cost. limits destination regions are limited to 50 concurrent ami copies.you cannot copy a paravirtual (pv) ami to a region that does not support pv amis. for more information, see .you can share an ami with another aws account. sharing an ami does not affect the ownership of the ami. the owning account is charged for the storage in the region. for more information, see . if you copy an ami that has been shared with your account, you are the owner of the target ami in your account. the owner of the source ami is charged standard amazon ebs or amazon s3 transfer fees, and you are charged for the storage of the target ami in the destination region. resource permissionsto copy an ami that was shared with you from another account, the owner of the source ami must grant you read permissions for the storage that backs the ami, either the associated ebs snapshot (for an amazon ebs-backed ami) or an associated s3 bucket (for an instance store-backed ami). if the shared ami has encrypted snapshots, the owner must share the key or keys with you as well. the following table shows encryption support for various ami-copying scenarios. while it is possible to copy an unencrypted snapshot to yield an encrypted snapshot, you cannot copy an encrypted snapshot to yield an unencrypted one. noteencrypting during the  action applies only to amazon ebs-backed amis. because an instance store-backed ami does not rely on snapshots, you cannot use copying to change its encryption status. by default (i.e., without specifying encryption parameters), the backing snapshot of an ami is copied with its original encryption status. copying an ami backed by an unencrypted snapshot results in an identical target snapshot that is also unencrypted. if the source ami is backed by an encrypted snapshot, copying it results in an identical target snapshot that is encrypted by the same customer master key (cmk). copying an ami backed by multiple snapshots preserves, by default, the source encryption status in each target snapshot. if you specify encryption parameters while copying an ami, you can encrypt or re-encrypt its backing snapshots. the following example shows a non-default case that supplies encryption parameters to the  action in order to change the target ami's encryption state. copy an unencrypted source ami to an encrypted target ami in this scenario, an ami backed by an unencrypted root snapshot is copied to an ami with an encrypted root snapshot. the  action is invoked with two encryption parameters, including a cmk. as a result, the encryption status of the root snapshot changes, so that the target ami is backed by a root snapshot containing the same data as the source snapshot, but encrypted using the specified key. you incur storage costs for the snapshots in both amis, as well as charges for any instances you launch from either ami. noteenabling  has the same effect as setting the  parameter to  for all snapshots in the ami.  setting the  parameter encrypts the single snapshot for this instance. if you do not specify the  parameter, the default cmk is used to encrypt the snapshot copy. for more information about copying amis with encrypted snapshots, see . you can copy an ami as follows. prerequisitecreate or obtain an ami backed by an amazon ebs snapshot. note that you can use the amazon ec2 console to search a wide variety of amis provided by aws. for more information, see  and . to copy an ami using the console open the amazon ec2 console at . from the console navigation bar, select the region that contains the ami. in the navigation pane, choose images, amis to display the list of amis available to you in the region. select the ami to copy and choose actions, copy ami. in the copy ami dialog box, specify the following information and then choose copy ami: destination region: the region in which to copy the ami.name: a name for the new ami. you can include operating system information in the name, as we do not provide this information when displaying details about the ami.description: by default, the description includes information about the source ami so that you can distinguish a copy from its original. you can change this description as needed.encryption: select this field to encrypt the target snapshots, or to re-encrypt them using a different key. if you have enabled , the encryption option is set and cannot be unset from the ami console. master key: the kms key to used to encrypt the target snapshots.we display a confirmation page to let you know that the copy operation has been initiated and to provide you with the id of the new ami. to check on the progress of the copy operation immediately, follow the provided link. to check on the progress later, choose done, and then when you are ready, use the navigation bar to switch to the target region (if applicable) and locate your ami in the list of amis. the initial status of the target ami is  and the operation is complete when the status is . to copy an ami using the aws cliyou can copy an ami using the  command. you must specify both the source and destination regions. you specify the source region using the  parameter. you can specify the destination region using either the  parameter or an environment variable. for more information, see . when you encrypt a target snapshot during copying, you must specify these additional parameters:  and . to copy an ami using the tools for windows powershellyou can copy an ami using the  command. you must specify both the source and destination regions. you specify the source region using the  parameter. you can specify the destination region using either the  parameter or the  command. for more information, see . when you encrypt a target snapshot during copying, you must specify these additional parameters:  and . you can stop a pending ami copy as follows. to stop an ami copy operation using the console open the amazon ec2 console at . from the navigation bar, select the destination region from the region selector. in the navigation pane, choose amis. select the ami to stop copying and choose actions, deregister. when asked for confirmation, choose continue. to stop an ami copy operation using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)
capacity reservation sharing enables capacity reservation owners to share their reserved capacity with other aws accounts or within an aws organization. this enables you to create and manage capacity reservations centrally, and share the reserved capacity across multiple aws accounts or within your aws organization. in this model, the aws account that owns the capacity reservation (owner) shares it with other aws accounts (consumers). consumers can launch instances into capacity reservations that are shared with them in the same way that they launch instances into capacity reservations that they own in their own account. the capacity reservation owner is responsible for managing the capacity reservation and the instances that they launch into it. owners cannot modify instances that consumers launch into capacity reservations that they have shared. consumers are responsible for managing the instances that they launch into capacity reservations shared with them. consumers cannot view or modify instances owned by other consumers or by the capacity reservation owner. a capacity reservation owner can share a capacity reservation with: specific aws accounts inside or outside of its aws organizationan organizational unit inside its aws organizationits entire aws organizationtopics to share a capacity reservation, you must own it in your aws account. you cannot share a capacity reservation that has been shared with you.you can only share capacity reservations for shared tenancy instances. you cannot share capacity reservations for dedicated tenancy instances.capacity reservation sharing is not available to new aws accounts or aws accounts that have a limited billing history. new accounts that are linked to a qualified master (payer) account or are linked through an aws organization are exempt from this restriction.to share a capacity reservation with your aws organization or an organizational unit in your aws organization, you must enable sharing with aws organizations. for more information, see  in the aws ram user guide.capacity reservation sharing integrates with aws resource access manager (aws ram). aws ram is a service that enables you to share your aws resources with any aws account or through aws organizations. with aws ram, you share resources that you own by creating a resource share. a resource share specifies the resources to share, and the consumers with whom to share them. consumers can be individual aws accounts, or organizational units or an entire organization from aws organizations. for more information about aws ram, see the . to ensure that resources are distributed across the availability zones for a region, we independently map availability zones to names for each account. this could lead to availability zone naming differences across accounts. for example, the availability zone  for your aws account might not have the same location as  for another aws account. to identify the location of your capacity reservations relative to your accounts, you must use the availability zone id (az id). the az id is a unique and consistent identifier for an availability zone across all aws accounts. for example,  is an az id for the  region and it is the same location in every aws account. to view the az ids for the availability zones in your account open the aws ram console at . the az ids for the current region are displayed in the your az id panel on the right-hand side of the screen. when you share a capacity reservation that you own with other aws accounts, you enable them to launch instances into your reserved capacity. if you share an open capacity reservation, keep the following in mind as it could lead to unintended capacity reservation usage: if consumers have running instances that match the capacity reservation's attributes, have the  parameter set to , and are not yet running in reserved capacity, they automatically use the shared capacity reservation.if consumers launch instances that have matching attributes (instance type, platform, and availability zone) and have the  parameter set to , they automatically launch into the shared capacity reservation.to share a capacity reservation, you must add it to a resource share. a resource share is an aws ram resource that lets you share your resources across aws accounts. a resource share specifies the resources to share, and the consumers with whom they are shared. when you share a capacity reservation using the amazon ec2 console, you add it to an existing resource share. to add the capacity reservation to a new resource share, you must create the resource share using the . if you are part of an organization in aws organizations and sharing within your organization is enabled, consumers in your organization are automatically granted access to the shared capacity reservation. otherwise, consumers receive an invitation to join the resource share and are granted access to the shared capacity reservation after accepting the invitation. you can share a capacity reservation that you own using the amazon ec2 console, aws ram console, or the aws cli. to share a capacity reservation that you own using the amazon ec2 console open the amazon ec2 console at . in the navigation pane, choose capacity reservations. choose the capacity reservation to share and choose actions, share reservation. select the resource share to which to add the capacity reservation and choose share capacity reservation. it could take a few minutes for consumers to get access to the shared capacity reservation. to share a capacity reservation that you own using the aws ram consolesee  in the aws ram user guide. to share a capacity reservation that you own using the aws cliuse the  command. the capacity reservation owner can unshare a shared capacity reservation at any time. when you unshare a shared capacity reservation, the following rules apply: instances owned by consumers that were running in the shared capacity at the time of unsharing continue to run normally outside of the reserved capacity, and the capacity is restored to the capacity reservation subject to amazon ec2 capacity availability.consumers with whom the capacity reservation was shared can no longer launch new instances into the reserved capacity.to unshare a shared capacity reservation that you own, you must remove it from the resource share. you can do this using the amazon ec2 console, aws ram console, or the aws cli. to unshare a shared capacity reservation that you own using the amazon ec2 console open the amazon ec2 console at . in the navigation pane, choose capacity reservations. choose the capacity reservation to unshare and choose the sharing tab. the sharing tab lists the resource shares to which the capacity reservation has been added. select the resource share from which to remove the capacity reservation and choose remove from resource share. to unshare a shared capacity reservation that you own using the aws ram consolesee  in the aws ram user guide. to unshare a shared capacity reservation that you own using the aws cliuse the  command. owners and consumers can identify shared capacity reservations using the amazon ec2 console and aws cli to identify a shared capacity reservation using the amazon ec2 console open the amazon ec2 console at . in the navigation pane, choose capacity reservations. the screen lists capacity reservations that you own and capacity reservations that are shared with you. the owner column shows the aws account id of the capacity reservation owner.  next to the aws account id indicates that you are the owner. to identify a shared capacity reservation using the aws cliuse the  command. the command returns the capacity reservations that you own and capacity reservations that are shared with you.  shows the aws account id of the capacity reservation owner. the owner of a shared capacity reservation can view its usage at any time using the amazon ec2 console and the aws cli. to view capacity reservation usage using the amazon ec2 console open the amazon ec2 console at . in the navigation pane, choose capacity reservations. select the capacity reservation for which to view the usage and choose the usage tab. the aws account id column shows the account ids of the consumers currently using the capacity reservation. the launched instances column shows the number of instances each consumer currently has running in the reserved capacity. to view capacity reservation usage using the aws cliuse the  command.  shows the account id of the account using the capacity reservation.  shows the number of instances the consumer currently has running in the reserved capacity. owners are responsible for managing and canceling their shared capacity reservations. owners cannot modify instances running in the shared capacity reservation that are owned by other accounts. owners remain responsible for managing instances that they launch into the shared capacity reservation. consumers are responsible for managing their instances that are running the shared capacity reservation. consumers cannot modify the shared capacity reservation in any way, and they cannot view or modify instances that are owned by other consumers or the capacity reservation owner. there are no additional charges for sharing capacity reservations. the capacity reservation owner is billed for instances that they run inside the capacity reservation and for unused reserved capacity. consumers are billed for the instances that they run inside the shared capacity reservation. all capacity reservation usage counts toward the capacity reservation owner's on-demand instance limits. this includes: unused reserved capacityusage by instances owned by the capacity reservation ownerusage by instances owned by consumersinstances launched into the shared capacity by consumers count towards the capacity reservation owner's on-demand instance limit. consumers' instance limits are a sum of their own on-demand instance limits and the capacity available in the shared capacity reservations to which they have access. 
when you purchase a reserved instance, you can choose between a standard or convertible offering class. the reserved instance applies to a single instance type, platform, scope, and tenancy over a term. if your computing needs change, you may be able to modify or exchange your reserved instance, depending on the offering class. offering classes may also have additional restrictions or limitations. the following are the differences between standard and convertible offering classes. standard and convertible reserved instances can be purchased to apply to instances in a specific availability zone (zonal reserved instances), or to instances in a region (regional reserved instances). for more information and examples, see .  if you want to purchase capacity reservations that recur on a daily, weekly, or monthly basis, a scheduled reserved instance may meet your needs. for more information, see . 
amazon ec2 and amazon ebs are integrated with aws cloudtrail, a service that provides a record of actions taken by a user, role, or an aws service in amazon ec2 and amazon ebs. cloudtrail captures all api calls for amazon ec2 and amazon ebs as events, including calls from the console and from code calls to the apis. if you create a trail, you can enable continuous delivery of cloudtrail events to an amazon s3 bucket, including events for amazon ec2 and amazon ebs. if you don't configure a trail, you can still view the most recent events in the cloudtrail console in event history. using the information collected by cloudtrail, you can determine the request that was made to amazon ec2 and amazon ebs, the ip address from which the request was made, who made the request, when it was made, and additional details.  to learn more about cloudtrail, see the . cloudtrail is enabled on your aws account when you create the account. when activity occurs in amazon ec2 and amazon ebs, that activity is recorded in a cloudtrail event along with other aws service events in event history. you can view, search, and download recent events in your aws account. for more information, see .  for an ongoing record of events in your aws account, including events for amazon ec2 and amazon ebs, create a trail. a trail enables cloudtrail to deliver log files to an amazon s3 bucket. by default, when you create a trail in the console, the trail applies to all regions. the trail logs events from all regions in the aws partition and delivers the log files to the amazon s3 bucket that you specify. additionally, you can configure other aws services to further analyze and act upon the event data collected in cloudtrail logs. for more information, see:   and all amazon ec2 and amazon ebs actions are logged by cloudtrail and are documented in the . for example, calls to the , , or  actions generate entries in the cloudtrail log files.  every event or log entry contains information about who generated the request. the identity information helps you determine the following:  whether the request was made with root or iam user credentials.whether the request was made with temporary security credentials for a role or federated user.whether the request was made by another aws service.for more information, see the . a trail is a configuration that enables delivery of events as log files to an amazon s3 bucket that you specify. cloudtrail log files contain one or more log entries. an event represents a single request from any source and includes information about the requested action, the date and time of the action, request parameters, and so on. cloudtrail log files are not an ordered stack trace of the public api calls, so they do not appear in any specific order.  the following log file record shows that a user terminated an instance. use aws cloudtrail to audit the users that connect to your instances via ec2 instance connect. to audit ssh activity via ec2 instance connect using the aws cloudtrail console open the aws cloudtrail console at . verify that you are in the correct region. in the navigation pane, choose event history. for filter, choose event source, ec2-instance-connect.amazonaws.com. (optional) for time range, select a time range. choose the refresh events icon. the page displays the events that correspond to the  api calls. expand an event using the arrow to view additional details, such as the user name and aws access key that was used to make the ssh connection, and the source ip address. to display the full event information in json format, choose view event. the requestparameters field contains the destination instance id, os user name, and public key that were used to make the ssh connection. if you have configured your aws account to collect cloudtrail events in an s3 bucket, you can download and audit the information programmatically. for more information, see  in the aws cloudtrail user guide. 
you can detach an amazon ebs volume from an instance explicitly or by terminating the instance. however, if the instance is running, you must first unmount the volume from the instance. if an ebs volume is the root device of an instance, you must stop the instance before you can detach the volume. when a volume with an aws marketplace product code is detached from an instance, the product code is no longer associated with the instance. importantafter you detach a volume, you are still charged for volume storage as long as the storage amount exceeds the limit of the aws free tier. you must delete a volume to avoid incurring further charges. for more information, see . this example unmounts the volume and then explicitly detaches it from the instance. this is useful when you want to terminate an instance or attach a volume to a different instance. to verify that the volume is no longer attached to the instance, see . you can reattach a volume that you detached (without unmounting it), but it might not get the same mount point. if there were writes to the volume in progress when it was detached, the data on the volume might be out of sync. you can get directions for volumes on a windows instance from  in the amazon ec2 user guide for windows instances. to detach an ebs volume using the console from your linux instance, use the following command to unmount the  device. open the amazon ec2 console at . in the navigation pane, choose volumes.  select a volume and choose actions, detach volume.  in the confirmation dialog box, choose yes, detach.  to detach an ebs volume from an instance using the command line after unmounting the volume, you can use one of the following commands to detach it. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)the following are common problems encountered when detaching volumes, and how to resolve them. noteto guard against the possibility of data loss, take a snapshot of your volume before attempting to unmount it. forced detachment of a stuck volume can cause damage to the file system or the data it contains or an inability to attach a new volume using the same device name, unless you reboot the instance. if you encounter problems while detaching a volume through the amazon ec2 console, it may be helpful to use the describe-volumes cli command to diagnose the issue. for more information, see .if your volume stays in the  state, you can force the detachment by choosing force detach. use this option only as a last resort to detach a volume from a failed instance, or if you are detaching a volume with the intention of deleting it. the instance doesn't get an opportunity to flush file system caches or file system metadata. if you use this option, you must perform the file system check and repair procedures. if you've tried to force the volume to detach multiple times over several minutes and it stays in the  state, you can post a request for help to the . to help expedite a resolution, include the volume id and describe the steps that you've already taken.when you attempt to detach a volume that is still mounted, the volume can become stuck in the  state while it is trying to detach. the following output from describe-volumes shows an example of this condition: when you encounter this state, detachment can be delayed indefinitely until you unmount the volume, force detachment, reboot the instance, or all three. 
amazon web services (aws) automatically provides data that you can use to monitor your amazon elastic block store (amazon ebs) volumes. topics for additional monitoring information, see  and . volume status checks enable you to better understand, track, and manage potential inconsistencies in the data on an amazon ebs volume. they are designed to provide you with the information that you need to determine whether your amazon ebs volumes are impaired, and to help you control how a potentially inconsistent volume is handled. volume status checks are automated tests that run every 5 minutes and return a pass or fail status. if all checks pass, the status of the volume is . if a check fails, the status of the volume is . if the status is , the checks may still be in progress on the volume. you can view the results of volume status checks to identify any impaired volumes and take any necessary actions. when amazon ebs determines that a volume's data is potentially inconsistent, the default is that it disables i/o to the volume from any attached ec2 instances, which helps to prevent data corruption. after i/o is disabled, the next volume status check fails, and the volume status is . in addition, you'll see an event that lets you know that i/o is disabled, and that you can resolve the impaired status of the volume by enabling i/o to the volume. we wait until you enable i/o to give you the opportunity to decide whether to continue to let your instances use the volume, or to run a consistency check using a command, such as fsck, before doing so. notevolume status is based on the volume status checks, and does not reflect the volume state. therefore, volume status does not indicate volumes in the  state (for example, when a volume is incapable of accepting i/o.) if the consistency of a particular volume is not a concern, and you'd prefer that the volume be made available immediately if it's impaired, you can override the default behavior by configuring the volume to automatically enable i/o. if you enable the auto-enable io volume attribute ( in the api), the volume status check continues to pass. in addition, you'll see an event that lets you know that the volume was determined to be potentially inconsistent, but that its i/o was automatically enabled. this enables you to check the volume's consistency or replace it at a later time. the i/o performance status check compares actual volume performance to the expected performance of a volume and alerts you if the volume is performing below expectations. this status check is only available for  volumes that are attached to an instance and is not valid for general purpose ssd (), throughput optimized hdd (), cold hdd (), or magnetic () volumes. the i/o performance status check is performed once every minute and cloudwatch collects this data every 5 minutes, so it may take up to 5 minutes from the moment you attach a  volume to an instance for this check to report the i/o performance status. importantwhile initializing  volumes that were restored from snapshots, the performance of the volume may drop below 50 percent of its expected level, which causes the volume to display a  state in the i/o performance status check. this is expected, and you can ignore the  state on  volumes while you are initializing them. for more information, see . the following table lists statuses for amazon ebs volumes. to view and work with status checks, you can use the amazon ec2 console, the api, or the command line interface. to view status checks in the console open the amazon ec2 console at . in the navigation pane, choose volumes. the volume status column displays the operational status of each volume. to view the status details of a volume, select the volume and choose status checks. if you have a volume with a failed status check (status is impaired), see . alternatively, you can choose events in the navigator to view all the events for your instances and volumes. for more information, see . to view volume status information with the command line you can use one of the following commands to view the status of your amazon ebs volumes. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)when amazon ebs determines that a volume's data is potentially inconsistent, it disables i/o to the volume from any attached ec2 instances by default. this causes the volume status check to fail, and creates a volume status event that indicates the cause of the failure.  to automatically enable i/o on a volume with potential data inconsistencies, change the setting of the auto-enabled io volume attribute ( in the api). for more information about changing this attribute, see . each event includes a start time that indicates the time at which the event occurred, and a duration that indicates how long i/o for the volume was disabled. the end time is added to the event when i/o for the volume is enabled. volume status events include one of the following descriptions: awaiting action: enable iovolume data is potentially inconsistent. i/o is disabled for the volume until you explicitly enable it. the event description changes to io enabled after you explicitly enable i/o. io enabledi/o operations were explicitly enabled for this volume. io auto-enabledi/o operations were automatically enabled on this volume after an event occurred. we recommend that you check for data inconsistencies before continuing to use the data. normalfor  volumes only. volume performance is as expected. degradedfor  volumes only. volume performance is below expectations. severely degradedfor  volumes only. volume performance is well below expectations. stalledfor  volumes only. volume performance is severely impacted. you can view events for your volumes using the amazon ec2 console, the api, or the command line interface.  to view events for your volumes in the console open the amazon ec2 console at . in the navigation pane, choose events. all instances and volumes that have events are listed. you can filter by volume to view only volume status. you can also filter on specific status types. select a volume to view its specific event. if you have a volume where i/o is disabled, see . if you have a volume where i/o performance is below normal, this might be a temporary condition due to an action you have taken (for example, creating a snapshot of a volume during peak usage, running the volume on an instance that cannot support the i/o bandwidth required, accessing data on the volume for the first time, etc.). to view events for your volumes with the command line you can use one of the following commands to view event information for your amazon ebs volumes. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)use the following options if a volume is impaired because the volume's data is potentially inconsistent. topics the simplest option is to enable i/o and then perform a data consistency check on the volume while the volume is still attached to its amazon ec2 instance. to perform a consistency check on an attached volume stop any applications from using the volume. enable i/o on the volume. open the amazon ec2 console at . in the navigation pane, choose volumes. select the volume on which to enable i/o operations.  in the details pane, choose enable volume io, and then choose yes, enable. check the data on the volume. run the fsck command. (optional) review any available application or system logs for relevant error messages. if the volume has been impaired for more than 20 minutes, you can contact the aws support center. choose troubleshoot, and then in the troubleshoot status checks dialog box, choose contact support to submit a support case. to enable i/o for a volume with the command line you can use one of the following commands to view event information for your amazon ebs volumes. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)use the following procedure to check the volume outside your production environment. importantthis procedure may cause the loss of write i/os that were suspended when volume i/o was disabled. to perform a consistency check on a volume in isolation stop any applications from using the volume. detach the volume from the instance. open the amazon ec2 console at . in the navigation pane, choose volumes.  select the volume to detach. choose actions, force detach volume. you'll be prompted for confirmation. enable i/o on the volume. in the navigation pane, choose volumes.  select the volume that you detached in the previous step. in the details pane, choose enable volume io, and then choose yes, enable. attach the volume to another instance. for more information, see  and . check the data on the volume. run the fsck command. (optional) review any available application or system logs for relevant error messages. if the volume has been impaired for more than 20 minutes, you can contact the aws support center. choose troubleshoot, and then in the troubleshooting dialog box, choose contact support to submit a support case. to enable i/o for a volume with the command line you can use one of the following commands to view event information for your amazon ebs volumes. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)if you want to remove the volume from your environment, simply delete it. for information about deleting a volume, see . if you have a recent snapshot that backs up the data on the volume, you can create a new volume from the snapshot. for more information, see . when amazon ebs determines that a volume's data is potentially inconsistent, it disables i/o to the volume from any attached ec2 instances by default. this causes the volume status check to fail, and creates a volume status event that indicates the cause of the failure. if the consistency of a particular volume is not a concern, and you prefer that the volume be made available immediately if it's impaired, you can override the default behavior by configuring the volume to automatically enable i/o. if you enable the auto-enabled io volume attribute ( in the api), i/o between the volume and the instance is automatically re-enabled and the volume's status check will pass. in addition, you'll see an event that lets you know that the volume was in a potentially inconsistent state, but that its i/o was automatically enabled. when this event occurs, you should check the volume's consistency and replace it if necessary. for more information, see . this procedure explains how to view and modify the auto-enabled io attribute of a volume. to view the auto-enabled io attribute of a volume in the console open the amazon ec2 console at . in the navigation pane, choose volumes.  select the volume and choose status checks. auto-enabled io displays the current setting (enabled or disabled) for your volume. to modify the auto-enabled io attribute of a volume in the console open the amazon ec2 console at . in the navigation pane, choose volumes.  select the volume and choose actions, change auto-enable io setting. alternatively, choose the status checks tab, and for auto-enabled io, choose edit. select the auto-enable volume io check box to automatically enable i/o for an impaired volume. to disable the feature, clear the check box. choose save. to view or modify the autoenableio attribute of a volume with the command line you can use one of the following commands to view the  attribute of your amazon ebs volumes. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)to modify the  attribute of a volume, you can use one of the commands below.  (aws cli) (aws tools for windows powershell)
amazon linux 2 2.0.20190618 or later and ubuntu 20.04 or later comes preconfigured with ec2 instance connect. for other supported linux distributions, you must set up instance connect for every instance that will support using instance connect. this is a one-time requirement for each instance. topics limitations the following linux distributions are supported:amazon linux 2 (any version)ubuntu 16.04 or laterif you configured the  and  settings for ssh authentication, the ec2 instance connect installation will not update them. as a result, you cannot use instance connect.prerequisites verify the general prerequisites for connecting to your instance using ssh. for more information, see . install an ssh client on your local computer. your local computer most likely has an ssh client installed by default. you can check for an ssh client by typing ssh at the command line. if your local computer doesn't recognize the command, you can install an ssh client. for information about installing an ssh client on linux or macos x, see . for information about installing an ssh client on windows 10, see . install the aws cli on your local computer. to configure the iam permissions, you must use the aws cli. for more information about installing the aws cli, see  in the aws command line interface user guide. [ubuntu] install the aws cli on your instance. to install ec2 instance connect on an ubuntu instance, you must use the aws cli on the instance. for more information about installing the aws cli, see  in the aws command line interface user guide. you must configure the following network access to your instance so that you can install ec2 instance connect and enable your users to connect to your instance: ensure that the security group associated with your instance  on port 22 from your ip address. the default security group for the vpc does not allow incoming ssh traffic by default. the security group created by the launch wizard allows incoming ssh traffic by default. for more information, see .(browser-based client) we recommend that your instance allows inbound ssh traffic from the . use the  filter for the  parameter to get the ip address ranges in the ec2 instance connect subset. for more information, see  in the amazon web services general reference.installing ec2 instance connect configures the ssh daemon on the instance. the procedure for installing ec2 instance connect is different for instances launched using amazon linux 2 and ubuntu. to install ec2 instance connect on an instance launched with amazon linux 2 connect to your instance using ssh. use the ssh key pair that was assigned to your instance when you launched it and the default user name of the ami that you used to launch your instance. for amazon linux 2, the default user name is . for example, if your instance was launched using amazon linux 2, your instance's public dns name is , and the key pair is , use the following command to ssh into your instance: for more information about connecting to your instance, see . install the ec2 instance connect package on your instance. for amazon linux 2, use the yum install command. you should see four new files in the  folder: (optional) verify that instance connect was successfully installed on your instance. use the sudo less command to check that the  file was correctly updated as follows: instance connect was successfully installed if the  and  lines in the  file contain the following values:  sets the  file to look up the keys from the instance metadata sets the system user as  noteif you previously configured  and , the instance connect installation will not change the values and you will not be able to use instance connect.to install ec2 instance connect on an instance launched with ubuntu 16.04 or later connect to your instance using ssh. use the ssh key pair that was assigned to your instance when you launched it and use the default user name of the ami that you used to launch your instance. for an ubuntu ami, the user name is . if your instance was launched using ubuntu, your instance's public dns name is , and the key pair is , use the following command to ssh into your instance: for more information about connecting to your instance, see . (optional) ensure your instance has the latest ubuntu ami. for ubuntu, use the following commands to update all the packages on your instance. install the instance connect package on your instance. for ubuntu, use the sudo apt-get command to install the  package. you should see four new files in the  folder: (optional) verify that instance connect was successfully installed on your instance. use the sudo less command to check that the  was correctly updated as follows: instance connect was successfully installed if the  and  lines in the  file contain the following values:  sets the  file to look up the keys from the instance metadata sets the system user as  noteif you previously configured  and , the instance connect installation will not change the values and you will not be able to use instance connect.for more information about the ec2 instance connect package, see  on the github website. the ec2 instance connect cli provides a similar interface to standard ssh calls, which includes querying ec2 instance information, generating and publishing ephemeral public keys, and establishing an ssh connection through a single command, . notethere is no need to install the ec2 instance connect cli if users will only use the browser-based client or an ssh client to connect to an instance. to install the ec2 instance connect cli packageuse  to install the  package. for more information, see  on the github website, and  on the python package index (pypi) website. for your iam users to connect to an instance using ec2 instance connect, you must grant them permission to push the public key to the instance. for more information, see  in the iam user guide. the following instructions explain how to create the policy and attach it using the aws cli. for instructions that use the aws management console, see  and  in the iam user guide. to grant an iam user permission for ec2 instance connect (aws cli) create a json policy document that includes the following: the  action. this grants an iam user permission to push the public key to an instance. with , consider restricting access to specific ec2 instances. otherwise, all iam users with this permission can connect to all ec2 instances.the  condition. this specifies the name of the os user that can push the public key to an instance. use the default user name for the ami that you used to launch the instance. for example, the default user name for amazon linux 2 is  and  for ubuntu.the  action. this is required when using the ec2 instance connect cli because the wrapper calls this action. iam users might already have permission to call this action from another policy.the following is an example policy document. you can omit the statement for the  action if your users will only use an ssh client to connect to your instances. you can replace the specified instances in  to grant users access to all ec2 instances using ec2 instance connect. the preceding policy allows access to specific instances, identified by their instance id. alternatively, you can use resource tags to control access to an instance. attribute-based access control is an authorization strategy that defines permissions based on tags that can be attached to users and aws resources. for example, the following policy allows an iam user to access an instance only if that instance has a resource tag with key= and value=. for more information about using tags to control access to your aws resources, see  in the iam user guide. use the  command to create a new managed policy, and specify the json document that you created to use as the content for the new policy. use the  command to attach the managed policy to the specified iam user. for the  parameter, specify the friendly name (not the arn) of the iam user. 
because of the way that amazon ec2 virtualizes disks, the first write to any location on some instance store volumes performs more slowly than subsequent writes. for most applications, amortizing this cost over the lifetime of the instance is acceptable. however, if you require high disk performance, we recommend that you initialize your drives by writing once to every drive location before production use. notesome instance types with direct-attached solid state drives (ssd) and trim support provide maximum performance at launch time, without initialization. for information about the instance store for each instance type, see . if you require greater flexibility in latency or throughput, we recommend using amazon ebs. to initialize the instance store volumes, use the following  commands, depending on the store to initialize (for example,  or ). notemake sure to unmount the drive before performing this command.initialization can take a long time (about 8 hours for an extra large instance). to initialize the instance store volumes, use the following commands on the , , , , , and  instance types: to perform initialization on all instance store volumes at the same time, use the following command:  configuring drives for raid initializes them by writing to every drive location. when configuring software-based raid, make sure to change the minimum reconstruction speed:  
the following examples show launch configurations that you can use with the  command to create a spot fleet request. for more information, see . notefor spot fleet, you can't specify an network interface id in a launch specification. make sure you omit the  parameter in your launch specification.        the following example specifies a single launch specification without an availability zone or subnet. the spot fleet launches the instances in the lowest-priced availability zone that has a default subnet. the price you pay does not exceed the on-demand price. the following examples specify two launch specifications with different availability zones or subnets, but the same instance type and ami. availability zones the spot fleet launches the instances in the default subnet of the lowest-priced availability zone that you specified. subnets you can specify default subnets or nondefault subnets, and the nondefault subnets can be from a default vpc or a nondefault vpc. the spot service launches the instances in whichever subnet is in the lowest-priced availability zone. you can't specify different subnets from the same availability zone in a spot fleet request. if the instances are launched in a default vpc, they receive a public ipv4 address by default. if the instances are launched in a nondefault vpc, they do not receive a public ipv4 address by default. use a network interface in the launch specification to assign a public ipv4 address to instances launched in a nondefault vpc. when you specify a network interface, you must include the subnet id and security group id using the network interface. the following examples specify two launch configurations with different instance types, but the same ami and availability zone or subnet. the spot fleet launches the instances using the specified instance type with the lowest price. availability zone subnet we recommended that you use the default maximum price, which is the on-demand price. if you prefer, you can specify a maximum price for the fleet request and maximum prices for individual launch specifications. the following examples specify a maximum price for the fleet request and maximum prices for two of the three launch specifications. the maximum price for the fleet request is used for any launch specification that does not specify a maximum price. the spot fleet launches the instances using the instance type with the lowest price. availability zone subnet the following example uses the  allocation strategy. the launch specifications have different instance types but the same ami and availability zone or subnet. the spot fleet distributes the 30 instances across the three launch specifications, such that there are 10 instances of each type. for more information, see . availability zone subnet a best practice to increase the chance that a spot request can be fulfilled by ec2 capacity in the event of an outage in one of the availability zones is to diversify across zones. for this scenario, include each availability zone available to you in the launch specification. and, instead of using the same subnet each time, use three unique subnets (each mapping to a different zone).  availability zone subnet the following examples use instance weighting, which means that the price is per unit hour instead of per instance hour. each launch configuration lists a different instance type and a different weight. the spot fleet selects the instance type with the lowest price per unit hour. the spot fleet calculates the number of spot instances to launch by dividing the target capacity by the instance weight. if the result isn't an integer, the spot fleet rounds it up to the next integer, so that the size of your fleet is not below its target capacity. if the  request is successful, spot provisions 4 of these instances. divide 20 by 6 for a total of 3.33 instances, then round up to 4 instances. if the  request is successful, spot provisions 7 of these instances. divide 20 by 3 for a total of 6.66 instances, then round up to 7 instances. for more information, see . availability zone subnet to ensure that you always have instance capacity, you can include a request for on-demand capacity in your spot fleet request. if there is capacity, the on-demand request is always fulfilled. the balance of the target capacity is fulfilled as spot if there is capacity and availability. the following example specifies the desired target capacity as 10, of which 5 must be on-demand capacity. spot capacity is not specified; it is implied in the balance of the target capacity minus the on-demand capacity. amazon ec2 launches 5 capacity units as on-demand, and 5 capacity units (10-5=5) as spot if there is available amazon ec2 capacity and availability.  for more information, see . 
enhanced networking uses single root i/o virtualization (sr-iov) to provide high-performance networking capabilities on . sr-iov is a method of device virtualization that provides higher i/o performance and lower cpu utilization when compared to traditional virtualized network interfaces. enhanced networking provides higher bandwidth, higher packet per second (pps) performance, and consistently lower inter-instance latencies. there is no additional charge for using enhanced networking. topics depending on your instance type, enhanced networking can be enabled using one of the following mechanisms: elastic network adapter (ena)the elastic network adapter (ena) supports network speeds of up to 100 gbps for supported instance types.a1, c5, c5a, c5d, c5n, c6g,  f1, g3, g4, h1, i3, i3en, inf1, , m5, m5a, m5ad, m5d, m5dn, m5n, m6g,  p2, p3, r4, r5, r5a, r5ad, r5d, r5dn, r5n, r6g,  t3, t3a, , , , , , x1, x1e, and z1d instances use the elastic network adapter for enhanced networking. intel 82599 virtual function (vf) interfacethe intel 82599 virtual function interface supports network speeds of up to 10 gbps for supported instance types.c3, c4, d2, i2, m4 (excluding ), and r3 instances use the intel 82599 vf interface for enhanced networking. for information about the supported network speed for each instance type, see . if your instance type supports the elastic network adapter for enhanced networking, follow the procedures in . if your instance type supports the intel 82599 vf interface for enhanced networking, follow the procedures in . 
amis that are backed by amazon ebs snapshots can take advantage of amazon ebs encryption. snapshots of both data and root volumes can be encrypted and attached to an ami. you can launch instances and copy images with full ebs encryption support included. encryption parameters for these operations are supported in all regions where aws kms is available. ec2 instances with encrypted ebs volumes are launched from amis in the same way as other instances. in addition, when you launch an instance from an ami backed by unencrypted ebs snapshots, you can encrypt some or all of the volumes during launch.  like ebs volumes, snapshots in amis can be encrypted by either your default aws key management service customer master key (cmk), or to a customer managed key that you specify. you must in all cases have permission to use the selected key. amis with encrypted snapshots can be shared across aws accounts. for more information, see . amazon ec2 instances are launched from amis using the  action with parameters supplied through block device mapping, either by means of the aws management console or directly using the amazon ec2 api or cli. for more information about block device mapping, see . for examples of controlling block device mapping from the aws cli, see . by default, without explicit encryption parameters, a  action maintains the existing encryption state of an ami's source snapshots while restoring ebs volumes from them. if  is enabled, all volumes created from the ami (whether from encrypted or unencrypted snapshots) will be encrypted. if encryption by default is not enabled, then the instance maintains the encryption state of the ami. you can also launch an instance and simultaneously apply a new encryption state to the resulting volumes by supplying encryption parameters. consequently, the following behaviors are observed: launch with no encryption parameters an unencrypted snapshot is restored to an unencrypted volume, unless encryption by default is enabled, in which case all the newly created volumes will be encrypted.an encrypted snapshot that you own is restored to a volume that is encrypted to the same cmk.an encrypted snapshot that you do not own (for example, the ami is shared with you) is restored to a volume that is encrypted by your aws account's default cmk.the default behaviors can be overridden by supplying encryption parameters. the available parameters are  and . setting only the  parameter results in the following: instance launch behaviors with  set, but no  specified an unencrypted snapshot is restored to an ebs volume that is encrypted by your aws account's default cmk.an encrypted snapshot that you own is restored to an ebs volume encrypted by the same cmk. (in other words, the  parameter has no effect.)an encrypted snapshot that you do not own (i.e., the ami is shared with you) is restored to a volume that is encrypted by your aws account's default cmk. (in other words, the  parameter has no effect.)setting both the  and  parameters allows you to specify a non-default cmk for an encryption operation. the following behaviors result: instance with both  and  set an unencrypted snapshot is restored to an ebs volume encrypted by the specified cmk.an encrypted snapshot is restored to an ebs volume encrypted not to the original cmk, but instead to the specified cmk.submitting a  without also setting the  parameter results in an error. the following sections provide examples of launching instances from amis using non-default encryption parameters. in each of these scenarios, parameters supplied to the  action result in a change of encryption state during restoration of a volume from a snapshot. notefor detailed console procedures to launch an instance from an ami, see .for documentation of the  api, see .for documentation of the  command in the aws command line interface, see . in this example, an ami backed by an unencrypted snapshot is used to launch an ec2 instance with an encrypted ebs volume.  the  parameter alone results in the volume for this instance being encrypted. providing a  parameter is optional. if no key id is specified, the aws account's default cmk is used to encrypt the volume. to encrypt the volume to a different cmk that you own, supply the  parameter.  in this example, an ami backed by an encrypted snapshot is used to launch an ec2 instance with an ebs volume encrypted by a new cmk.   if you own the ami and supply no encryption parameters, the resulting instance has a volume encrypted by the same key as the snapshot. if the ami is shared rather than owned by you, and you supply no encryption parameters, the volume is encrypted by your default cmk. with encryption parameters supplied as shown, the volume is encrypted by the specified cmk. in this more complex example, an ami backed by multiple snapshots (each with its own encryption state) is used to launch an ec2 instance with a newly encrypted volume and a re-encrypted volume.  in this scenario, the  action is supplied with encryption parameters for each of the source snapshots. when all possible encryption parameters are specified, the resulting instance is the same regardless of whether you own the ami. amazon ec2 amis are copied using the  action, either through the aws management console or directly using the amazon ec2 api or cli. by default, without explicit encryption parameters, a  action maintains the existing encryption state of an ami's source snapshots during copy. you can also copy an ami and simultaneously apply a new encryption state to its associated ebs snapshots by supplying encryption parameters. consequently, the following behaviors are observed: copy with no encryption parameters an unencrypted snapshot is copied to another unencrypted snapshot, unless encryption by default is enabled, in which case all the newly created snapshots will be encrypted.an encrypted snapshot that you own is copied to a snapshot encrypted with the same key.an encrypted snapshot that you do not own (that is, the ami is shared with you) is copied to a snapshot that is encrypted by your aws account's default cmk.all of these default behaviors can be overridden by supplying encryption parameters. the available parameters are  and . setting only the  parameter results in the following: copy-image behaviors with  set, but no  specified an unencrypted snapshot is copied to a snapshot encrypted by the aws account's default cmk.an encrypted snapshot is copied to a snapshot encrypted by the same cmk. (in other words, the  parameter has no effect.)an encrypted snapshot that you do not own (i.e., the ami is shared with you) is copied to a volume that is encrypted by your aws account's default cmk. (in other words, the  parameter has no effect.)setting both the  and  parameters allows you to specify a customer managed cmk for an encryption operation. the following behaviors result: copy-image behaviors with both  and  set an unencrypted snapshot is copied to a snapshot encrypted by the specified cmk.an encrypted snapshot is copied to a snapshot encrypted not to the original cmk, but instead to the specified cmk.submitting a  without also setting the  parameter results in an error. the following section provides an example of copying an ami using non-default encryption parameters, resulting in a change of encryption state. notefor detailed console procedures to copy an ami, see .for documentation of the  api, see .for documentation of the command  in the aws command line interface, see . in this scenario, an ami backed by an unencrypted root snapshot is copied to an ami with an encrypted root snapshot. the  action is invoked with two encryption parameters, including a cmk. as a result, the encryption status of the root snapshot changes, so that the target ami is backed by a root snapshot containing the same data as the source snapshot, but encrypted using the specified key. you incur storage costs for the snapshots in both amis, as well as charges for any instances you launch from either ami. noteenabling  has the same effect as setting the  parameter to  for all snapshots in the ami.  setting the  parameter encrypts the single snapshot for this instance. if you do not specify the  parameter, the default cmk is used to encrypt the snapshot copy. noteyou can also copy an image with multiple snapshots and configure the encryption state of each individually. 
an amazon ec2 instance transitions through different states from the moment you launch it through to its termination. the following illustration represents the transitions between instance states. notice that you can't stop and start an instance store-backed instance. for more information about instance store-backed instances, see .  the following table provides a brief description of each instance state and indicates whether it is billed or not. notethe table indicates billing for instance usage only. some aws resources, such as amazon ebs volumes and elastic ip addresses, incur charges regardless of the instance's state. for more information, see  in the aws billing and cost management user guide. noterebooting an instance doesn't start a new instance billing period because the instance stays in the  state. when you launch an instance, it enters the  state. the instance type that you specified at launch determines the hardware of the host computer for your instance. we use the amazon machine image (ami) you specified at launch to boot the instance. after the instance is ready for you, it enters the  state. you can connect to your running instance and use it the way that you'd use a computer sitting in front of you. as soon as your instance transitions to the  state, you're billed for each second, with a one-minute minimum, that you keep the instance running, even if the instance remains idle and you don't connect to it. for more information, see  and . if your instance fails a status check or is not running your applications as expected, and if the root volume of your instance is an amazon ebs volume, you can stop and start your instance to try to fix the problem. when you stop your instance, it enters the  state, and then the  state. we don't charge usage or data transfer fees for your instance after you stop it, but we do charge for the storage for any amazon ebs volumes. while your instance is in the  state, you can modify certain attributes of the instance, including the instance type. when you start your instance, it enters the  state, and in most cases, we move the instance to a new host computer. (your instance might stay on the same host computer if there are no problems with the host computer.) when you stop and start your instance, you lose any data on the instance store volumes on the previous host computer. your instance retains its private ipv4 address, which means that an elastic ip address associated with the private ipv4 address or network interface is still associated with your instance. if your instance has an ipv6 address, it retains its ipv6 address. each time you transition an instance from  to , we charge per second when the instance is running, with a minimum of one minute every time you start your instance. for more information, see . when you hibernate an instance, we signal the operating system to perform hibernation (suspend-to-disk), which saves the contents from the instance memory (ram) to your amazon ebs root volume. we persist the instance's amazon ebs root volume and any attached amazon ebs data volumes. when you start your instance, the amazon ebs root volume is restored to its previous state and the ram contents are reloaded. previously attached data volumes are reattached and the instance retains its instance id. when you hibernate your instance, it enters the  state, and then the  state. we don't charge usage for a hibernated instance when it is in the  state, but we do charge while it is in the  state, unlike when you  without hibernating it. we don't charge usage for data transfer fees, but we do charge for the storage for any amazon ebs volumes, including storage for the ram data. when you start your hibernated instance, it enters the  state, and in most cases, we move the instance to a new host computer. your instance might stay on the same host computer if there are no problems with the host computer. your instance retains its private ipv4 address, which means that an elastic ip address associated with the private ipv4 address or network interface is still associated with your instance. if your instance has an ipv6 address, it retains its ipv6 address. for more information, see . you can reboot your instance using the amazon ec2 console, a command line tool, and the amazon ec2 api. we recommend that you use amazon ec2 to reboot your instance instead of running the operating system reboot command from your instance. rebooting an instance is equivalent to rebooting an operating system. the instance remains on the same host computer and maintains its public dns name, private ip address, and any data on its instance store volumes. it typically takes a few minutes for the reboot to complete, but the time it takes to reboot depends on the instance configuration. rebooting an instance doesn't start a new instance billing period; per second billing continues without a further one-minute minimum charge. for more information, see . an instance is scheduled to be retired when aws detects the irreparable failure of the underlying hardware hosting the instance. when an instance reaches its scheduled retirement date, it is stopped or terminated by aws. if your instance root device is an amazon ebs volume, the instance is stopped, and you can start it again at any time. if your instance root device is an instance store volume, the instance is terminated, and cannot be used again. for more information, see . when you've decided that you no longer need an instance, you can terminate it. as soon as the status of an instance changes to  or , you stop incurring charges for that instance. if you enable termination protection, you can't terminate the instance using the console, cli, or api. after you terminate an instance, it remains visible in the console for a short while, and then the entry is automatically deleted. you can also describe a terminated instance using the cli and api. resources (such as tags) are gradually disassociated from the terminated instance, therefore may no longer be visible on the terminated instance after a short while. you can't connect to or recover a terminated instance.  each amazon ebs-backed instance supports the  attribute, which controls whether the instance stops or terminates when you initiate shutdown from within the instance itself (for example, by using the shutdown command on linux). the default behavior is to stop the instance. you can modify the setting of this attribute while the instance is running or stopped. each amazon ebs volume supports the  attribute, which controls whether the volume is deleted or preserved when you terminate the instance it is attached to. the default is to delete the root device volume and preserve any other ebs volumes. for more information, see . the following table summarizes the key differences between rebooting, stopping, hibernating, and terminating your instance. operating system shutdown commands always terminate an instance store-backed instance. you can control whether operating system shutdown commands stop or terminate an amazon ebs-backed instance. for more information, see . 
amazon ebs provides the following volume types, which differ in performance characteristics and price, so that you can tailor your storage performance and cost to the needs of your applications. the volumes types fall into two categories: ssd-backed volumes optimized for transactional workloads involving frequent read/write operations with small i/o size, where the dominant performance attribute is iopshdd-backed volumes optimized for large streaming workloads where throughput (measured in mib/s) is a better performance measure than iopsthere are several factors that can affect the performance of ebs volumes, such as instance configuration, i/o characteristics, and workload demand. for more information about getting the most out of your ebs volumes, see . for more information about pricing, see . the following table describes the use cases and performance characteristics for each volume type. the default volume type is general purpose ssd (). * the throughput limit is between 128 mib/s and 250 mib/s, depending on the volume size. volumes smaller than 170 gib deliver a maximum throughput of 128 mib/s. volumes larger than 170 gib but smaller than 334 gib deliver a maximum throughput of 250 mib/s if burst credits are available. volumes larger than or equal to 334 gib deliver 250 mib/s regardless of burst credits. older  volumes might not reach full performance unless you modify the volume. for more information, see . † maximum iops and throughput are guaranteed only on  provisioned with more than 32,000 iops. other instances guarantee up to 32,000 iops and 500 mib/s. older  volumes might not reach full performance unless you modify the volume. for more information, see . †† to achieve this throughput, you must have an instance that supports . the following table describes previous-generation ebs volume types. if you need higher performance or performance consistency than previous-generation volumes can provide, we recommend that you consider using general purpose ssd () or other current volume types. for more information, see . general purpose ssd () volumes offer cost-effective storage that is ideal for a broad range of workloads. these volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 iops for extended periods of time. between a minimum of 100 iops (at 33.33 gib and below) and a maximum of 16,000 iops (at 5,334 gib and above), baseline performance scales linearly at 3 iops per gib of volume size. aws designs  volumes to deliver their provisioned performance 99% of the time. a  volume can range in size from 1 gib to 16 tib. the performance of  volumes is tied to volume size, which determines the baseline performance level of the volume and how quickly it accumulates i/o credits; larger volumes have higher baseline performance levels and accumulate i/o credits faster. i/o credits represent the available bandwidth that your  volume can use to burst large amounts of i/o when more than the baseline performance is needed. the more credits your volume has for i/o, the more time it can burst beyond its baseline performance level and the better it performs when more performance is needed. the following diagram shows the burst-bucket behavior for .  each volume receives an initial i/o credit balance of 5.4 million i/o credits, which is enough to sustain the maximum burst performance of 3,000 iops for 30 minutes. this initial credit balance is designed to provide a fast initial boot cycle for boot volumes and to provide a good bootstrapping experience for other applications. volumes earn i/o credits at the baseline performance rate of 3 iops per gib of volume size. for example, a 100 gib  volume has a baseline performance of 300 iops.  when your volume requires more than the baseline performance i/o level, it draws on i/o credits in the credit balance to burst to the required performance level, up to a maximum of 3,000 iops. when your volume uses fewer i/o credits than it earns in a second, unused i/o credits are added to the i/o credit balance. the maximum i/o credit balance for a volume is equal to the initial credit balance (5.4 million i/o credits). when the baseline performance of a volume is higher than maximum burst performance, i/o credits are never spent. if the volume is attached to an instance built on the , the burst balance is not reported. for other instances, the reported burst balance is 100%. the burst duration of a volume is dependent on the size of the volume, the burst iops required, and the credit balance when the burst begins. this is shown in the following equation: the following table lists several volume sizes and the associated baseline performance of the volume (which is also the rate at which it accumulates i/o credits), the burst duration at the 3,000 iops maximum (when starting with a full credit balance), and the time in seconds that the volume would take to refill an empty credit balance. * the baseline performance of the volume exceeds the maximum burst performance. what happens if i empty my i/o credit balance?if your  volume uses all of its i/o credit balance, the maximum iops performance of the volume remains at the baseline iops performance level (the rate at which your volume earns credits) and the volume's maximum throughput is reduced to the baseline iops multiplied by the maximum i/o size. throughput can never exceed 250 mib/s. when i/o demand drops below the baseline level and unused credits are added to the i/o credit balance, the maximum iops performance of the volume again exceeds the baseline. for example, a 100 gib  volume with an empty credit balance has a baseline performance of 300 iops and a throughput limit of 75 mib/s (300 i/o operations per second * 256 kib per i/o operation = 75 mib/s). the larger a volume is, the greater the baseline performance is and the faster it replenishes the credit balance. for more information about how iops are measured, see . if you notice that your volume performance is frequently limited to the baseline level (due to an empty i/o credit balance), you should consider using a larger  volume (with a higher baseline performance level) or switching to an  volume for workloads that require sustained iops performance greater than 16,000 iops. for information about using cloudwatch metrics and alarms to monitor your burst bucket balance, see . throughput for a  volume can be calculated using the following formula, up to the throughput limit of 250 mib/s: assuming v = volume size, i = i/o size, r = i/o rate, and t = throughput, this can be simplified to:  the smallest volume size that achieves the maximum throughput is given by:  provisioned iops ssd () volumes are designed to meet the needs of i/o-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. unlike , which uses a bucket and credit model to calculate performance, an  volume allows you to specify a consistent iops rate when you create the volume, and amazon ebs delivers the provisioned performance 99.9 percent of the time. an  volume can range in size from 4 gib to 16 tib. you can provision from 100 iops up to 64,000 iops per volume on  and up to 32,000 on other instances. the maximum ratio of provisioned iops to requested volume size (in gib) is 50:1. for example, a 100 gib volume can be provisioned with up to 5,000 iops. on a supported instance type, any volume 1,280 gib in size or greater allows provisioning up to the 64,000 iops maximum (50 × 1,280 gib = 64,000). an  volume provisioned with up to 32,000 iops supports a maximum i/o size of 256 kib and yields as much as 500 mib/s of throughput. with the i/o size at the maximum, peak throughput is reached at 2,000 iops. a volume provisioned with more than 32,000 iops (up to the cap of 64,000 iops) supports a maximum i/o size of 16 kib and yields as much as 1,000 mib/s of throughput. the following graph illustrates these performance characteristics:  your per-i/o latency experience depends on the iops provisioned and your workload profile. for the best i/o latency experience, ensure that you provision iops to meet the i/o profile of your workload. notesome aws accounts created before 2012 might have access to availability zones in us-west-1 or ap-northeast-1 that do not support provisioned iops ssd () volumes. if you are unable to create an  volume (or launch an instance with an  volume in its block device mapping) in one of these regions, try a different availability zone in the region. you can verify that an availability zone supports  volumes by creating a 4 gib  volume in that zone. throughput optimized hdd () volumes provide low-cost magnetic storage that defines performance in terms of throughput rather than iops. this volume type is a good fit for large, sequential workloads such as amazon emr, etl, data warehouses, and log processing. bootable  volumes are not supported.  throughput optimized hdd () volumes, though similar to cold hdd () volumes, are designed to support frequently accessed data. this volume type is optimized for workloads involving large, sequential i/o, and we recommend that customers with workloads performing small, random i/o use . for more information, see . like ,  uses a burst-bucket model for performance. volume size determines the baseline throughput of your volume, which is the rate at which the volume accumulates throughput credits. volume size also determines the burst throughput of your volume, which is the rate at which you can spend credits when they are available. larger volumes have higher baseline and burst throughput. the more credits your volume has, the longer it can drive i/o at the burst level. the following diagram shows the burst-bucket behavior for .  subject to throughput and throughput-credit caps, the available throughput of an  volume is expressed by the following formula: for a 1-tib  volume, burst throughput is limited to 250 mib/s, the bucket fills with credits at 40 mib/s, and it can hold up to 1 tib-worth of credits. larger volumes scale these limits linearly, with throughput capped at a maximum of 500 mib/s. after the bucket is depleted, throughput is limited to the baseline rate of 40 mib/s per tib.  on volume sizes ranging from 0.5 to 16 tib, baseline throughput varies from 20 to a cap of 500 mib/s, which is reached at 12.5 tib as follows: burst throughput varies from 125 mib/s to a cap of 500 mib/s, which is reached at 2 tib as follows: the following table states the full range of base and burst throughput values for : the following diagram plots the table values:  notewhen you create a snapshot of a throughput optimized hdd () volume, performance may drop as far as the volume's baseline value while the snapshot is in progress. for information about using cloudwatch metrics and alarms to monitor your burst bucket balance, see . cold hdd () volumes provide low-cost magnetic storage that defines performance in terms of throughput rather than iops. with a lower throughput limit than ,  is a good fit for large, sequential cold-data workloads. if you require infrequent access to your data and are looking to save costs,  provides inexpensive block storage. bootable  volumes are not supported. cold hdd () volumes, though similar to throughput optimized hdd () volumes, are designed to support infrequently accessed data. notethis volume type is optimized for workloads involving large, sequential i/o, and we recommend that customers with workloads performing small, random i/o use . for more information, see . like ,  uses a burst-bucket model for performance. volume size determines the baseline throughput of your volume, which is the rate at which the volume accumulates throughput credits. volume size also determines the burst throughput of your volume, which is the rate at which you can spend credits when they are available. larger volumes have higher baseline and burst throughput. the more credits your volume has, the longer it can drive i/o at the burst level.  subject to throughput and throughput-credit caps, the available throughput of an  volume is expressed by the following formula: for a 1-tib  volume, burst throughput is limited to 80 mib/s, the bucket fills with credits at 12 mib/s, and it can hold up to 1 tib-worth of credits. larger volumes scale these limits linearly, with throughput capped at a maximum of 250 mib/s. after the bucket is depleted, throughput is limited to the baseline rate of 12 mib/s per tib.  on volume sizes ranging from 0.5 to 16 tib, baseline throughput varies from 6 mib/s to a maximum of 192 mib/s, which is reached at 16 tib as follows: burst throughput varies from 40 mib/s to a cap of 250 mib/s, which is reached at 3.125 tib as follows: the following table states the full range of base and burst throughput values for : the following diagram plots the table values:  notewhen you create a snapshot of a cold hdd () volume, performance may drop as far as the volume's baseline value while the snapshot is in progress. for information about using cloudwatch metrics and alarms to monitor your burst bucket balance, see . magnetic volumes are backed by magnetic drives and are suited for workloads where data is accessed infrequently, and scenarios where low-cost storage for small volume sizes is important. these volumes deliver approximately 100 iops on average, with burst capability of up to hundreds of iops, and they can range in size from 1 gib to 1 tib. notemagnetic is a previous generation volume type. for new applications, we recommend using one of the newer volume types. for more information, see . for information about using cloudwatch metrics and alarms to monitor your burst bucket balance, see . for optimal throughput results using hdd volumes, plan your workloads with the following considerations in mind. the  and  bucket sizes vary according to volume size, and a full bucket contains enough tokens for a full volume scan. however, larger  and  volumes take longer for the volume scan to complete due to per-instance and per-volume throughput limits. volumes attached to smaller instances are limited to the per-instance throughput rather than the  or  throughput limits. both  and  are designed for performance consistency of 90% of burst throughput 99% of the time. non-compliant periods are approximately uniformly distributed, targeting 99% of expected total throughput each hour. the following table shows ideal scan times for volumes of various size, assuming full buckets and sufficient instance throughput. in general, scan times are expressed by this formula: for example, taking the performance consistency guarantees and other optimizations into account, an  customer with a 5-tib volume can expect to complete a full volume scan in 2.91 to 3.27 hours.  similarly, an  customer with a 5-tib volume can expect to complete a full volume scan in 5.83 to 6.54 hours.  * these scan times assume an average queue depth (rounded to the nearest whole number) of four or more when performing 1 mib of sequential i/o. therefore if you have a throughput-oriented workload that needs to complete scans quickly (up to 500 mib/s), or requires several full volume scans a day, use . if you are optimizing for cost, your data is relatively infrequently accessed, and you don’t need more than 250 mib/s of scanning performance, then use . the performance model for  and  volumes is optimized for sequential i/os, favoring high-throughput workloads, offering acceptable performance on workloads with mixed iops and throughput, and discouraging workloads with small, random i/o. for example, an i/o request of 1 mib or less counts as a 1 mib i/o credit. however, if the i/os are sequential, they are merged into 1 mib i/o blocks and count only as a 1 mib i/o credit.  throughput for  and  volumes is always determined by the smaller of the following: throughput limits of the volumethroughput limits of the instanceas for all amazon ebs volumes, we recommend that you select an appropriate ebs-optimized ec2 instance in order to avoid network bottlenecks. for more information, see . you can monitor the burst-bucket level for , , and  volumes using the ebs  metric available in amazon cloudwatch. this metric shows the percentage of i/o credits (for ) or throughput credits (for  and ) remaining in the burst bucket. for more information about the  metric and other metrics related to i/o, see . cloudwatch also allows you to set an alarm that notifies you when the  value falls to a certain level. for more information, see . 
amazon elastic compute cloud (amazon ec2) provides scalable computing capacity in the amazon web services (aws) cloud. using amazon ec2 eliminates your need to invest in hardware up front, so you can develop and deploy applications faster. you can use amazon ec2 to launch as many or as few virtual servers as you need, configure security and networking, and manage storage. amazon ec2 enables you to scale up or down to handle changes in requirements or spikes in popularity, reducing your need to forecast traffic. for more information about cloud computing, see  amazon ec2 provides the following features: virtual computing environments, known as instancespreconfigured templates for your instances, known as amazon machine images (amis), that package the bits you need for your server (including the operating system and additional software)various configurations of cpu, memory, storage, and networking capacity for your instances, known as instance typessecure login information for your instances using key pairs (aws stores the public key, and you store the private key in a secure place)storage volumes for temporary data that's deleted when you stop or terminate your instance, known as instance store volumespersistent storage volumes for your data using amazon elastic block store (amazon ebs), known as amazon ebs volumesmultiple physical locations for your resources, such as instances and amazon ebs volumes, known as regions and availability zonesa firewall that enables you to specify the protocols, ports, and source ip ranges that can reach your instances using security groupsstatic ipv4 addresses for dynamic cloud computing, known as elastic ip addressesmetadata, known as tags, that you can create and assign to your amazon ec2 resourcesvirtual networks you can create that are logically isolated from the rest of the aws cloud, and that you can optionally connect to your own network, known as virtual private clouds (vpcs)for more information about the features of amazon ec2, see the . for more information about running your website on aws, see . first, you need to get set up to use amazon ec2. after you are set up, you are ready to complete the getting started tutorial for amazon ec2. whenever you need more information about an amazon ec2 feature, you can read the technical documentation. get up and running basics networking and security storage working with linux instances  in the aws systems manager user guideif you have questions about whether aws is right for you, . if you have technical questions about amazon ec2, use the .  you can provision amazon ec2 resources, such as instances and volumes, directly using amazon ec2. you can also provision amazon ec2 resources using other services in aws. for more information, see the following documentation: to automatically distribute incoming application traffic across multiple instances, use elastic load balancing. for more information, see the . to get a managed relational database in the cloud, use amazon relational database service (amazon rds) to launch a database instance. although you can set up a database on an ec2 instance, amazon rds offers the advantage of handling your database management tasks, such as patching the software, backing up, and storing the backups. for more information, see the . to make it easier to manage docker containers on a cluster of ec2 instances, use amazon elastic container service (amazon ecs). for more information, see the  or the . to monitor basic statistics for your instances and amazon ebs volumes, use amazon cloudwatch. for more information, see the . to detect potentially authorized or malicious use of your ec2 instances, use amazon guardduty. for more information see the . amazon ec2 provides a web-based user interface, the amazon ec2 console. if you've signed up for an aws account, you can access the amazon ec2 console by signing into the aws management console and selecting ec2 from the console home page. if you prefer to use a command line interface, you have the following options: aws command line interface (cli)provides commands for a broad set of aws products, and is supported on windows, mac, and linux. to get started, see . for more information about the commands for amazon ec2, see  in the aws cli command reference. aws tools for windows powershellprovides commands for a broad set of aws products for those who script in the powershell environment. to get started, see the . for more information about the cmdlets for amazon ec2, see the . amazon ec2 provides a query api. these requests are http or https requests that use the http verbs get or post and a query parameter named . for more information about the api actions for amazon ec2, see  in the amazon ec2 api reference. if you prefer to build applications using language-specific apis instead of submitting a request over http or https, aws provides libraries, sample code, tutorials, and other resources for software developers. these libraries provide basic functions that automate tasks such as cryptographically signing your requests, retrying requests, and handling error responses, making it is easier for you to get started. for more information, see . when you sign up for aws, you can get started with amazon ec2 for free using the . amazon ec2 provides the following purchasing options for instances: on-demand instancespay for the instances that you use by the second, with no long-term commitments or upfront payments. savings plansyou can reduce your amazon ec2 costs by making a commitment to a consistent amount of usage, in usd per hour, for a term of 1 or 3 years. reserved instancesyou can reduce your amazon ec2 costs by making a commitment to a specific instance configuration, including instance type and region, for a term of 1 or 3 years. spot instancesrequest unused ec2 instances, which can reduce your amazon ec2 costs significantly. for a complete list of charges and prices for amazon ec2, see . to calculate the cost of a sample provisioned environment, see . to see your bill, go to the billing and cost management dashboard in the . your bill contains links to usage reports that provide details about your bill. to learn more about aws account billing, see . if you have questions concerning aws billing, accounts, and events, . for an overview of trusted advisor, a service that helps you optimize the costs, security, and performance of your aws environment, see . amazon ec2 supports the processing, storage, and transmission of credit card data by a merchant or service provider, and has been validated as being compliant with payment card industry (pci) data security standard (dss). for more information about pci dss, including how to request a copy of the aws pci compliance package, see .  
when you launch an ec2 instance, it is assigned a public ip address and a public dns (domain name system) name that you can use to reach it from the internet. because there are so many hosts in the amazon web services domain, these public names must be quite long for each name to remain unique. a typical amazon ec2 public dns name looks something like this: , where the name consists of the amazon web services domain, the service (in this case, ), the region, and a form of the public ip address. dynamic dns services provide custom dns host names within their domain area that can be easy to remember and that can also be more relevant to your host's use case; some of these services are also free of charge. you can use a dynamic dns provider with amazon ec2 and configure the instance to update the ip address associated with a public dns name each time the instance starts. there are many different providers to choose from, and the specific details of choosing a provider and registering a name with them are outside the scope of this guide. importantthis information applies to amazon linux. for information about other distributions, see their specific documentation. to use dynamic dns with amazon ec2 sign up with a dynamic dns service provider and register a public dns name with their service. this procedure uses the free service from  as an example. configure the dynamic dns update client. after you have a dynamic dns service provider and a public dns name registered with their service, point the dns name to the ip address for your instance. many providers (including ) allow you to do this manually from your account page on their website, but many also support software update clients. if an update client is running on your ec2 instance, your dynamic dns record is updated each time the ip address changes, as after a shutdown and restart. in this example, you install the noip2 client, which works with the service provided by . enable the extra packages for enterprise linux (epel) repository to gain access to the noip2 client. noteamazon linux instances have the gpg keys and repository information for the epel repository installed by default; however, red hat and centos instances must first install the  package before you can enable the epel repository. for more information and to download the latest version of this package, see . for amazon linux 2: for amazon linux ami: install the  package. create the configuration file. enter the login and password information when prompted and answer the subsequent questions to configure the client. enable the noip service. for amazon linux 2: for amazon linux ami: start the noip service. for amazon linux 2: for amazon linux ami: this command starts the client, which reads the configuration file () that you created earlier and updates the ip address for the public dns name that you chose. verify that the update client has set the correct ip address for your dynamic dns name. allow a few minutes for the dns records to update, and then try to connect to your instance using ssh with the public dns name that you configured in this procedure. 
you can use amazon cloudwatch to collect metrics and logs from the operating systems for your ec2 instances. you can use the cloudwatch agent to collect both system metrics and log files from amazon ec2 instances and on-premises servers. the agent supports both windows server and linux, and enables you to select the metrics to be collected, including sub-resource metrics such as per-cpu core. we recommend that you use the agent to collect metrics and logs instead of using the monitoring scripts. for more information, see  in the amazon cloudwatch user guide. importantwe recommend that you use the cloudwatch agent to collect metrics and logs. the information about the monitoring scripts is provided for customers who are still using the old monitoring scripts to gather information from their linux instances. the old monitoring scripts are no longer supported.  the monitoring scripts demonstrate how to produce and consume custom metrics for amazon cloudwatch. these sample perl scripts comprise a fully functional example that reports memory, swap, and disk space utilization metrics for a linux instance.  standard amazon cloudwatch usage charges for custom metrics apply to your use of these scripts. for more information, see the  pricing page. topics the monitoring scripts were tested on instances using the following systems: amazon linux 2amazon linux ami 2014.09.2 and laterred hat enterprise linux 6.9 and 7.4suse linux enterprise server 12ubuntu server 14.04 and 16.04ensure that the scripts have permission to call the following actions by associating an iam role with your instance: cloudwatch:putmetricdatacloudwatch:getmetricstatisticscloudwatch:listmetricsec2:describetagsfor more information, see . with some versions of linux, you must install additional perl modules before you can use the monitoring scripts. to install the required packages on amazon linux 2 and amazon linux ami log on to your instance. for more information, see . at a command prompt, install packages as follows: to install the required packages on ubuntu log on to your instance. for more information, see . at a command prompt, install packages as follows: to install the required packages on red hat enterprise linux 7 log on to your instance. for more information, see . at a command prompt, install packages as follows: to install the required packages on red hat enterprise linux 6.9 log on to your instance. for more information, see . at a command prompt, install packages as follows: run cpan as an elevated user: press enter through the prompts until you see the following prompt: at the cpan prompt, run each of the below commands: run one command and it installs, and when you return to the cpan prompt, run the next command. press enter like before when prompted to continue through the process:  to install the required packages on suse log on to your instance. for more information, see . on servers running suse linux enterprise server 12, you might need to download the  package. you can download and install this package using the following commands: install the required packages as follows: the following steps show you how to download, uncompress, and configure the cloudwatch monitoring scripts on an ec2 linux instance. to download, install, and configure the monitoring scripts at a command prompt, move to a folder where you want to store the monitoring scripts and run the following command to download them: run the following commands to install the monitoring scripts you downloaded: the package for the monitoring scripts contains the following files: cloudwatchclient.pm – shared perl module that simplifies calling amazon cloudwatch from other scripts.mon-put-instance-data.pl – collects system metrics on an amazon ec2 instance (memory, swap, disk space utilization) and sends them to amazon cloudwatch.mon-get-instance-stats.pl – queries amazon cloudwatch and displays the most recent utilization statistics for the ec2 instance on which this script is executed.awscreds.template – file template for aws credentials that stores your access key id and secret access key.license.txt – text file containing the apache 2.0 license.notice.txt – copyright notice.this script collects memory, swap, and disk space utilization data on the current system. it then makes a remote call to amazon cloudwatch to report the collected data as custom metrics. options   examplesthe following examples assume that you provided an iam role or  file. otherwise, you must provide credentials using the  and  parameters for these commands. the following example performs a simple test run without posting data to cloudwatch. the following example collects all available memory metrics and sends them to cloudwatch, counting cache and buffer memory as used the following example collects aggregated metrics for an auto scaling group and sends them to amazon cloudwatch without reporting individual instance metrics. the following example collects aggregated metrics for instance type, ami id and region, and sends them to amazon cloudwatch without reporting individual instance metrics to set a cron schedule for metrics reported to cloudwatch, start editing the crontab using the crontab -e command. add the following command to report memory and disk space utilization to cloudwatch every five minutes: if the script encounters an error, it writes the error message in the system log. this script queries cloudwatch for statistics on memory, swap, and disk space metrics within the time interval provided using the number of most recent hours. this data is provided for the amazon ec2 instance on which this script is executed.  options   exampleto get utilization statistics for the last 12 hours, run the following command: the following is an example response: after you successfully run the  script, you can view your custom metrics in the amazon cloudwatch console. to view custom metrics run  as described previously. open the cloudwatch console at . choose view metrics. for viewing, your custom metrics posted by the script are displayed with the prefix . the cloudwatchclient.pm module caches instance metadata locally. if you create an ami from an instance where you have run the monitoring scripts, any instances launched from the ami within the cache ttl (default: six hours, 24 hours for auto scaling groups) emit metrics using the instance id of the original instance. after the cache ttl time period passes, the script retrieves fresh data and the monitoring scripts use the instance id of the current instance. to immediately correct this, remove the cached data using the following command: 
an amazon machine image (ami) is a template that contains a software configuration (for example, an operating system, an application server, and applications). from an ami, you launch an instance, which is a copy of the ami running as a virtual server in the cloud. you can launch multiple instances of an ami, as shown in the following figure.  your instances keep running until you stop or terminate them, or until they fail. if an instance fails, you can launch a new one from the ami. an instance is a virtual server in the cloud. its configuration at launch is a copy of the ami that you specified when you launched the instance. you can launch different types of instances from a single ami. an instance type essentially determines the hardware of the host computer used for your instance. each instance type offers different compute and memory capabilities. select an instance type based on the amount of memory and computing power that you need for the application or software that you plan to run on the instance. for more information about the hardware specifications for each amazon ec2 instance type, see . after you launch an instance, it looks like a traditional host, and you can interact with it as you would any computer. you have complete control of your instances; you can use sudo to run commands that require root privileges. your aws account has a limit on the number of instances that you can have running. for more information about this limit, and how to request an increase, see  in the amazon ec2 general faq.  the root device for your instance contains the image used to boot the instance. for more information, see . your instance may include local storage volumes, known as instance store volumes, which you can configure at launch time with block device mapping. for more information, see . after these volumes have been added to and mapped on your instance, they are available for you to mount and use. if your instance fails, or if your instance is stopped or terminated, the data on these volumes is lost; therefore, these volumes are best used for temporary data. to keep important data safe, you should use a replication strategy across multiple instances, or store your persistent data in amazon s3 or amazon ebs volumes. for more information, see . use aws identity and access management (iam) to control access to your aws resources, including your instances. you can create iam users and groups under your aws account, assign security credentials to each, and control the access that each has to resources and services in aws. for more information, see .restrict access by only allowing trusted hosts or networks to access ports on your instance. for example, you can restrict ssh access by restricting incoming traffic on port 22. for more information, see .review the rules in your security groups regularly, and ensure that you apply the principle of least privilege—only open up permissions that you require. you can also create different security groups to deal with instances that have different security requirements. consider creating a bastion security group that allows external logins, and keep the remainder of your instances in a group that does not allow external logins.disable password-based logins for instances launched from your ami. passwords can be found or cracked, and are a security risk. for more information, see . for more information about sharing amis safely, see .you can stop or terminate a running instance at any time. stopping an instancewhen an instance is stopped, the instance performs a normal shutdown, and then transitions to a  state. all of its amazon ebs volumes remain attached, and you can start the instance again at a later time.  you are not charged for additional instance usage while the instance is in a stopped state. a minimum of one minute is charged for every transition from a stopped state to a running state. if the instance type was changed while the instance was stopped, you will be charged the rate for the new instance type after the instance is started. all of the associated amazon ebs usage of your instance, including root device usage, is billed using typical amazon ebs prices.  when an instance is in a stopped state, you can attach or detach amazon ebs volumes. you can also create an ami from the instance, and you can change the kernel, ram disk, and instance type. terminating an instancewhen an instance is terminated, the instance performs a normal shutdown. the root device volume is deleted by default, but any attached amazon ebs volumes are preserved by default, determined by each volume's  attribute setting. the instance itself is also deleted, and you can't start the instance again at a later time. to prevent accidental termination, you can disable instance termination. if you do so, ensure that the  attribute is set to  for the instance. to control the behavior of an instance shutdown, such as  in linux or  in windows, set the  instance attribute to  or  as desired. instances with amazon ebs volumes for the root device default to , and instances with instance-store root devices are always terminated as the result of an instance shutdown. for more information, see . amazon web services (aws) publishes many  that contain common software configurations for public use. in addition, members of the aws developer community have published their own custom amis. you can also create your own custom ami or amis; doing so enables you to quickly and easily start new instances that have everything you need. for example, if your application is a website or a web service, your ami could include a web server, the associated static content, and the code for the dynamic pages. as a result, after you launch an instance from this ami, your web server starts, and your application is ready to accept requests. all amis are categorized as either backed by amazon ebs, which means that the root device for an instance launched from the ami is an amazon ebs volume, or backed by instance store, which means that the root device for an instance launched from the ami is an instance store volume created from a template stored in amazon s3. the description of an ami indicates the type of root device (either  or ). this is important because there are significant differences in what you can do with each type of ami. for more information about these differences, see .  you can deregister an ami when you have finished using it. after you deregister an ami, you can't use it to launch new instances. existing instances launched from the ami are not affected. therefore, if you are also finished with the instances launched from these amis, you should terminate them. 
amazon ebs snapshots are the preferred backup tool on amazon ec2 due to their speed, convenience, and cost. when creating a volume from a snapshot, you recreate its state at a specific point in the past with all data intact. by attaching a volume created from a snapshot to an instance, you can duplicate data across regions, create test environments, replace a damaged or corrupted production volume in its entirety, or retrieve specific files and directories and transfer them to another attached volume. for more information, see . you can use the following procedure to replace an ebs volume with another volume created from a previous snapshot of that volume. you must detach the current volume and then attach the new volume. note that ebs volumes can only be attached to ec2 instances in the same availability zone. to replace a volume create a volume from the snapshot and write down the id of the new volume. for more information, see . on the volumes page, select the check box for the volume to replace. on the description tab, find attachment information and write down the device name of the volume (for example,  or  for a root volume, or  or ) and the id of the instance.  (optional) before you can detach the root volume of an instance, you must stop the instance. if you are not replacing the root volume, you can continue to the next step without stopping the instance. otherwise, to stop the instance, from attachment information, hover over the instance id, right-click, and open the instance in a new browser tab. choose actions, instance state, stop. leave the tab with the instances page open and return to the browser tab with the volumes page. with the volume still selected, choose actions, detach volume. when prompted for confirmation, choose yes, detach. clear the check box for this volume. select the check box for the new volume that you created in step 1. choose actions, attach volume. enter the instance id and device name that you wrote down in step 2, and then choose attach. (optional) if you stopped the instance, you must restart it. return to the browser tab with the instances page and choose actions, instance state, start. connect to your instance and mount the volume. for more information, see . 
an instance is scheduled to be retired when aws detects irreparable failure of the underlying hardware hosting the instance. when an instance reaches its scheduled retirement date, it is stopped or terminated by aws. if your instance root device is an amazon ebs volume, the instance is stopped, and you can start it again at any time. starting the stopped instance migrates it to new hardware. if your instance root device is an instance store volume, the instance is terminated, and cannot be used again. topics for more information about types of instance events, see . warning the instance retirement email contains a very important sentence:  "due to this degradation your instance could already be unreachable." please make sure you check to see if your instance is still reachable and perform a stop/start if it is not reachable. if it is reachable, you may be fine to continue using your instance until the scheduled retirement date, but you should try to remediate at the earliest time you can without impacting your business as it will cut down on the chances of having prolonged/increased impact. if your instance is scheduled for retirement, you'll receive an email prior to the event with the instance id and retirement date. this email is sent to the address that's associated with your account; the same email address that you use to log in to the aws management console. if you use an email account that you do not check regularly, then you can use the amazon ec2 console or the command line to determine if any of your instances are scheduled for retirement. to update the contact information for your account, go to the  page. to identify instances scheduled for retirement using the console open the amazon ec2 console. in the navigation pane, choose ec2 dashboard. under scheduled events, you can see the events associated with your amazon ec2 instances and volumes, organized by region. if you have an instance with a scheduled event listed, select its link below the region name to go to the events page. the events page lists all resources with events associated with them. to view instances that are scheduled for retirement, select instance resources from the first filter list, and then instance stop or retirement from the second filter list. if the filter results show that an instance is scheduled for retirement, select it, and note the date and time in the start time field in the details pane. this is your instance retirement date. to identify instances scheduled for retirement using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)there are a number of actions available to you when your instance is scheduled for retirement. the action you take depends on whether your instance root device is an amazon ebs volume, or an instance store volume. if you do not know what your instance root device type is, you can find out using the amazon ec2 console or the command line. to determine your instance root device type using the console in the navigation pane, select events. use the filter lists to identify retiring instances, as demonstrated in the procedure above, . in the resource id column, select the instance id to go to the instances page.  select the instance and locate the root device type field in the description tab. if the value is , then your instance is ebs-backed. if the value is , then your instance is instance store-backed. to determine your instance root device type using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)you can perform one of the actions listed below in order to preserve the data on your retiring instance. it's important that you take this action before the instance retirement date to prevent unforeseen downtime and data loss.  warningif your instance store-backed instance passes its retirement date, it is terminated and you cannot recover the instance or any data that was stored on it. regardless of the root device of your instance, the data on instance store volumes is lost when the instance is retired, even if they are attached to an ebs-backed instance. 
linux amazon machine images use one of two types of virtualization: paravirtual (pv) or hardware virtual machine (hvm). the main differences between pv and hvm amis are the way in which they boot and whether they can take advantage of special hardware extensions (cpu, network, and storage) for better performance. for the best performance, we recommend that you use current generation instance types and hvm amis when you launch your instances. for more information about current generation instance types, see . if you are using previous generation instance types and would like to upgrade, see . hvm amishvm amis are presented with a fully virtualized set of hardware and boot by executing the master boot record of the root block device of your image. this virtualization type provides the ability to run an operating system directly on top of a virtual machine without any modification, as if it were run on the bare-metal hardware. the amazon ec2 host system emulates some or all of the underlying hardware that is presented to the guest. unlike pv guests, hvm guests can take advantage of hardware extensions that provide fast access to the underlying hardware on the host system. for more information on cpu virtualization extensions available in amazon ec2, see  on the intel website. hvm amis are required to take advantage of enhanced networking and gpu processing. in order to pass through instructions to specialized network and gpu devices, the os needs to be able to have access to the native hardware platform; hvm virtualization provides this access. for more information, see  and . all instance types support hvm amis. to find an hvm ami, verify that the virtualization type of the ami is set to , using the console or the  command. pv amispv amis boot with a special boot loader called pv-grub, which starts the boot cycle and then chain loads the kernel specified in the  file on your image. paravirtual guests can run on host hardware that does not have explicit support for virtualization, but they cannot take advantage of special hardware extensions such as enhanced networking or gpu processing. historically, pv guests had better performance than hvm guests in many cases, but because of enhancements in hvm virtualization and the availability of pv drivers for hvm amis, this is no longer true. for more information about pv-grub and its use in amazon ec2, see . the following previous generation instance types support pv amis: c1, c3, hs1, m1, m3, m2, and t1. current generation instance types do not support pv amis. the following aws regions support pv instances: asia pacific (tokyo), asia pacific (singapore), asia pacific (sydney), europe (frankfurt), europe (ireland), south america (são paulo), us east (n. virginia), us west (n. california), and us west (oregon). to find a pv ami, verify that the virtualization type of the ami is set to , using the console or the  command. pv on hvmparavirtual guests traditionally performed better with storage and network operations than hvm guests because they could leverage special drivers for i/o that avoided the overhead of emulating network and disk hardware, whereas hvm guests had to translate these instructions to emulated hardware. now pv drivers are available for hvm guests, so operating systems that cannot be ported to run in a paravirtualized environment can still see performance advantages in storage and network i/o by using them. with these pv on hvm drivers, hvm guests can get the same, or better, performance than paravirtual guests. 
security groups enable you to control traffic to your instance, including the kind of traffic that can reach your instance. for example, you can allow computers from only your home network to access your instance using ssh. if your instance is a web server, you can allow all ip addresses to access your instance using http or https, so that external users can browse the content on your web server. your default security groups and newly created security groups include default rules that do not enable you to access your instance from the internet. for more information, see  and . to enable network access to your instance, you must allow inbound traffic to your instance. to open a port for inbound traffic, add a rule to a security group that you associated with your instance when you launched it. to connect to your instance, you must set up a rule to authorize ssh traffic from your computer's public ipv4 address. to allow ssh traffic from additional ip address ranges, add another rule for each range you need to authorize. if you've enabled your vpc for ipv6 and launched your instance with an ipv6 address, you can connect to your instance using its ipv6 address instead of a public ipv4 address. your local computer must have an ipv6 address and must be configured to use ipv6. if you need to enable network access to a windows instance, see  in the amazon ec2 user guide for windows instances. decide who requires access to your instance; for example, a single host or a specific network that you trust such as your local computer's public ipv4 address. the security group editor in the amazon ec2 console can automatically detect the public ipv4 address of your local computer for you. alternatively, you can use the search phrase "what is my ip address" in an internet browser, or use the following service: . if you are connecting through an isp or from behind your firewall without a static ip address, you need to find out the range of ip addresses used by client computers. warningif you use , you enable all ipv4 addresses to access your instance using ssh. if you use , you enable all ipv6 address to access your instance. this is acceptable for a short time in a test environment, but it's unsafe for production environments. in production, you authorize only a specific ip address or range of addresses to access your instance. decide whether you'll support ssh access to your instances using ec2 instance connect. if you will not use ec2 instance connect, consider uninstalling it or denying the following action in your iam policies: . for more information, see  and . security groups act as a firewall for associated instances, controlling both inbound and outbound traffic at the instance level. you must add rules to a security group that enable you to connect to your linux instance from your ip address using ssh. to add a rule to a security group for inbound ssh traffic over ipv4 (console) in the navigation pane of the amazon ec2 console, choose instances. select your instance and look at the description tab; security groups lists the security groups that are associated with the instance. choose view inbound rules to display a list of the rules that are in effect for the instance. in the navigation pane, choose security groups. select one of the security groups associated with your instance. in the details pane, on the inbound tab, choose edit. in the dialog, choose add rule, and then choose ssh from the type list. in the source field, choose my ip to automatically populate the field with the public ipv4 address of your local computer. alternatively, choose custom and specify the public ipv4 address of your computer or network in cidr notation. for example, if your ipv4 address is , specify  to list this single ipv4 address in cidr notation. if your company allocates addresses from a range, specify the entire range, such as . for information about finding your ip address, see . choose save. if you launched an instance with an ipv6 address and want to connect to your instance using its ipv6 address, you must add rules that allow inbound ipv6 traffic over ssh. to add a rule to a security group for inbound ssh traffic over ipv6 (console) open the amazon ec2 console at . in the navigation pane, choose security groups. select the security group for your instance. choose inbound, edit, add rule. for type, choose ssh. in the source field, specify the ipv6 address of your computer in cidr notation. for example, if your ipv6 address is , specify  to list the single ip address in cidr notation. if your company allocates addresses from a range, specify the entire range, such as . choose save. notebe sure to run the following commands on your local system, not on the instance itself. for more information about these command line interfaces, see . to add a rule to a security group using the command line find the security group that is associated with your instance using one of the following commands:  (aws cli)  (aws tools for windows powershell) both commands return a security group id, which you use in the next step. add the rule to the security group using one of the following commands:  (aws cli)  (aws tools for windows powershell) the  command needs an  parameter, which describes the protocol, port range, and ip address range to be used for the security group rule. the following command creates the  parameter: you can assign a security group to an instance when you launch the instance. when you add or remove rules, those changes are automatically applied to all instances to which you've assigned the security group. after you launch an instance, you can change its security groups. for more information, see  in the amazon vpc user guide. 
aggregate statistics are available for the instances that have detailed monitoring enabled. instances that use basic monitoring are not included in the aggregates. in addition, amazon cloudwatch does not aggregate data across regions. therefore, metrics are completely separate between regions. before you can get statistics aggregated across instances, you must enable detailed monitoring (at an additional charge), which provides data in 1-minute periods. this example shows you how to use detailed monitoring to get the average cpu usage for your ec2 instances. because no dimension is specified, cloudwatch returns statistics for all dimensions in the  namespace. importantthis technique for retrieving all dimensions across an aws namespace does not work for custom namespaces that you publish to amazon cloudwatch. with custom namespaces, you must specify the complete set of dimensions that are associated with any given data point to retrieve statistics that include the data point.  to display average cpu utilization across your instances (console) open the cloudwatch console at . in the navigation pane, choose metrics. choose the ec2 namespace and then choose across all instances. choose the row that contains cpuutilization, which displays a graph for the metric for all your ec2 instances. to name the graph, choose the pencil icon. to change the time range, select one of the predefined values or choose custom. to change the statistic or the period for the metric, choose the graphed metrics tab. choose the column heading or an individual value, and then choose a different value. to get average cpu utilization across your instances (aws cli)use the  command as follows to get the average of the cpuutilization metric across your instances. the following is example output: 
amazon cloudwatch events enables you to automate your aws services and respond automatically to system events such as application availability issues or resource changes. events from aws services are delivered to cloudwatch events in near real time. you can write simple rules to indicate which events are of interest to you, and the automated actions to take when an event matches a rule. the actions that can be automatically triggered include the following: invoking an aws lambda functioninvoking amazon ec2 run commandrelaying the event to amazon kinesis data streamsactivating an aws step functions state machinenotifying an amazon sns topic or an amazon sqs queuesome examples of using cloudwatch events with amazon ec2 include: activating a lambda function whenever a new amazon ec2 instance starts.notifying an amazon sns topic when an amazon ebs volume is created or modified.sending a command to one or more amazon ec2 instances using amazon ec2 run command whenever a certain event in another aws service occurs.for more information, see the . 
you can delete your instance when you no longer need it. this is referred to as terminating your instance. as soon as the state of an instance changes to  or , you stop incurring charges for that instance. you can't connect to or start an instance after you've terminated it. however, you can launch additional instances using the same ami. if you'd rather stop and start your instance, or hibernate it, see  or . for more information, see . topics after you terminate an instance, it remains visible in the console for a short while, and then the entry is automatically deleted. you cannot delete the terminated instance entry yourself. after an instance is terminated, resources such as tags and volumes are gradually disassociated from the instance and may no longer be visible on the terminated instance after a short while. when an instance terminates, the data on any instance store volumes associated with that instance is deleted.  by default, amazon ebs root device volumes are automatically deleted when the instance terminates. however, by default, any additional ebs volumes that you attach at launch, or any ebs volumes that you attach to an existing instance persist even after the instance terminates. this behavior is controlled by the volume's  attribute, which you can modify. for more information, see . you can prevent an instance from being terminated accidentally by someone using the aws management console, the cli, and the api. this feature is available for both amazon ec2 instance store-backed and amazon ebs-backed instances. each instance has a  attribute with the default value of  (the instance can be terminated through amazon ec2). you can modify this instance attribute while the instance is running or stopped (in the case of amazon ebs-backed instances). for more information, see . you can control whether an instance should stop or terminate when shutdown is initiated from the instance using an operating system command for system shutdown. for more information, see . if you run a script on instance termination, your instance might have an abnormal termination, because we have no way to ensure that shutdown scripts run. amazon ec2 attempts to shut an instance down cleanly and run any system shutdown scripts; however, certain events (such as hardware failure) may prevent these system shutdown scripts from running. when an ec2 instance is terminated using the  command, the following is registered at the os level: the api request will send a button press event to the guest.various system services will be stopped as a result of the button press event. systemd handles a graceful shutdown of the system. graceful shutdown is triggered by the acpi shutdown button press event from the hypervisor.acpi shutdown will be initiated.the instance will shut down when the graceful shutdown process exits. there is no configurable os shutdown time. you can terminate an instance using the aws management console or the command line. to terminate an instance using the console before you terminate the instance, verify that you won't lose any data by checking that your amazon ebs volumes won't be deleted on termination and that you've copied any data that you need from your instance store volumes to amazon ebs or amazon s3. open the amazon ec2 console at . in the navigation pane, choose instances. select the instance, and choose actions, instance state, terminate. choose yes, terminate when prompted for confirmation. to terminate an instance using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)by default, you can terminate your instance using the amazon ec2 console, command line interface, or api. to prevent your instance from being accidentally terminated using amazon ec2, you can enable termination protection for the instance. the  attribute controls whether the instance can be terminated using the console, cli, or api. by default, termination protection is disabled for your instance. you can set the value of this attribute when you launch the instance, while the instance is running, or while the instance is stopped (for amazon ebs-backed instances).  the  attribute does not prevent you from terminating an instance by initiating shutdown from the instance (using an operating system command for system shutdown) when the  attribute is set. for more information, see . limitationsyou can't enable termination protection for spot instances—a spot instance is terminated when the spot price exceeds the amount you're willing to pay for spot instances. however, you can prepare your application to handle spot instance interruptions. for more information, see . the  attribute does not prevent amazon ec2 auto scaling from terminating an instance. for instances in an auto scaling group, use the following amazon ec2 auto scaling features instead of amazon ec2 termination protection: to prevent instances that are part of an auto scaling group from terminating on scale in, use instance protection. for more information, see  in the amazon ec2 auto scaling user guide.to prevent amazon ec2 auto scaling from terminating unhealthy instances, suspend the  process. for more information, see  in the amazon ec2 auto scaling user guide.to specify which instances amazon ec2 auto scaling should terminate first, choose a termination policy. for more information, see  in the amazon ec2 auto scaling user guide.to enable termination protection for an instance at launch time open the amazon ec2 console at . on the dashboard, choose launch instance and follow the directions in the wizard. on the configure instance details page, select the enable termination protection check box. to enable termination protection for a running or stopped instance select the instance, and choose actions, instance settings, change termination protection. choose yes, enable. to disable termination protection for a running or stopped instance select the instance, and choose actions, instance settings, change termination protection. choose yes, disable. to enable or disable termination protection using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)by default, when you initiate a shutdown from an amazon ebs-backed instance (using a command such as shutdown or poweroff), the instance stops (note that halt does not issue a poweroff command and, if used, the instance will not terminate; instead, it will place the cpu into hlt and the instance will remain running). you can change this behavior using the  attribute for the instance so that it terminates instead. you can update this attribute while the instance is running or stopped.  you can update the  attribute using the amazon ec2 console or the command line. the  attribute only applies when you perform a shutdown from the operating system of the instance itself; it does not apply when you stop an instance using the  api or the amazon ec2 console. to change the shutdown behavior of an instance using the console open the amazon ec2 console at . in the navigation pane, choose instances. select the instance, and choose actions, instance settings, change shutdown behavior. the current behavior is already selected. to change the behavior, select an option from the shutdown behavior list, and then choose apply. to change the shutdown behavior of an instance using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)when an instance terminates, amazon ec2 uses the value of the  attribute for each attached amazon ebs volume to determine whether to preserve or delete the volume. the default value for the  attribute differs depending on whether the volume is the root volume of the instance or a non-root volume attached to the instance. root volumeby default, the  attribute for the root volume of an instance is set to . therefore, the default is to delete the root volume of the instance when the instance terminates. the  attribute can be set by the creator of an ami as well as by the person who launches an instance. when the attribute is changed by the creator of an ami or by the person who launches an instance, the new setting overrides the original ami default setting. we recommend that you verify the default setting for the  attribute after you launch an instance with an ami. non-root volumeby default, when you , its  attribute is set to . therefore, the default is to preserve these volumes. after the instance terminates, you can take a snapshot of the preserved volume or attach it to another instance. you must delete a volume to avoid incurring further charges. for more information, see . to verify the value of the  attribute for an ebs volume that is in use, look at the instance's block device mapping. for more information, see . you can change the value of the  attribute for a volume when you launch the instance or while the instance is running. topics using the console, you can change the  attribute when you launch an instance. to change this attribute for a running instance, you must use the command line. to change the root volume of an instance to persist at launch using the console open the amazon ec2 console at . from the console dashboard, select launch instance. on the choose an amazon machine image (ami) page, choose an ami and choose select. follow the wizard to complete the choose an instance type and configure instance details pages. on the add storage page, deselect the delete on termination check box for the root volume. complete the remaining wizard pages, and then choose launch. you can verify the setting by viewing details for the root device volume on the instance's details pane. next to block devices, choose the entry for the root device volume. by default, delete on termination is . if you change the default behavior, delete on termination is . when you launch an ebs-backed instance, you can use one of the following commands to change the root device volume to persist. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)for example, add the following option to your  command: specify the following in : you can use one of the following commands to change the root device volume of a running ebs-backed instance to persist. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)for example, use the following command: specify the following in : if your instance is in the  state for longer than usual, it will eventually be cleaned up (terminated) by automated processes within the amazon ec2 service. for more information, see .  
by default, iam users don't have permission to create or modify amazon ec2 resources, or perform tasks using the amazon ec2 api. (this means that they also can't do so using the amazon ec2 console or cli.) to allow iam users to create or modify resources and perform tasks, you must create iam policies that grant iam users permission to use the specific resources and api actions they'll need, and then attach those policies to the iam users or groups that require those permissions. when you attach a policy to a user or group of users, it allows or denies the users permission to perform the specified tasks on the specified resources. for more general information about iam policies, see  in the iam user guide. for more information about managing and creating custom iam policies, see . getting started an iam policy must grant or deny permissions to use one or more amazon ec2 actions. it must also specify the resources that can be used with the action, which can be all resources, or in some cases, specific resources. the policy can also include conditions that you apply to the resource.  amazon ec2 partially supports resource-level permissions. this means that for some ec2 api actions, you cannot specify which resource a user is allowed to work with for that action. instead, you have to allow users to work with all resources for that action.  
you can select an ami to use based on the following characteristics: region (see )operating systemarchitecture (32-bit or 64-bit)the owner of an ami determines its availability by specifying launch permissions. launch permissions fall into the following categories. amazon and the amazon ec2 community provide a large selection of public amis. for more information, see . developers can charge for their amis. for more information, see . all amis are categorized as either backed by amazon ebs or backed by instance store. the former means that the root device for an instance launched from the ami is an amazon ebs volume created from an amazon ebs snapshot. the latter means that the root device for an instance launched from the ami is an instance store volume created from a template stored in amazon s3. for more information, see . the following table summarizes the important differences when using the two types of amis. * by default, amazon ebs-backed instance root volumes have the  flag set to . for information about how to change this flag so that the volume persists after termination, see . to determine the root device type of an ami using the console open the amazon ec2 console. in the navigation pane, click amis, and select the ami. check the value of root device type in the details tab as follows: if the value is , this is an amazon ebs-backed ami.if the value is , this is an instance store-backed ami.to determine the root device type of an ami using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)you can stop an amazon ebs-backed instance, but not an amazon ec2 instance store-backed instance. stopping causes the instance to stop running (its status goes from  to  to ). a stopped instance persists in amazon ebs, which allows it to be restarted. stopping is different from terminating; you can't restart a terminated instance. because amazon ec2 instance store-backed instances can't be stopped, they're either running or terminated. for more information about what happens and what you can do while an instance is stopped, see . instances that use an instance store volume for the root device automatically have instance store available (the root volume contains the root partition and you can store additional data). you can add persistent storage to your instance by attaching one or more amazon ebs volumes. any data on an instance store volume is deleted when the instance fails or terminates. for more information, see . instances that use amazon ebs for the root device automatically have an amazon ebs volume attached. the volume appears in your list of volumes like any other. with most instance types, amazon ebs-backed instances don't have instance store volumes by default. you can add instance store volumes or additional amazon ebs volumes using a block device mapping. for more information, see . instances launched from an amazon ebs-backed ami launch faster than instances launched from an instance store-backed ami. when you launch an instance from an instance store-backed ami, all the parts have to be retrieved from amazon s3 before the instance is available. with an amazon ebs-backed ami, only the parts required to boot the instance need to be retrieved from the snapshot before the instance is available. however, the performance of an instance that uses an amazon ebs volume for its root device is slower for a short time while the remaining parts are retrieved from the snapshot and loaded into the volume. when you stop and restart the instance, it launches quickly, because the state is stored in an amazon ebs volume. to create linux amis backed by instance store, you must create an ami from your instance on the instance itself using the amazon ec2 ami tools.  ami creation is much easier for amis backed by amazon ebs. the  api action creates your amazon ebs-backed ami and registers it. there's also a button in the aws management console that lets you create an ami from a running instance. for more information, see . with amis backed by instance store, you're charged for instance usage and storing your ami in amazon s3. with amis backed by amazon ebs, you're charged for instance usage, amazon ebs volume storage and usage, and storing your ami as an amazon ebs snapshot. with amazon ec2 instance store-backed amis, each time you customize an ami and create a new one, all of the parts are stored in amazon s3 for each ami. so, the storage footprint for each customized ami is the full size of the ami. for amazon ebs-backed amis, each time you customize an ami and create a new one, only the changes are stored. so the storage footprint for subsequent amis you customize after the first is much smaller, resulting in lower ami storage charges.  when an amazon ebs-backed instance is stopped, you're not charged for instance usage; however, you're still charged for volume storage. as soon as you start your instance, we charge a minimum of one minute for usage. after one minute, we charge only for the seconds used. for example, if you run an instance for 20 seconds and then stop it, we charge for a full one minute. if you run an instance for 3 minutes and 40 seconds, we charge for exactly 3 minutes and 40 seconds of usage. we charge you for each second, with a one-minute minimum, that you keep the instance running, even if the instance remains idle and you don't connect to it. 
you can launch an instance using the launch instance wizard. the launch instance wizard specifies all the launch parameters required for launching an instance. where the launch instance wizard provides a default value, you can accept the default or specify your own value. at the very least, you need to select an ami and a key pair to launch an instance. before you launch your instance, be sure that you are set up. for more information, see . importantwhen you launch an instance that's not within the , you are charged for the time that the instance is running, even if it remains idle. topics open the amazon ec2 console at . in the navigation bar at the top of the screen, the current region is displayed (for example, us east (ohio)). select a region for the instance that meets your needs. this choice is important because some amazon ec2 resources can be shared between regions, while others can't. for more information, see . from the amazon ec2 console dashboard, choose launch instance. when you launch an instance, you must select a configuration, known as an amazon machine image (ami). an ami contains the information required to create a new instance. for example, an ami might contain the software required to act as a web server, such as linux, apache, and your website. when you launch an instance, you can either select an ami from the list, or you can select a systems manager parameter that points to an ami id. for more information, see . on the choose an amazon machine image (ami) page, use one of two options to choose an ami. either , or . by searching the list of amis select the type of ami to use in the left pane:quick starta selection of popular amis to help you get started quickly. to select an ami that is eligible for the free tier, choose free tier only in the left pane. these amis are marked free tier eligible.my amisthe private amis that you own, or private amis that have been shared with you. to view amis that are shared with you, choose shared with me in the left pane.aws marketplacean online store where you can buy software that runs on aws, including amis. for more information about launching an instance from the aws marketplace, see .community amisthe amis that aws community members have made available for others to use. to filter the list of amis by operating system, choose the appropriate check box under operating system. you can also filter by architecture and root device type. check the root device type listed for each ami. notice which amis are the type that you need, either  (backed by amazon ebs) or  (backed by instance store). for more information, see .  check the virtualization type listed for each ami. notice which amis are the type that you need, either  or . for example, some instance types require hvm. for more information, see . choose an ami that meets your needs, and then choose select. by systems manager parameter choose search by systems manager parameter (at top right). for systems manager parameter, select a parameter. the corresponding ami id appears next to currently resolves to. choose search. the amis that match the ami id appear in the list. select the ami from the list, and choose select. on the choose an instance type page, select the hardware configuration and size of the instance to launch. larger instance types have more cpu and memory. for more information, see . to remain eligible for the free tier, choose the t2.micro instance type (or the t3.micro instance type in regions where t2.micro is unavailable). for more information, see . by default, the wizard displays current generation instance types, and selects the first available instance type based on the ami that you selected. to view previous generation instance types, choose all generations from the filter list. noteto set up an instance quickly for testing purposes, choose review and launch to accept the default configuration settings, and launch your instance. otherwise, to configure your instance further, choose next: configure instance details. on the configure instance details page, change the following settings as necessary (expand advanced details to see all the settings), and then choose next: add storage: number of instances: enter the number of instances to launch. tipto ensure faster instance launches, break up large requests into smaller batches. for example, create five separate launch requests for 100 instances each instead of one launch request for 500 instances.(optional) to help ensure that you maintain the correct number of instances to handle demand on your application, you can choose launch into auto scaling group to create a launch configuration and an auto scaling group. auto scaling scales the number of instances in the group according to your specifications. for more information, see the .purchasing option: choose request spot instances to launch a spot instance. this adds and removes options from this page. set your maximum price, and optionally update the request type, interruption behavior, and request validity. for more information, see .network: select the vpc, or to create a new vpc, choose create new vpc to go the amazon vpc console. when you have finished, return to the wizard and choose refresh to load your vpc in the list.subnet: you can launch an instance in a subnet associated with an availability zone, local zone, or outpost. to launch the instance in an availability zone, select the subnet into which to launch your instance. you can select no preference to let aws choose a default subnet in any availability zone. to create a new subnet, choose create new subnet to go to the amazon vpc console. when you are done, return to the wizard and choose refresh to load your subnet in the list. to launch the instance in a local zone, select a subnet that you created in the local zone.  to launch an instance in an outpost, select a subnet in a vpc that you associated with an outpost. auto-assign public ip: specify whether your instance receives a public ipv4 address. by default, instances in a default subnet receive a public ipv4 address and instances in a nondefault subnet do not. you can select enable or disable to override the subnet's default setting. for more information, see .auto-assign ipv6 ip: specify whether your instance receives an ipv6 address from the range of the subnet. select enable or disable to override the subnet's default setting. this option is only available if you've associated an ipv6 cidr block with your vpc and subnet. for more information, see  in the amazon vpc user guide.placement group: a placement group determines the placement strategy of your instances. select an existing placement group, or create a new one. this option is only available if you've selected an instance type that supports placement groups. for more information, see .capacity reservation: specify whether to launch the instance into shared capacity or an existing capacity reservation. for more information, see .iam role: select an aws identity and access management (iam) role to associate with the instance. for more information, see .cpu options: choose specify cpu options to specify a custom number of vcpus during launch. set the number of cpu cores and threads per core. for more information, see .shutdown behavior: select whether the instance should stop or terminate when shut down. for more information, see .stop - hibernate behavior: to enable hibernation, select this check box. this option is only available if your instance meets the hibernation prerequisites. for more information, see .enable termination protection: to prevent accidental termination, select this check box. for more information, see .monitoring: select this check box to enable detailed monitoring of your instance using amazon cloudwatch. additional charges apply. for more information, see .ebs-optimized instance: an amazon ebs-optimized instance uses an optimized configuration stack and provides additional, dedicated capacity for amazon ebs i/o. if the instance type supports this feature, select this check box to enable it. additional charges apply. for more information, see .tenancy: if you are launching your instance into a vpc, you can choose to run your instance on isolated, dedicated hardware (dedicated) or on a dedicated host (dedicated host). additional charges may apply. for more information, see  and .t2/t3 unlimited: select this check box to enable applications to burst beyond the baseline for as long as needed. additional charges may apply. for more information, see .file systems: choose add file system to mount one or more amazon efs file systems to your instance. for more information, see .network interfaces: if you selected a specific subnet, you can specify up to two network interfaces for your instance: for network interface, select new network interface to let aws create a new interface, or select an existing, available network interface.for primary ip, enter a private ipv4 address from the range of your subnet, or leave auto-assign to let aws choose a private ipv4 address for you.for secondary ip addresses, choose add ip to assign more than one private ipv4 address to the selected network interface.(ipv6-only) for ipv6 ips, choose add ip, and enter an ipv6 address from the range of the subnet, or leave auto-assign to let aws choose one for you.choose add device to add a secondary network interface. a secondary network interface can reside in a different subnet of the vpc, provided it's in the same availability zone as your instance.for more information, see . if you specify more than one network interface, your instance cannot receive a public ipv4 address. additionally, if you specify an existing network interface for eth0, you cannot override the subnet's public ipv4 setting using auto-assign public ip. for more information, see . kernel id: (only valid for paravirtual (pv) amis) select use default unless you want to use a specific kernel.ram disk id: (only valid for paravirtual (pv) amis) select use default unless you want to use a specific ram disk. if you have selected a kernel, you may need to select a specific ram disk with the drivers to support it.metadata accessible: you can enable or disable access to the instance metadata. for more information, see .metadata version: if you enable access to the instance metadata, you can choose to require the use of instance metadata service version 2 when requesting instance metadata. for more information, see .metadata token response hop limit: if you enable instance metadata, you can set the allowable number of network hops for the metadata token. for more information, see .user data: you can specify user data to configure an instance during launch, or to run a configuration script. to attach a file, select the as file option and browse for the file to attach.the ami you selected includes one or more volumes of storage, including the root device volume. on the add storage page, you can specify additional volumes to attach to the instance by choosing add new volume. configure each volume as follows, and then choose next: add tags. type: select instance store or amazon ebs volumes to associate with your instance. the types of volume available in the list depend on the instance type you've chosen. for more information, see  and .device: select from the list of available device names for the volume. snapshot: enter the name or id of the snapshot from which to restore a volume. you can also search for available shared and public snapshots by typing text into the snapshot field. snapshot descriptions are case-sensitive.size: for ebs volumes, you can specify a storage size. even if you have selected an ami and instance that are eligible for the free tier, to stay within the free tier, you must stay under 30 gib of total storage. for more information, see .volume type: for ebs volumes, select a volume type. for more information, see .iops: if you have selected a provisioned iops ssd volume type, then you can enter the number of i/o operations per second (iops) that the volume can support.delete on termination: for amazon ebs volumes, select this check box to delete the volume when the instance is terminated. for more information, see .encrypted: if the instance type supports ebs encryption, you can specify the encryption state of the volume. if you have enabled encryption by default in this region, the default cmk is selected for you. you can select a different key or disable encryption. for more information, see .on the add tags page, specify  by providing key and value combinations. you can tag the instance, the volumes, or both. for spot instances, you can tag the spot instance request only. choose add another tag to add more than one tag to your resources. choose next: configure security group when you are done. on the configure security group page, use a security group to define firewall rules for your instance. these rules specify which incoming network traffic is delivered to your instance. all other traffic is ignored. (for more information about security groups, see .) select or create a security group as follows, and then choose review and launch. to select an existing security group, choose select an existing security group, and select your security group. you can't edit the rules of an existing security group, but you can copy them to a new group by choosing copy to new. then you can add rules as described in the next step.to create a new security group, choose create a new security group. the wizard automatically defines the launch-wizard-x security group and creates an inbound rule to allow you to connect to your instance over ssh (port 22).you can add rules to suit your needs. for example, if your instance is a web server, open ports 80 (http) and 443 (https) to allow internet traffic. to add a rule, choose add rule, select the protocol to open to network traffic, and then specify the source. choose my ip from the source list to let the wizard add your computer's public ip address. however, if you are connecting through an isp or from behind your firewall without a static ip address, you need to find out the range of ip addresses used by client computers. warningrules that enable all ip addresses () to access your instance over ssh or rdp are acceptable for this short exercise, but are unsafe for production environments. you should authorize only a specific ip address or range of addresses to access your instance. on the review instance launch page, check the details of your instance, and make any necessary changes by choosing the appropriate edit link. when you are ready, choose launch. in the select an existing key pair or create a new key pair dialog box, you can choose an existing key pair, or create a new one. for example, choose choose an existing key pair, then select the key pair you created when getting set up. for more information, see . importantif you choose the proceed without key pair option, you won't be able to connect to the instance unless you choose an ami that is configured to allow users another way to log in. to launch your instance, select the acknowledgment check box, then choose launch instances. (optional) you can create a status check alarm for the instance (additional fees may apply). (if you're not sure, you can always add one later.) on the confirmation screen, choose create status check alarms and follow the directions. for more information, see . if the instance fails to launch or the state immediately goes to  instead of , see . 
memory optimized instances are designed to deliver fast performance for workloads that process large data sets in memory. r5, r5a, and r5n instances these instances are well suited for the following: high-performance, relational (mysql) and nosql (mongodb, cassandra) databases.distributed web scale cache stores that provide in-memory caching of key-value type data (memcached and redis).in-memory databases using optimized data storage formats and analytics for business intelligence (for example, sap hana).applications performing real-time processing of big unstructured data (financial services, hadoop/spark clusters).high-performance computing (hpc) and electronic design automation (eda) applications.bare metal instances, such as , provide your applications with direct access to physical resources of the host server, such as processors and memory. these instances are well suited for the following: workloads that require access to low-level hardware features (for example, intel vt) that are not available or fully supported in virtualized environmentsapplications that require a non-virtualized environment for licensing or supportfor more information, see . r6g instances these instances are powered by aws graviton2 processors and are ideal for running memory-intensive workloads, such as the following: open-source databases (for example, mysql, mariadb, and postgresql)in-memory caches (for example, memcached, redis, and keydb)bare metal instances, such as , provide your applications with direct access to physical resources of the host server, such as processors and memory. for more information, see . high memory instanceshigh memory instances (, , , , and ) offer 6 tib, 9 tib, 12 tib, 18 tib, and 24 tib of memory per instance. these instances are designed to run large in-memory databases, including production deployments of the sap hana in-memory database, in the cloud. they offer bare metal performance with direct access to host hardware. for more information, see  and . x1 instances these instances are well suited for the following: in-memory databases such as sap hana, including sap-certified support for business suite s/4hana, business suite on hana (soh), business warehouse on hana (bw), and data mart solutions on hana. for more information, see .big-data processing engines such as apache spark or presto.high-performance computing (hpc) applications.for more information, see . x1e instances these instances are well suited for the following: high-performance databases.in-memory databases such as sap hana. for more information, see .memory-intensive enterprise applications.for more information, see . z1d instances these instances deliver both high compute and high memory and are well-suited for the following: electronic design automation (eda)relational database workloads instances provide your applications with direct access to physical resources of the host server, such as processors and memory. these instances are well suited for the following: workloads that require access to low-level hardware features (for example, intel vt) that are not available or fully supported in virtualized environmentsapplications that require a non-virtualized environment for licensing or supportfor more information, see . topics the following is a summary of the hardware specifications for memory optimized instances. * each logical processor is a hyperthread on 224 cores. for more information about the hardware specifications for each amazon ec2 instance type, see . for more information about specifying cpu options, see . x1 instances include intel scalable m​​emory buffers, providing 300 gib/s of sustainable memory-read bandwidth and 140 gib/s of sustainable memory-write bandwidth. for more information about how much ram can be enabled for memory optimized instances, see . memory optimized instances have high memory and require 64-bit hvm amis to take advantage of that capacity. hvm amis provide superior performance in comparison to paravirtual (pv) amis on memory optimized instances. for more information, see . r4 instances feature up to 64 vcpus and are powered by two aws-customized intel xeon processors based on e5-2686v4 that feature high-memory bandwidth and larger l3 caches to boost the performance of in-memory applications. x1e and x1 instances feature up to 128 vcpus and are powered by four intel xeon e7-8880 v3 processors that feature high-memory bandwidth and larger l3 caches to boost the performance of in-memory applications. high memory instances (, , and ) are the first instances to be powered by an eight-socket platform with the latest generation intel xeon platinum 8176m (skylake) processors that are optimized for mission-critical enterprise workloads. high memory instances with 18 tb and 24 tb of memory ( and ) are the first instances powered by an 8-socket platform with 2nd generation intel xeon scalable 8280l (cascade lake) processors. memory optimized instances enable increased cryptographic performance through the latest intel aes-ni feature, support intel transactional synchronization extensions (tsx) to boost the performance of in-memory transactional data processing, and support advanced vector extensions 2 (intel avx2) processor instructions to expand most integer commands to 256 bits. some memory optimized instances provide the ability to control processor c-states and p-states on linux. c-states control the sleep levels that a core can enter when it is inactive, while p-states control the desired performance (measured by cpu frequency) from a core. for more information, see . you can enable enhanced networking on supported instance types to provide lower latencies, lower network jitter, and higher packet-per-second (pps) performance. most applications do not consistently need a high level of network performance, but can benefit from access to increased bandwidth when they send or receive data. for more information, see . the following is a summary of network performance for memory optimized instances that support enhanced networking. * instances of this type launched after march 12, 2020 provide network performance of 100 gbps. instances of this type launched before march 12, 2020 might only provide network performance of 25 gbps. to ensure that instances launched before march 12, 2020 have a network performance of 100 gbps, contact your account team to upgrade your instance at no additional cost. † these instances use a network i/o credit mechanism to allocate network bandwidth to instances based on average bandwidth utilization. they accrue credits when their bandwidth is below their baseline bandwidth, and can use these credits when they perform network data transfers. for more information, open a support case and ask about baseline bandwidth for the specific instance types that you are interested in. if you use a linux ami with kernel version 4.4 or later and use all the ssd-based instance store volumes available to your instance, you get the iops (4,096 byte block size) performance listed in the following table (at queue depth saturation). otherwise, you get lower iops performance. * for these instances, you can get up to the specified performance. as you fill the ssd-based instance store volumes for your instance, the number of write iops that you can achieve decreases. this is due to the extra work the ssd controller must do to find available space, rewrite existing data, and erase unused space so that it can be rewritten. this process of garbage collection results in internal write amplification to the ssd, expressed as the ratio of ssd write operations to user write operations. this decrease in performance is even larger if the write operations are not in multiples of 4,096 bytes or not aligned to a 4,096-byte boundary. if you write a smaller amount of bytes or bytes that are not aligned, the ssd controller must read the surrounding data and store the result in a new location. this pattern results in significantly increased write amplification, increased latency, and dramatically reduced i/o performance. ssd controllers can use several strategies to reduce the impact of write amplification. one such strategy is to reserve space in the ssd instance storage so that the controller can more efficiently manage the space available for write operations. this is called over-provisioning. the ssd-based instance store volumes provided to an instance don't have any space reserved for over-provisioning. to reduce write amplification, we recommend that you leave 10% of the volume unpartitioned so that the ssd controller can use it for over-provisioning. this decreases the storage that you can use, but increases performance even if the disk is close to full capacity. for instance store volumes that support trim, you can use the trim command to notify the ssd controller whenever you no longer need data that you've written. this provides the controller with more free space, which can reduce write amplification and increase performance. for more information, see . the following is a summary of features for memory optimized instances. * the root device volume must be an amazon ebs volume. for more information, see the following: memory optimized instances provide a high number of vcpus, which can cause launch issues with operating systems that have a lower vcpu limit. we strongly recommend that you use the latest amis when you launch memory optimized instances. the following amis support launching memory optimized instances: amazon linux 2 (hvm)amazon linux ami 2016.03 (hvm) or laterubuntu server 14.04 lts (hvm)red hat enterprise linux 7.1 (hvm)suse linux enterprise server 12 sp1 (hvm)windows server 2019windows server 2016windows server 2012 r2windows server 2012windows server 2008 r2 64-bitwindows server 2008 sp2 64-bitr5 and r5d instances feature a 3.1 ghz intel xeon platinum 8000 series processor from either the first generation (skylake-sp) or second generation (cascade lake).r5a and r5ad instances feature a 2.5 ghz amd epyc 7000 series processor.r6g instances feature an aws graviton2 processor based on 64-bit arm architecture.instances built on the nitro system have the following requirements:  must be installed must be installedthe following linux amis meet these requirements: amazon linux 2amazon linux ami 2018.03ubuntu 14.04 (with  kernel) or laterred hat enterprise linux 7.4 or latersuse linux enterprise server 12 sp2 or latercentos 7.4.1708 or laterfreebsd 11.1 or laterdebian gnu/linux 9 or laterinstances with an aws graviton processors have the following requirements: use an ami for the 64-bit arm architecture.support booting through uefi with acpi tables and support acpi hot-plug of pci devices.the following amis meet these requirements: amazon linux 2 (64-bit arm)ubuntu 16.04 or later (64-bit arm)red hat enterprise linux 8.0 or later (64-bit arm)suse linux enterprise server 15 or later (64-bit arm)instances built on the nitro system instances support a maximum of 28 attachments, including network interfaces, ebs volumes, and nvme instance store volumes. for more information, see .launching a bare metal instance boots the underlying server, which includes verifying all hardware and firmware components. this means that it can take 20 minutes from the time the instance enters the running state until it becomes available over the network.to attach or detach ebs volumes or secondary network interfaces from a bare metal instance requires pcie native hotplug support. amazon linux 2 and the latest versions of the amazon linux ami support pcie native hotplug, but earlier versions do not. you must enable the following linux kernel configuration options: bare metal instances use a pci-based serial device rather than an i/o port-based serial device. the upstream linux kernel and the latest amazon linux amis support this device. bare metal instances also provide an acpi spcr table to enable the system to automatically use the pci-based serial device. the latest windows amis automatically use the pci-based serial device.you can't launch x1 instances using a windows server 2008 sp2 64-bit ami, except for  instances.you can't launch x1e instances using a windows server 2008 sp2 64-bit ami.with earlier versions of the windows server 2008 r2 64-bit ami, you can't launch  and  instances. if you experience this issue, update to the latest version of this ami.there is a limit on the total number of instances that you can launch in a region, and there are additional limits on some instance types. for more information, see  in the amazon ec2 faq.
you can use aws config to record configuration changes for dedicated hosts, and for instances that are launched, stopped, or terminated on them. you can then use the information captured by aws config as a data source for license reporting. aws config records configuration information for dedicated hosts and instances individually, and pairs this information through relationships. there are three reporting conditions: aws config recording status—when on, aws config is recording one or more aws resource types, which can include dedicated hosts and dedicated instances. to capture the information required for license reporting, verify that hosts and instances are being recorded with the following fields.host recording status—when enabled, the configuration information for dedicated hosts is recorded.instance recording status—when enabled, the configuration information for dedicated instances is recorded.if any of these three conditions are disabled, the icon in the edit config recording button is red. to derive the full benefit of this tool, ensure that all three recording methods are enabled. when all three are enabled, the icon is green. to edit the settings, choose edit config recording. you are directed to the set up aws config page in the aws config console, where you can set up aws config and start recording for your hosts, instances, and other supported resource types. for more information, see  in the aws config developer guide. noteaws config records your resources after it discovers them, which might take several minutes.  after aws config starts recording configuration changes to your hosts and instances, you can get the configuration history of any host that you have allocated or released and any instance that you have launched, stopped, or terminated. for example, at any point in the configuration history of a dedicated host, you can look up how many instances are launched on that host, along with the number of sockets and cores on the host. for any of those instances, you can also look up the id of its amazon machine image (ami). you can use this information to report on licensing for your own server-bound software that is licensed per-socket or per-core. you can view configuration histories in any of the following ways: by using the aws config console. for each recorded resource, you can view a timeline page, which provides a history of configuration details. to view this page, choose the gray icon in the config timeline column of the dedicated hosts page. for more information, see  in the aws config developer guide.by running aws cli commands. first, you can use the  command to get a list of all hosts and instances. then, you can use the  command to get the configuration details of a host or instance for a specific time interval. for more information, see  in the aws config developer guide.by using the aws config api in your applications. first, you can use the  action to get a list of all hosts and instances. then, you can use the  action to get the configuration details of a host or instance for a specific time interval.for example, to get a list of all of your dedicated hosts from aws config, run a cli command such as the following. to obtain the configuration history of a dedicated host from aws config, run a cli command such as the following. to manage aws config settings using the console open the amazon ec2 console at . on the dedicated hosts page, choose edit config recording. in the aws config console, follow the steps provided to turn on recording. for more information, see . for more information, see . to activate aws config using the command line or api aws cli:  in the aws config developer guide.amazon ec2 api: .
you can attach an available ebs volume to one or more of your instances that is in the same availability zone as the volume. prerequisites determine how many volumes you can attach to your instance. for more information, see .determine whether you can attach your volume to multiple instances and enable multi-attach. for more information, see .if a volume is encrypted, it can only be attached to an instance that supports amazon ebs encryption. for more information, see .if a volume has an aws marketplace product code:the volume can only be attached to a stopped instance.you must be subscribed to the aws marketplace code that is on the volume.the configuration (instance type, operating system) of the instance must support that specific aws marketplace code. for example, you cannot take a volume from a windows instance and attach it to a linux instance.aws marketplace product codes are copied from the volume to the instance.to attach an ebs volume to an instance using the console open the amazon ec2 console at . in the navigation pane, choose elastic block store, volumes. select an available volume and choose actions, attach volume. for instance, start typing the name or id of the instance. select the instance from the list of options (only instances that are in the same availability zone as the volume are displayed). for device, you can keep the suggested device name, or type a different supported device name. for more information, see . choose attach. connect to your instance and mount the volume. for more information, see . to attach an ebs volume to an instance using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)
some resource-creating amazon ec2 api actions enable you to specify tags when you create the resource. for more information, see . to enable users to tag resources on creation, they must have permissions to use the action that creates the resource, such as  or . if tags are specified in the resource-creating action, amazon performs additional authorization on the  action to verify if users have permissions to create tags. therefore, users must also have explicit permissions to use the  action.  in the iam policy definition for the  action, use the  element with the  condition key to give tagging permissions to the action that creates the resource. the following example demonstrates a policy that allows users to launch instances and apply any tags to instances and volumes during launch. users are not permitted to tag any existing resources (they cannot call the  action directly). similarly, the following policy allows users to create volumes and apply any tags to the volumes during volume creation. users are not permitted to tag any existing resources (they cannot call the  action directly). the  action is only evaluated if tags are applied during the resource-creating action. therefore, a user that has permissions to create a resource (assuming there are no tagging conditions) does not require permissions to use the  action if no tags are specified in the request. however, if the user attempts to create a resource with tags, the request fails if the user does not have permissions to use the  action. the  action is also evaluated if tags are provided in a launch template. for an example policy, see . you can use additional conditions in the  element of your iam policies to control the tag keys and values that can be applied to resources. the following condition keys can be used with the examples in the preceding section: : to indicate that a particular tag key or tag key and value must be present in a request. other tags can also be specified in the request. use with the  condition operator to enforce a specific tag key and value combination, for example, to enforce the tag =: use with the  condition operator to enforce a specific tag key in the request; for example, to enforce the tag key : : to enforce the tag keys that are used in the request. use with the  modifier to enforce specific tag keys if they are provided in the request (if tags are specified in the request, only specific tag keys are allowed; no other tags are allowed). for example, the tag keys  or  are allowed: use with the  modifier to enforce the presence of at least one of the specified tag keys in the request. for example, at least one of the tag keys  or  must be present in the request: these condition keys can be applied to resource-creating actions that support tagging, as well as the  and  actions. to learn whether an amazon ec2 api action supports tagging, see  in the iam user guide.  to force users to specify tags when they create a resource, you must use the  condition key or the  condition key with the  modifier on the resource-creating action. the  action is not evaluated if a user does not specify tags for the resource-creating action. for conditions, the condition key is not case-sensitive and the condition value is case-sensitive. therefore, to enforce the case-sensitivity of a tag key, use the  condition key, where the tag key is specified as a value in the condition. for example iam policies, see . for more information about multi-value conditions, see  in the iam user guide.  
when you launch an instance in amazon ec2, you have the option of passing user data to the instance that can be used to perform common automated configuration tasks and even run scripts after the instance starts. you can pass two types of user data to amazon ec2: shell scripts and cloud-init directives. you can also pass this data into the launch wizard as plain text, as a file (this is useful for launching instances using the command line tools), or as base64-encoded text (for api calls). if you are interested in more complex automation scenarios, consider using aws cloudformation and aws opsworks. for more information, see the  and the . for information about running commands on your windows instance at launch, see  and  in the amazon ec2 user guide for windows instances. in the following examples, the commands from the  are converted to a shell script and a set of cloud-init directives that executes when the instance launches. in each example, the following tasks are executed by the user data: the distribution software packages are updated.the necessary web server, , and  packages are installed.the  service is started and turned on via systemctl.the  is added to the apache group.the appropriate ownership and file permissions are set for the web directory and the files contained within it.a simple web page is created to test the web server and php engine.topics the following examples assume that your instance has a public dns name that is reachable from the internet. for more information, see . you must also configure your security group to allow ssh (port 22), http (port 80), and https (port 443) connections. for more information about these prerequisites, see . also, these instructions are intended for use with amazon linux 2, and the commands and directives may not work for other linux distributions. for more information about other distributions, such as their support for cloud-init, see their specific documentation. if you are familiar with shell scripting, this is the easiest and most complete way to send instructions to an instance at launch. adding these tasks at boot time adds to the amount of time it takes to boot the instance. you should allow a few minutes of extra time for the tasks to complete before you test that the user script has finished successfully. importantby default, user data scripts and cloud-init directives run only during the boot cycle when you first launch an instance. you can update your configuration to ensure that your user data scripts and cloud-init directives run every time you restart your instance. for more information, see  in the aws knowledge center. user data shell scripts must start with the  characters and the path to the interpreter you want to read the script (commonly /bin/bash). for a great introduction on shell scripting, see  at the linux documentation project (). scripts entered as user data are executed as the  user, so do not use the sudo command in the script. remember that any files you create will be owned by ; if you need non-root users to have file access, you should modify the permissions accordingly in the script. also, because the script is not run interactively, you cannot include commands that require user feedback (such as yum update without the  flag). the cloud-init output log file () captures console output so it is easy to debug your scripts following a launch if the instance does not behave the way you intended. when a user data script is processed, it is copied to and executed from . the script is not deleted after it is run. be sure to delete the user data scripts from  before you create an ami from the instance. otherwise, the script will exist in this directory on any instance launched from the ami. you can specify instance user data when you launch the instance. if the root volume of the instance is an ebs volume, you can also stop the instance and update its user data. follow the procedure for launching an instance at , but when you get to  in that procedure, copy your shell script in the user data field, and then complete the launch procedure. in the example script below, the script creates and configures our web server. allow enough time for the instance to launch and execute the commands in your script, and then check to see that your script has completed the tasks that you intended. for our example, in a web browser, enter the url of the php test file the script created. this url is the public dns address of your instance followed by a forward slash and the file name. you should see the php information page. if you are unable to see the php information page, check that the security group you are using contains a rule to allow http (port 80) traffic. for more information, see . (optional) if your script did not accomplish the tasks you were expecting it to, or if you just want to verify that your script completed without errors, examine the cloud-init output log file at  and look for error messages in the output.  for additional debugging information, you can create a mime multipart archive that includes a cloud-init data section with the following directive: this directive sends command output from your script to . for more information about cloud-init data formats and creating mime multi part archive, see . to modify instance user data open the amazon ec2 console at . in the navigation pane, choose instances. select the instance and choose actions, instance state, stop. warningwhen you stop an instance, the data on any instance store volumes is erased. to keep data from instance store volumes, be sure to back it up to persistent storage. when prompted for confirmation, choose yes, stop. it can take a few minutes for the instance to stop. with the instance still selected, choose actions, instance settings, view/change user data. you can't change the user data if the instance is running, but you can view it. in the view/change user data dialog box, update the user data, and then choose save. restart the instance. the new user data is visible on your instance after you restart it; however, user data scripts are not executed. the cloud-init package configures specific aspects of a new amazon linux instance when it is launched; most notably, it configures the  file for the ec2-user so you can log in with your own private key. for more information, see . the cloud-init user directives can be passed to an instance at launch the same way that a script is passed, although the syntax is different. for more information about cloud-init, go to . importantby default, user data scripts and cloud-init directives run only during the boot cycle when you first launch an instance. you can update your configuration to ensure that your user data scripts and cloud-init directives run every time you restart your instance. for more information, see  in the aws knowledge center. adding these tasks at boot time adds to the amount of time it takes to boot an instance. you should allow a few minutes of extra time for the tasks to complete before you test that your user data directives have completed. to pass cloud-init directives to an instance with user data follow the procedure for launching an instance at , but when you get to  in that procedure, enter your cloud-init directive text in the user data field, and then complete the launch procedure. in the example below, the directives create and configure a web server on amazon linux 2. the  line at the top is required in order to identify the commands as cloud-init directives. allow enough time for the instance to launch and execute the directives in your user data, and then check to see that your directives have completed the tasks you intended. for our example, in a web browser, enter the url of the php test file the directives created. this url is the public dns address of your instance followed by a forward slash and the file name. you should see the php information page. if you are unable to see the php information page, check that the security group you are using contains a rule to allow http (port 80) traffic. for more information, see . (optional) if your directives did not accomplish the tasks you were expecting them to, or if you just want to verify that your directives completed without errors, examine the output log file at  and look for error messages in the output. for additional debugging information, you can add the following line to your directives: this directive sends runcmd output to . you can use the aws cli to specify, modify, and view the user data for your instance. for information about viewing user data from your instance using instance metadata, see . on windows, you can use the aws tools for windows powershell instead of using the aws cli. for more information, see  in the amazon ec2 user guide for windows instances. example: specify user data at launchto specify user data when you launch your instance, use the  command with the  parameter. with run-instances, the aws cli performs base64 encoding of the user data for you. the following example shows how to specify a script as a string on the command line: the following example shows how to specify a script using a text file. be sure to use the  prefix to specify the file. the following is an example text file with a shell script. example: modify the user data of a stopped instanceyou can modify the user data of a stopped instance using the  command. with modify-instance-attribute, the aws cli does not perform base64 encoding of the user data for you. on a linux computer, use the base64 command to encode the user data. on a windows computer, use the certutil command to encode the user data. before you can use this file with the aws cli, you must remove the first (begin certificate) and last (end certificate) lines. use the  and  parameters to use the encoded text file to specify the user data. be sure to use the  prefix to specify the file. example: clear the user data of a stopped instanceto delete the existing user data, use the  command as follows: example: view user datato retrieve the user data for an instance, use the  command. with describe-instance-attribute, the aws cli does not perform base64 decoding of the user data for you. the following is example output with the user data base64 encoded. on a linux computer , use the  option to get the encoded user data and the base64 command to decode it. on a windows computer, use the  option to get the coded user data and the certutil command to decode it. note that the encoded output is stored in a file and the decoded output is stored in another file. the following is example output. 
amazon ebs multi-attach enables you to attach a single provisioned iops ssd (io1) volume to up to 16 nitro-based instances that are in the same availability zone. you can attach multiple multi-attach enabled volumes to an instance or set of instances. each instance to which the volume is attached has full read and write permission to the shared volume. multi-attach makes it easier for you to achieve higher application availability in clustered linux applications that manage concurrent write operations. features multi-attach enabled volumes support many of the features that are supported by regular amazon ebs volumes, including: topics multi-attach enabled volumes do not support i/o fencing. i/o fencing protocols control write access in a shared storage environment to maintain data consistency. your applications must provide write ordering for the attached instances to maintain data consistency.multi-attach enabled volumes can be attached to up to 16 instances built on the  that are in the same availability zone.multi-attach is supported exclusively on .multi-attach is available in the , , , and  regions.multi-attach enabled volumes can't be created as boot volumes.multi-attach enabled volumes can be attached to one block device mapping per instance.you can't enable or disable multi-attach after volume creation.you can't change the volume type, size, or provisioned iops of a multi-attach enabled volume.multi-attach can't be enabled during instance launch using either the amazon ec2 console or runinstances api.multi-attach enabled volumes that have an issue at the amazon ebs infrastructure layer are unavailable to all attached instances. issues at the amazon ec2 or networking layer might only impact some attached instances.each attached instance is able to drive its maximum iops performance up to the volume's maximum provisioned performance. however, the aggregate performance of all of the attached instances can't exceed the volume's maximum provisioned performance. if the attached instances' demand for iops is higher than the volume's provisioned iops, the volume will not exceed its provisioned performance. for example, say you create an  multi-attach enabled volume with  provisioned iops and you attach it to an  instance and a  instance. the  and  instances support a maximum of  and  iops respectively. each instance can drive its maximum iops as it is less than the volume's provisioned iops of . however, if both instances drive i/o to the volume simultaneously, their combined iops can't exceed the volume's provisioned performance of . the volume will not exceed  iops. to achieve consistent performance, it is best practice to balance i/o driven from attached instances across the sectors of a multi-attach enabled volume. multi-attach enabled volumes can be managed in much the same way that you would manage any other amazon ebs volume. however, in order to use the multi-attach functionality, you must enable it for the volume. when you create a new volume, multi-attach is disabled by default. contents you can enable multi-attach for an amazon ebs volume during creation only. use one of the following methods to enable multi-attach for an amazon ebs volume during creation. to enable multi-attach during volume creation open the amazon ec2 console at . in the navigation pane, choose volumes. choose create volume. for volume type, choose provisioned iops ssd (io1). for size and iops, choose the required volume size and the number of iops to provision. for availability zone, choose the same availability zone that the instances are in. for multi-attach, choose enable. choose create volume. to enable multi-attach during volume creationuse the  command and specify the  parameter. you attach a multi-attach enabled volume to an instance in the same way that you attach a regular volume. for more information, see . multi-attach enabled volumes are deleted on instance termination if the last attached instance is terminated and if that instance is configured to delete the volume on termination. if the volume is attached to multiple instances that have different delete on termination settings in their volume block device mappings, the last attached instance's block device mapping setting determines the delete on termination behavior. to ensure predictable delete on termination behavior, enable or disable delete on termination for all of the instances to which the volume is attached. by default, when a volume is attached to an instance the delete on termination setting for the block device mapping is set to false. if you want to turn on delete on termination for a multi-attach enabled volume, modify the block device mapping. if you want the volume to be deleted when the attached instances are terminated, enable delete on termination in the block device mapping for all of the attached instances. if you want to retain the volume after the attached instances have been terminated, disable delete on termination in the block device mapping for all of the attached instances. for more information, see . you can modify an instance's delete on termination setting at launch or after it has launched. if you enable or disable delete on termination during instance launch, the settings apply only to volumes that are attached at launch. if you attach a volume to an instance after launch, you must explicitly set the delete on termination behavior for that volume. you can modify an instance's delete on termination setting using the command line tools only. to modify the delete on termination setting for an existing instanceuse the  command and specify the  attribute in the . specify the following in . you can monitor a multi-attach enabled volume using the cloudwatch metrics for amazon ebs volumes. for more information, see . data is aggregated across all of the attached instances. you can't monitor metrics for individual attached instances. there are no additional charges for using amazon ebs multi-attach. you are billed the standard charges that apply to provisioned iops ssd (io1) volumes. for more information, see . 
when you hibernate an instance, we signal the operating system to perform hibernation (suspend-to-disk). hibernation saves the contents from the instance memory (ram) to your amazon ebs root volume. we persist the instance's amazon ebs root volume and any attached amazon ebs data volumes. when you start your instance: the amazon ebs root volume is restored to its previous statethe ram contents are reloadedthe processes that were previously running on the instance are resumedpreviously attached data volumes are reattached and the instance retains its instance idyou can hibernate an instance only if it's  and it meets the . if an instance or application takes a long time to bootstrap and build a memory footprint to become fully productive, you can use hibernation to pre-warm the instance. to pre-warm the instance, you: launch it with hibernation enabled. bring it to a desired state. hibernate it, ready to be resumed to the same state as needed. we don't charge usage for a hibernated instance when it is in the  state. we do charge for instance usage while the instance is in the  state, when the contents of the ram are transferred to the amazon ebs root volume. (this is different from when you  without hibernating it.) we don't charge usage for data transfer fees. however, we do charge for storage of any amazon ebs volumes, including storage for the ram contents. if you no longer need an instance, you can terminate it at any time, including when it is in a  (hibernated) state. for more information, see . notefor information about using hibernation on windows instances, see  in the amazon ec2 user guide for windows instances. topics the following diagram shows a basic overview of the hibernation process.  when you hibernate a running instance, the following happens: when you initiate hibernation, the instance moves to the  state. we signal the operating system to perform hibernation (suspend-to-disk). the hibernation freezes all of the processes, saves the contents of the ram to the amazon ebs root volume, and then performs a regular shutdown.after the shutdown is complete, the instance moves to the  state.any amazon ebs volumes remain attached to the instance, and their data persists, including the saved contents of the ram.in most cases, the instance is migrated to a new underlying host computer when it's started. this is also what happens when you stop and start an instance.when you start the instance, the instance boots up and the operating system reads in the contents of the ram from the amazon ebs root volume, before unfreezing processes to resume its state.the instance retains its private ipv4 addresses and any ipv6 addresses when hibernated and started. we release the public ipv4 address and assign a new one when you start it.the instance retains its associated elastic ip addresses. you're charged for any elastic ip addresses associated with a hibernated instance. with ec2-classic, an elastic ip address is disassociated from your instance when you hibernate it. for more information, see .when you hibernate a classiclink instance, it's unlinked from the vpc to which it was linked. you must link the instance to the vpc again after starting it. for more information, see .for information about how hibernation differs from reboot, stop, and terminate, see . to hibernate an instance, the following prerequisites must be in place: supported instance families - c3, c4, c5, m3, m4, m5, r3, r4, r5, and t2.instance ram size - must be less than 150 gb.instance size - not supported for bare metal instances.supported amis (must be an hvm ami that supports hibernation): amazon linux 2 ami released 2019.08.29 or later.amazon linux ami 2018.03 released 2018.11.16 or later.ubuntu 18.04 lts - bionic ami released with serial number 20190722.1 or later.*ubuntu 16.04 lts - xenial ami.* ( is required.) *we recommend disabling kaslr on instances with ubuntu 18.04 lts - bionic and ubuntu 16.04 lts - xenial. for more information, see . to configure your own ami to support hibernation, see . support for other versions of ubuntu and other operating systems is coming soon. for information about the supported amis for windows, see  in the amazon ec2 user guide for windows instances. root volume type - must be an amazon ebs volume, not an instance store volume.supported amazon ebs volume types - general purpose ssd () or provisioned iops ssd (). if you choose a provisioned iops ssd () volume type, to achieve optimum performance for hibernation, you must provision the ebs volume with the appropriate iops. for more information, see .amazon ebs root volume size - must be large enough to store the ram contents and accommodate your expected usage, for example, os or applications. if you enable hibernation, space is allocated on the root volume at launch to store the ram.amazon ebs root volume encryption - to use hibernation, the root volume must be encrypted to ensure the protection of sensitive content that is in memory at the time of hibernation. when ram data is moved to the amazon ebs root volume, it is always encrypted. encryption of the root volume is enforced at instance launch. use one of the following three options to ensure that the root volume is an encrypted amazon ebs volume:ebs “single-step” encryption: you can launch encrypted ebs-backed ec2 instances from an unencrypted ami and also enable hibernation at the same time. for more information, see .ebs encryption by default: you can enable ebs encryption by default to ensure all new ebs volumes created in your aws account are encrypted. this way, you can enable hibernation for your instances without specifying encryption intent at instance launch. for more information, see .encrypted ami: you can enable ebs encryption by using an encrypted ami to launch your instance. if your ami does not have an encrypted root snapshot, you can copy it to a new ami and request encryption. for more information, see  and .enable hibernation at launch - you cannot enable hibernation on an existing instance (running or stopped). for more information, see .purchasing options - this feature is available for on-demand instances and reserved instances. it is not available for spot instances. for more information, see .the following actions are not supported for hibernation:changing the instance type or size of a hibernated instancecreating snapshots or amis from instances for which hibernation is enabledcreating snapshots or amis from hibernated instancesyou can't stop or hibernate instance store-backed instances.*you can't hibernate an instance that has more than 150 gb of ram.you cannot hibernate an instance that is in an auto scaling group or used by amazon ecs. if your instance is in an auto scaling group and you try to hibernate it, the amazon ec2 auto scaling service marks the stopped instance as unhealthy, and may terminate it and launch a replacement instance. for more information, see  in the amazon ec2 auto scaling user guide.we do not support keeping an instance hibernated for more than 60 days. to keep the instance for longer than 60 days, you must start the hibernated instance, stop the instance, and start it.we constantly update our platform with upgrades and security patches, which can conflict with existing hibernated instances. we notify you about critical updates that require a start for hibernated instances so that we can perform a shutdown or a reboot to apply the necessary upgrades and security patches.*for c3 and r3 instances that are enabled for hibernation, do not use instance store volumes. to hibernate an instance that was launched using your own ami, you must first configure your ami to support hibernation. for more information, see . if you use one of the  (except ubuntu 16.04 lts), or if you create an ami based on one of the supported amis, you do not need to configure it to support hibernation. these amis are preconfigured to support hibernation. to configure ubuntu 16.04 lts to support hibernation, you need to install the linux-aws-hwe kernel package version 4.15.0-1058-aws or later and the ec2-hibinit-agent. for the configuration steps, choose the ubuntu 16.04 - xenial tab below. to configure an amazon linux 2 ami to support hibernation update to the latest kernel to 4.14.138-114.102 or later using the following command. install the  package from the repositories using the following command. reboot the instance using the following command. confirm that the kernel version is updated to 4.14.138-114.102 or later using the following command. stop the instance and create an ami. for more information, see . to configure an amazon linux ami to support hibernation update to the latest kernel to 4.14.77-70.59 or later using the following command. install the  package from the repositories using the following command. reboot the instance using the following command. confirm that the kernel version is updated to 4.14.77-70.59 or greater using the following command. stop the instance and create an ami. for more information, see . to configure an ubuntu 18.04 lts ami to support hibernation update to the latest kernel to 4.15.0-1044 or later using the following commands. install the  package from the repositories using the following command. reboot the instance using the following command. confirm that the kernel version is updated to 4.15.0-1044 or later using the following command. to configure an ubuntu 16.04 lts ami to support hibernation update to the latest kernel to 4.15.0-1058-aws or later using the following commands. notethe linux-aws-hwe kernel package is fully supported by canonical. the package will continue to receive regular updates until standard support for ubuntu 16.04 lts ends in april 2021, and will receive additional security updates until the extended security maintenance support ends in 2024. for more information, see  on the canonical ubuntu blog. install the  package from the repositories using the following command. reboot the instance using the following command. confirm that the kernel version is updated to 4.15.0-1058-aws or later using the following command. to hibernate an instance, it must first be enabled for hibernation. to enable hibernation, you must do it while launching the instance. importantyou can't enable or disable hibernation for an instance after you launch it. to enable hibernation using the console follow the  procedure. on the choose an amazon machine image (ami) page, select an ami that supports hibernation. for more information about supported amis, see . on the choose an instance type page, select a supported instance type, and choose next: configure instance details. for information about supported instance types, see . on the configure instance details page, for stop - hibernate behavior, select the enable hibernation as an additional stop behavior check box. on the add storage page, for the root volume, specify the following information:  for size (gib), enter the amazon ebs root volume size. the volume must be large enough to store the ram contents and accommodate your expected usage.for volume type, select a supported amazon ebs volume type (general purpose ssd () or provisioned iops ssd ()).for encryption, select the encryption key for the volume. if you enabled encryption by default in this aws region, the default encryption key is selected.for more information about the prerequisites for the root volume, see . continue as prompted by the wizard. when you've finished reviewing your options on the review instance launch page, choose launch. for more information, see . to enable hibernation using the aws cliuse the  command to launch an instance. specify the ebs root volume parameters using the  parameter, and enable hibernation using the  parameter. specify the following in : notethe value for  must match the root device name associated with the ami. to find the root device name, use the  command, as follows:   if you enabled encryption by default in this aws region, you can omit . to enable hibernation using the aws tools for windows powershelluse the  command to launch an instance. specify the ebs root volume by first defining the block device mapping, and then adding it to the command using the  parameter. enable hibernation using the  parameter. notethe value for  must match the root device name associated with the ami. to find the root device name, use the  command, as follows:   if you enabled encryption by default in this aws region, you can omit  from the block device mapping.   to view if an instance is enabled for hibernation using the console open the amazon ec2 console at . in the navigation pane, choose instances. select the instance and, in the details pane, inspect stop - hibernation behavior. enabled indicates that the instance is enabled for hibernation. to view if an instance is enabled for hibernation using the aws cliuse the  command and specify the  parameter to filter instances that are enabled for hibernation. the following field in the output indicates that the instance is enabled for hibernation. to view if an instance is enabled for hibernation using the aws tools for windows powershelluse the  command and specify the  parameter to filter instances that are enabled for hibernation. the output lists the ec2 instances that are enabled for hibernation.  to run hibernation on a newly launched instance with ubuntu 16.04 lts - xenial or ubuntu 18.04 lts - bionic released with serial 20190722.1 or later, we recommend disabling kaslr (kernel address space layout randomization). on ubuntu 16.04 lts or ubuntu 18.04 lts, kaslr is enabled by default. kaslr is a standard linux kernel security feature that helps to mitigate exposure to and ramifications of yet-undiscovered memory access vulnerabilities by randomizing the base address value of the kernel. with kaslr enabled, there is a possibility that the instance might not resume after it has been hibernated. to learn more about kaslr, see . to disable kaslr on an instance launched with ubuntu connect to your instance using ssh. for more information, see . open the  file in your editor of choice. edit the  line to append the  option to its end, as shown in the following example. save the file and exit your editor. run the following command to rebuild the grub configuration. reboot the instance. confirm that  has been added when running the following command. the output of the command should include the  option. you can hibernate an instance if the instance is  and meets the . if an instance cannot hibernate successfully, a normal shutdown occurs. to hibernate an amazon ebs-backed instance using the console open the amazon ec2 console at . in the navigation pane, choose instances. select an instance, and choose actions, instance state, stop - hibernate. if stop - hibernate is disabled, the instance is already hibernated or stopped, or it can't be hibernated. for more information, see . in the confirmation dialog box, choose yes, stop - hibernate. it can take a few minutes for the instance to hibernate. the instance state changes to stopping while the instance is hibernating, and then stopped when the instance has hibernated. to hibernate an amazon ebs-backed instance using the aws cliuse the  command and specify the  parameter. to hibernate an amazon ebs-backed instance using the aws tools for windows powershelluse the  command and specify the  parameter.   to view if hibernation was initiated on an instance using the console open the amazon ec2 console at . in the navigation pane, choose instances. select the instance and, in the details pane, inspect state transition reason message. the message client.userinitiatedhibernate: user initiated hibernate indicates that hibernation was initiated on the instance. to view if hibernation was initiated on an instance using the aws cliuse the  command and specify the  filter to see the instances on which hibernation was initiated. the following field in the output indicates that hibernation was initiated on the instance. to view if hibernation was initiated on an instance using the aws tools for windows powershelluse the  command and specify the  filter to see the instances on which hibernation was initiated. the output lists the ec2 instances on which hibernation was initiated.  start a hibernated instance by starting it in the same way that you would start a stopped instance. to start a hibernated instance using the console open the amazon ec2 console at . in the navigation pane, choose instances. select a hibernated instance, and choose actions, instance state, start. it can take a few minutes for the instance to enter the  state. during this time, the instance  show the instance in a failed state until the instance has started. to start a hibernated instance using the aws cliuse the  command. to start a hibernated instance using the aws tools for windows powershelluse the  command. use this information to help diagnose and fix issues that you might encounter when hibernating an instance. if you try to hibernate an instance too quickly after you've launched it, you get an error. you must wait for about two minutes after launch before hibernating. if it takes a long time for your hibernating instance to transition from the  state to , and if the memory state is not restored after you start, this could indicate that hibernation was not properly configured. check the instance system log and look for messages that are related to hibernation. to access the system log,  to the instance or use the  command. find the log lines from the . if the log lines indicate a failure or the log lines are missing, there was most likely a failure configuring hibernation at launch. for example, the following message indicates that the instance root volume is not large enough:  if the last log line from the  is , hibernation was successfully configured. if you do not see any logs from these processes, your ami might not support hibernation. for information about supported amis, see . if you used your own ami, make sure that you followed the instructions for . if you hibernated your instance and it appears "stuck" in the  state, you can forcibly stop it. for more information, see . 
dedicated instances are amazon ec2 instances that run in a virtual private cloud (vpc) on hardware that's dedicated to a single customer. dedicated instances that belong to different aws accounts are physically isolated at a hardware level, even if those accounts are linked to a single payer account. however, dedicated instances may share hardware with other instances from the same aws account that are not dedicated instances. notea dedicated host is also a physical server that's dedicated for your use. with a dedicated host, you have visibility and control over how instances are placed on the server. for more information, see . each instance that you launch into a vpc has a tenancy attribute. this attribute has the following values. after you launch an instance, there are some limitations to changing its tenancy. you cannot change the tenancy of an instance from  to  or  after you've launched it.you cannot change the tenancy of an instance from  or  to  after you've launched it.you can change the tenancy of an instance from  to , or from  to  after you've launched it. for more information, see . each vpc has a related instance tenancy attribute. this attribute has the following values. you can change the instance tenancy of a vpc from  to  after you create it. you cannot change the instance tenancy of a vpc to . to create dedicated instances, you can do the following: create the vpc with the instance tenancy set to  (all instances launched into this vpc are dedicated instances).create the vpc with the instance tenancy set to , and specify a tenancy of  for any instances when you launch them.some aws services or their features won't work with a vpc with the instance tenancy set to . check the service's documentation to confirm if there are any limitations. some instance types cannot be launched into a vpc with the instance tenancy set to . for more information about supported instances types, see . when you launch an amazon ebs-backed dedicated instance, the ebs volume doesn't run on single-tenant hardware. to guarantee that sufficient capacity is available to launch dedicated instances, you can purchase dedicated reserved instances. for more information, see .  when you purchase a dedicated reserved instance, you are purchasing the capacity to launch a dedicated instance into a vpc at a much reduced usage fee; the price break in the usage charge applies only if you launch an instance with dedicated tenancy. when you purchase a reserved instance with default tenancy, it applies only to a running instance with  tenancy; it would not apply to a running instance with  tenancy. you can't use the modification process to change the tenancy of a reserved instance after you've purchased it. however, you can exchange a convertible reserved instance for a new convertible reserved instance with a different tenancy. you can use amazon ec2 auto scaling to launch dedicated instances. for more information, see  in the amazon ec2 auto scaling user guide. you can configure automatic recovery for a dedicated instances if it becomes impaired due to an underlying hardware failure or a problem that requires aws involvement to repair. for more information, see . you can run a dedicated spot instance by specifying a tenancy of  when you create a spot instance request. for more information, see . pricing for dedicated instances is different to pricing for on-demand instances. for more information, see the . you can leverage the benefits of running on dedicated tenancy hardware with . t3 dedicated instances launch in unlimited mode by default, and they provide a baseline level of cpu performance with the ability to burst to a higher cpu level when required by your workload. the t3 baseline performance and ability to burst are governed by cpu credits. because of the burstable nature of the t3 instance types, we recommend that you monitor how your t3 instances use the cpu resources of the dedicated hardware for the best performance. t3 dedicated instances are intended for customers with diverse workloads that display random cpu behavior, but that ideally have average cpu usage at or below the baseline usages. for more information, see . amazon ec2 has systems in place to identify and correct variability in performance. however, it is still possible to experience short term variability if you launch multiple t3 dedicated instances that have correlated cpu usage patterns. for these more demanding or correlated workloads, we recommend using m5 or m5a dedicated instances rather than t3 dedicated instances. you can create a vpc with an instance tenancy of  to ensure that all instances launched into the vpc are dedicated instances. alternatively, you can specify the tenancy of the instance during launch.  topics when you create a vpc, you have the option of specifying its instance tenancy. if you're using the amazon vpc console, you can create a vpc using the vpc wizard or the your vpcs page. to create a vpc with an instance tenancy of dedicated (vpc wizard) open the amazon vpc console at . from the dashboard, choose start vpc wizard. select a vpc configuration, and then choose select. on the next page of the wizard, choose dedicated from the hardware tenancy list. choose create vpc. to create a vpc with an instance tenancy of dedicated (create vpc dialog box) open the amazon vpc console at . in the navigation pane, choose your vpcs, and then create vpc. for tenancy, choose dedicated. specify the cidr block, and choose yes, create. to set the tenancy option when you create a vpc using the command line  (aws cli) (aws tools for windows powershell)if you launch an instance into a vpc that has an instance tenancy of , your instance is automatically a dedicated instance, regardless of the tenancy of the instance. you can launch a dedicated instance using the amazon ec2 launch instance wizard. to launch a dedicated instance into a default tenancy vpc using the console open the amazon ec2 console at . choose launch instance. on the choose an amazon machine image (ami) page, select an ami and choose select. on the choose an instance type page, select the instance type and choose next: configure instance details. noteensure that you choose an instance type that's supported as a dedicated instance. for more information, see . on the configure instance details page, select a vpc and subnet. choose dedicated - run a dedicated instance from the tenancy list, and then next: add storage. continue as prompted by the wizard. when you've finished reviewing your options on the review instance launch page, choose launch to choose a key pair and launch the dedicated instance. for more information about launching an instance with a tenancy of , see . to set the tenancy option for an instance during launch using the command line  (aws cli) (aws tools for windows powershell)to display tenancy information for your vpc using the console open the amazon vpc console at . in the navigation pane, choose your vpcs. check the instance tenancy of your vpc in the tenancy column. if the tenancy column is not displayed, choose edit table columns (the gear-shaped icon), tenancy in the show/hide columns dialog box, and then close. to display tenancy information for your instance using the console open the amazon ec2 console at . in the navigation pane, choose instances. check the tenancy of your instance in the tenancy column. if the tenancy column is not displayed, do one of the following:  choose show/hide columns (the gear-shaped icon), tenancy in the show/hide columns dialog box, and then close.select the instance. the description tab in the details pane displays information about the instance, including its tenancy.to describe the tenancy of your vpc using the command line  (aws cli) (aws tools for windows powershell)to describe the tenancy of your instance using the command line  (aws cli) (aws tools for windows powershell)to describe the tenancy value of a reserved instance using the command line  (aws cli) (aws tools for windows powershell)to describe the tenancy value of a reserved instance offering using the command line  (aws cli) (aws tools for windows powershell)depending on your instance type and platform, you can change the tenancy of a stopped dedicated instance to  after launching it. the next time the instance starts, it's started on a dedicated host that's allocated to your account. for more information about allocating and working with dedicated hosts, and the instance types that can be used with dedicated hosts, see . similarly, you can change the tenancy of a stopped dedicated host instance to  after launching it. the next time the instance starts, it's started on single-tenant hardware that we control. to change the tenancy of an instance using the console open the amazon ec2 console at . in the navigation pane, choose instances and select your instance. choose actions, instance state, stop. choose actions, instance settings, modify instance placement. in the tenancy list, choose whether to run your instance on dedicated hardware or on a dedicated host. choose save. to modify the tenancy value of an instance using the command line  (aws cli) (aws tools for windows powershell)you can change the instance tenancy attribute of a vpc from  to . modifying the instance tenancy of the vpc does not affect the tenancy of any existing instances in the vpc. the next time you launch an instance in the vpc, it has a tenancy of , unless you specify otherwise during launch. you cannot change the instance tenancy attribute of a vpc to . you can modify the instance tenancy attribute of a vpc using the aws cli, an aws sdk, or the amazon ec2 api only. to modify the instance tenancy attribute of a vpc using the aws cli use the  command to specify the id of the vpc and instance tenancy value. the only supported value is . 
modules are written in yaml, a data serialization standard. a module's yaml file consists of a single document, representing the module and its attributes. the following table lists the available module attributes. the following table lists the available environment variables. the following should be noted when constructing your module yaml files: the triple hyphen () denotes the explicit start of a document.the  tag tells the yaml parser which constructor to call when creating the object from the data stream. you can find the constructor inside the  file.the  tag tells the yaml parser to not attempt to determine the type of data, and instead interpret the content as a string literal.the pipe character () tells the yaml parser that the value is a literal-style scalar. in this case, the parser includes all whitespace. this is important for modules because indentation and newline characters are kept.the yaml standard indent is two spaces, which can be seen in the following examples. ensure that you maintain standard indentation (for example, four spaces for python) for your script and then indent the entire content two spaces inside the module file.example one (): 
kernel live patching for amazon linux 2 enables you to apply security vulnerability and critical bug patches to a running linux kernel, without reboots or disruptions to running applications. this allows you to benefit from improved service and application availability, while keeping your infrastructure secure and up to date.  aws releases two types of kernel live patches for amazon linux 2: security updates—include updates for linux common vulnerabilities and exposures (cve). these updates are typically rated as important or critical using the amazon linux security advisory ratings. they generally map to a common vulnerability scoring system (cvss) score of 7 and higher. in some cases, aws might provide updates before a cve is assigned. in these cases, the patches might appear as bug fixes.bug fixes—include fixes for critical bugs and stability issues that are not associated with cves.aws provides kernel live patches for an amazon linux 2 kernel version for up to 3 months after its release. after the 3-month period, you must update to a later kernel version to continue to receive kernel live patches. amazon linux 2 kernel live patches are made available as signed rpm packages in the existing amazon linux 2 repositories. the patches can be installed on individual instances using existing yum workflows, or they can be installed on a group of managed instances using aws systems manager. kernel live patching on amazon linux 2 is provided at no additional cost. topics kernel live patching is supported on amazon ec2 instances and  running amazon linux 2. to use kernel live patching on amazon linux 2, you must use: a 64-bit (x86_64) architecture that is supported by amazon linux 2amazon linux 2 with kernel version  or laternotethe 64-bit arm (arm64) architecture is not supported. you can enable and use kernel live patching on individual instances using the command line on the instance itself, or you can enable and use kernel live patching on a group of managed instances using aws systems manager. the following sections explain how to enable and use kernel live patching on individual instances using the command line. for more information about enabling and using kernel live patching on a group of managed instances, see  in the aws systems manager user guide. topics kernel live patching is disabled by default on amazon linux 2. to use live patching, you must install the yum plugin for kernel live patching and enable the live patching functionality. to enable kernel live patching kernel live patches are available for amazon linux 2 with kernel version  or later. to check your kernel version, run the following command. if you already have a supported kernel version, skip this step. if you do not have a supported kernel version, run the following commands to update the kernel to the latest version and to reboot the instance. install the yum plugin for kernel live patching. enable the yum plugin for kernel live patching. this command also installs the latest version of the kernel live patch rpm from the configured repositories. to confirm that the yum plugin for kernel live patching has installed successfully, run the following command. when you enable kernel live patching, an empty kernel live patch rpm is automatically applied. if kernel live patching was successfully enabled, this command returns a list that includes the initial empty kernel live patch rpm. update and start the kpatch service. this service loads all of the kernel live patches upon initialization or at boot.  configure the amazon linux 2 kernel live patching repository, which contains the kernel live patches. amazon linux security alerts are published to the amazon linux security center. for more information about the amazon linux 2 security alerts, which include alerts for kernel live patches, see the . kernel live patches are prefixed with . the amazon linux security center might not list kernel live patches that address bugs. you can also discover the available kernel live patches for advisories and cves using the command line. to list all available kernel live patches for advisoriesuse the following command. the following shows example output. to list all available kernel live patches for cvesuse the following command. the following shows example output. you apply kernel live patches using the yum package manager in the same way that you would apply regular updates. the yum plugin for kernel live patching manages the kernel live patches that are to be applied and eliminates the need to reboot. tipwe recommend that you update your kernel regularly using kernel live patching to ensure that it remains secure and up to date. you can choose to apply a specific kernel live patch, or to apply any available kernel live patches along with your regular security updates. to apply a specific kernel live patch get the kernel live patch version using one of the commands described in . apply the kernel live patch for your amazon linux 2 kernel. for example, the following command applies a kernel live patch for amazon linux 2 kernel version . to apply any available kernel live patches along with your regular security updatesuse the following command. omit the  option to include bug fixes. importantthe kernel version is not updated after applying kernel live patches. the version is only updated to the new version after the instance is rebooted. an amazon linux 2 kernel receives kernel live patches for a period of three months. after the three month period has lapsed, no new kernel live patches are released for that kernel version. to continue to receive kernel live patches after the three-month period, you must reboot the instance to move to the new kernel version, which will then continue receiving kernel live patches for the next three months. to check the support window for your kernel version, run . to view the applied kernel live patchesuse the following command. the command returns a list of the loaded and installed security update kernel live patches. the following is example output. notea single kernel live patch can include and install multiple live patches. if you no longer need to use kernel live patching, you can disable it at any time. to disable kernel live patching remove the rpm packages for the applied kernel live patches. uninstall the yum plugin for kernel live patching. reboot the instance. kernel live patching has the following limitations: while applying a kernel live patch, you can't perform hibernation, use advanced debugging tools (such as systemtap, kprobes, and ebpf-based tools), or access ftrace output files used by the kernel live patching infrastructure.amazon linux 2 instances with 64-bit arm (arm64) architecture are not supported.for frequently asked questions about kernel live patching for amazon linux 2, see the . 
we recommend that you regularly patch, update, and secure the operating system and applications on your ec2 instances. you can use  to automate the process of installing security-related updates for both the operating system and applications. alternatively, you can use any automatic update services or recommended processes for installing updates that are provided by the application vendor. 
you can get a list of some types of resource using the amazon ec2 console. you can get a list of each type of resource using its corresponding command or api action. if you have many resources, you can filter the results to include only the resources that match certain criteria. topics advanced search allows you to search using a combination of filters to achieve precise results. you can filter by keywords, user-defined tag keys, and predefined resource attributes. the specific search types available are: search by keyword to search by keyword, type or paste what you’re looking for in the search box, and then choose enter. for example, to search for a specific instance, you can type the instance id. search by fields you can also search by fields, tags, and attributes associated with a resource. for example, to find all instances in the stopped state: in the search box, start typing instance state. as you type, you'll see a list of suggested fields. select instance state from the list. select stopped from the list of suggested values. to further refine your list, select the search box for more search options. advanced search you can create advanced queries by adding multiple filters. for example, you can search by tags and see instances for the flying mountain project running in the production stack, and then search by attributes to see all t2.micro instances, or all instances in us-west-2a, or both.  inverse search you can search for resources that do not match a specified value. for example, to list all instances that are not terminated, search by the instance state field, and prefix the terminated value with an exclamation mark (!). partial search when searching by field, you can also enter a partial string to find all resources that contain the string in that field. for example, search by instance type, and then type t2 to find all t2.micro, t2.small or t2.medium instances. regular expression regular expressions are useful when you need to match the values in a field with a specific pattern. for example, search by the name tag, and then type ^s.* to see all instances with a name tag that starts with an 's'. regular expression search is not case-sensitive. after you have the precise results of your search, you can bookmark the url for easy reference. in situations where you have thousands of instances, filters and bookmarks can save you a great deal of time; you don’t have to run searches repeatedly. combining search filters in general, multiple filters with the same key field (for example, tag:name, search, instance state) are automatically joined with or. this is intentional, as the vast majority of filters would not be logical if they were joined with and. for example, you would get zero results for a search on instance state=running and instance state=stopped. in many cases, you can granulate the results by using complementary search terms on different key fields, where the and rule is automatically applied instead. if you search for tag: name:=all values and tag:instance state=running, you get search results that contain both those criteria. to fine-tune your results, simply remove one filter in the string until the results fit your requirements. you can view the most common amazon ec2 resource types using the console. to view additional resources, use the command line interface or the api actions. to list ec2 resources using the console open the amazon ec2 console at . in the navigation pane, choose the option that corresponds to the resource, such as amis or instances. the page displays all the available resources. you can perform filtering and sorting of the most common resource types using the amazon ec2 console. for example, you can use the search bar on the instances page to sort instances by tags, attributes, or keywords.  you can also use the search field on each page to find resources with specific attributes or values. you can use regular expressions to search on partial or multiple strings. for example, to find all instances that are using the mysg security group, enter  in the search field. the results will include any values that contain  as a part of the string, such as  and. to limit your results to mysg only, enter  in the search field. to list all the instances whose type is either  or , enter  in the search field.  to list volumes in the  availability zone with a status of  in the navigation pane, choose volumes. click on the search box, select attachment status from the menu, and then select detached. (a detached volume is available to be attached to an instance in the same availability zone.) click on the search box again, select state, and then select available.  click on the search box again, select availability zone, and then select . any volumes that meet this criteria are displayed. to list public 64-bit linux amis backed by amazon ebs in the navigation pane, choose amis. in the filter pane, select public images, ebs images, and then your linux distribution from the filter lists. type  in the search field. any amis that meet this criteria are displayed. each resource type has a corresponding cli command and api action that you use to list resources of that type. the resulting lists of resources can be long, so it can be faster and more useful to filter the results to include only the resources that match specific criteria. filtering considerations you can specify multiple filters and multiple filter values in a single request.you can use wildcards with the filter values. an asterisk (*) matches zero or more characters, and a question mark (?) matches zero or one character.filter values are case sensitive.your search can include the literal values of the wildcard characters; you just need to escape them with a backslash before the character. for example, a value of  searches for the literal string .supported filters to see the supported filters for each amazon ec2 resource, see the following documentation: aws cli: the  commands in the .tools for windows powershell: the  commands in the .query api: the  api actions in the .example example: specify a single filteryou can list your amazon ec2 instances using . without filters, the response contains information for all your resources. you can use the following command to include only the running instances in your output.   to list only the instance ids for your running instances, add the  parameter as follows.   the following is example output:   example example: specify multiple filters or filter valuesif you specify multiple filters or multiple filter values, the resource must match all filters to be included in the results.you can you the following command to list all instances whose type is either  or .   you can use the following command to list all stopped instances whose type is .   example example: use wildcards in a filter valueif you specify database as the filter value for the  filter when describing ebs snapshots using , the command returns only the snapshots whose description is "database".   the * wildcard matches zero or more characters. if you specify *database* as the filter value, the command returns only snapshots whose description includes the word database.   the ? wildcard matches exactly 1 character. if you specify database? as the filter value, the command returns only snapshots whose description is "database" or "database" followed by one character.   if you specify , the command returns only snapshots whose description is "database" followed by up to four characters. it excludes descriptions with "database" followed by five or more characters.   example example: filter based on datewith the aws cli, you can use jmespath to filter results using expressions. for example, the following  command displays the ids of all snapshots created by your aws account (represented by 123456789012) before the specified date (represented by 2020-03-31). if you do not specify the owner, the results include all public snapshots.   the following command displays the ids of all snapshots created in the specified date range.   filter based on tagsfor examples of how to filter a list of resources according to their tags, see . 
the  provides the tools for developing, testing, and building afis. you can use the fpga developer ami on any ec2 instance with at least 32 gb of system memory (for example, c5, m4, and r4 instances). for more information, see the documentation for the . 
you can convert an instance store-backed linux ami that you own to an amazon ebs-backed linux ami.  importantyou can't convert an instance store-backed windows ami to an amazon ebs-backed windows ami and you cannot convert an ami that you do not own. to convert an instance store-backed ami to an amazon ebs-backed ami launch an amazon linux instance from an amazon ebs-backed ami. for more information, see . amazon linux instances have the aws cli and ami tools pre-installed. upload the x.509 private key that you used to bundle your instance store-backed ami to your instance. we use this key to ensure that only you and amazon ec2 can access your ami. create a temporary directory on your instance for your x.509 private key as follows: copy your x.509 private key from your computer to the  directory on your instance, using a secure copy tool such as . the my-private-key parameter in the following command is the private key you use to connect to your instance with ssh. for example: set environment variables for your aws access key and secret key. prepare an amazon ebs volume for your new ami. create an empty amazon ebs volume in the same availability zone as your instance using the  command. note the volume id in the command output. importantthis amazon ebs volume must be the same size or larger than the original instance store root volume. attach the volume to your amazon ebs-backed instance using the  command. create a folder for your bundle. download the bundle for your instance store-based ami to  using the  command. reconstitute the image file from the bundle using the  command. change directories to the bundle folder. run the  command. copy the files from the unbundled image to the new amazon ebs volume. probe the volume for any new partitions that were unbundled. list the block devices to find the device name to mount. in this example, the partition to mount is , but your device name will likely be different. if your volume is not partitioned, then the device to mount will be similar to  (without a device partition trailing digit). create a mount point for the new amazon ebs volume and mount the volume. open the  file on the ebs volume with your favorite text editor (such as vim or nano) and remove any entries for instance store (ephemeral) volumes. because the amazon ebs volume is mounted on , the  file is located at . in this example, the last line should be removed. unmount the volume and detach it from the instance. create an ami from the new amazon ebs volume as follows. create a snapshot of the new amazon ebs volume. check to see that your snapshot is complete. identify the processor architecture, virtualization type, and the kernel image () used on the original ami with the describe-images command. you need the ami id of the original instance store-backed ami for this step. in this example, the architecture is  and the kernel image id is . use these values in the following step. if the output of the above command also lists an  id, take note of that as well. register your new ami with the snapshot id of your new amazon ebs volume and the values from the previous step. if the previous command output listed an  id, include that in the following command with . (optional) after you have tested that you can launch an instance from your new ami, you can delete the amazon ebs volume that you created for this procedure. 
before you can launch an instance, you must select an ami to use. as you select an ami, consider the following requirements you might have for the instances that you'll launch: the regionthe operating systemthe architecture: 32-bit (), 64-bit (), or 64-bit arm (arm64)the root device type: amazon ebs or instance storethe provider (for example, amazon web services)additional software (for example, sql server)if you need to find a windows ami, see  in the amazon ec2 user guide for windows instances. topics you can find linux amis using the amazon ec2 console. you can select from the list of amis when you use the launch wizard to launch an instance, or you can search through all available amis using the images page. ami ids are unique to each aws region. to find a linux ami using the launch wizard open the amazon ec2 console at . from the navigation bar, select the region in which to launch your instances. you can select any region that's available to you, regardless of your location. from the console dashboard, choose launch instance. on the quick start tab, select from one of the commonly used amis in the list. if you don't see the ami that you need, select the my amis, aws marketplace, or community amis tab to find additional amis. for more information, see . to find a linux ami using the images page open the amazon ec2 console at . from the navigation bar, select the region in which to launch your instances. you can select any region that's available to you, regardless of your location. in the navigation pane, choose amis. (optional) use the filter options to scope the list of displayed amis to see only the amis that interest you. for example, to list all linux amis provided by aws, select public images. choose the search bar and select owner from the menu, then select amazon images. choose the search bar again to select platform and then the operating system from the list provided. (optional) choose the show/hide columns icon to select which image attributes to display, such as the root device type. alternatively, you can select an ami from the list and view its properties in the details tab. before you select an ami, it's important that you check whether it's backed by instance store or by amazon ebs and that you are aware of the effects of this difference. for more information, see . to launch an instance from this ami, select it and then choose launch. for more information about launching an instance using the console, see . if you're not ready to launch the instance now, make note of the ami id for later. you can use aws cli commands for amazon ec2 to list only the linux amis that meet your needs. after locating an ami that meets your needs, make note of its id so that you can use it to launch instances. for more information, see  in the aws command line interface user guide. the  command supports filtering parameters. for example, use the  parameter to display public amis owned by amazon. you can add the following filter to the previous command to display only amis backed by amazon ebs. importantomitting the  flag from the describe-images command will return all images for which you have launch permissions, regardless of ownership. amazon ec2 provides aws systems manager public parameters for aws-maintained public amis that you can use when launching instances. for example, the ec2-provided parameter  is available in all regions and always points to the latest version of the amazon linux 2 ami in a given region.  the amazon ec2 ami public parameters are available from the following paths: you can view a list of all linux amis in the current aws region by using the following command in the aws cli. to launch an instance using a public parameterthe following example uses the ec2-provided public parameter to launch an  instance using the latest amazon linux 2 ami. to specify the parameter in the command, use the following syntax: , where  is the standard prefix and  is the path and name of the public parameter. in this example, the  and  parameters are not included. for , the default is 1. if you have a default vpc and a default security group, they are used. for more information, see  in the aws systems manager user guide and . when you launch an instance using the ec2 launch wizard in the console, you can either select an ami from the list, or you can select an aws systems manager parameter that points to an ami id. if you use automation code to launch your instances, you can specify the systems manager parameter instead of the ami id. a systems manager parameter is a customer-defined key-value pair that you can create in systems manager parameter store. the parameter store provides a central store to externalize your application configuration values. for more information, see  in the aws systems manager user guide. when you create a parameter that points to an ami id, make sure that you specify the data type as . this data type ensures that when the parameter is created or modified, the parameter value is validated as an ami id. for more information, see  in the aws systems manager user guide. topics by using systems manager parameters to point to ami ids, you can make it easier for your users to select the correct ami when launching instances, and you can simplify the maintenance of automation code. easier for users if you require instances to be launched using a specific ami, and if that ami is updated regularly, we recommend that you require your users to select a systems manager parameter to find the ami. by requiring your users to select a systems manager parameter, you can ensure that the latest ami is used to launch instances. for example, every month in your organization you might create a new version of your ami that has the latest operating system and application patches. you also require your users to launch instances using the latest version of your ami. to ensure that your users use the latest version, you can create a systems manager parameter (for example, ) that points to the correct ami id. each time a new version of the ami is created, you update the ami id value in the parameter so that it always points to the latest ami. your users don't need to know about the periodic updates to the ami, because they continue to select the same systems manager parameter every time. by having users select a systems manager parameter, you make it easier for them to select the correct ami for an instance launch. simplify automation code maintenance if you use automation code to launch your instances, you can specify the systems manager parameter instead of the ami id. if a new version of the ami is created, you change the ami id value in the parameter so that it points to the latest ami. the automation code that references the parameter doesn’t need to be modified every time a new version of the ami is created. this greatly simplifies maintenance of automation and helps drive down deployment costs. noterunning instances are not affected when you change the ami id to which the systems manager parameter points. you can launch an instance using the the console or the aws cli. instead of specifying an ami id, you can specify an aws systems manager parameter that points to an ami id. to find a linux ami using a systems manager parameter (console) open the amazon ec2 console at . from the navigation bar, select the region in which to launch your instances. you can select any region that's available to you, regardless of your location. from the console dashboard, choose launch instance. choose search by systems manager parameter (at top right). for systems manager parameter, select a parameter. the corresponding ami id appears next to currently resolves to. choose search. the amis that match the ami id appear in the list. select the ami from the list, and choose select. for more information about launching an instance from an ami using the launch wizard, see . to launch an instance using an aws systems manager parameter instead of an ami id (aws cli)the following example uses the systems manager parameter  to launch an  instance. the parameter points to an ami id. to specify the parameter in the command, use the following syntax: , where  is the standard prefix and  is the unique parameter name. note that the parameter name is case-sensitive. backslashes for the parameter name are only necessary when the parameter is part of a hierarchy, for example, . you can omit the backslash if the parameter is not part of a hierarchy. in this example, the  and  parameters are not included. for , the default is 1. if you have a default vpc and a default security group, they are used. to launch an instance using a specific version of an aws systems manager parameter (aws cli)systems manager parameters have version support. each iteration of a parameter is assigned a unique version number. you can reference the version of the parameter as follows , where  is the unique version number. by default, the latest version of the parameter is used when no version is specified. the following example uses version 2 of the parameter. in this example, the  and  parameters are not included. for , the default is 1. if you have a default vpc and a default security group, they are used. to launch an instance using a public parameter provided by awsamazon ec2 provides systems manager public parameters for public amis provided by aws. for example, the public parameter /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2 is available in all regions and always points to the latest version of the amazon linux 2 ami in the region. if you use systems manager parameters that point to ami ids in the launch instance wizard, you must add  and  to your iam policy.  grants your iam users the permission to view and select systems manager parameters.  grants your iam users the permission to get the values of the systems manager parameters. you can also restrict access to specific systems manager parameters. for more information, see . amis and systems manager parameters are region specific. to use the same systems manager parameter name across regions, create a systems manager parameter in each region with the same name (for example, ). in each region, point the systems manager parameter to an ami in that region. when you launch an instance using the amazon ec2 console, the choose an amazon machine image (ami) page includes a list of popular amis on the quick start tab. if you want to automate launching an instance using one of these quick start amis, you'll need to programatically locate the id of the current version of the ami. to locate the current version of a quick start ami, you can enumerate all amis with its ami name, and then find the one with the most recent creation date. example example: find the current amazon linux 2 ami   example example: find the current amazon linux ami   example example: find the current ubuntu server 16.04 lts ami   example example: find the current red hat enterprise linux 7.5 ami   example example: find the current suse linux enterprise server 15 ami   
to activate the grid virtual applications on g3 and g4 instances (nvidia grid virtual workstation is enabled by default), you must define the product type for the driver in the  file. to activate grid virtual applications on linux instances create the  file from the provided template file. open the  file in your favorite text editor. find the  line, and set it equal to 0. then add a line with . save the file and exit. reboot the instance to pick up the new configuration. 
to start using capacity reservations, you create the capacity reservation in the required availability zone. then, you can launch instances into the reserved capacity, view its capacity utilization in real time, and increase or decrease its capacity as needed.  by default, capacity reservations automatically match new instances and running instances that have matching attributes (instance type, platform, and availability zone). this means that any instance with matching attributes automatically runs in the capacity reservation. however, you can also target a capacity reservation for specific workloads. this enables you to explicitly control which instances are allowed to run in that reserved capacity. you can specify how the reservation ends. you can choose to manually cancel the capacity reservation or end it automatically at a specified time. if you specify an end time, the capacity reservation is canceled within an hour of the specified time. for example, if you specify 5/31/2019, 13:30:55, the capacity reservation is guaranteed to end between 13:30:55 and 14:30:55 on 5/31/2019. after a reservation ends, you can no longer target instances to the capacity reservation. instances running in the reserved capacity continue to run uninterrupted. if instances targeting a capacity reservation are stopped, you cannot restart them until you remove their capacity reservation targeting preference or configure them to target a different capacity reservation. contents after you create the capacity reservation, the capacity is available immediately. the capacity remains reserved for your use as long as the capacity reservation is active, and you can launch instances into it at any time. if the capacity reservation is open, new instances and existing instances that have matching attributes automatically run in the capacity reservation's capacity. if the capacity reservation is , instances must specifically target it to run in the reserved capacity. your request to create a capacity reservation could fail if one of the following is true: amazon ec2 does not have sufficient capacity to fulfill the request. either try again at a later time, try a different availability zone, or try a smaller capacity. if your application is flexible across instance types and sizes, try different instance attributes.the requested quantity exceeds your on-demand instance limit for the selected instance family. increase your on-demand instance limit for the instance family and try again. for more information, see .to create a capacity reservation using the console open the amazon ec2 console at . choose capacity reservations, and then choose create capacity reservation. on the create a capacity reservation page, configure the following settings in the instance details section. the instance type, platform, and availability zone of the instances that you launch must match the instance type, platform, and availability zone that you specify here or the capacity reservation is not applied. for example, if an open capacity reservation doesn't match, an instance launch that targets that capacity reservation explicitly will fail. instance type—the type of instance to launch into the reserved capacity. launch ebs-optimized instances—specify whether to reserve the capacity for ebs-optimized instances. this option is selected by default for some instance types. for more information about ebs-optimized instances, see . attach instance store at launch—specify whether instances launched into the capacity reservation use temporary block-level storage. the data on an instance store volume persists only during the life of the associated instance. platform—the operating system for your instances. availability zone—the availability zone in which to reserve the capacity. tenancy—specify whether to run on shared hardware (default) or a dedicated instance. quantity—the number of instances for which to reserve capacity. if you specify a quantity that exceeds your remaining on-demand instance limit for the selected instance type, the request is denied. configure the following settings in the reservation details section: reservation ends—choose one of the following options: manually—reserve the capacity until you explicitly cancel it.specific time—cancel the capacity reservation automatically at the specified date and time.instance eligibility—choose one of the following options: open—(default) the capacity reservation matches any instance that has matching attributes (instance type, platform, and availability zone). if you launch an instance with matching attributes, it is placed into the reserved capacity automatically.targeted—the capacity reservation only accepts instances that have matching attributes (instance type, platform, and availability zone), and that explicitly target the reservation.choose request reservation. to create a capacity reservation using the aws cliuse the  command: you can launch instances into an existing capacity reservation if it has matching attributes (instance type, platform, and availability zone) and sufficient capacity. launching an instance into a capacity reservation reduces its available capacity by the number of instances launched. for example, if you launch three instances, the capacity reservation's available capacity is reduced by three. to launch instances into an existing capacity reservation using the console open the launch instance wizard by choosing launch instances from dashboard or instances. select an amazon machine image (ami) and an instance type. complete the configure instance details page. for capacity reservation, choose one of the following options: open — launches the instances into any capacity reservation that has matching attributes and sufficient capacity for the number of instances you selected. if there is no matching capacity reservation with sufficient capacity, the instance uses on-demand capacity. — launches the instances into this specific capacity reservation. if this capacity reservation does not have sufficient capacity for the number of instances you selected, the instance launch fails.none — prevents the instances from launching into a capacity reservation.complete the remaining steps to launch the instances. to launch an instance into an existing capacity reservation using the aws cliuse the  command and specify the  parameter. the following example launches a  instance into any open capacity reservation that has matching attributes and available capacity: the following example launches a  instance into a  capacity reservation: you can change an active capacity reservation's attributes after you have created it. you cannot modify a capacity reservation after it has expired or after you have explicitly canceled it. when modifying a capacity reservation, you can only increase or decrease the quantity and change the way in which it is released. you cannot change a capacity reservation's instance type, ebs optimization, instance store settings, platform, availability zone, or instance eligibility. if you need to modify any of these attributes, we recommend that you cancel the reservation, and then create a new one with the required attributes. if you specify a new quantity that exceeds your remaining on-demand instance limit for the selected instance type, the update fails. to modify a capacity reservation using the console open the amazon ec2 console at . choose capacity reservations, select the capacity reservation to modify, and then choose edit. modify the quantity or reservation ends options as needed, and choose save changes. to modify a capacity reservation using the aws cliuse the  command: you can modify the following capacity reservation settings for a stopped instance at any time: start in any capacity reservation that has matching attributes (instance type, platform, and availability zone) and available capacity.start the instance in a specific capacity reservation.prevent the instance from starting in a capacity reservation.to modify an instance's capacity reservation settings using the console open the amazon ec2 console at . choose instances and select the instance to modify. stop the instance if it is not already stopped. choose actions, modify capacity reservation settings. for capacity reservation, choose one of the following options: open — starts the instance in any open capacity reservation that has matching attributes (instance type, platform, and availability zone) and available capacity. if you do not have a matching capacity reservation with available capacity, the instance uses on-demand capacity. — runs the instance in this specific capacity reservation. if the instance attributes (instance type, platform, and availability zone) do not match those of the capacity reservation, or if the selected capacity reservation does not have sufficient capacity, the instance launch fails.none — prevents the instance from running in a capacity reservation.to modify an instance's capacity reservation settings using the aws cliuse the  command: capacity reservations have the following possible states: —the capacity is available for use.—the capacity reservation expired automatically at the date and time specified in your reservation request. the reserved capacity is no longer available for your use.—the capacity reservation was manually canceled. the reserved capacity is no longer available for your use.—the capacity reservation request was successful but the capacity provisioning is still pending.—the capacity reservation request has failed. a request can fail due to invalid request parameters, capacity constraints, or instance limit constraints. you can view a failed request for 60 minutes.to view your capacity reservations using the console open the amazon ec2 console at . choose capacity reservations and select a capacity reservation to view. choose view launched instances for this reservation. to view your capacity reservations using the aws cliuse the  command: you can cancel a capacity reservation at any time if you no longer need the reserved capacity. when you cancel a capacity reservation, the capacity is released immediately, and it is no longer reserved for your use. you can cancel empty capacity reservations and capacity reservations that have running instances. if you cancel a capacity reservation that has running instances, the instances continue to run normally outside of the capacity reservation at standard on-demand instance rates or at a discounted rate if you have a matching savings plan or regional reserved instance. after you cancel a capacity reservation, instances that target it can no longer launch. modify these instances so that they either target a different capacity reservation, launch into any open capacity reservation with matching attributes and sufficient capacity, or avoid launching into a capacity reservation. for more information, see . to cancel a capacity reservation using the console open the amazon ec2 console at . choose capacity reservations and select the capacity reservation to cancel. choose cancel reservation, cancel reservation. to cancel a capacity reservation using the aws cliuse the  command: 
to create an amazon ebs-backed linux ami, start from an instance that you've launched from an existing amazon ebs-backed linux ami. this can be an ami you have obtained from the aws marketplace, an ami you have created using the  or , or any other ami you can access. after you customize the instance to suit your needs, create and register a new ami, which you can use to launch new instances with these customizations. the procedures described below work for amazon ec2 instances backed by encrypted amazon ebs volumes (including the root volume) as well as for unencrypted volumes. the ami creation process is different for instance store-backed amis. for more information about the differences between amazon ebs-backed and instance store-backed instances, and how to determine the root device type for your instance, see . for more information about creating an instance store-backed linux ami, see . for more information about creating an amazon ebs-backed windows ami, see  in the amazon ec2 user guide for windows instances. first, launch an instance from an ami that's similar to the ami that you'd like to create. you can connect to your instance and customize it. when the instance is configured correctly, ensure data integrity by stopping the instance before you create an ami, then create the image. when you create an amazon ebs-backed ami, we automatically register it for you. amazon ec2 powers down the instance before creating the ami to ensure that everything on the instance is stopped and in a consistent state during the creation process. if you're confident that your instance is in a consistent state appropriate for ami creation, you can tell amazon ec2 not to power down and reboot the instance. some file systems, such as xfs, can freeze and unfreeze activity, making it safe to create the image without rebooting the instance. during the ami-creation process, amazon ec2 creates snapshots of your instance's root volume and any other ebs volumes attached to your instance. you're charged for the snapshots until you deregister the ami and delete the snapshots. for more information, see . if any volumes attached to the instance are encrypted, the new ami only launches successfully on instances that support amazon ebs encryption. for more information, see . depending on the size of the volumes, it can take several minutes for the ami-creation process to complete (sometimes up to 24 hours). you may find it more efficient to create snapshots of your volumes before creating your ami. this way, only small, incremental snapshots need to be created when the ami is created, and the process completes more quickly (the total time for snapshot creation remains the same). for more information, see . after the process completes, you have a new ami and snapshot created from the root volume of the instance. when you launch an instance using the new ami, we create a new ebs volume for its root volume using the snapshot. if you add instance-store volumes or ebs volumes to your instance in addition to the root device volume, the block device mapping for the new ami contains information for these volumes, and the block device mappings for instances that you launch from the new ami automatically contain information for these volumes. the instance-store volumes specified in the block device mapping for the new instance are new and don't contain any data from the instance store volumes of the instance you used to create the ami. the data on ebs volumes persists. for more information, see . notewhen you create a new instance from an ebs-backed ami, you should initialize both its root volume and any additional ebs storage before putting it into production. for more information, see . you can create an ami using the aws management console or the command line. the following diagram summarizes the process for creating an amazon ebs-backed ami from a running ec2 instance. start with an existing ami, launch an instance, customize it, create a new ami from it, and finally launch an instance of your new ami. the steps in the following diagram match the steps in the procedure below.  to create an ami from an instance using the console select an appropriate ebs-backed ami to serve as a starting point for your new ami, and configure it as needed before launch. for more information, see . choose launch to launch an instance of the ebs-backed ami that you've selected. accept the default values as you step through the wizard. for more information, see . while the instance is running, connect to it. you can perform any of the following actions on your instance to customize it for your needs: install software and applicationscopy datareduce start time by deleting temporary files, defragmenting your hard drive, and zeroing out free spaceattach additional amazon ebs volumes(optional) create snapshots of all the volumes attached to your instance. for more information about creating snapshots, see . in the navigation pane, choose instances, select your instance, and then choose actions, image, create image. tipif this option is disabled, your instance isn't an amazon ebs-backed instance. in the create image dialog box, specify the following information, and then choose create image. image name – a unique name for the image.image description – an optional description of the image, up to 255 characters.no reboot – this option is not selected by default. amazon ec2 shuts down the instance, takes snapshots of any attached volumes, creates and registers the ami, and then reboots the instance. select no reboot to avoid having your instance shut down. warningif you select no reboot, we can't guarantee the file system integrity of the created image.instance volumes – the fields in this section enable you to modify the root volume, and add additional amazon ebs and instance store volumes. for information about each field, pause on the i icon next to each field to display field tooltips. some important points are listed below.to change the size of the root volume, locate root in the volume type column, and for size (gib), type the required value.if you select delete on termination, when you terminate the instance created from this ami, the ebs volume is deleted. if you clear delete on termination, when you terminate the instance, the ebs volume is not deleted. for more information, see .to add an amazon ebs volume, choose add new volume (which adds a new row). for volume type, choose ebs, and fill in the fields in the row. when you launch an instance from your new ami, additional volumes are automatically attached to the instance. empty volumes must be formatted and mounted. volumes based on a snapshot must be mounted.to add an instance store volume, see . when you launch an instance from your new ami, additional volumes are automatically initialized and mounted. these volumes do not contain data from the instance store volumes of the running instance on which you based your ami.to view the status of your ami while it is being created, in the navigation pane, choose amis. initially, the status is  but should change to  after a few minutes. (optional) to view the snapshot that was created for the new ami, choose snapshots. when you launch an instance from this ami, we use this snapshot to create its root device volume. launch an instance from your new ami. for more information, see . the new running instance contains all of the customizations that you applied in previous steps. you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)if you have a snapshot of the root device volume of an instance, you can create an ami from this snapshot using the aws management console or the command line. to create an ami from a snapshot using the console open the amazon ec2 console at . in the navigation pane, under elastic block store, choose snapshots. choose the snapshot and choose actions, create image. in the create image from ebs snapshot dialog box, complete the fields to create your ami, then choose create. if you're re-creating a parent instance, then choose the same options as the parent instance. architecture: choose i386 for 32-bit or x86_64 for 64-bit.root device name: enter the appropriate name for the root volume. for more information, see .virtualization type: choose whether instances launched from this ami use paravirtual (pv) or hardware virtual machine (hvm) virtualization. for more information, see .(pv virtualization type only) kernel id and ram disk id: choose the aki and ari from the lists. if you choose the default aki or don't choose an aki, you must specify an aki every time you launch an instance using this ami. in addition, your instance may fail the health checks if the default aki is incompatible with the instance.(optional) block device mappings: add volumes or expand the default size of the root volume for the ami. for more information about resizing the file system on your instance for a larger volume, see .to create an ami from a snapshot using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)
the following examples show launch configurations that you can use with the  command to create an ec2 fleet. for more information about the  parameters, see the . topics the following example specifies the minimum parameters required in an ec2 fleet: a launch template, target capacity, and default purchasing option. the launch template is identified by its launch template id and version number. the target capacity for the fleet is 2 instances, and the default purchasing option is , which results in the fleet launching 2 spot instances. the following example specifies the minimum parameters required in an ec2 fleet: a launch template, target capacity, and default purchasing option. the launch template is identified by its launch template id and version number. the target capacity for the fleet is 2 instances, and the default purchasing option is , which results in the fleet launching 2 on-demand instances. the following example specifies the total target capacity of 2 instances for the fleet, and a target capacity of 1 on-demand instance. the default purchasing option is . the fleet launches 1 on-demand instance as specified, but needs to launch one more instance to fulfill the total target capacity. the purchasing option for the difference is calculated as  –  = , which results in the fleet launching 1 spot instance. if the allocation strategy for spot instances is not specified, the default allocation strategy, which is , is used. the following example uses the  allocation strategy. the three launch specifications, which override the launch template, have different instance types but the same weighted capacity and subnet. the total target capacity is 2 instances and the default purchasing option is . the ec2 fleet launches 2 spot instances using the instance type of the launch specification with the lowest price. you can configure a fleet to use on-demand capacity reservations first when launching on-demand instances by setting the usage strategy for capacity reservations to . and if multiple instance pools have unused capacity reservations, the chosen on-demand allocation strategy is applied. in this example, the on-demand allocation strategy is . in this example, there are 15 available unused capacity reservations. this is more than the fleet's target on-demand capacity of 12 on-demand instances. the account has the following 15 unused capacity reservations in 3 different pools. the number of capacity reservations in each pool is indicated by . the following fleet configuration shows only the pertinent configurations for this example. the on-demand allocation strategy is , and the usage strategy for capacity reservations is . the total target capacity is 12, and the default target capacity type is . notethe fleet type must be . capacity reservations are not supported for other fleet types. after you create the  fleet using the preceding configuration, the following 12 instances are launched to meet the target capacity: 5 c4.large on-demand instances in us-east-1a – c4.large in us-east-1a is prioritized first, and there are 5 available unused c4.large capacity reservations5 c3.large on-demand instances in us-east-1a – c3.large in us-east-1a is prioritized second, and there are 5 available unused c3.large capacity reservations2 c5.large on-demand instances in us-east-1a – c5.large in us-east-1a is prioritized third, and there are 5 available unused c5.large capacity reservations of which only 2 are needed to meet the target capacityafter the fleet is launched, you can run  to see how many unused capacity reservations are remaining. in this example, you should see the following response, which shows that all of the c4.large and c3.large capacity reservations were used, with 3 c5.large capacity reservations remaining unused. you can configure a fleet to use on-demand capacity reservations first when launching on-demand instances by setting the usage strategy for capacity reservations to . and if the number of unused capacity reservations is less than the on-demand target capacity, the remaining on-demand target capacity is launched according to the chosen on-demand allocation strategy. in this example, the on-demand allocation strategy is . in this example, there are 15 available unused capacity reservations. this is less than the fleet's on-demand target capacity of 16 on-demand instances. the account has the following 15 unused capacity reservations in 3 different pools. the number of capacity reservations in each pool is indicated by . the following fleet configuration shows only the pertinent configurations for this example. the on-demand allocation strategy is , and the usage strategy for capacity reservations is . the total target capacity is 16, and the default target capacity type is . notethe fleet type must be . capacity reservations are not supported for other fleet types. after you create the  fleet using the preceding configuration, the following 16 instances are launched to meet the target capacity: 6 c4.large on-demand instances in us-east-1a – c4.large in us-east-1a is prioritized first, and there are 5 available unused c4.large capacity reservations. the capacity reservations are used first to launch 5 on-demand instances plus an additional on-demand instance is launched according to the on-demand allocation strategy, which is  in this example.5 c3.large on-demand instances in us-east-1a – c3.large in us-east-1a is prioritized second, and there are 5 available unused c3.large capacity reservations5 c5.large on-demand instances in us-east-1a – c5.large in us-east-1a is prioritized third, and there are 5 available unused c5.large capacity reservationsafter the fleet is launched, you can run  to see how many unused capacity reservations are remaining. in this example, you should see the following response, which shows that all of the capacity reservations in all of the pools were used. you can configure a fleet to use on-demand capacity reservations first when launching on-demand instances by setting the usage strategy for capacity reservations to . and if multiple instance pools have unused capacity reservations, the chosen on-demand allocation strategy is applied. in this example, the on-demand allocation strategy is . in this example, there are 15 available unused capacity reservations. this is more than the fleet's target on-demand capacity of 12 on-demand instances. the account has the following 15 unused capacity reservations in 3 different pools. the number of capacity reservations in each pool is indicated by . the following fleet configuration shows only the pertinent configurations for this example. the on-demand allocation strategy is , and the usage strategy for capacity reservations is . the total target capacity is 12, and the default target capacity type is . in this example, the on-demand instance price is: m5.large – $0.096 per hourm4.xlarge – $0.20 per hourm4.2xlarge – $0.40 per hournotethe fleet type must be . capacity reservations are not supported for other fleet types. after you create the  fleet using the preceding configuration, the following 12 instances are launched to meet the target capacity: 5 m5.large on-demand instances in us-east-1a – m5.large in us-east-1a is the lowest price, and there are 5 available unused m5.large capacity reservations5 m4.xlarge on-demand instances in us-east-1a – m4.xlarge in us-east-1a is the next lowest price, and there are 5 available unused m4.xlarge capacity reservations2 m4.2xlarge on-demand instances in us-east-1a – m4.2xlarge in us-east-1a is the third lowest price, and there are 5 available unused m4.2xlarge capacity reservations of which only 2 are needed to meet the target capacityafter the fleet is launched, you can run  to see how many unused capacity reservations are remaining. in this example, you should see the following response, which shows that all of the m5.large and m4.xlarge capacity reservations were used, with 3 m4.2xlarge capacity reservations remaining unused. you can configure a fleet to use on-demand capacity reservations first when launching on-demand instances by setting the usage strategy for capacity reservations to . and if the number of unused capacity reservations is less than the on-demand target capacity, the remaining on-demand target capacity is launched according to the chosen on-demand allocation strategy. in this example, the on-demand allocation strategy is . in this example, there are 15 available unused capacity reservations. this is less than the fleet's on-demand target capacity of 16 on-demand instances. the account has the following 15 unused capacity reservations in 3 different pools. the number of capacity reservations in each pool is indicated by . the following fleet configuration shows only the pertinent configurations for this example. the on-demand allocation strategy is , and the usage strategy for capacity reservations is . the total target capacity is 16, and the default target capacity type is . in this example, the on-demand instance price is: m5.large – $0.096 per hourm4.xlarge – $0.20 per hourm4.2xlarge – $0.40 per hournotethe fleet type must be . capacity reservations are not supported for other fleet types. after you create the  fleet using the preceding configuration, the following 16 instances are launched to meet the target capacity: 6 m5.large on-demand instances in us-east-1a – m5.large in us-east-1a is the lowest price, and there are 5 available unused m5.large capacity reservations. the capacity reservations are used first to launch 5 on-demand instances plus an additional on-demand instance is launched according to the on-demand allocation strategy, which is  in this example.5 m4.xlarge on-demand instances in us-east-1a – m4.xlarge in us-east-1a is the next lowest price, and there are 5 available unused m4.xlarge capacity reservations5 m4.2xlarge on-demand instances in us-east-1a – m4.2xlarge in us-east-1a is the third lowest price, and there are 5 available unused m4.2xlarge capacity reservationsafter the fleet is launched, you can run  to see how many unused capacity reservations are remaining. in this example, you should see the following response, which shows that all of the capacity reservations in all of the pools were used. 
cloudwatch metrics are statistical data that you can use to view, analyze, and set alarms on the operational behavior of your volumes.  the following table describes the types of monitoring data available for your amazon ebs volumes. when you get data from cloudwatch, you can include a  request parameter to specify the granularity of the returned data. this is different than the period that we use when we collect the data (5-minute periods). we recommend that you specify a period in your request that is equal to or larger than the collection period to ensure that the returned data is valid. you can get the data using either the cloudwatch api or the amazon ec2 console. the console takes the raw data from the cloudwatch api and displays a series of graphs based on the data. depending on your needs, you might prefer to use either the data from the api or the graphs in the console. amazon elastic block store (amazon ebs) sends data points to cloudwatch for several metrics. amazon ebs general purpose ssd (gp2), throughput optimized hdd (st1) , cold hdd (sc1), and magnetic (standard) volumes automatically send five-minute metrics to cloudwatch. provisioned iops ssd (io1) volumes automatically send one-minute metrics to cloudwatch. data is only reported to cloudwatch when the volume is attached to an instance. some of these metrics have differences on nitro-based instances. for a list of instance types based on the nitro system, see . the  namespace includes the following metrics. topics the  namespace includes the following metrics for ebs volumes. to get information about the available disk space from the operating system on an instance, see .  namespace includes the following metrics for . the supported dimension is the volume id (). all available statistics are filtered by volume id. for the , the supported dimension is the volume id (). all available statistics are filtered by volume id. for the , the supported dimensions are the snapshot id () and the availability zone (). after you create a volume, you can view the volume's monitoring graphs in the amazon ec2 console. select a volume on the volumes page in the console and choose monitoring. the following table lists the graphs that are displayed. the column on the right describes how the raw data metrics from the cloudwatch api are used to produce each graph. the period for all the graphs is 5 minutes. for the average latency graphs and average size graphs, the average is calculated over the total number of operations (read or write, whichever is applicable to the graph) that completed during the period. 
to retrieve dynamic data from within a running instance, use the following uri.  this example shows how to retrieve the high-level instance identity categories. for more information about dynamic data and examples of how to retrieve it, see . 
use the amazon linux 2 virtual machine (vm) images for on-premises development and testing. these images are available for use on the following virtualization platforms: vmwarekvmvirtualbox (oracle vm)microsoft hyper-vto use the amazon linux 2 virtual machine images with one of the supported virtualization platforms, do the following: the  boot image includes the initial configuration information that is needed to boot your new vm, such as the network configuration, host name, and user data. notethe  boot image includes only the configuration information required to boot the vm. it does not include the amazon linux 2 operating system files. to generate the  boot image, you need two configuration files: —this file includes the hostname and static network settings for the vm.—this file configures user accounts, and specifies their passwords, key pairs, and access mechanisms. by default, the amazon linux 2 vm image creates a  user account. you use the  configuration file to set the password for the default user account.to create the  boot disc create a new folder named  and navigate into it. create the  configuration file. create a new file named . open the  file using your preferred editor and add the following. replace vm_hostname with a vm host name of your choice, and configure the network settings as required. save and close the  configuration file. for an example  configuration file that specifies a vm hostname (), configures the default network interface (), and specifies static ip addresses for the necessary network devices, see the . create the  configuration file. create a new file named . open the  file using your preferred editor and add the following. replace plain_text_password with a password of your choice for the default  user account. (optional) by default, cloud-init applies network settings each time the vm boots. add the following to prevent cloud-init from applying network settings at each boot, and to retain the network settings applied during the first boot. save and close the  configuration file. you can also create additional user accounts and specify their access mechanisms, passwords, and key pairs. for more information about the supported directives, see . for an example  file that creates three additional users and specifies a custom password for the default  user account, see the . create the  boot image using the  and  configuration files. for linux, use a tool such as genisoimage. navigate into the  folder, and execute the following command. for macos, use a tool such as hdiutil. navigate one level up from the  folder, and execute the following command. we offer a different amazon linux 2 vm image for each of the supported virtualization platforms. download the correct vm image for your chosen platform: to boot and connect to your new vm, you must have the  boot image (created in step 1) and an amazon linux 2 vm image (downloaded in step 2). the steps vary depending on your chosen vm platform. the vm image for vmware is made available in the ovf format. to boot the vm using vmware vsphere create a new datastore for the  file, or add it to an existing datastore. deploy the ovf template, but do not start the vm yet. in the navigator panel, right-click the new virtual machine and choose edit settings. on the virtual hardware tab, for new device, choose cd/dvd drive, and then choose add. for new cd/dvd drive, choose datastore iso file. select the datastore to which you added the  file, browse to and select the  file, and then choose ok. for new cd/dvd drive, select connect, and then choose ok. after you have associated the datastore with the vm, you should be able to boot it. to boot the vm using kvm open the create new vm wizard. for step 1, choose import existing disk image. for step 2, browse to and select the vm image. for os type and version, choose linux and red hat enterprise linux 7.0 respectively. for step 3, specify the amount of ram and the number of cpus to use. for step 4, enter a name for the new vm and select customize configuration before install, and choose finish. in the configuration window for the vm, choose add hardware. in the add new virtual hardware window, choose storage. in the storage configuration, choose select or create custom storage. for device type, choose cdrom device. choose manage, browse local, and then navigate to and select the  file. choose finish. choose begin installation. to boot the vm using oracle virtualbox open the create virtual machine wizard. when prompted to select a type and a version, select linux and rhel (64-bit) respectively. when prompted to configure the hard disk, choose use an existing virtual hard disk file, select the vm image, and then choose create. before you start the vm, you must load the  file in the virtual machine's virtual optical drive: select the new vm, choose settings, and then choose storage. in the storage devices list, under controller: ide, choose the empty optical drive. in the attributes section for the optical drive, choose the browse button, select choose virtual optical disk file, and then select the  file. after you have added the  file to the virtual optical drive, you should be able to boot the vm. the vm image for microsoft hyper-v is compressed into a zip file. you must extract the contents of the zip file. to boot the vm using microsoft hyper-v open the new virtual machine wizard. when prompted to select a generation, select generation 1. when prompted to configure the network adapter, for connection choose external. when prompted to connect a virtual hard disk, choose use an existing virtual hard disk, choose browse, and then navigate to and select the vm image. choose finish to create the vm. right-click the new vm and choose settings. in the settings window, under ide controller 1, choose dvd drive. for the dvd drive, choose image file and then browse to and select the  file. apply the changes and start the vm. after the vm has booted, log in using one of the user accounts that is defined in the  configuration file. for virtualization platforms other than vmware, you can disconnect the  boot image from the vm after you have logged in for the first time. 
you can view detailed information about your snapshots. to view snapshot information using the console open the amazon ec2 console at . choose snapshots in the navigation pane.  to reduce the list, choose an option from the filter list. for example, to view only your snapshots, choose owned by me. you can also filter your snapshots using tags and snapshot attributes. choose the search bar to view the available tags and attributes. to view more information about a snapshot, select it. to view snapshot information using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)example example: filter based on tagsthe following command describes the snapshots with the tag stack=production.   example example: filter based on volumethe following command describes the snapshots created from the specified volume.   example example: filter based on snapshot agewith the aws cli, you can use jmespath to filter results using expressions. for example, the following command displays the ids of all snapshots created by your aws account (represented by 123456789012) before the specified date (represented by 2020-03-31). if you do not specify the owner, the results include all public snapshots.   the following command displays the ids of all snapshots created in the specified date range.   
to ensure the best iops performance from your ssd instance store volumes on linux, we recommend that you use the most recent version of amazon linux, or another linux ami with a kernel version of 3.8 or later. if you do not use a linux ami with a kernel version of 3.8 or later, your instance won't achieve the maximum iops performance available for these instance types. like other instance store volumes, you must map the ssd instance store volumes for your instance when you launch it. the data on an ssd instance volume persists only for the life of its associated instance. for more information, see . the following instances offer non-volatile memory express (nvme) solid state drives (ssd) instance store volumes: c5d, g4, i3, i3en, f1, m5ad, m5d, , r5ad, r5d, and z1d. to access nvme volumes, the  must be installed. the following amis meet this requirement: amazon linux 2amazon linux ami 2018.03ubuntu 14.04 (with  kernel) or laterred hat enterprise linux 7.4 or latersuse linux enterprise server 12 sp2 or latercentos 7.4.1708 or laterfreebsd 11.1 or laterdebian gnu/linux 9 or laterafter you connect to your instance, you can list the nvme devices using the lspci command. the following is example output for an  instance, which supports four nvme devices. if you are using a supported operating system but you do not see the nvme devices, verify that the nvme module is loaded using the following command. amazon linux, amazon linux 2, ubuntu 14/16, red hat enterprise linux, suse linux enterprise server, centos 7 ubuntu 18 the nvme volumes are compliant with the nvme 1.0e specification. you can use the nvme commands with your nvme volumes. with amazon linux, you can install the  package from the repo using the yum install command. with other supported versions of linux, you can download the  package if it's not available in the image. the data on nvme instance storage is encrypted using an xts-aes-256 block cipher implemented in a hardware module on the instance. the encryption keys are generated using the hardware module and are unique to each nvme instance storage device. all encryption keys are destroyed when the instance is stopped or terminated and cannot be recovered. you cannot disable this encryption and you cannot provide your own encryption key. the following instances support instance store volumes that use non-nvme ssds to deliver high random i/o performance: c3, g2, i2, m3, r3, and x1. for more information about the instance store volumes support by each instance type, see . the following instances support ssd volumes with trim: c5d, f1, g4, i2, i3, i3en, m5ad, m5d, , r3, r5ad, r5d, and z1d. instance store volumes that support trim are fully trimmed before they are allocated to your instance. these volumes are not formatted with a file system when an instance launches, so you must format them before they can be mounted and used. for faster access to these volumes, you should skip the trim operation when you format them.  with instance store volumes that support trim, you can use the trim command to notify the ssd controller when you no longer need data that you've written. this provides the controller with more free space, which can reduce write amplification and increase performance. on linux, use the  command to enable periodic trim. 
the  mode is a credit configuration option for burstable performance instances. it can be enabled or disabled at any time for a running or stopped instance. you can set  as the default credit option at the account level per aws region, per burstable performance instance family, so that all new burstable performance instances in the account launch using the default credit option. notet3 and t3a instances are launched as  by default. t2 instances are launched as  by default. you can change the default at the account level per aws region. for more information, see . if a burstable performance instance configured as  depletes its cpu credit balance, it can spend surplus credits to burst beyond the . when its cpu utilization falls below the baseline, it uses the cpu credits that it earns to pay down the surplus credits that it spent earlier. the ability to earn cpu credits to pay down surplus credits enables amazon ec2 to average the cpu utilization of an instance over a 24-hour period. if the average cpu usage over a 24-hour period exceeds the baseline, the instance is billed for the additional usage at a flat additional rate per vcpu-hour. the following graph shows the cpu usage of a . the baseline cpu utilization for a  is 30%. if the instance runs at 30% cpu utilization or less on average over a 24-hour period, there is no additional charge because the cost is already covered by the instance hourly price. however, if the instance runs at 40% cpu utilization on average over a 24-hour period, as shown in the graph, the instance is billed for the additional 10% cpu usage at a flat additional rate per vcpu-hour.  for more information about the baseline utilization per vcpu for each instance type and how many credits each instance type earns, see the . when determining whether you should use a burstable performance instance in  mode, such as a t3, or a fixed performance instance, such as an m5, you need to determine the breakeven cpu usage. the breakeven cpu usage for a burstable performance instance is the point at which a burstable performance instance costs the same as a fixed performance instance. the breakeven cpu usage helps you determine the following: if the average cpu usage over a 24-hour period is at or below the breakeven cpu usage, use a burstable performance instance in  mode so that you can benefit from the lower price of a burstable performance instance while getting the same performance as a fixed performance instance.if the average cpu usage over a 24-hour period is above the breakeven cpu usage, the burstable performance instance will cost more than the equivalently-sized fixed performance instance. if a t3 instance continuously bursts at 100% cpu, you end up paying approximately 1.5 times the price of an equivalently-sized m5 instance.the following graph shows the breakeven cpu usage point where a  costs the same as an . the breakeven cpu usage point for a  is 42.5%. if the average cpu usage is at 42.5%, the cost of running the  is the same as an , and is more expensive if the average cpu usage is above 42.5%. if the workload needs less than 42.5% average cpu usage, you can benefit from the lower price of the  while getting the same performance as an .  the following table shows how to calculate the breakeven cpu usage threshold so that you can determine when it's less expensive to use a burstable performance instance in  mode or a fixed performance instance. the columns in the table are labeled a through k. the table provides the following information: column a shows the instance type, .column b shows the number of vcpus for the .column c shows the price of a  per hour.column d shows the price of an  per hour.column e shows the price difference between the  and the . column f shows the baseline utilization per vcpu of the , which is 30%. at the baseline, the hourly cost of the instance covers the cost of the cpu usage.column g shows the flat additional rate per vcpu-hour that an instance is charged if it bursts at 100% cpu after it has depleted its earned credits.column h shows the flat additional rate per vcpu-minute that an instance is charged if it bursts at 100% cpu after it has depleted its earned credits.column i shows the number of additional minutes that the  can burst per hour at 100% cpu while paying the same price per hour as an .column j shows the additional cpu usage (in %) over baseline that the instance can burst while paying the same price per hour as an .column k shows the breakeven cpu usage (in %) that the  can burst without paying more than the . anything above this, and the  costs more than the .the following table shows the breakeven cpu usage (in %) for t3 instance types compared to the similarly-sized m5 instance types. if the average cpu utilization of an instance is at or below the baseline, the instance incurs no additional charges. because an instance earns a  in a 24-hour period (for example, a  instance can earn a maximum of 288 credits in a 24-hour period), it can spend surplus credits up to that maximum without being charged. however, if cpu utilization stays above the baseline, the instance cannot earn enough credits to pay down the surplus credits that it has spent. the surplus credits that are not paid down are charged at a flat additional rate per vcpu-hour. surplus credits that were spent earlier are charged when any of the following occurs: the spent surplus credits exceed the  the instance can earn in a 24-hour period. spent surplus credits above the maximum are charged at the end of the hour.the instance is stopped or terminated.the instance is switched from  to .spent surplus credits are tracked by the cloudwatch metric . surplus credits that are charged are tracked by the cloudwatch metric . for more information, see . t2 standard instances receive , but t2 unlimited instances do not. a t2 unlimited instance can burst beyond the baseline at any time with no additional charge, as long as its average cpu utilization is at or below the baseline over a rolling 24-hour window or its lifetime, whichever is shorter. as such, t2 unlimited instances do not require launch credits to achieve high performance immediately after launch. if a t2 instance is switched from  to , any accrued launch credits are removed from the  before the remaining  is carried over. notet3 and t3a instances never receive launch credits. t3 and t3a instances launch as  by default. t2 instances launch as  by default, but you can enable  at launch. you can switch from  to , and from  to , at any time on a running or stopped instance. for more information, see  and . you can set  as the default credit option at the account level per aws region, per burstable performance instance family, so that all new burstable performance instances in the account launch using the default credit option. for more information, see . you can check whether your burstable performance instance is configured as  or  using the amazon ec2 console or the aws cli. for more information, see  and .  is a cloudwatch metric that tracks the number of credits accrued by an instance.  is a cloudwatch metric that tracks the number of surplus credits spent by an instance. when you change an instance configured as  to , the following occurs: the  value remains unchanged and is carried over. the  value is immediately charged.when a  instance is switched to , the following occurs: the  value containing accrued earned credits is carried over.for t2 standard instances, any launch credits are removed from the  value, and the remaining  value containing accrued earned credits is carried over.to see if your instance is spending more credits than the baseline provides, you can use cloudwatch metrics to track usage, and you can set up hourly alarms to be notified of credit usage. for more information, see . 
the maximum transmission unit (mtu) of a network connection is the size, in bytes, of the largest permissible packet that can be passed over the connection. the larger the mtu of a connection, the more data that can be passed in a single packet. ethernet packets consist of the frame, or the actual data you are sending, and the network overhead information that surrounds it. ethernet frames can come in different formats, and the most common format is the standard ethernet v2 frame format. it supports 1500 mtu, which is the largest ethernet packet size supported over most of the internet. the maximum supported mtu for an instance depends on its instance type. all amazon ec2 instance types support 1500 mtu, and many current instance sizes support 9001 mtu, or jumbo frames. topics jumbo frames allow more than 1500 bytes of data by increasing the payload size per packet, and thus increasing the percentage of the packet that is not packet overhead. fewer packets are needed to send the same amount of usable data. however, outside of a given aws region (ec2-classic), a single vpc, or a vpc peering connection, you will experience a maximum path of 1500 mtu. vpn connections and traffic sent over an internet gateway are limited to 1500 mtu. if packets are over 1500 bytes, they are fragmented, or they are dropped if the  flag is set in the ip header. jumbo frames should be used with caution for internet-bound traffic or any traffic that leaves a vpc. packets are fragmented by intermediate systems, which slows down this traffic. to use jumbo frames inside a vpc and not slow traffic that's bound for outside the vpc, you can configure the mtu size by route, or use multiple elastic network interfaces with different mtu sizes and different routes. for instances that are collocated inside a cluster placement group, jumbo frames help to achieve the maximum network throughput possible, and they are recommended in this case. for more information, see . you can use jumbo frames for traffic between your vpcs and your on-premises networks over aws direct connect. for more information, and for how to verify jumbo frame capability, see  in the aws direct connect user guide. all  support jumbo frames. the following previous generation instances support jumbo frames: c3, g2, i2, m3, and r3. for more information about supported mtu sizes for transit gateways, see  in amazon vpc transit gateways. path mtu discovery (pmtud) is used to determine the maximum transmission unit (mtu) of a network path. path mtu is the maximum packet size between the originating host and the receiving host. if a host sends a packet that's larger than the mtu of the receiving host or that's larger than the mtu of a device along the path, the receiving host or device returns the following icmp message:  (type 3, code 4). this instructs the original host to adjust the mtu until the packet can be transmitted. by default, security groups do not allow any inbound icmp traffic. however, security groups are stateful, therefore icmp responses to outbound requests are allowed to flow in, regardless of security group rules. therefore, you do not need to explicitly add an inbound icmp rule to ensure that your instance can receive the icmp message response. for more information about configuring icmp rules in a network acl, see  in the amazon vpc user guide. importantpath mtu discovery does not guarantee that jumbo frames will not be dropped by some routers. an internet gateway in your vpc will forward packets up to 1500 bytes only. 1500 mtu packets are recommended for internet traffic. you can check the path mtu between two hosts using the tracepath command, which is part of the  package that is available by default on many linux distributions, including amazon linux.  to check path mtu using tracepathuse the following command to check the path mtu between your ec2 instance and another host. you can use a dns name or an ip address as the destination. if the destination is another ec2 instance, verify that the security group allows inbound udp traffic. this example checks the path mtu between an ec2 instance and . in this example, the path mtu is 1500. some instances are configured to use jumbo frames, and others are configured to use standard frame sizes. you may want to use jumbo frames for network traffic within your vpc or you may want to use standard frames for internet traffic. whatever your use case, we recommend verifying that your instance will behave the way you expect it to. you can use the procedures in this section to check your network interface's mtu setting and modify it if needed. to check the mtu setting on a linux instanceyou can check the current mtu value using the following ip command. note that in the example output, mtu 9001 indicates that this instance uses jumbo frames. to set the mtu value on a linux instance you can set the mtu value using the ip command. the following command sets the desired mtu value to 1500, but you could use 9001 instead. (optional) to persist your network mtu setting after a reboot, modify the following configuration files, based on your operating system type. for amazon linux 2, add the following line to the  file: add the following line to the  file: for amazon linux, add the following lines to your  file. for other linux distributions, consult their specific documentation.(optional) reboot your instance and verify that the mtu setting is correct. if you experience connectivity issues between your ec2 instance and an amazon redshift cluster when using jumbo frames, see  in the amazon redshift cluster management guide 
you can create a launch template that contains the configuration information to launch an instance. launch templates enable you to store launch parameters so that you do not have to specify them every time you launch an instance. for example, a launch template can contain the ami id, instance type, and network settings that you typically use to launch instances. when you launch an instance using the amazon ec2 console, an aws sdk, or a command line tool, you can specify the launch template to use. for each launch template, you can create one or more numbered launch template versions. each version can have different launch parameters. when you launch an instance from a launch template, you can use any version of the launch template. if you do not specify a version, the default version is used. you can set any version of the launch template as the default version—by default, it's the first version of the launch template. the following diagram shows a launch template with three versions. the first version specifies the instance type, ami id, subnet, and key pair to use to launch the instance. the second version is based on the first version and also specifies a security group for the instance. the third version uses different values for some of the parameters. version 2 is set as the default version. if you launched an instance from this launch template, the launch parameters from version 2 would be used if no other version were specified.  topics the following rules apply to launch templates and launch template versions: you are limited to creating 5,000 launch templates per region and 10,000 versions per launch template.launch template parameters are optional. however, you must ensure that your request to launch an instance includes all required parameters. for example, if your launch template does not include an ami id, you must specify both the launch template and an ami id when you launch an instance.launch template parameters are not fully validated when you create the launch template. if you specify incorrect values for parameters, or if you do not use supported parameter combinations, no instances can launch using this launch template. ensure that you specify the correct values for the parameters and that you use supported parameter combinations. for example, to launch an instance in a placement group, you must specify a supported instance type.you can tag a launch template, but you cannot tag a launch template version.launch template versions are numbered in the order in which they are created. when you create a launch template version, you cannot specify the version number yourself.a launch template can contain all or some of the parameters to launch an instance. when you launch an instance using a launch template, you can override parameters that are specified in the launch template. or, you can specify additional parameters that are not in the launch template. noteyou cannot remove launch template parameters during launch (for example, you cannot specify a null value for the parameter). to remove a parameter, create a new version of the launch template without the parameter and use that version to launch the instance. to launch instances, iam users must have permissions to use the  action. you must also have permissions to create or use the resources that are created or associated with the instance. you can use resource-level permissions for the  action to control the launch parameters that users can specify. alternatively, you can grant users permissions to launch an instance using a launch template. this enables you to manage launch parameters in a launch template rather than in an iam policy, and to use a launch template as an authorization vehicle for launching instances. for example, you can specify that users can only launch instances using a launch template, and that they can only use a specific launch template. you can also control the launch parameters that users can override in the launch template. for example policies, see . by default, iam users do not have permissions to work with launch templates. you can create an iam user policy that grants users permissions to create, modify, describe, and delete launch templates and launch template versions. you can also apply resource-level permissions to some launch template actions to control a user's ability to use specific resources for those actions. for more information, see the following example policies: . take care when granting users permissions to use the  and  actions. you cannot use resource-level permissions to control which resources users can specify in the launch template. to restrict the resources that are used to launch an instance, ensure that you grant permissions to create launch templates and launch template versions only to appropriate administrators. create a new launch template using parameters that you define, or use an existing launch template or an instance as the basis for a new launch template. topics to create a new launch template using defined parameters using the console open the amazon ec2 console at . in the navigation pane, choose launch templates, and then choose create launch template. for launch template name, enter a descriptive name for the launch template. for template version description, provide a brief description of the launch template version. to tag the launch template on creation, expand template tags, choose add tag, and then enter a tag key and value pair. for launch template contents, provide the following information: ami: an ami from which to launch the instance. to search through all available amis, choose search for ami. to select a commonly used ami, choose quick start. or, choose aws marketplace or community amis. you can use an ami that you own or .instance type: ensure that the instance type is compatible with the ami that you've specified. for more information, see . key pair name: the key pair for the instance. for more information, see .network platform: if applicable, whether to launch the instance into a vpc or ec2-classic. if you choose vpc, specify the subnet in the network interfaces section. if you choose classic, ensure that the specified instance type is supported in ec2-classic and specify the availability zone for the instance.security groups: one or more security groups to associate with the instance. if you add a network interface to the launch template, omit this setting and specify the security groups as part of the network interface specification. you cannot launch an instance from a launch template that specifies security groups and a network interface. for more information, see .for storage (volumes), specify volumes to attach to the instance besides the volumes specified by the ami (volume 1 (ami root)). to add a new volume, choose add new volume. volume type: the instance store or amazon ebs volumes with which to associate your instance. the type of volume depends on the instance type that you've chosen. for more information, see  and .device name: a device name for the volume.snapshot: the id of the snapshot from which to create the volume.size: for amazon ebs volumes, the storage size.volume type: for amazon ebs volumes, the volume type. for more information, see .iops: for the provisioned iops ssd volume type, the number of i/o operations per second (iops) that the volume can support.delete on termination: for amazon ebs volumes, whether to delete the volume when the instance is terminated. for more information, see .encrypted: if the instance type supports ebs encryption, you can enable encryption for the volume. if you have enabled encryption by default in this region, encryption is enabled for you. for more information, see .key: the cmk to use for ebs encryption. you can specify the arn of any customer master key (cmk) that you created using the aws key management service. if you specify a cmk, you must also use encrypted to enable encryption.for instance tags, specify  by providing key and value combinations. you can tag the instance, the volumes, or both. for network interfaces, you can specify up to two  for the instance. device index: the device number for the network interface, for example,  for the primary network interface. if you leave the field blank, aws creates the primary network interface.network interface: the id of the network interface, or leave blank to let aws create a new network interface.description: (optional) a description for the new network interface.subnet: the subnet in which to create a new network interface. for the primary network interface (), this is the subnet in which the instance is launched. if you've entered an existing network interface for , the instance is launched in the subnet in which the network interface is located.auto-assign public ip: whether to automatically assign a public ip address to the network interface with the device index of . this setting can only be enabled for a single, new network interface.primary ip: a private ipv4 address from the range of your subnet. leave blank to let aws choose a private ipv4 address for you.secondary ip: a secondary private ipv4 address from the range of your subnet. leave blank to let aws choose one for you.(ipv6-only) ipv6 ips: an ipv6 address from the range of the subnet.security group id: the id of a security group in your vpc with which to associate the network interface.delete on termination: whether the network interface is deleted when the instance is deleted.elastic fabric adapter: indicates whether the network interface is an elastic fabric adapter. for more information, see .for advanced details, expand the section to view the fields and specify any additional parameters for the instance. purchasing option: the purchasing model. choose request spot instances to request spot instances at the spot price, capped at the on-demand price, and choose customize to change the default spot instance settings. if you do not request a spot instance, ec2 launches an on-demand instance by default. for more information, see .iam instance profile: an aws identity and access management (iam) instance profile to associate with the instance. for more information, see .shutdown behavior: whether the instance should stop or terminate when shut down. for more information, see .stop - hibernate behavior: whether the instance is enabled for hibernation. this field is only valid for instances that meet the hibernation prerequisites. for more information, see .termination protection: whether to prevent accidental termination. for more information, see .detailed cloudwatch monitoring: whether to enable detailed monitoring of the instance using amazon cloudwatch. additional charges apply. for more information, see .elastic inference: an elastic inference accelerator to attach to your ec2 cpu instance. for more information, see  in the amazon elastic inference developer guide.t2/t3 unlimited: whether to enable applications to burst beyond the baseline for as long as needed. this field is only valid for t2, t3, and t3a instances. additional charges may apply. for more information, see .placement group name: specify a placement group in which to launch the instance. not all instance types can be launched in a placement group. for more information, see .ebs-optimized instance: provides additional, dedicated capacity for amazon ebs i/o. not all instance types support this feature, and additional charges apply. for more information, see .capacity reservation: specify whether to launch the instance into a specific capacity reservation (specify a capacity reservation id) or any open capacity reservation that has matching attributes (open), or prevent the instance from running in a capacity reservation even if there is a matching one (none). for more information, see .tenancy: choose whether to run your instance on shared hardware (shared), isolated, dedicated hardware (dedicated), or on a dedicated host (dedicated host). if you choose to launch the instance onto a dedicated host, you can specify whether to launch the instance into a host resource group or you can target a specific dedicated host. additional charges may apply. for more information, see  and .ram disk id: (only valid for paravirtual (pv) amis) a ram disk for the instance. if you have specified a kernel, you may need to specify a specific ram disk with the drivers to support it.kernel id: (only valid for paravirtual (pv) amis) a kernel for the instance.license manager: you can launch instances against the specified license configuration to track your license usage. for more information, see  in the aws license manager user guide.metadata accessible: whether to enable or disable access to the instance metadata. for more information, see .metadata version: if you enable access to the instance metadata, you can choose to require the use of instance metadata service version 2 when requesting instance metadata. for more information, see .metadata token response hop limit: if you enable instance metadata, you can set the allowable number of network hops for the metadata token. for more information, see .user data: you can specify user data to configure an instance during launch, or to run a configuration script. for more information, see .choose create launch template. to create a new launch template using defined parameters using the console open the amazon ec2 console at . in the navigation pane, choose launch templates, and then choose create launch template. for launch template name, enter a descriptive name for the launch template. to tag the launch template on creation, choose show tags, add tag, and then enter a tag key and value pair. for template version description, provide a brief description of the launch template version. for launch template contents, provide the following information: ami id: an ami from which to launch the instance. to search through all available amis, choose search for ami. to select a commonly used ami, choose quick start. or, choose aws marketplace or community amis. you can use an ami that you own or .instance type: ensure that the instance type is compatible with the ami that you've specified. for more information, see . key pair name: the key pair for the instance. for more information, see .network type: if applicable, whether to launch the instance into a vpc or ec2-classic. if you choose vpc, specify the subnet in the network interfaces section. if you choose classic, ensure that the specified instance type is supported in ec2-classic and specify the availability zone for the instance.security groups: one or more security groups to associate with the instance. for more information, see .for network interfaces, you can specify up to two  for the instance. device: the device number for the network interface, for example,  for the primary network interface. if you leave the field blank, aws creates the primary network interface.network interface: the id of the network interface, or leave blank to let aws create a new network interface.description: (optional) a description for the new network interface.subnet: the subnet in which to create a new network interface. for the primary network interface (), this is the subnet in which the instance is launched. if you've entered an existing network interface for , the instance is launched in the subnet in which the network interface is located.auto-assign public ip: whether to automatically assign a public ip address to the network interface with the device index of . this setting can only be enabled for a single, new network interface.primary ip: a private ipv4 address from the range of your subnet. leave blank to let aws choose a private ipv4 address for you.secondary ip: a secondary private ipv4 address from the range of your subnet. leave blank to let aws choose one for you.(ipv6-only) ipv6 ips: an ipv6 address from the range of the subnet.security group id: the id of a security group in your vpc with which to associate the network interface.delete on termination: whether the network interface is deleted when the instance is deleted.elastic fabric adapter: indicates whether the network interface is an elastic fabric adapter. for more information, see .for storage (volumes), specify volumes to attach to the instance besides the volumes specified by the ami. volume type: the instance store or amazon ebs volumes with which to associate your instance. the type of volume depends on the instance type that you've chosen. for more information, see  and .device name: a device name for the volume.snapshot: the id of the snapshot from which to create the volume.size: for amazon ebs volumes, the storage size.volume type: for amazon ebs volumes, the volume type. for more information, see .iops: for the provisioned iops ssd volume type, the number of i/o operations per second (iops) that the volume can support.delete on termination: for amazon ebs volumes, whether to delete the volume when the instance is terminated. for more information, see .encrypted: if the instance type supports ebs encryption, you can enable encryption for the volume. if you have enabled encryption by default in this region, encryption is enabled for you. for more information, see .key: the cmk to use for ebs encryption. you can specify the arn of any customer master key (cmk) that you created using the aws key management service. if you specify a cmk, you must also use encrypted to enable encryption.for instance tags, specify  by providing key and value combinations. you can tag the instance, the volumes, or both. for advanced details, expand the section to view the fields and specify any additional parameters for the instance. purchasing option: the purchasing model. choose request spot instances to request spot instances at the spot price, capped at the on-demand price, and choose customize spot parameters to change the default spot instance settings. if you do not request a spot instance, ec2 launches an on-demand instance by default. for more information, see .iam instance profile: an aws identity and access management (iam) instance profile to associate with the instance. for more information, see .shutdown behavior: whether the instance should stop or terminate when shut down. for more information, see .stop - hibernate behavior: whether the instance is enabled for hibernation. this field is only valid for instances that meet the hibernation prerequisites. for more information, see .termination protection: whether to prevent accidental termination. for more information, see .monitoring: whether to enable detailed monitoring of the instance using amazon cloudwatch. additional charges apply. for more information, see .t2/t3 unlimited: whether to enable applications to burst beyond the baseline for as long as needed. this field is only valid for t2 and t3 instances. additional charges may apply. for more information, see .placement group name: specify a placement group in which to launch the instance. not all instance types can be launched in a placement group. for more information, see .ebs-optimized instance: provides additional, dedicated capacity for amazon ebs i/o. not all instance types support this feature, and additional charges apply. for more information, see .tenancy: choose whether to run your instance on shared hardware (shared), isolated, dedicated hardware (dedicated), or on a dedicated host (dedicated host). if you choose to launch the instance onto a dedicated host, you can specify whether to launch the instance into a host resource group or you can target a specific dedicated host. additional charges may apply. for more information, see  and .ram disk id: a ram disk for the instance. if you have specified a kernel, you may need to specify a specific ram disk with the drivers to support it. only valid for paravirtual (pv) amis.kernel id: a kernel for the instance. only valid for paravirtual (pv) amis.user data: you can specify user data to configure an instance during launch, or to run a configuration script. for more information, see .choose create launch template. to create a launch template using the aws cli use the  command. the following example creates a launch template that specifies the following: a tag for the launch template (=)the instance type () and ami () to launchthe number of cores () and threads per core () for a total of 8 vcpus (4 cores x 2 threads)the subnet in which to launch the instance ()the template assigns a public ip address and an ipv6 address to the instance and creates a tag for the instance(=). the following is an example  file. the following is example output. to create a launch template from an existing launch template using the console open the amazon ec2 console at . in the navigation pane, choose launch templates, and then choose create launch template. for launch template name, enter a descriptive name for the launch template. for template version description, provide a brief description of the launch template version. to tag the launch template on creation, expand template tags, choose add tag, and then enter a tag key and value pair. expand source template, and for launch template namechoose a launch template on which to base the new launch template. for source template version, choose the launch template version on which to base the new launch template. adjust any launch parameters as required, and then choose create launch template. to create a launch template from an existing launch template using the console open the amazon ec2 console at . in the navigation pane, choose launch templates. choose create launch template. provide a name, description, and tags for the launch template. for source template, choose a launch template on which to base the new launch template. for source template version, choose the launch template version on which to base the new launch template. adjust any launch parameters as required, and then choose create launch template. to get launch template data from an instance using the aws cli use the  command and specify the instance id. you can use the output as a base to create a new launch template or launch template version. by default, the output includes a top-level  object, which cannot be specified in your launch template data. use the  option to exclude this object. the following is example output. you can write the output directly to a file, for example: to create a launch template from an instance using the console open the amazon ec2 console at . in the navigation pane, choose instances. select the instance, and choose actions, create template from instance. provide a name, description, and tags, and adjust the launch parameters as required. notewhen you create a launch template from an instance, the instance's network interface ids and ip addresses are not included in the template. choose create template from instance. you can create launch template versions for a specific launch template, set the default version, describe a launch template version, and delete versions that you no longer require. topics when you create a launch template version, you can specify new launch parameters or use an existing version as the base for the new version. for more information about the launch parameters, see . to create a launch template version using the console open the amazon ec2 console at . in the navigation pane, choose launch templates. select a launch template, and then choose actions, modify template (create new version). for template version description, enter a description for the launch template version. (optional) expand source template and select a version of the launch template to use as a base for the new launch template version. the new launch template version inherits the launch parameters from this launch template version. modify the launch parameters as required, and choose create launch template. to create a launch template version using the console open the amazon ec2 console at . in the navigation pane, choose launch templates. choose create launch template. for what would you like to do, choose create a new template version for launch template name, select the name of the existing launch template from the list. for template version description, enter a description for the launch template version. (optional) select a version of the launch template, or a version of a different launch template, to use as a base for the new launch template version. the new launch template version inherits the launch parameters from this launch template version. modify the launch parameters as required, and choose create launch template. to create a launch template version using the aws cli use the  command. you can specify a source version on which to base the new version. the new version inherits the launch parameters from this version, and you can override parameters using . the following example creates a new version based on version 1 of the launch template and specifies a different ami id. you can set the default version for the launch template. when you launch an instance from a launch template and do not specify a version, the instance is launched using the parameters of the default version. to set the default launch template version using the console open the amazon ec2 console at . in the navigation pane, choose launch templates. select the launch template and choose actions, set default version. for template version, select the version number to set as the default version and choose set as default version. to set the default launch template version using the console open the amazon ec2 console at . in the navigation pane, choose launch templates. select the launch template and choose actions, set default version. for default version, select the version number and choose set as default version. to set the default launch template version using the aws cli use the  command and specify the version that you want to set as the default. using the console, you can view all the versions of the selected launch template, or get a list of the launch templates whose latest or default version matches a specific version number. using the aws cli, you can describe all versions, individual versions, or a range of versions of a specified launch template. you can also describe all the latest versions or all the default versions of all the launch templates in your account. to describe a launch template version using the console open the amazon ec2 console at . in the navigation pane, choose launch templates. you can view a version of a specific launch template, or get a list of the launch templates whose latest or default version matches a specific version number. to view a version of a launch template: select the launch template. on the versions tab, from version, select a version to view its details.to get a list of all the launch templates whose latest version matches a specific version number: from the search bar, choose latest version, and then choose a version number.to get a list of all the launch templates whose default version matches a specific version number: from the search bar, choose default version, and then choose a version number.to describe a launch template version using the aws cli use the  command and specify the version numbers. in the following example, versions 1 and 3 are specified. to describe all the latest and default launch template versions in your account using the aws cli use the  command and specify , , or both. you must omit the launch template id and name in the call. you cannot specify version numbers. if you no longer require a launch template version, you can delete it. you cannot replace the version number after you delete it. you cannot delete the default version of the launch template; you must first assign a different version as the default. to delete a launch template version using the console open the amazon ec2 console at . in the navigation pane, choose launch templates. select the launch template and choose actions, delete template version. select the version to delete and choose delete. to delete a launch template version using the console open the amazon ec2 console at . in the navigation pane, choose launch templates. select the launch template and choose actions, delete template version. select the version to delete and choose delete launch template version. to delete a launch template version using the aws cli use the  command and specify the version numbers to delete. you can use the parameters contained in a launch template to launch an instance. you have the option to override or add launch parameters before you launch the instance. instances that are launched using a launch template are automatically assigned two tags with the keys  and . you cannot remove or edit these tags. to launch an instance from a launch template using the console open the amazon ec2 console at . in the navigation pane, choose launch templates. select the launch template and choose actions, launch instance from template. for source template version, select the launch template version to use. for number of instances, specify the number of instances to launch. (optional) you can override or add launch template parameters by changing and adding parameters in the instance details section. choose launch instance from template. to launch an instance from a launch template using the console open the amazon ec2 console at . in the navigation pane, choose launch templates. select the launch template and choose actions, launch instance from template. select the launch template version to use. (optional) you can override or add launch template parameters by changing and adding parameters in the instance details section. choose launch instance from template. to launch an instance from a launch template using the aws cli use the  command and specify the  parameter. optionally specify the launch template version to use. if you don't specify the version, the default version is used. to override a launch template parameter, specify the parameter in the  command. the following example overrides the instance type that's specified in the launch template (if any). if you specify a nested parameter that's part of a complex structure, the instance is launched using the complex structure as specified in the launch template plus any additional nested parameters that you specify. in the following example, the instance is launched with the tag  as well as any other tags that are specified in the launch template. if the launch template has an existing tag with a key of , the value is replaced with . in the following example, the instance is launched with a volume with the device name  as well as any other block device mappings that are specified in the launch template. if the launch template has an existing volume defined for , its values are replaced with the specified values. if the instance fails to launch or the state immediately goes to  instead of , see . you can create an auto scaling group and specify a launch template to use for the group. when amazon ec2 auto scaling launches instances in the auto scaling group, it uses the launch parameters defined in the associated launch template. for more information, see  in the amazon ec2 auto scaling user guide. before you can create an auto scaling group using a launch template, you must create a launch template that includes the parameters required to launch an instance in an auto scaling group, such as the id of the ami. the new console provides guidance to help you create a template that you can use with auto scaling. to create a launch template to use with auto scaling using the console open the amazon ec2 console at . in the navigation pane, choose launch templates, and then choose create launch template. for launch template name, enter a descriptive name for the launch template. for template version description, provide a brief description of the launch template version. under auto scaling guidance, select the checkbox to have amazon ec2 provide guidance to help create a template to use with auto scaling. modify the launch parameters as required. because you selected auto scaling guidance, some fields are required and some fields are not available. for considerations to keep in mind when creating a launch template, and for information about how to configure the launch parameters for auto scaling, see  in the amazon ec2 auto scaling user guide. choose create launch template. (optional) to create an auto scaling group using this launch template, in the next steps page, choose create auto scaling group. to create or update an amazon ec2 auto scaling group with a launch template using the aws cli use the  or the  command and specify the  parameter.you can create an ec2 fleet request and specify a launch template in the instance configuration. when amazon ec2 fulfills the ec2 fleet request, it uses the launch parameters defined in the associated launch template. you can override some of the parameters that are specified in the launch template. for more information, see . to create an ec2 fleet with a launch template using the aws cli use the  command. use the  parameter to specify the launch template and any overrides for the launch template.you can create a spot fleet request and specify a launch template in the instance configuration. when amazon ec2 fulfills the spot fleet request, it uses the launch parameters defined in the associated launch template. you can override some of the parameters that are specified in the launch template. for more information, see . to create a spot fleet request with a launch template using the aws cli use the  command. use the  parameter to specify the launch template and any overrides for the launch template.if you no longer require a launch template, you can delete it. deleting a launch template deletes all of its versions. to delete a launch template (console) open the amazon ec2 console at . in the navigation pane, choose launch templates. select the launch template and choose actions, delete template. choose delete launch template. to delete a launch template (aws cli) use the  (aws cli) command and specify the launch template. 
by default, your instance is enabled for basic monitoring. you can optionally enable detailed monitoring. after you enable detailed monitoring, the amazon ec2 console displays monitoring graphs with a 1-minute period for the instance. the following describes the data interval and charge for basic and detailed monitoring for instances. basic monitoringdata is available automatically in 5-minute periods at no charge. detailed monitoringdata is available in 1-minute periods for an additional charge.to get this level of data, you must specifically enable it for the instance. for the instances where you've enabled detailed monitoring, you can also get aggregated data across groups of similar instances. charges for detailed monitoring if you enable detailed monitoring, you are charged per metric that is sent to cloudwatch. you are not charged for data storage. for more information about pricing for detailed monitoring, see paid tier on the . for a pricing example, see example 1 - ec2 detailed monitoring on the . you can enable detailed monitoring on an instance as you launch it or after the instance is running or stopped. enabling detailed monitoring on an instance does not affect the monitoring of the ebs volumes attached to the instance. for more information, see . to enable detailed monitoring for an existing instance (console) open the amazon ec2 console at . in the navigation pane, choose instances. select the instance and choose actions, cloudwatch monitoring, enable detailed monitoring. in the enable detailed monitoring dialog box, choose yes, enable. choose close. to enable detailed monitoring when launching an instance (console)when launching an instance using the aws management console, select the monitoring check box on the configure instance details page. to enable detailed monitoring for an existing instance (aws cli)use the following  command to enable detailed monitoring for the specified instances. to enable detailed monitoring when launching an instance (aws cli)use the  command with the  flag to enable detailed monitoring. you can disable detailed monitoring on an instance as you launch it or after the instance is running or stopped. to disable detailed monitoring (console) open the amazon ec2 console at . in the navigation pane, choose instances. select the instance and choose actions, cloudwatch monitoring, disable detailed monitoring. in the disable detailed monitoring dialog box, choose yes, disable. choose close. to disable detailed monitoring (aws cli)use the following  command to disable detailed monitoring for the specified instances. 
use amazon ebs encryption as a straight-forward encryption solution for your ebs resources associated with your ec2 instances. with amazon ebs encryption, you aren't required to build, maintain, and secure your own key management infrastructure. amazon ebs encryption uses aws key management service (aws kms) customer master keys (cmk) when creating encrypted volumes and snapshots. encryption operations occur on the servers that host ec2 instances, ensuring the security of both data-at-rest and data-in-transit between an instance and its attached ebs storage. topics you can encrypt both the boot and data volumes of an ec2 instance. when you create an encrypted ebs volume and attach it to a supported instance type, the following types of data are encrypted: data at rest inside the volumeall data moving between the volume and the instanceall snapshots created from the volumeall volumes created from those snapshotsebs encrypts your volume with a data key using the industry-standard aes-256 algorithm. your data key is stored on-disk with your encrypted data, but not before ebs encrypts it with your cmk. your data key never appears on disk in plaintext. the same data key is shared by snapshots of the volume and any subsequent volumes created from those snapshots. for more information, see  in the aws key management service developer guide. amazon ebs works with aws kms to encrypt and decrypt your ebs volumes as follows: amazon ebs sends a  request to aws kms, so that it can decrypt the data key. amazon ebs sends a  request to aws kms, specifying the cmk that you chose for volume encryption. aws kms generates a new data key, encrypts it under the cmk that you chose for volume encryption, and sends the encrypted data key to amazon ebs to be stored with the volume metadata. when you attach an encrypted volume to an instance, amazon ec2 sends a  request to aws kms, specifying the encrypted data key. aws kms decrypts the encrypted data key and sends the decrypted data key to amazon ec2. amazon ec2 uses the plaintext data key in hypervisor memory to encrypt disk i/o to the volume. the plaintext data key persists in memory as long as the volume is attached to the instance. for more information, see  and  in the aws key management service developer guide. before you begin, verify that the following requirements are met. encryption is supported by all ebs volume types. you can expect the same iops performance on encrypted volumes as on unencrypted volumes, with a minimal effect on latency. you can access encrypted volumes the same way that you access unencrypted volumes. encryption and decryption are handled transparently, and they require no additional action from you or your applications. amazon ebs encryption is available on the instance types listed below. you can attach both encrypted and unencrypted volumes to these instance types simultaneously. general purpose: a1, m3, m4, m5, m5a, m5ad, m5d, m5dn, m5n, m6g,  t2, t3, and t3acompute optimized: c3, c4, c5, c5a, c5d, c5n, c6gmemory optimized: , r3, r4, r5, r5a, r5ad, r5d, r5dn, r5n, r6g,  , , , , , x1, x1e, and z1dstorage optimized: d2, , , i2, i3, and i3enaccelerated computing: f1, g2, g3, g4, inf1, p2, and p3when you configure a cmk as the default key for ebs encryption, the default key policy allows any iam user with access to the required kms actions to use this key to encrypt or decrypt ebs resources. you must grant iam users permission to call the following actions in order to use ebs encryption: to follow the principal of least privilege, do not allow full access to . instead, allow the user to create grants on the cmk only when the grant is created on the user's behalf by an aws service, as shown in the following example: for more information, see  in the default key policy section in the aws key management service developer guide. amazon ebs automatically creates a unique aws managed cmk in each region where you store aws resources. this key has the alias . by default, amazon ebs uses this key for encryption. alternatively, you can specify a symmetric customer managed cmk that you created as the default key for ebs encryption. using your own cmk gives you more flexibility, including the ability to create, rotate, and disable keys.  importantamazon ebs does not support asymmetric cmks. for more information, see  in the aws key management service developer guide. to configure the default key for ebs encryption for a region open the amazon ec2 console at . from the navigation bar, select the region. choose account attributes, settings. choose change the default key and then choose an available key. choose update. you can configure your aws account to enforce the encryption of the new ebs volumes and snapshot copies that you create. for example, amazon ebs encrypts the ebs volumes created when you launch an instance and the snapshots that you copy from an unencrypted snapshot. for examples of transitioning from unencrypted to encrypted ebs resources, see . encryption by default has no effect on existing ebs volumes or snapshots. considerations encryption by default is a region-specific setting. if you enable it for a region, you cannot disable it for individual volumes or snapshots in that region.when you enable encryption by default, you can launch an instance only if the instance type supports ebs encryption. for more information, see .when migrating servers using aws server migration service (sms), do not turn on encryption by default. if encryption by default is already on and you are experiencing delta replication failures, turn off encryption by default. instead, enable ami encryption when you create the replication job.to enable encryption by default for a region open the amazon ec2 console at . from the navigation bar, select the region. from the navigation pane, select ec2 dashboard. in the upper-right corner of the page, choose account attributes, settings. under ebs storage, select always encrypt new ebs volumes. choose update. you cannot change the cmk that is associated with an existing snapshot or encrypted volume. however, you can associate a different cmk during a snapshot copy operation so that the resulting copied snapshot is encrypted by the new cmk. you encrypt ebs volumes by enabling encryption, either using  or by enabling encryption when you create a volume that you want to encrypt. when you encrypt a volume, you can specify the symmetric cmk to use to encrypt the volume. if you do not specify a cmk, the key that is used for encryption depends on the encryption state of the source snapshot and its ownership. for more information, see the . you cannot change the cmk that is associated with an existing snapshot or volume. however, you can associate a different cmk during a snapshot copy operation so that the resulting copied snapshot is encrypted by the new cmk. when you create a new, empty ebs volume, you can encrypt it by enabling encryption for the specific volume creation operation. if you enabled ebs encryption by default, the volume is automatically encrypted. by default, the volume is encrypted to your default key for ebs encryption. alternatively, you can specify a different symmetric cmk for the specific volume creation operation. the volume is encrypted by the time it is first available, so your data is always secured. for detailed procedures, see . by default, the cmk that you selected when creating a volume encrypts the snapshots that you make from the volume and the volumes that you restore from those encrypted snapshots. you cannot remove encryption from an encrypted volume or snapshot, which means that a volume restored from an encrypted snapshot, or a copy of an encrypted snapshot, is always encrypted. public snapshots of encrypted volumes are not supported, but you can share an encrypted snapshot with specific accounts. for detailed directions, see . although there is no direct way to encrypt an existing unencrypted volume or snapshot, you can encrypt them by creating either a volume or a snapshot. if you enabled encryption by default, amazon ebs encrypts the resulting new volume or snapshot using your default key for ebs encryption. even if you have not enabled encryption by default, you can enable encryption when you create an individual volume or snapshot. whether you enable encryption by default or in individual creation operations, you can override the default key for ebs encryption and select a symmetric customer managed cmk. for more information, see  and . to encrypt the snapshot copy to a customer managed cmk, you must both enable encryption and specify the key, as shown in .  importantamazon ebs does not support asymmetric cmks. for more information, see  in the aws key management service developer guide. you can also apply new encryption states when launching an instance from an ebs-backed ami. this is because ebs-backed amis include snapshots of ebs volumes that can be encrypted as described. for more information, see . when you create an encrypted ebs resource, it is encrypted by your account's default key for ebs encryption unless you specify a different customer managed cmk in the volume creation parameters or the block device mapping for the ami or instance. for more information, see . the following examples illustrate how you can manage the encryption state of your volumes and snapshots. for a full list of encryption cases, see the . topics without encryption by default enabled, a volume restored from an unencrypted snapshot is unencrypted by default. however, you can encrypt the resulting volume by setting the  parameter and, optionally, the  parameter. the following diagram illustrates the process.  if you leave out the  parameter, the resulting volume is encrypted using your default key for ebs encryption. you must specify a key id to encrypt the volume to a different cmk. for more information, see . when you have enabled encryption by default, encryption is mandatory for volumes restored from unencrypted snapshots, and no encryption parameters are required for your default cmk to be used. the following diagram shows this simple default case:  if you want to encrypt the restored volume to a symmetric customer managed cmk, you must supply both the  and  parameters as shown in . without encryption by default enabled, a copy of an unencrypted snapshot is unencrypted by default. however, you can encrypt the resulting snapshot by setting the  parameter and, optionally, the  parameter. if you omit , the resulting snapshot is encrypted by your default cmk. you must specify a key id to encrypt the volume to a different symmetric cmk. the following diagram illustrates the process.  noteif you copy a snapshot and encrypt it to a new cmk, a complete (non-incremental) copy is always created, resulting in additional delay and storage costs. you can encrypt an ebs volume by copying an unencrypted snapshot to an encrypted snapshot and then creating a volume from the encrypted snapshot. for more information, see . when you have enabled encryption by default, encryption is mandatory for copies of unencrypted snapshots, and no encryption parameters are required if your default cmk is used. the following diagram illustrates this default case:  noteif you copy a snapshot and encrypt it to a new cmk, a complete (non-incremental) copy is always created, resulting in additional delay and storage costs. when the  action operates on an encrypted snapshot, you have the option of re-encrypting it with a different cmk. the following diagram illustrates the process. in this example, you own two cmks, cmk a and cmk b. the source snapshot is encrypted by cmk a. during volume creation, with the key id of cmk b specified as a parameter, the source data is automatically decrypted, then re-encrypted by cmk b.  noteif you copy a snapshot and encrypt it to a new cmk, a complete (non-incremental) copy is always created, resulting in additional delay and storage costs. for more information, see . the ability to encrypt a snapshot during copying allows you to apply a new symmetric cmk to an already-encrypted snapshot that you own. volumes restored from the resulting copy are only accessible using the new cmk. the following diagram illustrates the process. in this example, you own two cmks, cmk a and cmk b. the source snapshot is encrypted by cmk a. during copy, with the key id of cmk b specified as a parameter, the source data is automatically re-encrypted by cmk b.  noteif you copy a snapshot and encrypt it to a new cmk, a complete (non-incremental) copy is always created, resulting in additional delay and storage costs. in a related scenario, you can choose to apply new encryption parameters to a copy of a snapshot that has been shared with you. by default, the copy is encrypted with a cmk shared by the snapshot's owner. however, we recommend that you create a copy of the shared snapshot using a different cmk that you control. this protects your access to the volume if the original cmk is compromised, or if the owner revokes the cmk for any reason. for more information, see . when you have access to both an encrypted and unencrypted volume, you can freely transfer data between them. ec2 carries out the encryption and decryption operations transparently. for example, use the rsync command to copy the data. in the following command, the source data is located in  and the destination volume is mounted at . the following table describes the encryption outcome for each possible combination of settings.  * this is the default cmk used for ebs encryption for the aws account and region. by default this is a unique aws managed cmk for ebs, or you can specify a customer managed cmk. for more information, see . ** this is a customer managed cmk specified for the volume at launch time. this cmk is used instead of the default cmk for the aws account and region. you can manage encryption by default and the default customer master key (cmk) using the following api actions and cli commands. 
each linux instance launches with a default linux system user account. the default user name is determined by the ami that was specified when you launched the instance. for amazon linux 2 or the amazon linux ami, the user name is . for centos, the user name is . for debian, the user name is . for fedora, the user name is  or . for rhel, the user name is  or . for suse, the user name is  or . for ubuntu, the user name is . otherwise, if  and  don't work, check with your ami provider. notelinux system users should not be confused with aws identity and access management (iam) users. for more information, see  in the iam user guide. topics using the default user account is adequate for many applications. however, you may choose to add user accounts so that individuals can have their own files and workspaces. furthermore, creating user accounts for new users is much more secure than granting multiple (possibly inexperienced) users access to the default user account, because the default user account can cause a lot of damage to a system when used improperly. for more information, see . to enable users ssh access to your ec2 instance using a linux system user account, you must share the ssh key with the user. alternatively, you can use ec2 instance connect to provide access to users without the need to share and manage ssh keys. for more information, see . first create the user account, and then add the ssh public key that allows the user to connect to and log into the instance. to create a user account . you must provide the  file to the user for whom you are creating the user account. they must use this file to connect to the instance. retrieve the public key from the key pair that you created in the previous step. the command returns the public key, as shown in the following example. connect to the instance. use the adduser command to create the user account and add it to the system (with an entry in the  file). the command also creates a group and a home directory for the account. in this example, the user account is named . amazon linux and amazon linux 2 ubuntu include the  parameter to create the user account without a password. switch to the new account so that the directory and file that you create will have the proper ownership. the prompt changes from  to  to indicate that you have switched the shell session to the new account. add the ssh public key to the user account. first create a directory in the user's home directory for the ssh key file, then create the key file, and finally paste the public key into the key file, as described in the following sub-steps. create a  directory in the  home directory and change its file permissions to  (only the owner can read, write, or open the directory). importantwithout these exact file permissions, the user will not be able to log in. create a file named  in the  directory and change its file permissions to  (only the owner can read or write to the file). importantwithout these exact file permissions, the user will not be able to log in. open the  file using your favorite text editor (such as vim or nano). paste the public key that you retrieved in step 2 into the file and save the changes. importantensure that you paste the public key in one continuous line. the public key must not be split over multiple lines. the user should now be able to log into the  account on your instance, using the private key that corresponds to the public key that you added to the  file. for more information about the different methods of connecting to a linux instance, see . if a user account is no longer needed, you can remove that account so that it can no longer be used. use the userdel command to remove the user account from the system. when you specify the  parameter, the user's home directory and mail spool are deleted. to keep the user's home directory and mail spool, omit the  parameter. 
an elastic network interface is a logical networking component in a vpc that represents a virtual network card. it can include the following attributes: a primary private ipv4 address from the ipv4 address range of your vpcone or more secondary private ipv4 addresses from the ipv4 address range of your vpcone elastic ip address (ipv4) per private ipv4 addressone public ipv4 addressone or more ipv6 addressesone or more security groupsa mac addressa source/destination check flaga descriptionyou can create and configure network interfaces in your account and attach them to instances in your vpc. your account might also have requester-managed network interfaces, which are created and managed by aws services to enable you to use other resources and services. you cannot manage these network interfaces yourself. for more information, see . this aws resource is referred to as a network interface in the aws management console and the amazon ec2 api. therefore, we use "network interface" in this documentation instead of "elastic network interface". the term "network interface" in this documentation always means "elastic network interface". topics you can create a network interface, attach it to an instance, detach it from an instance, and attach it to another instance. the attributes of a network interface follow it as it's attached or detached from an instance and reattached to another instance. when you move a network interface from one instance to another, network traffic is redirected to the new instance. you can also modify the attributes of your network interface, including changing its security groups and managing its ip addresses. every instance in a vpc has a default network interface, called the primary network interface. you cannot detach a primary network interface from an instance. you can create and attach additional network interfaces. the maximum number of network interfaces that you can use varies by instance type. for more information, see . public ipv4 addresses for network interfacesin a vpc, all subnets have a modifiable attribute that determines whether network interfaces created in that subnet (and therefore instances launched into that subnet) are assigned a public ipv4 address. for more information, see  in the amazon vpc user guide. the public ipv4 address is assigned from amazon's pool of public ipv4 addresses. when you launch an instance, the ip address is assigned to the primary network interface that's created. when you create a network interface, it inherits the public ipv4 addressing attribute from the subnet. if you later modify the public ipv4 addressing attribute of the subnet, the network interface keeps the setting that was in effect when it was created. if you launch an instance and specify an existing network interface as the primary network interface, the public ipv4 address attribute is determined by this network interface. for more information, see . ipv6 addresses for network interfacesyou can associate an ipv6 cidr block with your vpc and subnet, and assign one or more ipv6 addresses from the subnet range to a network interface. all subnets have a modifiable attribute that determines whether network interfaces created in that subnet (and therefore instances launched into that subnet) are automatically assigned an ipv6 address from the range of the subnet. for more information, see  in the amazon vpc user guide. when you launch an instance, the ipv6 address is assigned to the primary network interface that's created. for more information, see . monitoring ip trafficyou can enable a vpc flow log on your network interface to capture information about the ip traffic going to and from a network interface. after you've created a flow log, you can view and retrieve its data in amazon cloudwatch logs. for more information, see  in the amazon vpc user guide. the following table lists the maximum number of network interfaces per instance type, and the maximum number of private ipv4 addresses and ipv6 addresses per network interface. the limit for ipv6 addresses is separate from the limit for private ipv4 addresses per network interface. not all instance types support ipv6 addressing. network interfaces, multiple private ipv4 addresses, and ipv6 addresses are only available for instances running in a vpc. ipv6 addresses are public and reachable over the internet. for more information, see . for more information about ipv6 in vpc, see  in the amazon vpc user guide. attaching multiple network interfaces to an instance is useful when you want to: create a management network.use network and security appliances in your vpc.create dual-homed instances with workloads/roles on distinct subnets.create a low-budget, high-availability solution.you can create a management network using network interfaces. in this scenario, the primary network interface (eth0) on the instance handles public traffic and the secondary network interface (eth1) handles backend management traffic and is connected to a separate subnet in your vpc that has more restrictive access controls. the public interface, which may or may not be behind a load balancer, has an associated security group that allows access to the server from the internet (for example, allow tcp port 80 and 443 from , or from the load balancer) while the private facing interface has an associated security group allowing ssh access only from an allowed range of ip addresses either within the vpc or from the internet, a private subnet within the vpc or a virtual private gateway. to ensure failover capabilities, consider using a secondary private ipv4 for incoming traffic on a network interface. in the event of an instance failure, you can move the interface and/or secondary private ipv4 address to a standby instance.  some network and security appliances, such as load balancers, network address translation (nat) servers, and proxy servers prefer to be configured with multiple network interfaces. you can create and attach secondary network interfaces to instances in a vpc that are running these types of applications and configure the additional interfaces with their own public and private ip addresses, security groups, and source/destination checking. you can place a network interface on each of your web servers that connects to a mid-tier network where an application server resides. the application server can also be dual-homed to a backend network (subnet) where the database server resides. instead of routing network packets through the dual-homed instances, each dual-homed instance receives and processes requests on the front end, initiates a connection to the backend, and then sends requests to the servers on the backend network. if one of your instances serving a particular function fails, its network interface can be attached to a replacement or hot standby instance pre-configured for the same role in order to rapidly recover the service. for example, you can use a network interface as your primary or secondary network interface to a critical service such as a database instance or a nat instance. if the instance fails, you (or more likely, the code running on your behalf) can attach the network interface to a hot standby instance. because the interface maintains its private ip addresses, elastic ip addresses, and mac address, network traffic begins flowing to the standby instance as soon as you attach the network interface to the replacement instance. users experience a brief loss of connectivity between the time the instance fails and the time that the network interface is attached to the standby instance, but no changes to the vpc route table or your dns server are required. you can attach a network interface to an instance when it's running (hot attach), when it's stopped (warm attach), or when the instance is being launched (cold attach).you can detach secondary network interfaces when the instance is running or stopped. however, you can't detach the primary network interface.you can move a network interface from one instance to another, if the instances are in the same availability zone and vpc but in different subnets.when launching an instance using the cli, api, or an sdk, you can specify the primary network interface and additional network interfaces.launching an amazon linux or windows server instance with multiple network interfaces automatically configures interfaces, private ipv4 addresses, and route tables on the operating system of the instance.a warm or hot attach of an additional network interface may require you to manually bring up the second interface, configure the private ipv4 address, and modify the route table accordingly. instances running amazon linux or windows server automatically recognize the warm or hot attach and configure themselves.attaching another network interface to an instance (for example, a nic teaming configuration) cannot be used as a method to increase or double the network bandwidth to or from the dual-homed instance.if you attach two or more network interfaces from the same subnet to an instance, you may encounter networking issues such as asymmetric routing. if possible, use a secondary private ipv4 address on the primary network interface instead. for more information, see . amazon linux amis may contain additional scripts installed by aws, known as ec2-net-utils. these scripts optionally automate the configuration of your network interfaces. these scripts are available for amazon linux only. use the following command to install the package on amazon linux if it's not already installed, or update it if it's installed and additional updates are available: the following components are part of ec2-net-utils: udev rules ()identifies network interfaces when they are attached, detached, or reattached to a running instance, and ensures that the hotplug script runs (). maps the mac address to a device name (, which generates ). hotplug scriptgenerates an interface configuration file suitable for use with dhcp (n). also generates a route configuration file (n). dhcp scriptwhenever the network interface receives a new dhcp lease, this script queries the instance metadata for elastic ip addresses. for each elastic ip address, it adds a rule to the routing policy database to ensure that outbound traffic from that address uses the correct network interface. it also adds each private ip address to the network interface as a secondary address. ec2ifup ethnextends the functionality of the standard ifup. after this script rewrites the configuration files n and n, it runs ifup. ec2ifdown ethnextends the functionality of the standard ifdown. after this script removes any rules for the network interface from the routing policy database, it runs ifdown. ec2ifscanchecks for network interfaces that have not been configured and configures them.this script isn't available in the initial release of ec2-net-utils. to list any configuration files that were generated by ec2-net-utils, use the following command: to disable the automation on a per-instance basis, you can add  to the corresponding n file. for example, use the following command to disable the automation for the eth1 interface: to disable the automation completely, you can remove the package using the following command: you can work with network interfaces using the amazon ec2 console or the command line. topics you can create a network interface in a subnet. you can't move the network interface to another subnet after it's created, and you can only attach the network interface to instances in the same availability zone. to create a network interface using the console open the amazon ec2 console at . in the navigation pane, choose network interfaces. choose create network interface. for description, enter a descriptive name. for subnet, select the subnet. for private ip (or ipv4 private ip), enter the primary private ipv4 address. if you don't specify an ipv4 address, we select an available private ipv4 address from within the selected subnet. (ipv6 only) if you selected a subnet that has an associated ipv6 cidr block, you can optionally specify an ipv6 address in the ipv6 ip field.  to create an elastic fabric adapter, select elastic fabric adapter. for security groups, select one or more security groups. (optional) choose add tag and enter a tag key and a tag value. choose yes, create. to create a network interface using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)to delete an instance, you must first detach the network interface. deleting a network interface releases all attributes associated with the interface and releases any private ip addresses or elastic ip addresses to be used by another instance. to delete a network interface using the console open the amazon ec2 console at . in the navigation pane, choose network interfaces. select a network interface and choose delete. in the delete network interface dialog box, choose yes, delete. to delete a network interface using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)you can view all the network interfaces in your account. to describe a network interface using the console open the amazon ec2 console at . in the navigation pane, choose network interfaces. select the network interface. to view the details, choose details. to describe a network interface using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)to describe a network interface attribute using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)you can specify an existing network interface or attach an additional network interface when you launch an instance. noteif an error occurs when attaching a network interface to your instance, this causes the instance launch to fail. to attach a network interface when launching an instance using the console open the amazon ec2 console at . choose launch instance. select an ami and instance type and choose next: configure instance details. on the configure instance details page, select a vpc for network, and a subnet for subnet. in the network interfaces section, the console enables you to specify up to two network interfaces (new, existing, or a combination) when you launch an instance. you can also enter a primary ipv4 address and one or more secondary ipv4 addresses for any new interface.  you can add additional network interfaces to the instance after you launch it. the total number of network interfaces that you can attach varies by instance type. for more information, see . noteif you specify more than one network interface, you cannot auto-assign a public ipv4 address to your instance.  (ipv6 only) if you're launching an instance into a subnet that has an associated ipv6 cidr block, you can specify ipv6 addresses for any network interfaces that you attach. under ipv6 ips, choose add ip. to add a secondary ipv6 address, choose add ip again. you can enter an ipv6 address from the range of the subnet, or leave the default auto-assign value to let amazon choose an ipv6 address from the subnet for you. choose next: add storage. on the add storage page, you can specify volumes to attach to the instance besides the volumes specified by the ami (such as the root device volume), and then choose next: add tags. on the add tags page, specify tags for the instance, such as a user-friendly name, and then choose next: configure security group. on the configure security group page, you can select a security group or create a new one. choose review and launch.  noteif you specified an existing network interface in step 5, the instance is associated with the security group for that network interface, regardless of any option that you select in this step. on the review instance launch page, details about the primary and additional network interface are displayed. review the settings, and then choose launch to choose a key pair and launch your instance. if you're new to amazon ec2 and haven't created any key pairs, the wizard prompts you to create one. to attach a network interface when launching an instance using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)you can attach a network interface to any of your stopped or running instances in your vpc, using either the instances or network interfaces pages of the amazon ec2 console.  noteif the public ipv4 address on your instance is released, it does not receive a new one if there is more than one network interface attached to the instance. for more information about the behavior of public ipv4 addresses, see . to attach a network interface to an instance using the instances page open the amazon ec2 console at . in the navigation pane, choose instances. choose actions, networking, attach network interface. in the attach network interface dialog box, select the network interface and choose attach. to attach a network interface to an instance using the network interfaces page open the amazon ec2 console at . in the navigation pane, choose network interfaces. select the network interface and choose attach. in the attach network interface dialog box, select the instance and choose attach. to attach a network interface to an instance using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)you can detach a secondary network interface that is attached to an ec2 instance at any time, using either the instances or network interfaces page of the amazon ec2 console. to detach a network interface from an instance using the instances page open the amazon ec2 console at . in the navigation pane, choose instances. choose actions, networking, detach network interface. in the detach network interface dialog box, select the network interface and choose detach. you can't use the amazon ec2 console to detach a network interface that is attached to a resource from another service, such as an elastic load balancing load balancer, a lambda function, a workspace, or a nat gateway. the network interfaces for those resources will be deleted when the resource is deleted. to detach a network interface from an instance using the network interfaces page open the amazon ec2 console at . in the navigation pane, choose network interfaces. select the network interface and check the description to verify that the network interface is attached to an instance, not another type of resource. if the resource is an ec2 instance, choose detach. if the network interface is the primary network interface for the instance, the detach button is disabled. when prompted for confirmation, choose yes, detach. if the network interface fails to detach from the instance, choose force detachment and then try again. we recommend that you choose this option only as a last resort. forcing a detachment can prevent you from attaching a different network interface on the same index until you restart the instance. it can also prevent the instance metadata from reflecting that the network interface was detached until you restart the instance. to detach a network interface using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)you can change the security groups that are associated with a network interface. when you create the security group, be sure to specify the same vpc as the subnet for the network interface. noteto change security group membership for interfaces owned by other services, such as elastic load balancing, use the console or command line interface for that service. to change the security group of a network interface using the console open the amazon ec2 console at . in the navigation pane, choose network interfaces. select the network interface and choose actions, change security groups. in the change security groups dialog box, select the security groups to use, and choose save. to change the security group of a network interface using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)the source/destination check attribute controls whether source/destination checking is enabled on the instance. disabling this attribute enables an instance to handle network traffic that isn't specifically destined for the instance. for example, instances running services such as network address translation, routing, or a firewall should set this value to . the default value is . to change source/destination checking for a network interface using the console open the amazon ec2 console at . in the navigation pane, choose network interfaces. select the network interface and choose actions, change source/dest check. in the dialog box, choose enabled (if enabling) or disabled (if disabling), and save. to change source/destination checking for a network interface using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)if you have an elastic ip address (ipv4), you can associate it with one of the private ipv4 addresses for the network interface. you can associate one elastic ip address with each private ipv4 address. you can associate an elastic ip address using the amazon ec2 console or the command line. to associate an elastic ip address using the console open the amazon ec2 console at . in the navigation pane, choose network interfaces. select the network interface and choose actions, associate address. in the associate elastic ip address dialog box, select the elastic ip address from the address list. for associate to private ip address, select the private ipv4 address to associate with the elastic ip address.  choose allow reassociation to allow the elastic ip address to be associated with the specified network interface if it's currently associated with another instance or network interface, and then choose associate address. to associate an elastic ip address using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)if the network interface has an elastic ip address (ipv4) associated with it, you can disassociate the address, and then either associate it with another network interface or release it back to the address pool. this is the only way to associate an elastic ip address with an instance in a different subnet or vpc using a network interface, as network interfaces are specific to a particular subnet. you can disassociate an elastic ip address using the amazon ec2 console or the command line. to disassociate an elastic ip address using the console open the amazon ec2 console at . in the navigation pane, choose network interfaces. select the network interface and choose actions, disassociate address. in the disassociate ip address dialog box, choose yes, disassociate. to disassociate an elastic ip address using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)you can assign one or more ipv6 addresses to a network interface. the network interface must be in a subnet that has an associated ipv6 cidr block. to assign a specific ipv6 address to the network interface, ensure that the ipv6 address is not already assigned to another network interface. open the amazon ec2 console at . in the navigation pane, choose network interfaces and select the network interface. choose actions, manage ip addresses. under ipv6 addresses, choose assign new ip. specify an ipv6 address from the range of the subnet. to let aws choose an address for you, leave the auto-assign value. choose yes, update. to assign an ipv6 address to a network interface using the command line you can use one of the following commands. for more information about these command line interfaces, see . (aws cli) (aws tools for windows powershell)you can unassign an ipv6 address from a network interface using the amazon ec2 console. open the amazon ec2 console at . in the navigation pane, choose network interfaces and select the network interface. choose actions, manage ip addresses. under ipv6 addresses, choose unassign for the ipv6 address to remove. choose yes, update. to unassign an ipv6 address from a network interface using the command line you can use one of the following commands. for more information about these command line interfaces, see . (aws cli) (aws tools for windows powershell)you can set the termination behavior for a network interface that's attached to an instance. you can specify whether the network interface should be automatically deleted when you terminate the instance to which it's attached. you can change the terminating behavior for a network interface using the amazon ec2 console or the command line. to change the termination behavior for a network interface using the console open the amazon ec2 console at . in the navigation pane, choose network interfaces. select the network interface and choose actions, change termination behavior. in the change termination behavior dialog box, select the delete on termination check box if you want the network interface to be deleted when you terminate an instance. to change the termination behavior for a network interface using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)you can change the description for a network interface using the amazon ec2 console or the command line. to change the description for a network interface using the console open the amazon ec2 console at . in the navigation pane, choose network interfaces. select the network interface and choose actions, change description. in the change description dialog box, enter a description for the network interface, and then choose save. to change the description for a network interface using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)tags are metadata that you can add to a network interface. tags are private and are only visible to your account. each tag consists of a key and an optional value. for more information about tags, see . to add or edit tags for a network interface using the console open the amazon ec2 console at . in the navigation pane, choose network interfaces. select the network interface. in the details pane, choose tags, add/edit tags. in the add/edit tags dialog box, choose create tag for each tag to create, and enter a key and optional value. when you're done, choose save. to add or edit tags for a network interface using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)
you are not billed for any instance usage while an instance is not in the  state. in other words, when you terminate an instance, you stop incurring charges for that instance as soon as its state changes to . if your instance remains in the  state longer than a few minutes, it might be delayed due to shutdown scripts being run by the instance. another possible cause is a problem with the underlying host computer. if your instance remains in the  state for several hours, amazon ec2 treats it as a stuck instance and forcibly terminates it. if it appears that your instance is stuck terminating and it has been longer than several hours, post a request for help to the . to help expedite a resolution, include the instance id and describe the steps that you've already taken. alternatively, if you have a support plan, create a technical support case in the . after you terminate an instance, it remains visible for a short while before being deleted. the state shows as . if the entry is not deleted after several hours, contact support. if you terminate all your instances, you may see that we launch a new instance for you. if you launch an instance, you may see that we terminate one of your instances. if you stop an instance, you may see that we terminate the instance and launch a new instance. generally, these behaviors mean that you've used amazon ec2 auto scaling or elastic beanstalk to scale your computing resources automatically based on criteria that you've defined. for more information, see the  or the . 
on a given volume configuration, certain i/o characteristics drive the performance behavior for your ebs volumes. ssd-backed volumes—general purpose ssd () and provisioned iops ssd ()—deliver consistent performance whether an i/o operation is random or sequential. hdd-backed volumes—throughput optimized hdd () and cold hdd ()—deliver optimal performance only when i/o operations are large and sequential. to understand how ssd and hdd volumes will perform in your application, it is important to know the connection between demand on the volume, the quantity of iops available to it, the time it takes for an i/o operation to complete, and the volume's throughput limits. iops are a unit of measure representing input/output operations per second. the operations are measured in kib, and the underlying drive technology determines the maximum amount of data that a volume type counts as a single i/o. i/o size is capped at 256 kib for ssd volumes and 1,024 kib for hdd volumes because ssd volumes handle small or random i/o much more efficiently than hdd volumes.  when small i/o operations are physically contiguous, amazon ebs attempts to merge them into a single i/o operation up to the maximum size. for example, for ssd volumes, a single 1,024 kib i/o operation counts as 4 operations (1,024÷256=4), while 8 contiguous i/o operations at 32 kib each count as 1 operation (8×32=256). however, 8 random i/o operations at 32 kib each count as 8 operations. each i/o operation under 32 kib counts as 1 operation. similarly, for hdd-backed volumes, both a single 1,024 kib i/o operation and 8 sequential 128 kib operations would count as one operation. however, 8 random 128 kib i/o operations would count as 8 operations. consequently, when you create an ssd-backed volume supporting 3,000 iops (either by provisioning an  volume at 3,000 iops or by sizing a  volume at 1000 gib), and you attach it to an ebs-optimized instance that can provide sufficient bandwidth, you can transfer up to 3,000 i/os of data per second, with throughput determined by i/o size.  the volume queue length is the number of pending i/o requests for a device. latency is the true end-to-end client time of an i/o operation, in other words, the time elapsed between sending an i/o to ebs and receiving an acknowledgement from ebs that the i/o read or write is complete. queue length must be correctly calibrated with i/o size and latency to avoid creating bottlenecks either on the guest operating system or on the network link to ebs. optimal queue length varies for each workload, depending on your particular application's sensitivity to iops and latency. if your workload is not delivering enough i/o requests to fully use the performance available to your ebs volume, then your volume might not deliver the iops or throughput that you have provisioned.  transaction-intensive applications are sensitive to increased i/o latency and are well-suited for ssd-backed  and  volumes. you can maintain high iops while keeping latency down by maintaining a low queue length and a high number of iops available to the volume. consistently driving more iops to a volume than it has available can cause increased i/o latency.  throughput-intensive applications are less sensitive to increased i/o latency, and are well-suited for hdd-backed  and  volumes. you can maintain high throughput to hdd-backed volumes by maintaining a high queue length when performing large, sequential i/o. for ssd-backed volumes, if your i/o size is very large, you may experience a smaller number of iops than you provisioned because you are hitting the throughput limit of the volume. for example, a  volume under 1000 gib with burst credits available has an iops limit of 3,000 and a volume throughput limit of 250 mib/s. if you are using a 256 kib i/o size, your volume reaches its throughput limit at 1000 iops (1000 x 256 kib = 250 mib). for smaller i/o sizes (such as 16 kib), this same volume can sustain 3,000 iops because the throughput is well below 250 mib/s. (these examples assume that your volume's i/o is not hitting the throughput limits of the instance.) for more information about the throughput limits for each ebs volume type, see .  for smaller i/o operations, you may see a higher-than-provisioned iops value as measured from inside your instance. this happens when the instance operating system merges small i/o operations into a larger operation before passing them to amazon ebs. if your workload uses sequential i/os on hdd-backed  and  volumes, you may experience a higher than expected number of iops as measured from inside your instance. this happens when the instance operating system merges sequential i/os and counts them in 1,024 kib-sized units. if your workload uses small or random i/os, you may experience a lower throughput than you expect. this is because we count each random, non-sequential i/o toward the total iops count, which can cause you to hit the volume's iops limit sooner than expected. whatever your ebs volume type, if you are not experiencing the iops or throughput you expect in your configuration, ensure that your ec2 instance bandwidth is not the limiting factor. you should always use a current-generation, ebs-optimized instance (or one that includes 10 gb/s network connectivity) for optimal performance. for more information, see . another possible cause for not experiencing the expected iops is that you are not driving enough i/o to the ebs volumes. you can monitor these i/o characteristics with each volume's . important metrics to consider include the following:  displays the burst bucket balance for , , and  volumes as a percentage of the remaining balance. when your burst bucket is depleted, volume i/o (for  volumes) or volume throughput (for  and  volumes) is throttled to the baseline. check the  value to determine whether your volume is being throttled for this reason.  hdd-backed  and  volumes are designed to perform best with workloads that take advantage of the 1,024 kib maximum i/o size. to determine your volume's average i/o size, divide by . the same calculation applies to read operations. if average i/o size is below 64 kib, increasing the size of the i/o operations sent to an  or  volume should improve performance.  noteif average i/o size is at or near 44 kib, you might be using an instance or kernel without support for indirect descriptors. any linux kernel 3.8 and above has this support, as well as any current-generation instance. if your i/o latency is higher than you require, check  to make sure your application is not trying to drive more iops than you have provisioned. if your application requires a greater number of iops than your volume can provide, you should consider using a larger  volume with a higher base performance level or an  volume with more provisioned iops to achieve faster latencies. for more information about amazon ebs i/o characteristics, see the following re:invent presentation: . 
when you modify an ebs volume, it goes through a sequence of states. the volume enters the  state, the  state, and finally the  state. at this point, the volume is ready to be further modified.  noterarely, a transient aws fault can result in a  state. this is not an indication of volume health; it merely indicates that the modification to the volume failed. if this occurs, retry the volume modification. while the volume is in the  state, your volume performance is in between the source and target configuration specifications. transitional volume performance will be no less than the source volume performance. if you are downgrading iops, transitional volume performance is no less than the target volume performance. volume modification changes take effect as follows: size changes usually take a few seconds to complete and take effect after a volume is in the  state.performance (iops) changes can take from a few minutes to a few hours to complete and are dependent on the configuration change being made.it may take up to 24 hours for a new configuration to take effect, and in some cases more, such as when the volume has not been fully initialized. typically, a fully used 1-tib volume takes about 6 hours to migrate to a new performance configuration. use one of the following methods to monitor the progress of a volume modification. topics use the following procedure to view the progress of one or more volume modifications. to monitor progress of a modification using the console open the amazon ec2 console at . in the navigation pane, choose volumes. select the volume. the volume modification state is displayed in the state column and in the state field in the details pane. in the following screenshot, the selected volume's modification state is optimizing. the next volume in the list has a state of modifying. notethe state column and field also show the volume's availability status, which can be creating, available, in-use, deleting, deleted, or error. choose the text in the state field to display before and after information about the most recent modification action, as shown in this screenshot. use the  command to view the progress of one or more volume modifications. the following example describes the volume modifications for two volumes. in the following example output, the volume modifications are still in the  state. progress is reported as a percentage. the next example describes all volumes with a modification state of either  or , and then filters and formats the results to show only modifications that were initiated on or after february 1, 2017: the following is example output with information about two volumes: with cloudwatch events, you can create a notification rule for volume modification events. you can use your rule to generate a notification message using  or to invoke a  in response to matching events. to monitor progress of a modification using cloudwatch events open the cloudwatch console at . choose events, create rule. for build event pattern to match events by service, choose custom event pattern. for build custom event pattern, replace the contents with the following and choose save. the following is example event data: 
each instance that you launch has an associated root device volume, which is either an amazon ebs volume or an instance store volume. you can use block device mapping to specify additional ebs volumes or instance store volumes to attach to an instance when it's launched. you can also attach additional ebs volumes to a running instance; see . however, the only way to attach instance store volumes to an instance is to use block device mapping to attach the volumes as the instance is launched. for more information about root device volumes, see . topics a block device is a storage device that moves data in sequences of bytes or bits (blocks). these devices support random access and generally use buffered i/o. examples include hard disks, cd-rom drives, and flash drives. a block device can be physically attached to a computer or accessed remotely as if it were physically attached to the computer. amazon ec2 supports two types of block devices:  instance store volumes (virtual devices whose underlying hardware is physically attached to the host computer for the instance)ebs volumes (remote storage devices)a block device mapping defines the block devices (instance store volumes and ebs volumes) to attach to an instance. you can specify a block device mapping as part of creating an ami so that the mapping is used by all instances launched from the ami. alternatively, you can specify a block device mapping when you launch an instance, so this mapping overrides the one specified in the ami from which you launched the instance. note that all nvme instance store volumes supported by an instance type are automatically enumerated and assigned a device name on instance launch; including them in your block device mapping has no effect. topics when you create a block device mapping, you specify the following information for each block device that you need to attach to the instance: the device name used within amazon ec2. the block device driver for the instance assigns the actual volume name when mounting the volume. the name assigned can be different from the name that amazon ec2 recommends. for more information, see .[instance store volumes] the virtual device: . note that the number and size of available instance store volumes for your instance varies by instance type.[nvme instance store volumes] these volumes are automatically enumerated and assigned a device name; including them in your block device mapping has no effect.[ebs volumes] the id of the snapshot to use to create the block device (snap-xxxxxxxx). this value is optional as long as you specify a volume size.[ebs volumes] the size of the volume, in gib. the specified size must be greater than or equal to the size of the specified snapshot.[ebs volumes] whether to delete the volume on instance termination ( or ). the default value is  for the root device volume and  for attached volumes. when you create an ami, its block device mapping inherits this setting from the instance. when you launch an instance, it inherits this setting from the ami.[ebs volumes] the volume type, which can be  for general purpose ssd,  for provisioned iops ssd,  for throughput optimized hdd,  for cold hdd, or  for magnetic. the default value is .[ebs volumes] the number of input/output operations per second (iops) that the volume supports. (not used with , , , or  volumes.)there are several caveats to consider when launching instances with amis that have instance store volumes in their block device mappings. some instance types include more instance store volumes than others, and some instance types contain no instance store volumes at all. if your instance type supports one instance store volume, and your ami has mappings for two instance store volumes, then the instance launches with one instance store volume.instance store volumes can only be mapped at launch time. you cannot stop an instance without instance store volumes (such as the ), change the instance to a type that supports instance store volumes, and then restart the instance with instance store volumes. however, you can create an ami from the instance and launch it on an instance type that supports instance store volumes, and map those instance store volumes to the instance.if you launch an instance with instance store volumes mapped, and then stop the instance and change it to an instance type with fewer instance store volumes and restart it, the instance store volume mappings from the initial launch still show up in the instance metadata. however, only the maximum number of supported instance store volumes for that instance type are available to the instance. notewhen an instance is stopped, all data on the instance store volumes is lost.depending on instance store capacity at launch time, m3 instances may ignore ami instance store block device mappings at launch unless they are specified at launch. you should specify instance store block device mappings at launch time, even if the ami you are launching has the instance store volumes mapped in the ami, to ensure that the instance store volumes are available when the instance launches. this figure shows an example block device mapping for an ebs-backed instance. it maps  to  and maps two ebs volumes, one to  and the other to . it also shows the ebs volume that is the root device volume, .  note that this example block device mapping is used in the example commands and apis in this topic. you can find example commands and apis that create block device mappings in  and . device names like  and  are used by amazon ec2 to describe block devices. the block device mapping is used by amazon ec2 to specify the block devices to attach to an ec2 instance. after a block device is attached to an instance, it must be mounted by the operating system before you can access the storage device. when a block device is detached from an instance, it is unmounted by the operating system and you can no longer access the storage device. with a linux instance, the device names specified in the block device mapping are mapped to their corresponding block devices when the instance first boots. the instance type determines which instance store volumes are formatted and mounted by default. you can mount additional instance store volumes at launch, as long as you don't exceed the number of instance store volumes available for your instance type. for more information, see . the block device driver for the instance determines which devices are used when the volumes are formatted and mounted. for more information, see . each ami has a block device mapping that specifies the block devices to attach to an instance when it is launched from the ami. an ami that amazon provides includes a root device only. to add more block devices to an ami, you must create your own ami. topics there are two ways to specify volumes in addition to the root volume when you create an ami. if you've already attached volumes to a running instance before you create an ami from the instance, the block device mapping for the ami includes those same volumes. for ebs volumes, the existing data is saved to a new snapshot, and it's this new snapshot that's specified in the block device mapping. for instance store volumes, the data is not preserved. for an ebs-backed ami, you can add ebs volumes and instance store volumes using a block device mapping. for an instance store-backed ami, you can add instance store volumes only by modifying the block device mapping entries in the image manifest file when registering the image. notefor m3 instances, you must specify instance store volumes in the block device mapping for the instance when you launch it. when you launch an m3 instance, instance store volumes specified in the block device mapping for the ami may be ignored if they are not specified as part of the instance block device mapping. to add volumes to an ami using the console open the amazon ec2 console. in the navigation pane, choose instances. select an instance and choose actions, image, create image. in the create image dialog box, choose add new volume. select a volume type from the type list and a device name from the device list. for an ebs volume, you can optionally specify a snapshot, volume size, and volume type. choose create image. to add volumes to an ami using the command line use the  aws cli command to specify a block device mapping for an ebs-backed ami. use the  aws cli command to specify a block device mapping for an instance store-backed ami. specify the block device mapping using the  parameter. arguments encoded in json can be supplied either directly on the command line or by reference to a file: to add an instance store volume, use the following mapping. to add an empty 100 gib  volume, use the following mapping. to add an ebs volume based on a snapshot, use the following mapping. to omit a mapping for a device, use the following mapping. alternatively, you can use the  parameter with the following commands (aws tools for windows powershell): you can easily enumerate the ebs volumes in the block device mapping for an ami. to view the ebs volumes for an ami using the console open the amazon ec2 console. in the navigation pane, choose amis. choose ebs images from the filter list to get a list of ebs-backed amis.  select the desired ami, and look at the details tab. at a minimum, the following information is available for the root device: root device type ()root device name (for example, )block devices (for example, )if the ami was created with additional ebs volumes using a block device mapping, the block devices field displays the mapping for those additional volumes as well. (this screen doesn't display instance store volumes.) to view the ebs volumes for an ami using the command line use the  (aws cli) command or  (aws tools for windows powershell) command to enumerate the ebs volumes in the block device mapping for an ami. by default, an instance that you launch includes any storage devices specified in the block device mapping of the ami from which you launched the instance. you can specify changes to the block device mapping for an instance when you launch it, and these updates overwrite or merge with the block device mapping of the ami. limitations for the root volume, you can only modify the following: volume size, volume type, and the delete on termination flag.when you modify an ebs volume, you can't decrease its size. therefore, you must specify a snapshot whose size is equal to or greater than the size of the snapshot specified in the block device mapping of the ami.topics you can add ebs volumes and instance store volumes to an instance when you launch it. note that updating the block device mapping for an instance doesn't make a permanent change to the block device mapping of the ami from which it was launched. to add volumes to an instance using the console open the amazon ec2 console. from the dashboard, choose launch instance. on the choose an amazon machine image (ami) page, select the ami to use and choose select. follow the wizard to complete the choose an instance type and configure instance details pages. on the add storage page, you can modify the root volume, ebs volumes, and instance store volumes as follows: to change the size of the root volume, locate the root volume under the type column, and change its size field.to suppress an ebs volume specified by the block device mapping of the ami used to launch the instance, locate the volume and click its delete icon.to add an ebs volume, choose add new volume, choose ebs from the type list, and fill in the fields (device, snapshot, and so on).to suppress an instance store volume specified by the block device mapping of the ami used to launch the instance, locate the volume, and choose its delete icon.to add an instance store volume, choose add new volume, select instance store from the type list, and select a device name from device.complete the remaining wizard pages, and choose launch. to add volumes to an instance using the aws cliuse the  aws cli command with the  option to specify a block device mapping for an instance at launch. for example, suppose that an ebs-backed ami specifies the following block device mapping: /dev/sdb=ephemeral0/dev/sdh=snap-1234567890abcdef0/dev/sdj=:100to prevent  from attaching to an instance launched from this ami, use the following mapping. to increase the size of  to 300 gib, specify the following mapping. notice that you don't need to specify the snapshot id for , because specifying the device name is enough to identify the volume. to increase the size of the root volume at instance launch, first call  with the id of the ami to verify the device name of the root volume. for example, . to override the size of the root volume, specify the device name of the root device used by the ami and the new volume size. to attach an additional instance store volume, , specify the following mapping. if the instance type doesn't support multiple instance store volumes, this mapping has no effect. if the instance supports nvme instance store volumes, they are automatically enumerated and assigned an nvme device name. to add volumes to an instance using the aws tools for windows powershelluse the  parameter with the  command (aws tools for windows powershell). you can use the  aws cli command to update the block device mapping of a running instance. you do not need to stop the instance before changing this attribute. for example, to preserve the root volume at instance termination, specify the following in . alternatively, you can use the  parameter with the  command (aws tools for windows powershell). you can easily enumerate the ebs volumes mapped to an instance. notefor instances launched before the release of the 2009-10-31 api, aws can't display the block device mapping. you must detach and reattach the volumes so that aws can display the block device mapping. to view the ebs volumes for an instance using the console open the amazon ec2 console. in the navigation pane, choose instances. in the search box, enter root device type, and then choose ebs. this displays a list of ebs-backed instances. select the desired instance and look at the details displayed in the description tab. at a minimum, the following information is available for the root device: root device type ()root device (for example, )block devices (for example, , , and )if the instance was launched with additional ebs volumes using a block device mapping, the block devices field displays those additional volumes as well as the root device. (this screen doesn't display instance store volumes.) to display additional information about a block device, choose its entry next to block devices. this displays the following information for the block device: ebs id (vol-xxxxxxxx)root device type ()attachment time (yyyy-mmthh:mm:ss.s**stzd)block device status (, , , )delete on termination (, )to view the ebs volumes for an instance using the command line use the  (aws cli) command or  (aws tools for windows powershell) command to enumerate the ebs volumes in the block device mapping for an instance. when you view the block device mapping for your instance, you can see only the ebs volumes, not the instance store volumes. you can use instance metadata to query the non-nvme instance store volumes in the block device mapping. nvme instance store volumes are not included. the base uri for all requests for instance metadata is . for more information, see . first, connect to your running instance. from the instance, use this query to get its block device mapping. the response includes the names of the block devices for the instance. for example, the output for an instance store–backed  instance looks like this. the  device is the root device as seen by the instance. the instance store volumes are named . the  device is for the page file. if you've also mapped ebs volumes, they appear as , , and so on. to get details about an individual block device in the block device mapping, append its name to the previous query, as shown here. the instance type determines the number of instance store volumes that are available to the instance. if the number of instance store volumes in a block device mapping exceeds the number of instance store volumes available to an instance, the additional volumes are ignored. to view the instance store volumes for your instance, run the lsblk command. to learn how many instance store volumes are supported by each instance type, see . 
 amazon ec2 enables you to share your amis with other aws accounts. you can allow all aws accounts to launch the ami (make the ami public), or only allow a few specific accounts to launch the ami (see ). you are not billed when your ami is launched by other aws accounts; only the accounts launching the ami are billed. amis with encrypted volumes cannot be made public. amis are a regional resource. therefore, sharing an ami makes it available in that region. to make an ami available in a different region, copy the ami to the region and then share it. for more information, see . to avoid exposing sensitive data when you share an ami, read the security considerations in  and follow the recommended actions.  if an ami has a product code, or contains a snapshot of an encrypted volume, you can't make it public. you can share the ami only with specific aws accounts. after you make an ami public, it is available in community amis when you launch an instance in the same region using the console. note that it can take a short while for an ami to appear in community amis after you make it public. it can also take a short while for an ami to be removed from community amis after you make it private again. to share a public ami using the console open the amazon ec2 console at . in the navigation pane, choose amis. select your ami from the list, and then choose actions, modify image permissions. choose public and choose save. each ami has a  property that controls which aws accounts, besides the owner's, are allowed to use that ami to launch instances. by modifying the  property of an ami, you can make the ami public (which grants launch permissions to all aws accounts) or share it with only the aws accounts that you specify. you can add or remove account ids from the list of accounts that have launch permissions for an ami. to make the ami public, specify the  group. you can specify both public and explicit launch permissions. to make an ami public use the  command as follows to add the  group to the  list for the specified ami. to verify the launch permissions of the ami, use the  command. (optional) to make the ami private again, remove the  group from its launch permissions. note that the owner of the ami always has launch permissions and is therefore unaffected by this command. 
the following table lists the categories of instance metadata. importantsome of the category names in the following table are placeholders for data that is unique to your instance. for example, mac represents the mac address for the network interface. you must replace the placeholders with the actual values. the following table lists the categories of dynamic data. 
amazon elastic inference (ei) is a resource you can attach to your amazon ec2 cpu instances to accelerate your deep learning (dl) inference workloads. amazon ei accelerators come in multiple sizes and are a cost-effective method to build intelligent capabilities into applications running on amazon ec2 instances.  amazon ei distributes model operations defined by tensorflow, apache mxnet, and the open neural network exchange (onnx) format through mxnet between low-cost, dl inference accelerators and the cpu of the instance.  for more information about amazon elastic inference, see the . 
with elastic volumes, you can dynamically modify the size, performance, and volume type of your amazon ebs volumes without detaching them. use the following process when modifying a volume: (optional) before modifying a volume that contains valuable data, it is a best practice to create a snapshot of the volume in case you need to roll back your changes. for more information, see . request the volume modification. monitor the progress of the volume modification. for more information, see . if the size of the volume was modified, extend the volume's file system to take advantage of the increased storage capacity. for more information, see . topics use the following procedure to modify an ebs volume. to modify an ebs volume using the console open the amazon ec2 console at . choose volumes, select the volume to modify, and then choose actions, modify volume. the modify volume window displays the volume id and the volume's current configuration, including type, size, and iops. you can change any or all of these settings in a single action. set new configuration values as follows: to modify the type, choose a value for volume type.to modify the size, enter an allowed integer value for size.if you chose provisioned iops ssd (io1) as the volume type, enter an allowed integer value for iops.after you have finished changing the volume settings, choose modify. when prompted for confirmation, choose yes. modifying volume size has no practical effect until you also extend the volume's file system to make use of the new storage capacity. for more information, see . use the  command to modify one or more configuration settings for a volume. for example, if you have a volume of type  with a size of 100 gib, the following command changes its configuration to a volume of type  with 10,000 iops and a size of 200 gib. the following is example output: modifying volume size has no practical effect until you also extend the volume's file system to make use of the new storage capacity. for more information, see . before you can modify a volume that was attached to an instance before november 3, 2016 23:40 utc, you must initialize volume modification support using one of the following actions: detach and attach the volumestop and start the instanceuse one of the following procedures to determine whether your instances are ready for volume modification. to determine whether your instances are ready using the console open the amazon ec2 console at . on the navigation pane, choose instances. choose the show/hide columns icon (the gear). select the launch time and block devices attributes and then choose close. sort the list of instances by the launch time column. for instances that were started before the cutoff date, check when the devices were attached. in the following example, you must initialize volume modification for the first instance because it was started before the cutoff date and its root volume was attached before the cutoff date. the other instances are ready because they were started after the cutoff date. to determine whether your instances are ready using the cliuse the following  command to determine whether the volume was attached before november 3, 2016 23:40 utc. the first line of the output for each instance shows its id and whether it was started before the cutoff date (true or false). the first line is followed by one or more lines that show whether each ebs volume was attached before the cutoff date (true or false). in the following example output, you must initialize volume modification for the first instance because it was started before the cutoff date and its root volume was attached before the cutoff date. the other instances are ready because they were started after the cutoff date. if you are using a supported instance type, you can use elastic volumes to dynamically modify the size, performance, and volume type of your amazon ebs volumes without detaching them. if you cannot use elastic volumes but you need to modify the root (boot) volume, you must stop the instance, modify the volume, and then restart the instance. after the instance has started, you can check the file system size to see if your instance recognizes the larger volume space. on linux, use the df -h command to check the file system size. if the size does not reflect your newly expanded volume, you must extend the file system of your device so that your instance can use the new space. for more information, see . 
several factors, including i/o characteristics and the configuration of your instances and volumes, can affect the performance of amazon ebs. customers who follow the guidance on our amazon ebs and amazon ec2 product detail pages typically achieve good performance out of the box. however, there are some cases where you may need to do some tuning in order to achieve peak performance on the platform. this topic discusses general best practices as well as performance tuning that is specific to certain use cases. we recommend that you tune performance with information from your actual workload, in addition to benchmarking, to determine your optimal configuration. after you learn the basics of working with ebs volumes, it's a good idea to look at the i/o performance you require and at your options for increasing amazon ebs performance to meet those requirements. aws updates to the performance of ebs volume types might not immediately take effect on your existing volumes. to see full performance on an older volume, you might first need to perform a  action on it. for more information, see . topics these tips represent best practices for getting optimal performance from your ebs volumes in a variety of user scenarios. on instances without support for ebs-optimized throughput, network traffic can contend with traffic between your instance and your ebs volumes; on ebs-optimized instances, the two types of traffic are kept separate. some ebs-optimized instance configurations incur an extra cost (such as c3, r3, and m3), while others are always ebs-optimized at no extra cost (such as m4, c4, c5, and d2). for more information, see . when you measure the performance of your ebs volumes, it is important to understand the units of measure involved and how performance is calculated. for more information, see . there is a relationship between the maximum performance of your ebs volumes, the size and number of i/o operations, and the time it takes for each action to complete. each of these factors (performance, i/o, and latency) affects the others, and different applications are more sensitive to one factor or another. for more information, see . there is a significant increase in latency when you first access each block of data on a new ebs volume that was created from a snapshot. you can avoid this performance hit using one of the following options: access each block prior to putting the volume into production. this process is called initialization (formerly known as pre-warming). for more information, see .enable fast snapshot restore on a snapshot to ensure that the ebs volumes created from it are fully-initialized at creation and instantly deliver all of their provisioned performance. for more information, see .when you create a snapshot of a throughput optimized hdd () or cold hdd () volume, performance may drop as far as the volume's baseline value while the snapshot is in progress. this behavior is specific to these volume types. other factors that can limit performance include driving more throughput than the instance can support, the performance penalty encountered while initializing volumes created from a snapshot, and excessive amounts of small, random i/o on the volume. for more information about calculating throughput for hdd volumes, see . your performance can also be impacted if your application isn’t sending enough i/o requests. this can be monitored by looking at your volume’s queue length and i/o size. the queue length is the number of pending i/o requests from your application to your volume. for maximum consistency, hdd-backed volumes must maintain a queue length (rounded to the nearest whole number) of 4 or more when performing 1 mib sequential i/o. for more information about ensuring consistent performance of your volumes, see  some workloads are read-heavy and access the block device through the operating system page cache (for example, from a file system). in this case, to achieve the maximum throughput, we recommend that you configure the read-ahead setting to 1 mib. this is a per-block-device setting that should only be applied to your hdd volumes. to examine the current value of read-ahead for your block devices, use the following command: block device information is returned in the following format: the device shown reports a read-ahead value of 256 (the default). multiply this number by the sector size (512 bytes) to obtain the size of the read-ahead buffer, which in this case is 128 kib. to set the buffer value to 1 mib, use the following command: verify that the read-ahead setting now displays 2,048 by running the first command again. only use this setting when your workload consists of large, sequential i/os. if it consists mostly of small, random i/os, this setting will actually degrade your performance. in general, if your workload consists mostly of small or random i/os, you should consider using a general purpose ssd () volume rather than  or . use a modern linux kernel with support for indirect descriptors. any linux kernel 3.8 and above has this support, as well as any current-generation ec2 instance. if your average i/o size is at or near 44 kib, you may be using an instance or kernel without support for indirect descriptors. for information about deriving the average i/o size from amazon cloudwatch metrics, see . to achieve maximum throughput on  or  volumes, we recommend applying a value of 256 to the  parameter (for linux kernel versions below 4.6) or the  parameter (for linux kernel version 4.6 and above). the appropriate parameter can be set in your os boot command line.  for example, in an amazon linux ami with an earlier kernel, you can add it to the end of the kernel line in the grub configuration found in : for a later kernel, the command would be similar to the following: reboot your instance for this setting to take effect. for more information, see . other linux distributions, especially those that do not use the grub boot loader, may require a different approach to adjusting the kernel parameters. for more information about ebs i/o characteristics, see the  re:invent presentation on this topic. some instance types can drive more i/o throughput than what you can provision for a single ebs volume. you can join multiple , , , or  volumes together in a raid 0 configuration to use the available bandwidth for these instances. for more information, see . amazon web services provides performance metrics for amazon ebs that you can analyze and view with amazon cloudwatch and status checks that you can use to monitor the health of your volumes. for more information, see . 
amazon elastic block store (amazon ebs) provides block level storage volumes for use with ec2 instances. ebs volumes behave like raw, unformatted block devices. you can mount these volumes as devices on your instances. ebs volumes that are attached to an instance are exposed as storage volumes that persist independently from the life of the instance. you can create a file system on top of these volumes, or use them in any way you would use a block device (such as a hard drive). you can dynamically change the configuration of a volume attached to an instance. we recommend amazon ebs for data that must be quickly accessible and requires long-term persistence. ebs volumes are particularly well-suited for use as the primary storage for file systems, databases, or for any applications that require fine granular updates and access to raw, unformatted, block-level storage. amazon ebs is well suited to both database-style applications that rely on random reads and writes, and to throughput-intensive applications that perform long, continuous reads and writes. with amazon ebs, you pay only for what you use. for more information about amazon ebs pricing, see the projecting costs section of the . topics ebs volumes are created in a specific availability zone, and can then be attached to any instances in that same availability zone. to make a volume available outside of the availability zone, you can create a snapshot and restore that snapshot to a new volume anywhere in that region. you can copy snapshots to other regions and then restore them to new volumes there, making it easier to leverage multiple aws regions for geographical expansion, data center migration, and disaster recovery.amazon ebs provides the following volume types: general purpose ssd (), provisioned iops ssd (), throughput optimized hdd (), and cold hdd (). the following is a summary of performance and use cases for each volume type.general purpose ssd volumes offer a base performance of 3 iops/gib, with the ability to burst to 3,000 iops for extended periods of time. these volumes are ideal for a broad range of use cases such as boot volumes, small and medium-size databases, and development and test environments. for more information, see .provisioned iops ssd volumes support up to 64,000 iops and 1,000 mib/s of throughput. this allows you to predictably scale to tens of thousands of iops per ec2 instance. for more information, see .throughput optimized hdd volumes provide low-cost magnetic storage that defines performance in terms of throughput rather than iops. these volumes are ideal for large, sequential workloads such as amazon emr, etl, data warehouses, and log processing. for more information, see .cold hdd volumes provide low-cost magnetic storage that defines performance in terms of throughput rather than iops. these volumes are ideal for large, sequential, cold-data workloads. if you require infrequent access to your data and are looking to save costs, these volumes provides inexpensive block storage. for more information, see .you can create your ebs volumes as encrypted volumes, in order to meet a wide range of data-at-rest encryption requirements for regulated/audited data and applications. when you create an encrypted ebs volume and attach it to a supported instance type, data stored at rest on the volume, disk i/o, and snapshots created from the volume are all encrypted. the encryption occurs on the servers that host ec2 instances, providing encryption of data-in-transit from ec2 instances to ebs storage. for more information, see .you can create point-in-time snapshots of ebs volumes, which are persisted to amazon s3. snapshots protect data for long-term durability, and they can be used as the starting point for new ebs volumes. the same snapshot can be used to instantiate as many volumes as you wish. these snapshots can be copied across aws regions. for more information, see . performance metrics, such as bandwidth, throughput, latency, and average queue length, are available through the aws management console. these metrics, provided by amazon cloudwatch, allow you to monitor the performance of your volumes to make sure that you are providing enough performance for your applications without paying for resources you don't need. for more information, see .
suppose that you start out running your app or website on a single ec2 instance, and over time, traffic increases to the point that you require more than one instance to meet the demand. you can launch multiple ec2 instances from your ami and then use elastic load balancing to distribute incoming traffic for your application across these ec2 instances. this increases the availability of your application. placing your instances in multiple availability zones also improves the fault tolerance in your application. if one availability zone experiences an outage, traffic is routed to the other availability zone. you can use amazon ec2 auto scaling to maintain a minimum number of running instances for your application at all times. amazon ec2 auto scaling can detect when your instance or application is unhealthy and replace it automatically to maintain the availability of your application. you can also use amazon ec2 auto scaling to scale your amazon ec2 capacity up or down automatically based on demand, using criteria that you specify. in this tutorial, we use amazon ec2 auto scaling with elastic load balancing to ensure that you maintain a specified number of healthy ec2 instances behind your load balancer. note that these instances do not need public ip addresses, because traffic goes to the load balancer and is then routed to the instances. for more information, see  and .  topics this tutorial assumes that you have already done the following: created a virtual private cloud (vpc) with one public subnet in two or more availability zones. launched an instance in the vpc. connected to the instance and customized it. for example, installing software and applications, copying data, and attaching additional ebs volumes. for information about setting up a web server on your instance, see . tested your application on your instance to ensure that your instance is configured correctly. created a custom amazon machine image (ami) from your instance. for more information, see  or . (optional) terminated the instance if you no longer need it. created an iam role that grants your application the access to aws it needs. for more information, see . use the following procedure to create a load balancer, create a launch configuration for your instances, create an auto scaling group with two or more instances, and associate the load balancer with the auto scaling group. to scale and load-balance your application open the amazon ec2 console at . on the navigation pane, under load balancing, choose load balancers. choose create load balancer. for application load balancer, choose create. on the configure load balancer page, do the following: for name, type a name for your load balancer. for example, my-lb. for scheme, keep the default value, internet-facing. for listeners, keep the default, which is a listener that accepts http traffic on port 80. for availability zones, select the vpc that you used for your instances. select an availability zone and then select the public subnet for that availability zone. repeat for a second availability zone. choose next: configure security settings. for this tutorial, you are not using a secure listener. choose next: configure security groups. on the configure security groups page, do the following: choose create a new security group. type a name and description for the security group, or keep the default name and description. this new security group contains a rule that allows traffic to the port configured for the listener. choose next: configure routing. on the configure routing page, do the following: for target group, keep the default, new target group. for name, type a name for the target group. keep protocol as http, port as 80, and target type as instance. for health checks, keep the default protocol and path. choose next: register targets. on the register targets page, choose next: review to continue to the next page, as we'll use amazon ec2 auto scaling to add ec2 instances to the target group. on the review page, choose create. after the load balancer is created, choose close. on the navigation pane, under auto scaling, choose launch configurations. if you are new to amazon ec2 auto scaling, you see a welcome page. choose create auto scaling group to start the create auto scaling group wizard, and then choose create launch configuration.otherwise, choose create launch configuration.on the choose ami page, select the my amis tab, and then select the ami that you created in . on the choose instance type page, select an instance type, and then choose next: configure details. on the configure details page, do the following: for name, type a name for your launch configuration (for example, my-launch-config). for iam role, select the iam role that you created in . (optional) if you need to run a startup script, expand advanced details and type the script in user data. choose skip to review. on the review page, choose edit security groups. you can select an existing security group or create a new one. this security group must allow http traffic and health checks from the load balancer. if your instances will have public ip addresses, you can optionally allow ssh traffic if you need to connect to the instances. when you are finished, choose review. on the review page, choose create launch configuration. when prompted, select an existing key pair, create a new key pair, or proceed without a key pair. select the acknowledgment check box, and then choose create launch configuration. after the launch configuration is created, you must create an auto scaling group. if you are new to amazon ec2 auto scaling and you are using the create auto scaling group wizard, you are taken to the next step automatically.otherwise, choose create an auto scaling group using this launch configuration.on the configure auto scaling group details page, do the following: for group name, type a name for the auto scaling group. for example, my-asg. for group size, type the number of instances (for example, 2). note that we recommend that you maintain approximately the same number of instances in each availability zone. select your vpc from network and your two public subnets from subnet. under advanced details, select receive traffic from one or more load balancers. select your target group from target groups. choose next: configure scaling policies. on the configure scaling policies page, choose review, as we will let amazon ec2 auto scaling maintain the group at the specified size. note that later on, you can manually scale this auto scaling group, configure the group to scale on a schedule, or configure the group to scale based on demand. on the review page, choose create auto scaling group. after the group is created, choose close. when a client sends a request to your load balancer, the load balancer routes the request to one of its registered instances. to test your load balancer verify that your instances are ready. from the auto scaling groups page, select your auto scaling group, and then choose the instances tab. initially, your instances are in the  state. when their states are , they are ready for use. verify that your instances are registered with the load balancer. from the target groups page, select your target group, and then choose the targets tab. if the state of your instances is , it's possible that they are still registering. when the state of your instances is , they are ready for use. after your instances are ready, you can test your load balancer as follows. from the load balancers page, select your load balancer. on the description tab, locate the dns name. this name has the following form: in a web browser, paste the dns name for the load balancer into the address bar and press enter. you'll see your website displayed. 
you can create a cloudwatch alarm that monitors cloudwatch metrics for one of your instances. cloudwatch will automatically send you a notification when the metric reaches a threshold you specify. you can create a cloudwatch alarm using the amazon ec2 console, or using the more advanced options provided by the cloudwatch console. to create an alarm using the cloudwatch consolefor examples, see  in the amazon cloudwatch user guide. to create an alarm using the amazon ec2 console open the amazon ec2 console at . in the navigation pane, choose instances. select the instance. on the monitoring tab located at the bottom of the page, choose create alarm. or, from the actions dropdown, choose cloudwatch monitoring, add/edit alarm. in the create alarm dialog box, do the following: choose create topic. for send a notification to, enter a name for the sns topic. for with these recipients, enter one or more email addresses to receive notification. specify the metric and the criteria for the policy. for example, you can leave the default settings for whenever (average of cpu utilization). for is, choose  and enter  percent. for for at least, enter  consecutive period of . choose create alarm. 
an amazon ec2 dedicated host is a physical server with ec2 instance capacity fully dedicated to your use. dedicated hosts allow you to use your existing per-socket, per-core, or per-vm software licenses, including windows server, microsoft sql server, suse, and linux enterprise server. for information about the configurations supported on dedicated hosts, see the . topics dedicated hosts and dedicated instances can both be used to launch amazon ec2 instances onto physical servers that are dedicated for your use. there are no performance, security, or physical differences between dedicated instances and instances on dedicated hosts. however, there are some differences between the two. the following table highlights some of the key differences between dedicated hosts and dedicated instances: dedicated hosts allow you to use your existing per-socket, per-core, or per-vm software licenses. when you bring your own license, you are responsible for managing your own licenses. however, amazon ec2 has features that help you maintain license compliance, such as instance affinity and targeted placement. these are the general steps to follow in order to bring your own volume licensed machine image into amazon ec2. verify that the license terms controlling the use of your machine images allow usage in a virtualized cloud environment.  after you have verified that your machine image can be used within amazon ec2, import it using vm import/export. for information about how to import your machine image, see the . after you import your machine image, you can launch instances from it onto active dedicated hosts in your account. when you run these instances, depending on the operating system, you might be required to activate these instances against your own kms server. noteto track how your images are used in aws, enable host recording in aws config. you can use aws config to record configuration changes to a dedicated host and use the output as a data source for license reporting. for more information, see .  support for multiple instance types on the same dedicated host is available for the following instance families: , , , , , and . for example, when you allocate an  dedicated host, you can use a host with 2 sockets and 48 physical cores on which you can run different instance types, such as  and . you can run any number of instances up to the core capacity associated with the host. for example, the table below shows the different instance type combinations you can run on a dedicated host. other instance families support only a single instance type on the same dedicated host. for more information about the instance families and instance type configurations supported on dedicated hosts see . before you allocate dedicated hosts, take note of the following limitations and restrictions: to run rhel, suse linux, and sql server on dedicated hosts, you must bring your own amis. rhel, suse linux, and sql server amis that are offered by aws or that are available on aws marketplace can't be used with dedicated hosts. for more information on how to create your own ami, see .up to two on-demand dedicated hosts per instance family, per region can be allocated. it is possible to request a limit increase: .the instances that run on a dedicated host can only be launched in a vpc.auto scaling groups are supported when using a launch template that specifies a host resource group. for more information, see  in the amazon ec2 auto scaling user guide.amazon rds instances are not supported.the aws free usage tier is not available for dedicated hosts.instance placement control refers to managing instance launches onto dedicated hosts. placement groups are not supported for dedicated hosts.the price for a dedicated host varies by payment option. topics on-demand billing is automatically activated when you allocate a dedicated host to your account. the on-demand price for a dedicated host varies by instance family and region. you pay per second (with a minimum of 60 seconds) for active dedicated host, regardless of the quantity or the size of instances that you choose to launch on it. for more information about on-demand pricing, see . you can release an on-demand dedicated host at any time to stop accruing charges for it. for information about releasing a dedicated host, see . dedicated host reservations provide a billing discount compared to running on-demand dedicated hosts. reservations are available in three payment options: no upfront—no upfront reservations provide you with a discount on your dedicated host usage over a term and do not require an upfront payment. available for a one-year term only.partial upfront—a portion of the reservation must be paid upfront and the remaining hours in the term are billed at a discounted rate. available in one-year and three-year terms.all upfront—provides the lowest effective price. available in one-year and three-year terms and covers the entire cost of the term upfront, with no additional future charges.you must have active dedicated hosts in your account before you can purchase reservations. each reservation covers a single, specific dedicated host in your account. reservations are applied to the instance family on the host, not the instance size. if you have three dedicated hosts with different instances sizes (, , and ) you can associate a single  reservation with all those dedicated hosts. the instance family and region of the reservation must match that of the dedicated hosts you want to associate it with.  when a reservation is associated with a dedicated host, the dedicated host can't be released until the reservation's term is over. for more information about reservation pricing, see . savings plans are a flexible pricing model that offers significant savings over on-demand instances. with savings plans, you make a commitment to a consistent amount of usage, in usd per hour, for a term of one or three years. this provides you with the flexibility to use the dedicated hosts that best meet your needs and continue to save money, instead of making a commitment to a specific dedicated host. for more information, see the . subject to microsoft licensing terms, you can bring your existing windows server and sql server licenses to dedicated hosts. there is no additional charge for software usage if you choose to bring your own licenses. in addition, you can also use windows server amis provided by amazon to run the latest versions of windows server on dedicated hosts. this is common for scenarios where you have existing sql server licenses eligible to run on dedicated hosts, but need windows server to run the sql server workload. windows server amis provided by amazon are supported on  only. for more information, see . 
after you , you must use file system–specific commands to extend the file system to the larger size. you can resize the file system as soon as the volume enters the  state. importantbefore extending a file system that contains valuable data, it is best practice to create a snapshot of the volume, in case you need to roll back your changes. for more information, see . if your linux ami uses the mbr partitioning scheme, you are limited to a boot volume size of up to 2 tib. for more information, see  and .  for information about extending a windows file system, see  in the amazon ec2 user guide for windows instances. for the following tasks, suppose that you have resized the boot volume of an instance from 8 gb to 16 gb and an additional volume from 8 gb to 30 gb. topics to verify the file system in use for each volume on your instance,  and run the df -ht command. example: file systems on an instance built on the nitro systemthe following example shows an instance built on the  that has a boot volume with an xfs file system and an additional volume with an xfs file system. example: file systems on a t2 instancethe following example shows a t2 instance that has a boot volume with an ext4 file system and an additional volume with an xfs file system. your ebs volume might have a partition that contains the file system and data. increasing the size of a volume does not increase the size of the partition. before you extend the file system on a resized volume, check whether the volume has a partition that must be extended to the new size of the volume. use the lsblk command to display information about the block devices attached to your instance. if a resized volume has a partition and the partition does not reflect the new size of the volume, use the growpart command to extend the partition. for information about extending an lvm partition, see . example: partitions on an instance built on the nitro systemthe following example shows the volumes on a nitro-based instance: the root volume, , has a partition, . while the size of the root volume reflects the new size, 16 gb, the size of the partition reflects the original size, 8 gb, and must be extended before you can extend the file system.the volume  has no partitions. the size of the volume reflects the new size, 30 gb.to extend the partition on the root volume, use the following growpart command. notice that there is a space between the device name and the partition number. you can verify that the partition reflects the increased volume size by using the lsblk command again. example: partitions on a t2 instancethe following example shows the volumes on a t2 instance: the root volume, , has a partition, . while the size of the volume is 16 gb, the size of the partition is still 8 gb and must be extended.the volume  has a partition, . while the size of the volume is 30g, the size of the partition is still 8 gb and must be extended.to extend the partition on each volume, use the following growpart commands. note that there is a space between the device name and the partition number. you can verify that the partitions reflect the increased volume size by using the lsblk command again. use a file system-specific command to resize each file system to the new volume capacity. for a file system other than the examples shown here, refer to the documentation for the file system for instructions. example: extend an ext2, ext3, or ext4 file systemuse the df -h command to verify the size of the file system for each volume. in this example, both  and  reflect the original size of the volumes, 8 gb. use the resize2fs command to extend the file system on each volume. you can verify that each file system reflects the increased volume size by using the df -h command again. example: extend an xfs file systemuse the df -h command to verify the size of the file system for each volume. in this example, each file system reflects the original volume size, 8 gb. to extend the xfs file system, install the xfs tools as follows, if they are not already installed. use the xfs_growfs command to extend the file system on each volume. in this example,  and  are the volume mount points shown in the output for df -h. you can verify that each file system reflects the increased volume size by using the df -h command again. 
if you have a need for a custom kernel on your amazon ec2 instances, you can start with an ami that is close to what you want, compile the custom kernel on your instance, and modify the  file to point to the new kernel. this process varies depending on the virtualization type that your ami uses. for more information, see . topics hvm instance volumes are treated like actual physical disks. the boot process is similar to that of a bare metal operating system with a partitioned disk and bootloader, which allows it to work with all currently supported linux distributions. the most common bootloader is grub, and the following section describes configuring grub to use a custom kernel. the following is an example of a  configuration file for an hvm ami. in this example, there are two kernel entries to choose from: amazon linux 2018.03 (the original kernel for this ami) and vanilla linux 4.16.4 (a newer version of the vanilla linux kernel from ). the vanilla entry was copied from the original entry for this ami, and the  and  paths were updated to the new locations. the  parameter points the bootloader to the first entry that it sees (in this case, the vanilla entry), and the  parameter points the bootloader to the next entry if there is a problem booting the first. by default, grub does not send its output to the instance console because it creates an extra boot delay. for more information, see . if you are installing a custom kernel, you should consider enabling grub output by deleting the  line and adding  and  lines to  as shown in the example below. importantavoid printing large amounts of debug information during the boot process; the serial console does not support high rate data transfer. you don't need to specify a fallback kernel in your  file, but we recommend that you have a fallback when you test a new kernel. grub can fall back to another kernel in the event that the new kernel fails. having a fallback kernel allows the instance to boot even if the new kernel isn't found. if your new vanilla linux kernel fails, the output will be similar to the example below. amazon machine images that use paravirtual (pv) virtualization use a system called pv-grub during the boot process. pv-grub is a paravirtual bootloader that runs a patched version of gnu grub 0.97. when you start an instance, pv-grub starts the boot process and then chain loads the kernel specified by your image's  file. pv-grub understands standard  or  commands, which allows it to work with all currently supported linux distributions. older distributions such as ubuntu 10.04 lts, oracle enterprise linux or centos 5.x require a special "ec2" or "xen" kernel package, while newer distributions include the required drivers in the default kernel package. most modern paravirtual amis use a pv-grub aki by default (including all of the paravirtual linux amis available in the amazon ec2 launch wizard quick start menu), so there are no additional steps that you need to take to use a different kernel on your instance, provided that the kernel you want to use is compatible with your distribution. the best way to run a custom kernel on your instance is to start with an ami that is close to what you want and then to compile the custom kernel on your instance and modify the  file as shown in  to boot with that kernel. you can verify that the kernel image for an ami is a pv-grub aki by executing the following  command with the amazon ec2 command line tools (substituting the kernel image id you want to check: check whether the  field starts with . topics pv-grub has the following limitations: you can't use the 64-bit version of pv-grub to start a 32-bit kernel or vice versa.you can't specify an amazon ramdisk image (ari) when using a pv-grub aki.aws has tested and verified that pv-grub works with these file system formats: ext2, ext3, ext4, jfs, xfs, and reiserfs. other file system formats might not work.pv-grub can boot kernels compressed using the gzip, bzip2, lzo, and xz compression formats.cluster amis don't support or need pv-grub, because they use full hardware virtualization (hvm). while paravirtual instances use pv-grub to boot, hvm instance volumes are treated like actual disks, and the boot process is similar to the boot process of a bare metal operating system with a partitioned disk and bootloader. pv-grub versions 1.03 and earlier don't support gpt partitioning; they support mbr partitioning only.if you plan to use a logical volume manager (lvm) with amazon ebs volumes, you need a separate boot partition outside of the lvm. then you can create logical volumes with the lvm.to boot pv-grub, a grub  file must exist in the image; the most common location for this file is . the following is an example of a  configuration file for booting an ami with a pv-grub aki. in this example, there are two kernel entries to choose from: amazon linux 2018.03 (the original kernel for this ami), and vanilla linux 4.16.4 (a newer version of the vanilla linux kernel from ). the vanilla entry was copied from the original entry for this ami, and the  and  paths were updated to the new locations. the  parameter points the bootloader to the first entry it sees (in this case, the vanilla entry), and the  parameter points the bootloader to the next entry if there is a problem booting the first. you don't need to specify a fallback kernel in your  file, but we recommend that you have a fallback when you test a new kernel. pv-grub can fall back to another kernel in the event that the new kernel fails. having a fallback kernel allows the instance to boot even if the new kernel isn't found.  pv-grub checks the following locations for , using the first one it finds:          note that pv-grub 1.03 and earlier only check one of the first two locations in this list. pv-grub akis are available in all amazon ec2 regions. there are akis for both 32-bit and 64-bit architecture types. most modern amis use a pv-grub aki by default. we recommend that you always use the latest version of the pv-grub aki, as not all versions of the pv-grub aki are compatible with all instance types. use the following  command to get a list of the pv-grub akis for the current region: note that pv-grub is the only aki available in the  region. you should verify that any ami you want to copy to this region is using a version of pv-grub that is available in this region. the following are the current aki ids for each region. register new amis using an hd0 aki. notewe continue to provide hd00 akis for backward compatibility in regions where they were previously available. ap-northeast-1, asia pacific (tokyo)   ap-southeast-1, asia pacific (singapore) region   ap-southeast-2, asia pacific (sydney)   eu-central-1, europe (frankfurt)   eu-west-1, europe (ireland)   sa-east-1, south america (são paulo)   us-east-1, us east (n. virginia)   us-gov-west-1, aws govcloud (us-west)   us-west-1, us west (n. california)   us-west-2, us west (oregon)   we recommend that you always use the latest version of the pv-grub aki, as not all versions of the pv-grub aki are compatible with all instance types. also, older versions of pv-grub are not available in all regions, so if you copy an ami that uses an older version to a region that does not support that version, you will be unable to boot instances launched from that ami until you update the kernel image. use the following procedures to check your instance's version of pv-grub and update it if necessary. to check your pv-grub version find the kernel id for your instance. the kernel id for this instance is . view the version information of that kernel id. this kernel image is pv-grub 1.05. if your pv-grub version is not the newest version (as shown in ), you should update it using the following procedure. to update your pv-grub version if your instance is using an older version of pv-grub, you should update it to the latest version. identify the latest pv-grub aki for your region and processor architecture from . stop your instance. your instance must be stopped to modify the kernel image used. modify the kernel image used for your instance. restart your instance. 
an elastic ip address is a static ipv4 address designed for dynamic cloud computing. an elastic ip address is associated with your aws account. with an elastic ip address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account.  an elastic ip address is a public ipv4 address, which is reachable from the internet. if your instance does not have a public ipv4 address, you can associate an elastic ip address with your instance to enable communication with the internet. for example, this allows you to connect to your instance from your local computer. we currently do not support elastic ip addresses for ipv6. topics the following are the basic characteristics of an elastic ip address: to use an elastic ip address, you first allocate one to your account, and then associate it with your instance or a network interface.when you associate an elastic ip address with an instance, it is also associated with the instance's primary network interface. when you associate an elastic ip address with a network interface that is attached to an instance, it is also associated with the instance.when you associate an elastic ip address with an instance or its primary network interface, the instance's public ipv4 address (if it had one) is released back into amazon's pool of public ipv4 addresses. you cannot reuse a public ipv4 address, and you cannot convert a public ipv4 address to an elastic ip address. for more information, see .you can disassociate an elastic ip address from a resource, and reassociate it with a different resource. any open connections to an instance continue to work for a time even after you disassociate its elastic ip address and reassociate it with another instance. we recommend that you reopen these connections using the reassociated elastic ip address.a disassociated elastic ip address remains allocated to your account until you explicitly release it.to ensure efficient use of elastic ip addresses, we impose a small hourly charge if an elastic ip address is not associated with a running instance, or if it is associated with a stopped instance or an unattached network interface. while your instance is running, you are not charged for one elastic ip address associated with the instance, but you are charged for any additional elastic ip addresses associated with the instance. for more information, see the section for elastic ip addresses on the .an elastic ip address is for use in a specific network border group only. when you associate an elastic ip address with an instance that previously had a public ipv4 address, the public dns host name of the instance changes to match the elastic ip address. we resolve a public dns host name to the public ipv4 address or the elastic ip address of the instance outside the network of the instance, and to the private ipv4 address of the instance from within the network of the instance. when you allocate an elastic ip address from an ip address pool that you have brought to your aws account, it does not count toward your elastic ip address limits.when you allocate the elastic ip addresses, you can associate the elastic ip addresses with a network border group. this is the location from which we advertise the cidr block. setting the network border group limits the cidr block to this group. if you do not specify the network border group, we set the border group containing all of the availability zones in the region (for example, ).the following sections describe how you can work with elastic ip addresses. topics you can allocate an elastic ip address from amazon's pool of public ipv4 addresses, or from a custom ip address pool that you have brought to your aws account. for more information about bringing your own ip address range to your aws account, see . you can allocate an elastic ip address using one of the following methods. to allocate an elastic ip address open the amazon ec2 console at . in the navigation pane, choose elastic ips. choose allocate elastic ip address. for scope, choose either vpc or ec2-classic depending on the scope in which it will be used. (vpc scope only) for public ipv4 address pool choose one of the following: amazon's pool of ip addresses—if you want an ipv4 address to be allocated from amazon's pool of ip addresses.my pool of public ipv4 addresses—if you want to allocate an ipv4 address from an ip address pool that you have brought to your aws account. this option is disabled if you do not have any ip address pools.customer owned pool of ipv4 addresses—if you want to allocate an ipv4 address from a pool created from your on-premises network for use with an aws outpost. this option is disabled if you do not have an aws outpost.choose allocate. to allocate an elastic ip address open the amazon ec2 console at . in the navigation pane, choose elastic ips. choose allocate new address. for ipv4 address pool, choose amazon pool. choose allocate, and close the confirmation screen. to allocate an elastic ip addressuse the  aws cli command. to allocate an elastic ip addressuse the  aws tools for windows powershell command. you can describe an elastic ip address using one of the following methods. to describe your elastic ip addresses open the amazon ec2 console at . in the navigation pane, choose elastic ips. select the elastic ip address to view and choose actions, view details. to describe your elastic ip addresses open the amazon ec2 console at . in the navigation pane, choose elastic ips.  select a filter from the resource attribute list to begin searching. you can use multiple filters in a single search. to describe your elastic ip addressesuse the  aws cli command. to describe your elastic ip addressesuse the  aws tools for windows powershell command. you can assign custom tags to your elastic ip addresses to categorize them in different ways, for example, by purpose, owner, or environment. this helps you to quickly find a specific elastic ip address based on the custom tags that you assigned to it. you can only tag elastic ip addresses that are in the vpc scope. notecost allocation tracking using elastic ip address tags is not supported. you can tag an elastic ip address using one of the following methods. to tag an elastic ip address open the amazon ec2 console at . in the navigation pane, choose elastic ips.  select the elastic ip address to tag and choose actions, view details.  in the tags section, choose manage tags. specify a tag key and value pair. (optional) choose add tag to add additional tags. choose save. to tag an elastic ip address open the amazon ec2 console at . in the navigation pane, choose elastic ips.  select the elastic ip address to tag and choose tags.  choose add/edit tags. in the add/edit tags dialog box, choose create tag, and then specify the key and value for the tag. (optional) choose create tag to add additional tags to the elastic ip address. choose save. to tag an elastic ip addressuse the  aws cli command. to tag an elastic ip addressuse the  aws tools for windows powershell command. the  command needs a  parameter, which specifies the key and value pair to be used for the elastic ip address tag. the following commands create the  parameter. if you're associating an elastic ip address with your instance to enable communication with the internet, you must also ensure that your instance is in a public subnet. for more information, see  in the amazon vpc user guide. you can associate an elastic ip address with an instance or network interface using one of the following methods. to associate an elastic ip address with an instance open the amazon ec2 console at . in the navigation pane, choose elastic ips.  select the elastic ip address to associate and choose actions, associate elastic ip address.  for resource type, choose instance. for instance, choose the instance with which to associate the elastic ip address. you can also enter text to search for a specific instance. (optional) for private ip address, specify a private ip address with which to associate the elastic ip address. choose associate. to associate an elastic ip address with a network interface open the amazon ec2 console at . in the navigation pane, choose elastic ips.  select the elastic ip address to associate and choose actions, associate elastic ip address.  for resource type, choose network interface. for network interface, choose the network interface with which to associate the elastic ip address. you can also enter text to search for a specific network interface. (optional) for private ip address, specify a private ip address with which to associate the elastic ip address. choose associate. to associate an elastic ip address with an instance open the amazon ec2 console at . in the navigation pane, choose elastic ips.  select an elastic ip address and choose actions, associate address.  select the instance from instance and then choose associate. to associate an elastic ip addressuse the  aws cli command. to associate an elastic ip addressuse the  aws tools for windows powershell command. you can disassociate an elastic ip address from an instance or network interface at any time. after you disassociate the elastic ip address, you can reassociate it with another resource. you can disassociate an elastic ip address using one of the following methods. to disassociate and reassociate an elastic ip address open the amazon ec2 console at . in the navigation pane, choose elastic ips. select the elastic ip address to disassociate, choose actions, disassociate elastic ip address. choose disassociate. to disassociate and reassociate an elastic ip address open the amazon ec2 console at . in the navigation pane, choose elastic ips.  select the elastic ip address, choose actions, and then select disassociate address.  choose disassociate address. to disassociate an elastic ip addressuse the  aws cli command. to disassociate an elastic ip addressuse the  aws tools for windows powershell command. if you no longer need an elastic ip address, we recommend that you release it using one of the following methods. the address that you release must not be associated with an instance. to release an elastic ip address open the amazon ec2 console at . in the navigation pane, choose elastic ips. select the elastic ip address to release and choose actions, release elastic ip addresses. choose release. to release an elastic ip address open the amazon ec2 console at . in the navigation pane, choose elastic ips. select the elastic ip address, choose actions, and then select release addresses. choose release when prompted. to release an elastic ip addressuse the  aws cli command. to release an elastic ip addressuse the  aws tools for windows powershell command. if you have released your elastic ip address, you might be able to recover it. the following rules apply: you cannot recover an elastic ip address if it has been allocated to another aws account, or if it will result in your exceeding your elastic ip address limit.you cannot recover tags associated with an elastic ip address.you can recover an elastic ip address using the amazon ec2 api or a command line tool only.to recover an elastic ip addressuse the  aws cli command and specify the ip address using the  parameter as follows. to recover an elastic ip addressuse the  aws tools for windows powershell command and specify the ip address using the  parameter as follows. if you intend to send email to third parties from an instance, we suggest that you provision one or more elastic ip addresses and provide them to aws. aws works with isps and internet anti-spam organizations to reduce the chance that your email sent from these addresses will be flagged as spam. in addition, assigning a static reverse dns record to your elastic ip address that is used to send email can help avoid having email flagged as spam by some anti-spam organizations. note that a corresponding forward dns record (record type a) pointing to your elastic ip address must exist before we can create your reverse dns record.  if a reverse dns record is associated with an elastic ip address, the elastic ip address is locked to your account and cannot be released from your account until the record is removed.  to remove email sending limits, or to provide us with your elastic ip addresses and reverse dns records, go to the  page. by default, all aws accounts are limited to five (5) elastic ip addresses per region, because public (ipv4) internet addresses are a scarce public resource. we strongly encourage you to use an elastic ip address primarily for the ability to remap the address to another instance in the case of instance failure, and to use dns hostnames for all other inter-node communication. to verify how many elastic ip addresses are in use, open the amazon ec2 console at  and choose elastic ips from the navigation pane. to verify your current account limit for elastic ip addresses, do one of the following: open the amazon ec2 console at , choose limits from the navigation pane, and enter ip in the search field.open the service quotas console at . on the dashboard, choose amazon elastic compute cloud (amazon ec2). if amazon elastic compute cloud (amazon ec2) is not listed on the dashboard, choose aws services, enter ec2 in the search field, and then choose amazon elastic compute cloud (amazon ec2). on the amazon ec2 service quotas screen, enter elastic ip in the search field and then choose elastic ip addresses for ec2. if you feel your architecture warrants additional elastic ip addresses, you can request a quota increase directly from the service quotas console. 
the following documentation can help you troubleshoot problems that you might have with your instance. topics for additional help with windows instances, see  in the amazon ec2 user guide for windows instances. 
after you have successfully launched and logged into your amazon linux instance, you can make changes to it. there are many different ways you can configure an instance to meet the needs of a specific application. the following are some common tasks to help get you started. topics the base distribution of amazon linux contains many software packages and utilities that are required for basic server operations. however, many more software packages are available in various software repositories, and even more packages are available for you to build from source code. for more information on installing and building software from these locations, see . amazon linux instances come pre-configured with an  account, but you may want to add other user accounts that do not have super-user privileges. for more information on adding and removing user accounts, see . the default time configuration for amazon linux instances uses amazon time sync service to set the system time on an instance. the default time zone is utc. for more information on setting the time zone for an instance or using your own time server, see . if you have your own network with a domain name registered to it, you can change the hostname of an instance to identify itself as part of that domain. you can also change the system prompt to show a more meaningful name without changing the hostname settings. for more information, see . you can configure an instance to use a dynamic dns service provider. for more information, see . when you launch an instance in amazon ec2, you have the option of passing user data to the instance that can be used to perform common configuration tasks and even run scripts after the instance starts. you can pass two types of user data to amazon ec2: cloud-init directives and shell scripts. for more information, see . 
you can create an amazon ebs volume and then attach to any ec2 instance in the same availability zone. if you create an encrypted ebs volume, you can only attach it to supported instance types. for more information, see . if you are creating a volume for a high-performance storage scenario, you should make sure to use a provisioned iops ssd () volume and attach it to an instance with enough bandwidth to support your application, such as an ebs-optimized instance or an instance with 10-gigabit network connectivity. the same advice holds for throughput optimized hdd () and cold hdd () volumes. for more information, see . empty ebs volumes receive their maximum performance the moment that they are available and do not require initialization (formerly known as pre-warming). however, storage blocks on volumes that were created from snapshots must be initialized (pulled down from amazon s3 and written to the volume) before you can access the block. this preliminary action takes time and can cause a significant increase in the latency of an i/o operation the first time each block is accessed. volume performance is achieved after all blocks have been downloaded and written to the volume. for most applications, amortizing this cost over the lifetime of the volume is acceptable. to avoid this initial performance hit in a production environment, you can force immediate initialization of the entire volume or enable fast snapshot restore. for more information, see . methods of creating a volume create and attach ebs volumes when you launch instances by specifying a block device mapping. for more information, see  and .create an empty ebs volume and attach it to a running instance. for more information, see  below.create an ebs volume from a previously created snapshot and attach it to a running instance. for more information, see  below.empty volumes receive their maximum performance the moment that they are available and do not require initialization. to create a empty ebs volume using the console open the amazon ec2 console at . from the navigation bar, select the region in which you would like to create your volume. this choice is important because some amazon ec2 resources can be shared between regions, while others can't. for more information, see . in the navigation pane, choose elastic block store, volumes. choose create volume. for volume type, choose a volume type. for more information, see . for size (gib), type the size of the volume. for more information, see . with a provisioned iops ssd volume, for iops, type the maximum number of input/output operations per second (iops) that the volume should support. for availability zone, choose the availability zone in which to create the volume. ebs volumes can only be attached to ec2 instances within the same availability zone. (optional) if the instance type supports ebs encryption and you want to encrypt the volume, select encrypt this volume and choose a cmk. if encryption by default is enabled in this region, ebs encryption is enabled and the default cmk for ebs encryption is chosen. you can choose a different cmk from master key or paste the full arn of any key that you can access. for more information, see . (optional) choose create additional tags to add tags to the volume. for each tag, provide a tag key and a tag value. for more information, see . choose create volume. the volume is ready for use when the volume status is available. to use your new volume, attach it to an instance, format it, and mount it. for more information, see . to create an empty ebs volume using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)volumes created from snapshots load lazily in the background. this means that there is no need to wait for all of the data to transfer from amazon s3 to your ebs volume before the instance can start accessing an attached volume and all its data. if your instance accesses data that hasn't yet been loaded, the volume immediately downloads the requested data from amazon s3, and then continues loading the rest of the volume data in the background. volume performance is achieved after all blocks are downloaded and written to the volume. to avoid the initial performance hit in a production environment, see . new ebs volumes that are created from encrypted snapshots are automatically encrypted. you can also encrypt a volume on-the-fly while restoring it from an unencrypted snapshot. encrypted volumes can only be attached to instance types that support ebs encryption. for more information, see . use the following procedure to create a volume from a snapshot. to create an ebs volume from a snapshot using the console open the amazon ec2 console at . from the navigation bar, select the region that your snapshot is located in. to use the snapshot to create a volume in a different region, copy your snapshot to the new region and then use it to create a volume in that region. for more information, see . in the navigation pane, choose elastic block store, volumes. choose create volume. for volume type, choose a volume type. for more information, see . for snapshot id, start typing the id or description of the snapshot from which you are restoring the volume, and choose it from the list of suggested options. (optional) select encrypt this volume to change the encryption state of your volume. this is optional if  is enabled. select a cmk from master key to specify a cmk other than the default cmk for ebs encryption. for size (gib), type the size of the volume, or verify that the default size of the snapshot is adequate. if you specify both a volume size and a snapshot, the size must be equal to or greater than the snapshot size. when you select a volume type and a snapshot, the minimum and maximum sizes for the volume are shown next to size. for more information, see . with a provisioned iops ssd volume, for iops, type the maximum number of input/output operations per second (iops) that the volume should support. for availability zone, choose the availability zone in which to create the volume. ebs volumes can only be attached to ec2 instances in the same availability zone. (optional) choose create additional tags to add tags to the volume. for each tag, provide a tag key and a tag value. choose create volume. to use your new volume, attach it to an instance and mount it. for more information, see . if you created a volume that is larger than the snapshot, you must extend the file system on the volume to take advantage of the extra space. for more information, see . to create an ebs volume from a snapshot using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)
the aws global infrastructure is built around aws regions and availability zones. regions provide multiple physically separated and isolated availability zones, which are connected through low-latency, high-throughput, and highly redundant networking. with availability zones, you can design and operate applications and databases that automatically fail over between zones without interruption. availability zones are more highly available, fault tolerant, and scalable than traditional single or multiple data center infrastructures. for more information about aws regions and availability zones, see . in addition to the aws global infrastructure, amazon ec2 offers the following features to support your data resiliency: copying amis across regionscopying ebs snapshots across regionsautomating ebs snapshots using amazon data lifecycle managermaintaining the health and availability of your fleet using amazon ec2 auto scalingdistributing incoming traffic across multiple instances in a single availability zone or multiple availability zones using elastic load balancing
aws provides various tools that you can use to monitor amazon ec2. you can configure some of these tools to do the monitoring for you, while some of the tools require manual intervention. topics you can use the following automated monitoring tools to watch amazon ec2 and report back to you when something is wrong: system status checks – monitor the aws systems required to use your instance to ensure that they are working properly. these checks detect problems with your instance that require aws involvement to repair. when a system status check fails, you can choose to wait for aws to fix the issue or you can resolve it yourself (for example, by stopping and restarting or terminating and replacing an instance). examples of problems that cause system status checks to fail include: loss of network connectivityloss of system powersoftware issues on the physical hosthardware issues on the physical host that impact network reachabilityfor more information, see . instance status checks – monitor the software and network configuration of your individual instance. these checks detect problems that require your involvement to repair. when an instance status check fails, typically you will need to address the problem yourself (for example, by rebooting the instance or by making modifications in your operating system). examples of problems that may cause instance status checks to fail include: failed system status checksmisconfigured networking or startup configurationexhausted memorycorrupted file systemincompatible kernelfor more information, see . amazon cloudwatch alarms – watch a single metric over a time period you specify, and perform one or more actions based on the value of the metric relative to a given threshold over a number of time periods. the action is a notification sent to an amazon simple notification service (amazon sns) topic or amazon ec2 auto scaling policy. alarms invoke actions for sustained state changes only. cloudwatch alarms will not invoke actions simply because they are in a particular state; the state must have changed and been maintained for a specified number of periods. for more information, see .amazon cloudwatch events – automate your aws services and respond automatically to system events. events from aws services are delivered to cloudwatch events in near real time, and you can specify automated actions to take when an event matches a rule you write. for more information, see .amazon cloudwatch logs – monitor, store, and access your log files from amazon ec2 instances, aws cloudtrail, or other sources. for more information, see the .cloudwatch agent – collect logs and system-level metrics from both hosts and guests on your ec2 instances and on-premises servers. for more information, see  in the amazon cloudwatch user guide.aws management pack for microsoft system center operations manager – links amazon ec2 instances and the windows or linux operating systems running inside them. the aws management pack is an extension to microsoft system center operations manager. it uses a designated computer in your datacenter (called a watcher node) and the amazon web services apis to remotely discover and collect information about your aws resources. for more information, see .another important part of monitoring amazon ec2 involves manually monitoring those items that the monitoring scripts, status checks, and cloudwatch alarms don't cover. the amazon ec2 and cloudwatch console dashboards provide an at-a-glance view of the state of your amazon ec2 environment. amazon ec2 dashboard shows:service health and scheduled events by regioninstance statestatus checksalarm statusinstance metric details (in the navigation pane choose instances, select an instance, and choose the monitoring tab)volume metric details (in the navigation pane choose volumes, select a volume, and choose the monitoring tab)amazon cloudwatch dashboard shows: current alarms and statusgraphs of alarms and resourcesservice health statusin addition, you can use cloudwatch to do the following: graph amazon ec2 monitoring data to troubleshoot issues and discover trendssearch and browse all your aws resource metricscreate and edit alarms to be notified of problemssee at-a-glance overviews of your alarms and aws resources
amazon ec2 provides different resources that you can create and use. some of these resources include images, instances, volumes, and snapshots. when you create a resource, we assign the resource a unique resource id. some resources can be tagged with values that you define, to help you organize and identify them. the following topics describe resources and tags, and how you can work with them. topics 
this example demonstrates how you can use both user data and instance metadata to configure your instances.  alice wants to launch four instances of her favorite database ami, with the first acting as master and the remaining three acting as replicas. when she launches them, she wants to add user data about the replication strategy for each replicant. she is aware that this data will be available to all four instances, so she needs to structure the user data in a way that allows each instance to recognize which parts are applicable to it. she can do this using the  instance metadata value, which will be unique for each instance. here is the user data that alice has constructed. the  data defines the first replicant's configuration,  defines the second replicant's configuration, and so on. alice decides to provide this data as an ascii string with a pipe symbol () delimiting the data for the separate instances. alice launches four instances using the  command, specifying the user data.  after they're launched, all instances have a copy of the user data and the common metadata shown here: ami id: ami-0abcdef1234567890reservation id: r-1234567890abcabc0public keys: none security group name: defaultinstance type: t2.microhowever, each instance has certain unique metadata. instance 1   instance 2   instance 3   instance 4   alice can use the  value to determine which portion of the user data is applicable to a particular instance. she connects to one of the instances, and retrieves the  for that instance to ensure it is one of the replicants:   for the following steps, the imdsv2 requests use the stored token from the preceding imdsv2 command, assuming the token has not expired. she saves the  as a variable.she saves the user data as a variable.finally, alice uses the cut command to extract the portion of the user data that is applicable to that instance.
amazon ebs fast snapshot restore enables you to create a volume from a snapshot that is fully-initialized at creation. this eliminates the latency of i/o operations on a block when it is accessed for the first time. volumes created using fast snapshot restore instantly deliver all of their provisioned performance. to get started, enable fast snapshot restore for specific snapshots in specific availability zones. each snapshot and availability zone pair refers to one fast snapshot restore. you can enable up to 50 fast snapshot restores per region. when you create a volume from one of these snapshots in one of its enabled availability zones, the volume is restored using fast snapshot restore. topics after you enable fast snapshot restore for a snapshot, it can be in one of the following states.  — a request was made to enable fast snapshot restore. — fast snapshot restore is being enabled. it takes 60 minutes per tib to optimize a snapshot. — fast snapshot restore is enabled. — a request was made to disable fast snapshot restore or a request to enable fast snapshot restore failed. — fast snapshot restore is disabled. you can enable fast snapshot restore again as needed.the number of volumes that receive the full performance benefit of fast snapshot restore is determined by the volume creation credits for the snapshot. there is one credit bucket per snapshot per availability zone. each volume that you create from a snapshot with fast snapshot restore enabled consumes one credit from the credit bucket. the size of a credit bucket depends on the size of the snapshot, not the size of the volumes created from the snapshot. the size of the credit bucket for each snapshot is calculated as follows: as you consume credits, the credit bucket is refilled over time. the refill rate for each credit bucket is calculated as follows: for example, if you enable fast snapshot restore for a snapshot with a size of 100 gib, the maximum size of its credit bucket is 10 credits and the refill rate is 10 credits per hour. when the credit bucket is full, you can create 10 initialized volumes from this snapshot simultaneously. you can use cloudwatch metrics to monitor the size of your credit buckets and the number of credits available in each bucket. for more information, see . after you create a volume from a snapshot with fast snapshot restore enabled, you can describe the volume using  and check the  field in the output to determine whether the volume was created as an initialized volume using fast snapshot restore. use the following procedure to enable fast snapshot restore for a snapshot. you must own the snapshot. you cannot enable fast snapshot restore on a snapshot that was shared with you. to enable or disable fast snapshot restore open the amazon ec2 console at . in the navigation pane, choose snapshots. select the snapshot. choose actions, manage fast snapshot restore. select or deselect availability zones, and then choose save. to track the state of fast snapshot restore as it is enabled, see fast snapshot restore on the description tab. to manage fast snapshot restore using the aws cli use the following procedure to view the state of fast snapshot restore for a snapshot. to view the state of fast snapshot restore using the console open the amazon ec2 console at . in the navigation pane, choose snapshots. select the snapshot. on the description tab, see fast snapshot restore, which indicates the state of fast snapshot restore. for example, "2 availability zones optimizing" or "2 availability zones enabled". to view snapshots with fast snapshot restore enabled using the aws cliuse the  command to describe the snapshots that are enabled for fast snapshot restore. the following is example output. when you create a volume from a snapshot that is enabled for fast snapshot restore in the availability zone for the volume, it is restored using fast snapshot restore. use the  command to view volumes that were created from a snapshot that is enabled for fast snapshot restore. the following is example output. you are billed for each minute that fast snapshot restore is enabled for a snapshot in a particular availability zone. charges are pro-rated with a minimum of one hour.  for example, if you enable fast snapshot restore for one snapshot in  for one month (30 days), you are billed $540 ( snapshot x  az x  hours x  per hour). if you enable fast snapshot restore for two snapshots in , , and  for the same period, you are billed $3240 ( snapshot x  azs x  hours x  per hour). for more information, see . 
the base distribution of amazon linux contains many software packages and utilities that are required for basic server operations. however, many more software packages are available in various software repositories, and even more packages are available for you to build from source code. topics it is important to keep software up-to-date. many packages in a linux distribution are updated frequently to fix bugs, add features, and protect against security exploits. for more information, see . by default, amazon linux instances launch with the following repositories enabled: amazon linux 2:  and amazon linux ami:  and  while there are many packages available in these repositories that are updated by amazon web services, there may be a package that you wish to install that is contained in another repository. for more information, see . for help finding packages in enabled repositories, see . for information about installing software on an amazon linux instance, see . not all software is available in software packages stored in repositories; some software must be compiled on an instance from its source code. for more information, see . amazon linux instances manage their software using the yum package manager. the yum package manager can install, remove, and update software, as well as manage all of the dependencies for each package. debian-based linux distributions, like ubuntu, use the apt-get command and dpkg package manager, so the yum examples in the following sections do not work for those distributions. 
you can deregister an ami when you have finished using it. after you deregister an ami, you can't use it to launch new instances. when you deregister an ami, it doesn't affect any instances that you've already launched from the ami. you'll continue to incur usage costs for these instances. therefore, if you are finished with these instances, you should terminate them. the procedure that you'll use to clean up your ami depends on whether it is backed by amazon ebs or instance store. for more information, see . notean ami must be owned by your account in order to deregister it. topics when you deregister an amazon ebs-backed ami, it doesn't affect the snapshot(s) that were created for the volume(s) of the instance during the ami creation process. you'll continue to incur storage costs for the snapshots. therefore, if you are finished with the snapshots, you should delete them.  the following diagram illustrates the process for cleaning up your amazon ebs-backed ami.  to clean up your amazon ebs-backed ami open the amazon ec2 console at . in the navigation pane, choose amis. select the ami, and take note of its id — this can help you find the correct snapshot in the next step. choose actions, and then deregister. when prompted for confirmation, choose continue. noteit may take a few minutes before the console removes the ami from the list. choose refresh to refresh the status. in the navigation pane, choose snapshots, and select the snapshot (look for the ami id in the description column). choose actions, and then choose delete snapshot. when prompted for confirmation, choose yes, delete. (optional) if you are finished with an instance that you launched from the ami, terminate it. in the navigation pane, choose instances. select the instance, choose actions, then instance state, and then terminate. when prompted for confirmation, choose yes, terminate. when you deregister an instance store-backed ami, it doesn't affect the files that you uploaded to amazon s3 when you created the ami. you'll continue to incur usage costs for these files in amazon s3. therefore, if you are finished with these files, you should delete them. the following diagram illustrates the process for cleaning up your instance store-backed ami.  to clean up your instance store-backed ami deregister the ami using the  command as follows. delete the bundle in amazon s3 using the  (ami tools) command as follows. (optional) if you are finished with an instance that you launched from the ami, you can terminate it using the  command as follows. (optional) if you are finished with the amazon s3 bucket that you uploaded the bundle to, you can delete the bucket. to delete an amazon s3 bucket, open the amazon s3 console, select the bucket, choose actions, and then choose delete. 
this topic explains how to verify the instance identity document using the rsa-2048 signature and the aws rsa-2048 public certificate. importantto validate the instance identity document using the rsa-2048 signature, you must request the aws rsa-2048 public certificate from .  to verify the instance identity document using the rsa-2048 signature and the aws rsa-2048 public certificate connect to the instance. retrieve the rsa-2048 signature from the instance metadata and add it to a file named . add the  header to the  file. retrieve the rsa-2048 signature from the instance metadata and append it to the  file. use one of the following commands depending on the imds version used by the instance. append the  footer to a new line in the  file. add the contents of the instance identity document from the instance metadata to a file named . use one of the following commands depending on the imds version used by the instance.add the aws rsa-2048 public certificate to a file named . create the  file. open the  file using your preferred text editor and add the contents of the aws rsa-2048 public certificate that you received from aws support. save and close the file. use the openssl smime command to verify the signature. include the  option to indicate that the signature needs to be verified, and the  option to indicate that the certificate does not need to be verified. if the signature is valid, the  message appears. if the signature cannot be verified, contact aws support. 
the following examples explain credit use for instances that are configured as . topics in this example, you see the cpu utilization of a  instance launched as , and how it spends earned and surplus credits to sustain cpu utilization. a  instance earns 144 cpu credits over a rolling 24-hour period, which it can redeem for 144 minutes of vcpu use. when it depletes its cpu credit balance (represented by the cloudwatch metric ), it can spend surplus cpu credits—that it has not yet earned—to burst for as long as it needs. because a  instance earns a maximum of 144 credits in a 24-hour period, it can spend surplus credits up to that maximum without being charged immediately. if it spends more than 144 cpu credits, it is charged for the difference at the end of the hour. the intent of the example, illustrated by the following graph, is to show how an instance can burst using surplus credits even after it depletes its . the following workflow references the numbered points on the graph: p1 – at 0 hours on the graph, the instance is launched as  and immediately begins to earn credits. the instance remains idle from the time it is launched—cpu utilization is 0%—and no credits are spent. all unspent credits are accrued in the credit balance. for the first 24 hours,  is at 0, and the  value reaches its maximum of 144. p2 – for the next 12 hours, cpu utilization is at 2.5%, which is below the 5% baseline. the instance earns more credits than it spends, but the  value cannot exceed its maximum of 144 credits. p3 – for the next 24 hours, cpu utilization is at 7% (above the baseline), which requires a spend of 57.6 credits. the instance spends more credits than it earns, and the  value reduces to 86.4 credits. p4 – for the next 12 hours, cpu utilization decreases to 2.5% (below the baseline), which requires a spend of 36 credits. in the same time, the instance earns 72 credits. the instance earns more credits than it spends, and the  value increases to 122 credits. p5 – for the next 5 hours, the instance bursts at 100% cpu utilization, and spends a total of 570 credits to sustain the burst. about an hour into this period, the instance depletes its entire  of 122 credits, and starts to spend surplus credits to sustain the high cpu utilization, totaling 448 surplus credits in this period (570-122=448). when the  value reaches 144 cpu credits (the maximum a  instance can earn in a 24-hour period), any surplus credits spent thereafter cannot be offset by earned credits. the surplus credits spent thereafter amounts to 304 credits (448-144=304), which results in a small additional charge at the end of the hour for 304 credits. p6 – for the next 13 hours, cpu utilization is at 5% (the baseline). the instance earns as many credits as it spends, with no excess to pay down the . the  value remains at 144 credits. p7 – for the last 24 hours in this example, the instance is idle and cpu utilization is 0%. during this time, the instance earns 144 credits, which it uses to pay down the .  in this example, you see the cpu utilization of a  instance launched as , and how it spends earned and surplus credits to sustain cpu utilization. a  instance earns 72 cpu credits over a rolling 24-hour period, which it can redeem for 72 minutes of vcpu use. when it depletes its cpu credit balance (represented by the cloudwatch metric ), it can spend surplus cpu credits—that it has not yet earned—to burst for as long as it needs. because a  instance earns a maximum of 72 credits in a 24-hour period, it can spend surplus credits up to that maximum without being charged immediately. if it spends more than 72 cpu credits, it is charged for the difference at the end of the hour. the intent of the example, illustrated by the following graph, is to show how an instance can burst using surplus credits even after it depletes its . you can assume that, at the start of the time line in the graph, the instance has an accrued credit balance equal to the maximum number of credits it can earn in 24 hours. the following workflow references the numbered points on the graph:  1 – in the first 10 minutes,  is at 0, and the  value remains at its maximum of 72. 2 – at 23:40, as cpu utilization increases, the instance spends cpu credits and the  value decreases. 3 – at around 00:47, the instance depletes its entire , and starts to spend surplus credits to sustain high cpu utilization. 4 – surplus credits are spent until 01:55, when the  value reaches 72 cpu credits. this is equal to the maximum a  instance can earn in a 24-hour period. any surplus credits spent thereafter cannot be offset by earned credits within the 24-hour period, which results in a small additional charge at the end of the hour. 5 – the instance continues to spend surplus credits until around 02:20. at this time, cpu utilization falls below the baseline, and the instance starts to earn credits at 3 credits per hour (or 0.25 credits every 5 minutes), which it uses to pay down the . after the  value reduces to 0, the instance starts to accrue earned credits in its  at 0.25 credits every 5 minutes.  calculating the billsurplus credits cost $0.05 per vcpu-hour. the instance spent approximately 25 surplus credits between 01:55 and 02:20, which is equivalent to 0.42 vcpu-hours. additional charges for this instance are 0.42 vcpu-hours x $0.05/vcpu-hour = $0.021, rounded to $0.02. here is the month-end bill for this t2 unlimited instance:  you can set billing alerts to be notified every hour of any accruing charges, and take action if required. 
aws compute optimizer provides amazon ec2 instance recommendations to help you improve performance, save money, or both. you can use these recommendations to decide whether to move to a new instance type. to make recommendations, compute optimizer analyzes your existing instance specifications and utilization metrics. the compiled data is then used to recommend which amazon ec2 instance types are best able to handle the existing workload. recommendations are returned along with per-hour instance pricing. this topic outlines how to view recommendations through the amazon ec2 console. for more information, see the . noteto get recommendations from compute optimizer, you must first opt in to compute optimizer. for more information, see  in the aws compute optimizer user guide. topics compute optimizer currently generates recommendations for m, c, r, t, and x instance types. other instance types are not considered by compute optimizer. if you're using other instance types, they will not be listed in the compute optimizer recommendations view. for information about these and other instance types, see . compute optimizer classifies its findings for ec2 instances as follows: under-provisioned – an ec2 instance is considered under-provisioned when at least one specification of your instance, such as cpu, memory, or network, does not meet the performance requirements of your workload. under-provisioned ec2 instances might lead to poor application performance. over-provisioned – an ec2 instance is considered over-provisioned when at least one specification of your instance, such as cpu, memory, or network, can be sized down while still meeting the performance requirements of your workload, and when no specification is under-provisioned. over-provisioned ec2 instances might lead to unnecessary infrastructure cost. optimized – an ec2 instance is considered optimized when all specifications of your instance, such as cpu, memory, and network, meet the performance requirements of your workload, and the instance is not over-provisioned. an optimized ec2 instance runs your workloads with optimal performance and infrastructure cost. for optimized instances, compute optimizer might sometimes recommend a new generation instance type. none – there are no recommendations for this instance. this might occur if you've been opted in to compute optimizer for less than 12 hours, or when the instance has been running for less than 30 hours, or when the instance type is not supported by compute optimizer. for more information, see  in the previous section.after you opt in to compute optimizer, you can view the findings that compute optimizer generates for your ec2 instances in the ec2 console. you can then access the compute optimizer console to view the recommendations. if you recently opted in, findings might not be reflected in the ec2 console for up to 12 hours.  to view a recommendation for an ec2 instance through the ec2 console open the amazon ec2 console at . in the navigation pane, choose instances. select an instance, and on the description tab, inspect the finding field. choose view detail. the instance opens in compute optimizer, where it is labeled as the current instance. up to three different instance type recommendations, labeled option 1, option 2, and option 3, are provided. the bottom half of the window shows recent cloudwatch metric data for the current instance: cpu utilization, memory utilization, network in, and network out.  (optional) in the compute optimizer console, choose the settings () icon to change the visible columns in the table, or to view the public pricing information for a different purchasing option for the current and recommended instance types. noteif you’ve purchased a reserved instance, your on-demand instance might be billed as a reserved instance. before you change your current instance type, first evaluate the impact on reserved instance utilization and coverage. determine whether you want to use one of the recommendations. decide whether to optimize for performance improvement, for cost reduction, or for a combination of the two. for more information, see  in the aws compute optimizer user guide. to view recommendations for all ec2 instances across all regions through the compute optimizer console open the compute optimizer console at . choose view recommendations for all ec2 instances. you can perform the following actions on the recommendations page: to filter recommendations to one or more aws regions, enter the name of the region in the filter by one or more regions text box, or choose one or more regions in the drop-down list that appears.  to view recommendations for resources in another account, choose account, and then select a different account id.  this option is available only if you are signed in to a master account of an organization, and you opted in all member accounts within the organization.  to clear the selected filters, choose clear filters.  to change the purchasing option that is displayed for the current and recommended instance types, choose the settings () icon , and then choose on-demand instances, reserved instances, standard 1-year no upfront, or reserved instances, standard 3-year no upfront.  to view details, such as additional recommendations and a comparison of utilization metrics, choose the finding (under-provisioned, over-provisioned, or optimized) listed next to the desired instance. for more information, see  in the aws compute optimizer user guide. before changing an instance type, consider the following: the recommendations don’t forecast your usage. recommendations are based on your historical usage over the most recent 14-day time period. be sure to choose an instance type that is expected to meet your future resource needs.focus on the graphed metrics to determine whether actual usage is lower than instance capacity. you can also view metric data (average, peak, percentile) in cloudwatch to further evaluate your ec2 instance recommendations. for example, notice how cpu percentage metrics change during the day and whether there are peaks that need to be accommodated. for more information, see  in the amazon cloudwatch user guide. compute optimizer might supply recommendations for burstable performance instances, which are t3, t3a, and t2 instances. if you periodically burst above the baseline, make sure that you can continue to do so based on the vcpus of the new instance type. for more information, see .if you’ve purchased a reserved instance, your on-demand instance might be billed as a reserved instance. before you change your current instance type, first evaluate the impact on reserved instance utilization and coverage.consider conversions to newer generation instances, where possible.when migrating to a different instance family, make sure the current instance type and the new instance type are compatible, for example, in terms of virtualization, architecture, or network type. for more information, see .finally, consider the performance risk rating that's provided for each recommendation. performance risk indicates the amount of effort you might need to spend in order to validate whether the recommended instance type meets the performance requirements of your workload. we also recommend rigorous load and performance testing before and after making any changes.there are other considerations when resizing an ec2 instance. for more information, see . additional resources 
when resources are created, we assign each resource a unique resource id. you can use resource ids to find your resources in the amazon ec2 console. if you are using a command line tool or the amazon ec2 api to work with amazon ec2, resource ids are required for certain commands. for example, if you are using the  aws cli command to stop an instance, you must specify the instance id in the command. resource id lengtha resource id takes the form of a resource identifier (such as  for a snapshot) followed by a hyphen and a unique combination of letters and numbers. starting in january 2016, we're gradually introducing longer length ids for amazon ec2 and amazon ebs resource types. the length of the alphanumeric character combination was in an 8-character format; the new ids are in a 17-character format, for example,  for an instance id. supported resource types have an opt-in period, during which you can choose a resource id format, and a deadline date, after which the resource defaults to the longer id format. after the deadline has passed for a specific resource type, you can no longer disable the longer id format for that resource type. different resource types have different opt-in periods and deadline dates. the following table lists the supported resource types, along with their opt-in periods and deadline dates. during the opt-in periodyou can enable or disable longer ids for a resource at any time during the opt-in period. after you've enabled longer ids for a resource type, any new resources that you create are created with a longer id. notea resource id does not change after it's created. therefore, enabling or disabling longer ids during the opt-in period does not affect your existing resource ids. depending on when you created your aws account, supported resource types may default to using longer ids. however, you can opt out of using longer ids until the deadline date for that resource type. for more information, see  in the amazon ec2 faqs. after the deadlineyou can’t disable longer ids for a resource type after its deadline date has passed. any new resources that you create are created with a longer id. you can enable or disable longer ids per iam user and iam role. by default, an iam user or role defaults to the same settings as the root user.  topics you can use the console and command line tools to view the resource types that support longer ids. to view your longer id settings using the console open the amazon ec2 console at . in the navigation bar at the top of the screen, select the region for which to view your longer id settings.  from the dashboard, under account attributes, choose resource id length management.  expand advanced resource id management to view the resource types that support longer ids and their deadline dates. to view your longer id settings using the command line use one of the following commands:  (aws cli)  (aws tools for windows powershell) to view longer id settings for a specific iam user or iam role using the command line use one of the following commands and specify the arn of an iam user, iam role, or root account user in the request.  (aws cli)  (aws tools for windows powershell) to view the aggregated longer id settings for a specific region using the command lineuse the  aws cli command to view the aggregated longer id setting for the entire region, as well as the aggregated longer id setting of all arns for each resource type. this command is useful for performing a quick audit to determine whether a specific region is fully opted in for longer ids. to identify users who have explicitly defined custom longer id settingsuse the  aws cli command to view the longer id format settings for the root user and all iam roles and iam users that have explicitly specified a longer id preference. this command is useful for identifying iam users and iam roles that have overridden the default longer id settings. you can use the console and command line tools to modify longer id settings for resource types that are still within their opt-in period. notethe aws cli and aws tools for windows powershell commands in this section are per-region only. they apply to the default region unless otherwise specified. to modify the settings for other regions, include the  parameter in the command. to modify longer id settings using the console open the amazon ec2 console at . in the navigation bar at the top of the screen, select the region for which to modify the longer id settings.  from the dashboard, under account attributes, choose resource id length management.  do one of the following: to enable longer ids for all supported resource types for all iam users across all regions, choose switch to longer ids, yes, switch to longer ids. importantiam users and iam roles need the  permission to perform this action.to modify longer id settings for a specific resource type for your iam user account, expand advanced resource id management, and then select the corresponding check box in the my iam role/user column to enable longer ids, or clear the check box to disable longer ids.to modify longer id settings for a specific resource type for all iam users, expand advanced resource id management, and then select the corresponding check box in the all iam roles/users column to enable longer ids, or clear the check box to disable longer ids.to modify longer id settings for your iam user account using the command line use one of the following commands: noteif you’re using these commands as the root user, then changes apply to the entire aws account, unless an iam user or role explicitly overrides these settings for themselves.  (aws cli) you can also use the command to modify the longer id settings for all supported resource types. to do this, replace the  parameter with . noteto disable longer ids, replace the  parameter with .  (aws tools for windows powershell) you can also use the command to modify the longer id settings for all supported resource types. to do this, replace the  parameter with . to modify longer id settings for a specific iam user or iam role using the command line use one of the following commands and specify the arn of an iam user, iam role, or root user in the request.  (aws cli) you can also use the command to modify the longer id settings for all supported resource types. to do this, specify  for the  parameter. noteto disable longer ids, replace the  parameter with .  (aws tools for windows powershell) you can also use the command to modify the longer id settings for all supported resource types. to do this, specify  for the  parameter. by default, iam users and roles do not have permission to use the following actions unless they're explicitly granted permission through their associated iam policies: for example, an iam role may have permission to use all amazon ec2 actions through an  element in the policy statement. to prevent iam users and roles from viewing or modifying the longer resource id settings for themselves or other users and roles in your account, ensure that the iam policy contains the following statement: we do not support resource-level permissions for the following actions: 
you can use the amazon elastic block store (amazon ebs) direct apis to create ebs snapshots, write data directly to your snapshots, read data on your snapshots, and identify the differences or changes between two snapshots. if you’re an independent software vendor (isv) who offers backup services for amazon ebs, the ebs direct apis make it more efficient and cost-effective to track incremental changes on your ebs volumes through snapshots. this can be done without having to create new volumes from snapshots, and then use amazon elastic compute cloud (amazon ec2) instances to compare the differences. you can create incremental snapshots directly from data on-premises into ebs volumes and the cloud to use for quick disaster recovery. with the ability to write and read snapshots, you can write your on-premises data to an ebs snapshot during a disaster. then after recovery, you can restore it back to aws or on-premises from the snapshot. you no longer need to build and maintain complex mechanisms to copy data to and from amazon ebs. this user guide provides an overview of the elements that make up the ebs direct apis, and examples of how to use them effectively. for more information about the actions, data types, parameters, and errors of the apis, see the . for more information about the supported aws regions, endpoints, and service quotas for the ebs direct apis, see  in the aws general reference. topics the following are the key elements that you should understand before getting started with the ebs direct apis. the price that you pay to use the ebs direct apis depends on the requests you make. for more information, see . snapshots are the primary means to back up data from your ebs volumes. with the ebs direct apis, you can also back up data from your on-premises disks to snapshots. to save storage costs, successive snapshots are incremental, containing only the volume data that changed since the previous snapshot. for more information, see . notepublic snapshots are not supported by the ebs direct apis. a block is a fragment of data within a snapshot. each snapshot can contain thousands of blocks. all blocks in a snapshot are of a fixed size. a block index is the offset position of a block within a snapshot, and it is used to identify the block. multiply the blockindex value with the blocksize value (blockindex * blocksize) to identify the logical offset of the data in the logical volume. a block token is the identifying hash of a block within a snapshot, and it is used to locate the block data. block tokens returned by ebs direct apis are temporary. they change on the expiry timestamp specified for them, or if you run another listsnapshotblocks or listchangedblocks request for the same snapshot. a checksum is a small-sized datum derived from a block of data for the purpose of detecting errors that were introduced during its transmission or storage. the ebs direct apis use checksums to validate data integrity. when you read data from an ebs snapshot, the service provides base64-encoded sha256 checksums for each block of data transmitted, which you can use for validation. when you write data to an ebs snapshot, you must provide a base64 encoded sha256 checksum for each block of data transmitted. the service validates the data received using the checksum provided. for more information, see  later in this guide. encryption protects your data by converting it into unreadable code that can be deciphered only by people who have access to the key used to encrypt it. you can use the ebs direct apis to read and write encrypted snapshots, but there are some limitations. for more information, see  later in this guide. the ebs direct apis consists of six actions. there are three read actions and three write actions. the read actions are listsnapshotblocks, listchangedblocks, and getsnapshotblock. the write actions are startsnapshot, putsnapshotblock, and completesnapshot. these actions are described in the following sections. the listsnapshotblocks action returns the block indexes and block tokens of blocks in the specified snapshot. the listchangedblocks action returns the block indexes and block tokens of blocks that are different between two specified snapshots of the same volume and snapshot lineage. the getsnapshotblock action returns the data in a block for the specified snapshot id, block index, and block token. the startsnapshot action starts a snapshot, either as an incremental snapshot of an existing one or as a new snapshot. the started snapshot remains in a pending state until it is completed using the completesnapshot action. the putsnapshotblock action adds data to a started snapshot in the form of individual blocks. you must specify a base64-encoded sha256 checksum for the block of data transmitted. the service validates the checksum after the transmission is completed. the request fails if the checksum computed by the service doesn’t match what you specified. the completesnapshot action completes a started snapshot that is in a pending state. the snapshot is then changed to a completed state. the following steps describe how to use the ebs direct apis to read snapshots: use the listsnapshotblocks action to view all block indexes and block tokens of blocks in a snapshot. or use the listchangedblocks action to view only the block indexes and block tokens of blocks that are different between two snapshots of the same volume and snapshot lineage. these actions help you identify the block tokens and block indexes of blocks for which you might want to get data. use the getsnapshotblock action, and specify the block index and block token of the block for which you want to get data. for examples of how to run these actions, see the  and  sections later in this guide. the following steps describe how to use the ebs direct apis to write incremental snapshots: use the startsnapshot action and specify a parent snapshot id to start a snapshot as an incremental snapshot of an existing one, or omit the parent snapshot id to start a new snapshot. this action returns the new snapshot id, which is in a pending state. use the putsnapshotblock action and specify the id of the pending snapshot to add data to it in the form of individual blocks. you must specify a base64-encoded sha256 checksum for the block of data transmitted. the service computes the checksum of the data received and validates it with the checksum that you specified. the action fails if the checksums don't match. when you're done adding data to the pending snapshot, use the completesnapshot action to start an asynchronous workflow that seals the snapshot and moves it to a completed state. repeat these steps to create a new, incremental snapshot using the previously created snapshot as the parent. for example, in the following diagram, snapshot a is the first new snapshot started. snapshot a is used as the parent snapshot to start snapshot b. snapshot b is used as the parent snapshot to start and create snapshot c. snapshots a, b, and c are incremental snapshots. snapshot a is used to create ebs volume 1. snapshot d is created from ebs volume 1. snapshot d is not an incremental snapshot of a, b or c.  for examples of how to run these actions, see the  and  sections later in this guide. an aws identity and access management (iam) user must have the following policies to use the ebs direct apis. for more information, see . be cautious when assigning the following policies to iam users. by assigning these policies, you might give access to a user who is denied access to the same resource through the amazon ec2 apis, such as the copysnapshot or createvolume actions. the following policy allows the read ebs direct apis to be used on all snapshots in a specific aws region. in the policy, replace  with the region of the snapshot. the following policy allows the read ebs direct apis to be used on snapshots with a specific key-value tag. in the policy, replace  with the key value of the tag, and  with the value of the tag. the following policy allows all of the read ebs direct apis to be used on all snapshots in the account only within a specific time range. this policy authorizes use of the ebs direct apis based on the  global condition key. in the policy, be sure to replace the date and time range shown with the date and time range for your policy. the following policy grants access to decrypt an encrypted snapshot using a specific key id from the aws key management service (aws kms). it grants access to encrypt new snapshots using the default aws kms key id for ebs snapshots. it also provides the ability to determine if encrypt by default is enabled on the account. in the policy, replace  with the region of the aws kms key,  with the id of the aws account of the key, and  with the id of the key used to encrypt the snapshot that you want to read with the ebs direct apis. for more information, see  in the iam user guide. the following policy allows the write ebs direct apis to be used on all snapshots in a specific aws region. in the policy, replace  with the region of the snapshot. the following policy allows the write ebs direct apis to be used on snapshots with a specific key-value tag. in the policy, replace  with the key value of the tag, and  with the value of the tag. the following policy allows all of the ebs direct apis to be used. it also allows the  action only if a parent snapshot id is specified. therefore, this policy blocks the ability to start new snapshots without using a parent snapshot. the following policy allows all of the ebs direct apis to be used. it also allows only the  tag key to be created for a new snapshot. this policy also ensures that the user has access to create tags. the  action is the only action that can specify tags. the following policy allows all of the write ebs direct apis to be used on all snapshots in the account only within a specific time range. this policy authorizes use of the ebs direct apis based on the  global condition key. in the policy, be sure to replace the date and time range shown with the date and time range for your policy. the following policy grants access to decrypt an encrypted snapshot using a specific key id from the aws key management service (aws kms). it grants access to encrypt new snapshots using the default aws kms key id for ebs snapshots. it also provides the ability to determine if encrypt by default is enabled on the account. in the policy, replace  with the region of the aws kms key,  with the id of the aws account of the key, and  with the id of the key used to encrypt the snapshot that you want to read with the ebs direct apis. for more information, see  in the iam user guide. if amazon ebs encryption by default is enabled on your aws account, you cannot start a new snapshot using an un-encrypted parent snapshot. you must first encrypt the parent snapshot by copying it. for more information, see  and . to start an encrypted snapshot, specify the amazon resource name (arn) of an aws kms key, or specify an encrypted parent snapshot in your startsnapshot request. if neither are specified, and amazon ebs encryption by default is enabled on the account, then the default key for ebs encryption is used to encrypt the new snapshot. you might need additional iam permissions to use the ebs direct apis with encryption. for more information, see the  section earlier in this guide. signature version 4 is the process to add authentication information to aws requests sent by http. for security, most requests to aws must be signed with an access key, which consists of an access key id and secret access key. these two keys are commonly referred to as your security credentials. for information about how to obtain credentials for your account, see . noteif you intend to manually create http requests, you must learn how to sign them. when you use the aws command line interface (aws cli) or one of the aws sdks to make requests to aws, these tools automatically sign the requests for you with the access key that you specify when you configure the tools. when you use these tools, you don't need to learn how to sign requests yourself. the process for calculating a signature can be broken into the following tasks, which are documented in the aws general reference:     for more information, see  in the aws general reference. the getsnapshotblock action returns data that is in a block of a snapshot, and the putsnapshotblock action adds data to a block in a snapshot. the block data that is transmitted is not signed as part of the signature version 4 signing process. as a result, checksums are used to validate the integrity of the data as follows: when you use the getsnapshotblock action, the response provides a base64-encoded sha256 checksum for the block data using the x-amz-checksum header, and the checksum algorithm using the x-amz-checksum-algorithm header. use the returned checksum to validate the integrity of the data. if the checksum that you generate doesn't match what amazon ebs provided, you should consider the data not valid and retry your request.when you use the putsnapshotblock action, your request must provide a base64-encoded sha256 checksum for the block data using the x-amz-checksum header, and the checksum algorithm using the x-amz-checksum-algorithm header. the checksum that you provide is validated against a checksum generated by amazon ebs to validate the integrity of the data. if the checksums do not correspond, the request fails.when you use the completesnapshot action, your request can optionally provide an aggregate base64-encoded sha256 checksum for the complete set of data added to the snapshot. provide the checksum using the x-amz-checksum header, the checksum algorithm using the x-amz-checksum-algorithm header, and the checksum aggregation method using the x-amz-checksum-aggregation-method header. to generate the aggregated checksum using the linear aggregation method, arrange the checksums for each written block in ascending order of their block index, concatenate them to form a single string, and then generate the checksum on the entire string using the sha256 algorithm. the checksums in these actions are part of the signature version 4 signing process. the  provides descriptions and syntax for each of the service’s actions and data types. you can also use one of the aws sdks to access an api that's tailored to the programming language or platform that you're using. for more information, see . the ebs direct apis require an aws signature version 4 signature. for more information, see .  the following  example request returns the block indexes and block tokens of blocks that are in snapshot . the  parameter limits the results to block indexes greater than , and the  parameter limits the results to the first  blocks. the following example response for the previous request lists the block indexes and block tokens in the snapshot. use the getsnapshotblock action and specify the block index and block token of the block for which you want to get data. the block tokens are valid until the expiry time listed.  the following  example request returns the block indexes and block tokens of blocks that are different between snapshots  and . the  parameter limits the results to block indexes greater than , and the  parameter limits the results to the first  blocks. the following example response for the previous request shows that block indexes , , , and  are different between the two snapshots. additionally, block indexes , and  exist only in the first snapshot id specified, and not in the second snapshot id because there is no second block token listed in the response. use the  action and specify the block index and block token of the block for which you want to get data. the block tokens are valid until the expiry time listed.  the following  example request returns the data in the block index  with block token , in snapshot . the following example response for the previous request shows the size of the data returned, the checksum to validate the data, and the algorithm used to generate the checksum. the binary data is transmitted in the body of the response and is represented as blockdata in the following example. the following  example request starts an  gib snapshot, using snapshot  as the parent snapshot. the new snapshot will be an incremental snapshot of the parent snapshot. the snapshot moves to an error state if there are no put or complete requests made for the snapshot within the specified  minute timeout period. the  client token ensures idempotency for the request. if the client token is omitted, the aws sdk automatically generates one for you. for more information about idempotency, see . the following example response for the previous request shows the snapshot id, aws account id, status, volume size in gib, and size of the blocks in the snapshot. the snapshot is started in a pending state. specify the snapshot id in a subsequent  request to write data to the snapshot. the following  example request writes  bytes of data to block index  on snapshot . the base64 encoded  checksum was generated using the  algorithm. the data is transmitted in the body of the request and is represented as blockdata in the following example. the following is example response for the previous request confirms the data length, checksum, and checksum algorithm for the data received by the service.  the following  example request completes snapshot . the command specifies that  blocks were written to the snapshot. the  checksum represents the checksum for the complete set of data written to a snapshot. the following is an example response for the previous request. the following examples show how to use the ebs direct apis using the aws command line interface (aws cli). for more information about installing and configuring the aws cli, see  and . the following  example command returns the block indexes and block tokens of blocks that are in snapshot . the  parameter limits the results to block indexes greater than , and the  parameter limits the results to the first  blocks. the following example response for the previous command lists the block indexes and block tokens in the snapshot. use the  command and specify the block index and block token of the block for which you want to get data. the block tokens are valid until the expiry time listed. the following  example command returns the block indexes and block tokens of blocks that are different between snapshots  and . the  parameter limits the results to block indexes greater than , and the  parameter limits the results to the first  blocks.. the following example response for the previous command shows that block indexes 0, 6000, 6001, 6002, and 6003 are different between the two snapshots. additionally, block indexes 6001, 6002, and 6003 exist only in the first snapshot id specified, and not in the second snapshot id because there is no second block token listed in the response. use the  command and specify the block index and block token of the block for which you want to get data. the block tokens are valid until the expiry time listed. the following  example command returns the data in the block index  with block token , in snapshot . the binary data is output to the  file in the  directory on a windows computer. if you run the command on a linux or unix computer, replace the output path with  to output the data to the  file in the  directory. the following example response for the previous command shows the size of the data returned, the checksum to validate the data, and the algorithm of the checksum. the binary data is automatically saved to the directory and file you specified in the request command. the following  example command starts an  gib snapshot, using snapshot  as the parent snapshot. the new snapshot will be an incremental snapshot of the parent snapshot. the snapshot moves to an error state if there are no put or complete requests made for the snapshot within the specified  minute timeout period. the  client token ensures idempotency for the request. if the client token is omitted, the aws sdk automatically generates one for you. for more information about idempotency, see . the following example response for the previous command shows the snapshot id, aws account id, status, volume size in gib, and size of the blocks in the snapshot. the snapshot is started in a  state. specify the snapshot id in subsequent  commands to write data to the snapshot, then use the  command to complete the snapshot and change its status to . the following  example command writes  bytes of data to block index  on snapshot . the base64 encoded  checksum was generated using the  algorithm. the data that is transmitted is in the  file. the following example response for the previous command confirms the data length, checksum, and checksum algorithm for the data received by the service. the following  example command completes snapshot . the command specifies that  blocks were written to the snapshot. the  checksum represents the checksum for the complete set of data written to a snapshot. for more information about checksums, see  earlier in this guide. the following is an example response for the previous command. you can run api requests concurrently. assuming putsnapshotblock latency is 100ms, then a thread can process 10 requests in one second. furthermore, assuming your client application creates multiple threads and connections (for example, 100 connections), it can make 1000 (10 * 100) requests per second in total. this will correspond to a throughput of around 500 mb per second. the following list contains few things to look for in your application: is each thread using a separate connection? if the connections are limited on the application then multiple threads will wait for the connection to be available and you will notice lower throughput.is there any wait time in the application between two put requests? this will reduce the effective throughput of a thread.the bandwidth limit on the instance – if bandwidth on the instance is shared by other applications, it could limit the available throughput for putsnapshotblock requests.be sure to take note of other workloads that might be running in the account to avoid bottlenecks. you should also build retry mechanisms into your ebs direct apis workflows to handle throttling, timeouts, and service unavailability. review the ebs direct apis service quotas to determine the maximum api requests that you can run per second. for more information, see  in the aws general reference. can a snapshot be accessed using the ebs direct apis if it has a pending status?no. the snapshot can be accessed only if it has a completed status. are the block indexes returned by the ebs direct apis in numerical order?yes. the block indexes returned are unique, and in numerical order. can i submit a request with a maxresults parameter value of under 100?no. the minimum maxresult parameter value you can use is 100. if you submit a request with a maxresult parameter value of under 100, and there are more than 100 blocks in the snapshot, then the api will return at least 100 results. can i run api requests concurrently?you can run api requests concurrently. be sure to take note of other workloads that might be running in the account to avoid bottlenecks. you should also build retry mechanisms into your ebs direct apis workflows to handle throttling, timeouts, and service unavailability. for more information, see .review the ebs direct apis service quotas to determine the api requests that you can run per second. for more information, see  in the aws general reference. when running the listchangedblocks action, is it possible to get an empty response even though there are blocks in the snapshot?yes. if the changed blocks are scarce in the snapshot, the response may be empty but the api will return a next page token value. use the next page token value to continue to the next page of results. you can confirm that you have reached the last page of results when the api returns a next page token value of null. if the nexttoken parameter is specified together with a startingblockindex parameter, which of the two is used?the nexttoken is used, and the startingblockindex is ignored. how long are the block tokens and next tokens valid?block tokens are valid for seven days, and next tokens are valid for 60 minutes. are encrypted snapshots supported?yes. encrypted snapshots can be accessed using the ebs direct apis.to access an encrypted snapshot, the user must have access to the key used to encrypt the snapshot, and the aws kms decrypt action. see the  section earlier in this guide for the aws kms policy to assign to a user. are public snapshots supported?public snapshots are not supported. does list snapshot block return all block indexes and block tokens in a snapshot, or only those that have data written to them?it returns only block indexes and tokens that have data written to them. can i get a history of the api calls made by the ebs direct apis on my account for security analysis and operational troubleshooting purposes?yes. to receive a history of ebs direct apis api calls made on your account, turn on aws cloudtrail in the aws management console. for more information, see . 
you can stop and start your instance if it has an amazon ebs volume as its root device. the instance retains its instance id, but can change as described in the  section. when you stop an instance, we shut it down. we don't charge usage for a stopped instance, or data transfer fees, but we do charge for the storage for any amazon ebs volumes. each time you start a stopped instance we charge a minimum of one minute for usage. after one minute, we charge only for the seconds you use. for example, if you run an instance for 20 seconds and then stop it, we charge for a full one minute. if you run an instance for 3 minutes and 40 seconds, we charge for exactly 3 minutes and 40 seconds of usage. while the instance is stopped, you can treat its root volume like any other volume, and modify it (for example, repair file system problems or update software). you just detach the volume from the stopped instance, attach it to a running instance, make your changes, detach it from the running instance, and then reattach it to the stopped instance. make sure that you reattach it using the storage device name that's specified as the root device in the block device mapping for the instance. if you decide that you no longer need an instance, you can terminate it. as soon as the state of an instance changes to  or , we stop charging for that instance. for more information, see . if you'd rather hibernate the instance, see . for more information, see . topics you can only stop an amazon ebs-backed instance. to verify the root device type of your instance, describe the instance and check whether the device type of its root volume is  (amazon ebs-backed instance) or  (instance store-backed instance). for more information, see . when you stop a running instance, the following happens: the instance performs a normal shutdown and stops running; its status changes to  and then .any amazon ebs volumes remain attached to the instance, and their data persists.any data stored in the ram of the host computer or the instance store volumes of the host computer is gone.in most cases, the instance is migrated to a new underlying host computer when it's started.the instance retains its private ipv4 addresses and any ipv6 addresses when stopped and started. we release the public ipv4 address and assign a new one when you start it.the instance retains its associated elastic ip addresses. you're charged for any elastic ip addresses associated with a stopped instance. with ec2-classic, an elastic ip address is dissociated from your instance when you stop it. for more information, see .when you stop and start a windows instance, the ec2config service performs tasks on the instance, such as changing the drive letters for any attached amazon ebs volumes. for more information about these defaults and how you can change them, see  in the amazon ec2 user guide for windows instances.if your instance is in an auto scaling group, the amazon ec2 auto scaling service marks the stopped instance as unhealthy, and may terminate it and launch a replacement instance. for more information, see  in the amazon ec2 auto scaling user guide.when you stop a classiclink instance, it's unlinked from the vpc to which it was linked. you must link the instance to the vpc again after starting it. for more information about classiclink, see .for more information, see . you can modify the following attributes of an instance only when it is stopped: instance typeuser datakernelram diskif you try to modify these attributes while the instance is running, amazon ec2 returns the  error. when an ec2 instance is stopped using the  command, the following is registered at the os level: the api request will send a button press event to the guest.various system services will be stopped as a result of the button press event. systemd handles a graceful shutdown of the system. graceful shutdown is triggered by the acpi shutdown button press event from the hypervisor.acpi shutdown will be initiated.the instance will shut down when the graceful shutdown process exits. there is no configurable os shutdown time.if the instance os does not shut down cleanly within four minutes, a hard reboot is performed.you can stop and start your amazon ebs-backed instance using the console or the command line. by default, when you initiate a shutdown from an amazon ebs-backed instance (using the shutdown or poweroff command), the instance stops. you can change this behavior so that it terminates instead. for more information, see . to stop and start an amazon ebs-backed instance using the console in the navigation pane, choose instances, and select the instance. choose actions, instance state, stop. if stop is disabled, either the instance is already stopped or its root device is an instance store volume. warningwhen you stop an instance, the data on any instance store volumes is erased. to keep data from instance store volumes, be sure to back it up to persistent storage. in the confirmation dialog box, choose yes, stop. it can take a few minutes for the instance to stop. while your instance is stopped, you can modify certain instance attributes. for more information, see . to start the stopped instance, select the instance, and choose actions, instance state, start. in the confirmation dialog box, choose yes, start. it can take a few minutes for the instance to enter the  state. to stop and start an amazon ebs-backed instance using the command line you can use one of the following commands. for more information about these command line interfaces, see .  and  (aws cli) and  (aws tools for windows powershell)you can change the instance type, user data, and ebs-optimization attributes of a stopped instance using the aws management console or the command line interface. you can't use the aws management console to modify the , kernel, or ram disk attributes. to modify an instance attribute to change the instance type, see .to change the user data for your instance, see .to enable or disable ebs–optimization for your instance, see .to change the  attribute of the root volume for your instance, see . you are not required to stop the instance to change this attribute. to modify an instance attribute using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)if you have stopped your amazon ebs-backed instance and it appears "stuck" in the  state, you can forcibly stop it. for more information, see . 
a spot instance is an unused ec2 instance that is available for less than the on-demand price. because spot instances enable you to request unused ec2 instances at steep discounts, you can lower your amazon ec2 costs significantly. the hourly price for a spot instance is called a spot price. the spot price of each instance type in each availability zone is set by amazon ec2, and adjusted gradually based on the long-term supply of and demand for spot instances. your spot instance runs whenever capacity is available and the maximum price per hour for your request exceeds the spot price. spot instances are a cost-effective choice if you can be flexible about when your applications run and if your applications can be interrupted. for example, spot instances are well-suited for data analysis, batch jobs, background processing, and optional tasks. for more information, see . topics before you get started with spot instances, you should be familiar with the following concepts: spot instance pool – a set of unused ec2 instances with the same instance type (for example, ), operating system, availability zone, and network platform.spot price – the current price of a spot instance per hour.spot instance request – requests a spot instance. the request provides the maximum price per hour that you are willing to pay for a spot instance. if you don't specify a maximum price, the default maximum price is the on-demand price. when the maximum price per hour for your request exceeds the spot price, amazon ec2 fulfills your request if capacity is available. a spot instance request is either one-time or persistent. amazon ec2 automatically resubmits a persistent spot instance request after the spot instance associated with the request is terminated. your spot instance request can optionally specify a duration for the spot instances.spot fleet – a set of spot instances that is launched based on criteria that you specify. the spot fleet selects the spot instance pools that meet your needs and launches spot instances to meet the target capacity for the fleet. by default, spot fleets are set to maintain target capacity by launching replacement instances after spot instances in the fleet are terminated. you can submit a spot fleet as a one-time request, which does not persist after the instances have been terminated. you can include on-demand instance requests in a spot fleet request.spot instance interruption – amazon ec2 terminates, stops, or hibernates your spot instance when the spot price exceeds the maximum price for your request or capacity is no longer available. amazon ec2 provides a spot instance interruption notice, which gives the instance a two-minute warning before it is interrupted.the following table lists the key differences between spot instances and on-demand instances. one strategy is to maintain a minimum level of guaranteed compute resources for your applications by launching a core group of on-demand instances, and supplementing them with spot instances when the opportunity arises.  another strategy is to launch spot instances with a specified duration (also known as spot blocks), which are designed not to be interrupted and will run continuously for the duration you select. in rare situations, spot blocks may be interrupted due to amazon ec2 capacity needs. in these cases, we provide a two-minute warning before we terminate an instance, and you are not charged for the terminated instances even if you used them. for more information, see . the first thing you need to do is get set up to use amazon ec2. it can also be helpful to have experience launching on-demand instances before launching spot instances. get up and running spot basics  working with spot instances working with spot fleets you can provision spot instances directly using amazon ec2. you can also provision spot instances using other services in aws. for more information, see the following documentation. amazon ec2 auto scaling and spot instancesyou can create launch templates or configurations with the maximum price that you are willing to pay, so that amazon ec2 auto scaling can launch spot instances. for more information, see  and  in the amazon ec2 auto scaling user guide. amazon emr and spot instancesthere are scenarios where it can be useful to run spot instances in an amazon emr cluster. for more information, see  and  in the amazon emr management guide. aws cloudformation templatesaws cloudformation enables you to create and manage a collection of aws resources using a template in json format. aws cloudformation templates can include the maximum price you are willing to pay. for more information, see . aws sdk for javayou can use the java programming language to manage your spot instances. for more information, see  and . aws sdk for .netyou can use the .net programming environment to manage your spot instances. for more information, see . you pay the spot price for spot instances, which is set by amazon ec2 and adjusted gradually based on the long-term supply of and demand for spot instances. if the maximum price for your request exceeds the current spot price, amazon ec2 fulfills your request if capacity is available. your spot instances run until you terminate them, capacity is no longer available, the spot price exceeds your maximum price, or your amazon ec2 auto scaling group terminates them during . spot instances with a predefined duration use a fixed hourly price that remains in effect for the spot instance while it runs. if you or amazon ec2 interrupts a running spot instance, you are charged for the seconds used or the full hour, or you receive no charge, depending on the operating system used and who interrupted the spot instance. for more information, see . to view the current (updated every five minutes) lowest spot price per aws region and instance type, see the  page. to view the spot price history for the past three months, use the amazon ec2 console or the  command (aws cli). for more information, see . we independently map availability zones to codes for each aws account. therefore, you can get different results for the same availability zone code (for example, ) between different accounts. you can view the savings made from using spot instances for a single spot fleet or for all spot instances. you can view the savings made in the last hour or the last three days, and you can view the average cost per vcpu hour and per memory (gib) hour. savings are estimated and may differ from actual savings because they do not include the billing adjustments for your usage. for more information about viewing savings information, see . to review your bill, go to your . your bill contains links to usage reports that provide details about your bill. for more information, see . if you have questions concerning aws billing, accounts, and events, . 
you can improve the security posture of your vpc by configuring amazon ec2 to use an interface vpc endpoint. interface endpoints are powered by aws privatelink, a technology that enables you to privately access amazon ec2 apis by restricting all network traffic between your vpc and amazon ec2 to the amazon network. with interface endpoints, you also don't need an internet gateway, a nat device, or a virtual private gateway. you are not required to configure aws privatelink, but it's recommended. for more information about aws privatelink and vpc endpoints, see . topics create an endpoint for amazon ec2 using the following service name:  — creates an endpoint for the amazon ec2 api actions.for more information, see  in the amazon vpc user guide. you can attach a policy to your vpc endpoint to control access to the amazon ec2 api. the policy specifies: the principal that can perform actions.the actions that can be performed.the resource on which the actions can be performed.importantwhen a non-default policy is applied to an interface vpc endpoint for amazon ec2, certain failed api requests, such as those failing from , might not be logged to aws cloudtrail or amazon cloudwatch. for more information, see  in the  amazon vpc user guide. the following example shows a vpc endpoint policy that denies permission to create unencrypted volumes or to launch instances with unencrypted volumes. the example policy also grants permission to perform all other amazon ec2 actions. 
the following issues prevent you from launching an instance. topics you get the  error when you try to launch a new instance or restart a stopped instance. if you get an  error when you try to launch a new instance or restart a stopped instance, you have reached the limit on the number of instances that you can launch in a region. when you create your aws account, we set default limits on the number of instances you can run on a per-region basis. you can request an instance limit increase on a per-region basis. for more information, see . you get the  error when you try to launch a new instance or restart a stopped instance. if you get an  error when you try to launch an instance or restart a stopped instance, aws does not currently have enough available on-demand capacity to service your request. to resolve the issue, try the following: wait a few minutes and then submit your request again; capacity can shift frequently.submit a new request with a reduced number of instances. for example, if you're making a single request to launch 15 instances, try making 3 requests for 5 instances, or 15 requests for 1 instance instead.if you're launching an instance, submit a new request without specifying an availability zone.if you're launching an instance, submit a new request using a different instance type (which you can resize at a later stage). for more information, see .if you are launching instances into a cluster placement group, you can get an insufficient capacity error. for more information, see .try creating an on-demand capacity reservation, which enables you to reserve amazon ec2 capacity for any duration. for more information, see .try purchasing reserved instances, which are a long-term capacity reservation. for more information, see .your instance goes from the  state to the  state immediately after restarting it. the following are a few reasons why an instance might immediately terminate: you've reached your ebs volume limit.an ebs snapshot is corrupt.the root ebs volume is encrypted and you do not have permissions to access the kms key for decryption.the instance store-backed ami that you used to launch the instance is missing a required part (an image.part.xx file).you can use the amazon ec2 console or aws command line interface to get the termination reason. to get the termination reason using the amazon ec2 console open the amazon ec2 console at . in the navigation pane, choose instances, and select the instance. in the description tab, note the reason next to the state transition reason label. to get the termination reason using the aws command line interface use the  command and specify the instance id. review the json response returned by the command and note the values in the  response element. the following code block shows an example of a  response element. to address the issue take one of the following actions depending on the termination reason you noted: if the reason is , you have reached your ebs volume limit. for more information, see . to submit a request to increase your amazon ebs volume limit, complete the aws support center  form. for more information, see .if the reason is , that typically indicates that the root volume is encrypted and that you do not have permissions to access the kms key for decryption. to get permissions to access the required kms key, add the appropriate kms permissions to your iam user. for more information, see  in the aws key management service developer guide.
you can create a point-in-time snapshot of an ebs volume and use it as a baseline for new volumes or for data backup. if you make periodic snapshots of a volume, the snapshots are incremental—the new snapshot saves only the blocks that have changed since your last snapshot. snapshots occur asynchronously; the point-in-time snapshot is created immediately, but the status of the snapshot is  until the snapshot is complete (when all of the modified blocks have been transferred to amazon s3), which can take several hours for large initial snapshots or subsequent snapshots where many blocks have changed. while it is completing, an in-progress snapshot is not affected by ongoing reads and writes to the volume. you can take a snapshot of an attached volume that is in use. however, snapshots only capture data that has been written to your amazon ebs volume at the time the snapshot command is issued. this might exclude any data that has been cached by any applications or the operating system. if you can pause any file writes to the volume long enough to take a snapshot, your snapshot should be complete. however, if you can't pause all file writes to the volume, you should unmount the volume from within the instance, issue the snapshot command, and then remount the volume to ensure a consistent and complete snapshot. you can remount and use your volume while the snapshot status is . to make snapshot management easier, you can tag your snapshots during creation or add tags afterward. for example, you can apply tags describing the original volume from which the snapshot was created, or the device name that was used to attach the original volume to an instance. for more information, see . snapshots that are taken from encrypted volumes are automatically encrypted. volumes that are created from encrypted snapshots are also automatically encrypted. the data in your encrypted volumes and any associated snapshots is protected both at rest and in motion. for more information, see . by default, only you can create volumes from snapshots that you own. however, you can share your unencrypted snapshots with specific aws accounts, or you can share them with the entire aws community by making them public. for more information, see . you can share an encrypted snapshot only with specific aws accounts. for others to use your shared, encrypted snapshot, you must also share the cmk key that was used to encrypt it. users with access to your encrypted snapshot must create their own personal copy of it and then use that copy. your copy of a shared, encrypted snapshot can also be re-encrypted using a different key. for more information, see . noteif you copy a snapshot and encrypt it to a new cmk, a complete (non-incremental) copy is always created, resulting in additional delay and storage costs. you can create multi-volume snapshots, which are point-in-time snapshots for all ebs volumes attached to an ec2 instance. you can also create lifecycle policies to automate the creation and retention of multi-volume snapshots. for more information, see . after the snapshots are created, each snapshot is treated as an individual snapshot. you can perform all snapshot operations, such as restore, delete, and copy across regions or accounts, just as you would with a single volume snapshot. you can also tag your multi-volume snapshots as you would a single volume snapshot. we recommend you tag your multiple volume snapshots to manage them collectively during restore, copy, or retention. multi-volume, crash-consistent snapshots are typically restored as a set. it is helpful to identify the snapshots that are in a crash-consistent set by tagging your set with the instance id, name, or other relevant details. you can also choose to automatically copy tags from the source volume to the corresponding snapshots. this helps you to set the snapshot metadata, such as access policies, attachment information, and cost allocation, to match the source volume.  after it's created, a multi-volume snapshot behaves like any other snapshot. you can perform all operations, such as restore, delete, and copy across regions and accounts. you can also tag your snapshots. we recommend that you tag your multi-volume snapshots to collectively manage them during restore, copy, or retention. after creating your snapshots, they appear in your ec2 console created at the exact point-in-time. the snapshots are collectively managed and, therefore, if any one snapshot for the volume set fails, all of the other snapshots display an error status. you can create, retain, and delete snapshots manually, or you can use amazon data lifecycle manager to manage your snapshots for you. for more information, see . the following considerations apply to creating snapshots: when you create a snapshot for an ebs volume that serves as a root device, you should stop the instance before taking the snapshot.you cannot create snapshots from instances for which hibernation is enabled.you cannot create snapshots from hibernated instances.although you can take a snapshot of a volume while a previous snapshot of that volume is in the  status, having multiple  snapshots of a volume can result in reduced volume performance until the snapshots complete.there is a limit of five  snapshots for a single , , or magnetic volume, and one  snapshot for a single  or  volume. if you receive a  error while trying to create multiple concurrent snapshots of the same volume, wait for one or more of the  snapshots to complete before creating another snapshot of that volume.when a snapshot is created from a volume with an aws marketplace product code, the product code is propagated to the snapshot.use the following procedure to create a snapshot from the specified volume. to create a snapshot using the console open the amazon ec2 console at . choose snapshots under elastic block store in the navigation pane. choose create snapshot. for select resource type, choose volume. for volume, select the volume. (optional) enter a description for the snapshot. (optional) choose add tag to add tags to your snapshot. for each tag, provide a tag key and a tag value. choose create snapshot. to create a snapshot using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)use the following procedure to create a snapshot from the volumes of an instance. to create multi-volume snapshots using the console open the amazon ec2 console at . choose snapshots under elastic block store in the navigation pane. choose create snapshot. for select resource type, choose instance. select the instance id for which you want to create simultaneous backups for all of the attached ebs volumes. multi-volume snapshots support up to 40 ebs volumes per instance. (optional) set exclude root volume. (optional) set copy tags from volume flag to automatically copy tags from the source volume to the corresponding snapshots. this sets snapshot metadata—such as access policies, attachment information, and cost allocation—to match the source volume. (optional) choose add tag to add tags to your snapshot. for each tag, provide a tag key and a tag value. choose create snapshot. during snapshot creation, the snapshots are managed together. if one of the snapshots in the volume set fails, the other snapshots are moved to error status for the volume set. you can monitor the progress of your snapshots using . after the snapshot creation process completes, cloudwatch generates an event that contains the status and all of the relevant snapshots details for the affected instance. to create multi-volume snapshots using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)you can copy snapshots, share snapshots, and create volumes from snapshots. for more information, see the following: 
after you no longer need an amazon ebs volume, you can delete it. after deletion, its data is gone and the volume can't be attached to any instance. however, before deletion, you can store a snapshot of the volume, which you can use to re-create the volume later.  to delete a volume, it must be in the  state (not attached to an instance). for more information, see . to delete an ebs volume using the console open the amazon ec2 console at . in the navigation pane, choose volumes.  select a volume and choose actions, delete volume.  in the confirmation dialog box, choose yes, delete.  to delete an ebs volume using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)
to help you understand the charges for your spot instances, amazon ec2 provides a data feed that describes your spot instance usage and pricing. this data feed is sent to an amazon s3 bucket that you specify when you subscribe to the data feed. data feed files arrive in your bucket typically once an hour, and each hour of usage is typically covered in a single data file. these files are compressed (gzip) before they are delivered to your bucket. amazon ec2 can write multiple files for a given hour of usage where files are large (for example, when file contents for the hour exceed 50 mb before compression). noteif you don't have a spot instance running during a certain hour, you don't receive a data feed file for that hour. topics the spot instance data feed file name uses the following format (with the date and hour in utc):  for example, if your bucket name is  and your prefix is , your file names are similar to the following: the spot instance data feed files are tab-delimited. each line in the data file corresponds to one instance hour and contains the fields listed in the following table. when you subscribe to the data feed, you must specify an amazon s3 bucket to store the data feed files. before you choose an amazon s3 bucket for the data feed, consider the following: you must have  permission to the bucket, which includes permission for the  and  actions. if you're the bucket owner, you have this permission by default. otherwise, the bucket owner must grant your aws account this permission. when you subscribe to a data feed, these permissions are used to update the bucket acl to give the aws data feed account  permission. the aws data feed account writes data feed files to the bucket. if your account doesn't have the required permissions, the data feed files cannot be written to the bucket. noteif you update the acl and remove the permissions for the aws data feed account, the data feed files cannot be written to the bucket. you must resubscribe to the data feed to receive the data feed files.each data feed file has its own acl (separate from the acl for the bucket). the bucket owner has  permission to the data files. the aws data feed account has read and write permissions.if you delete your data feed subscription, amazon ec2 doesn't remove the read and write permissions for the aws data feed account on either the bucket or the data files. you must remove these permissions yourself.to subscribe to your data feed, use the  command. the following is example output: to delete your data feed, use the  command. 
the yum package manager is a great tool for installing software, because it can search all of your enabled repositories for different software packages and also handle any dependencies in the software installation process. importantthis information applies to amazon linux. for information about other distributions, see their specific documentation. to install a package from a repository, use the yum install package command, replacing package with the name of the software to install. for example, to install the links text-based web browser, enter the following command. you can also use yum install to install rpm package files that you have downloaded from the internet. to do this, simply append the path name of an rpm file to the installation command instead of a repository package name. 
the amazon ec2 console provides a launch more like this wizard option that enables you to use a current instance as a base for launching other instances. this option automatically populates the amazon ec2 launch wizard with certain configuration details from the selected instance.  notethe launch more like this wizard option does not clone your selected instance; it only replicates some configuration details. to create a copy of your instance, first create an ami from it, then launch more instances from the ami.alternatively, create a  to store the launch parameters for your instances. the following configuration details are copied from the selected instance into the launch wizard: ami idinstance typeavailability zone, or the vpc and subnet in which the selected instance is locatedpublic ipv4 address. if the selected instance currently has a public ipv4 address, the new instance receives a public ipv4 address - regardless of the selected instance's default public ipv4 address setting. for more information about public ipv4 addresses, see .placement group, if applicableiam role associated with the instance, if applicableshutdown behavior setting (stop or terminate)termination protection setting (true or false)cloudwatch monitoring (enabled or disabled)amazon ebs-optimization setting (true or false)tenancy setting, if launching into a vpc (shared or dedicated)kernel id and ram disk id, if applicableuser data, if specifiedtags associated with the instance, if applicable security groups associated with the instancethe following configuration details are not copied from your selected instance; instead, the wizard applies their default settings or behavior: number of network interfaces: the default is one network interface, which is the primary network interface (eth0).storage: the default storage configuration is determined by the ami and the instance type.to use your current instance as a template on the instances page, select the instance you want to use. choose actions, and then launch more like this. the launch wizard opens on the review instance launch page. you can check the details of your instance, and make any necessary changes by clicking the appropriate edit link.  when you are ready, choose launch to select a key pair and launch your instance. if the instance fails to launch or the state immediately goes to  instead of , see . 
after you launch your instance, you can connect to it and use it the way that you'd use a computer sitting in front of you. the following instructions explain how to connect to your instance using an ssh client. if you receive an error while attempting to connect to your instance, see . for more connection options, see . before you connect to your linux instance, complete the following prerequisites. verify that the instance is readyafter you launch an instance, it can take a few minutes for the instance to be ready so that you can connect to it. check that your instance has passed its status checks. you can view this information in the status checks column on the instances page. verify the general prerequisites for connecting to your instancefor more information, see . install an ssh client on your local computer as neededyour local computer might have an ssh client installed by default. you can verify this by typing ssh at the command line. if your compute doesn't recognize the command, you can install an ssh client.   recent versions of windows server 2019 and windows 10 - openssh is included as an installable component. for information, see .earlier versions of windows - download and install openssh. for more information, see .linux and macos x - download and install openssh. for more information, see .use the following procedure to connect to your linux instance using an ssh client. if you receive an error while attempting to connect to your instance, see . to connect to your instance using ssh in a terminal window, use the ssh command to connect to the instance. you specify the path and file name of the private key (), the user name for your instance, and the public dns name or ipv6 address for your instance. for more information about how to find the private key, the user name for your instance, and the dns name or ipv6 address for an instance, see  and . to connect to your instance, use one of the following commands. (public dns) to connect using your instance's public dns name, enter the following command. (ipv6) alternatively, if your instance has an ipv6 address, to connect using your instance's ipv6 address, enter the following command. you see a response like the following: (optional) verify that the fingerprint in the security alert matches the fingerprint that you previously obtained in . if these fingerprints don't match, someone might be attempting a "man-in-the-middle" attack. if they match, continue to the next step. enter yes. you see a response like the following: one way to transfer files between your local computer and a linux instance is to use the secure copy protocol (scp). this section describes how to transfer files with scp. the procedure is similar to the procedure for connecting to an instance with ssh.  prerequisites verify the general prerequisites for transferring files to your instance. the general prerequisites for transferring files to an instance are the same as the general prerequisites for connecting to an instance. for more information, see . install an scp client most linux, unix, and apple computers include an scp client by default. if yours doesn't, the openssh project provides a free implementation of the full suite of ssh tools, including an scp client. for more information, see . the following procedure steps you through using scp to transfer a file. if you've already connected to the instance with ssh and have verified its fingerprints, you can start with the step that contains the scp command (step 4). to use scp to transfer a file transfer a file to your instance using the instance's public dns name, or the ipv6 address if your instance has one. for example, if the name of your private key file is , the file to transfer is , the user name for your instance is , and the public dns name of the instance is , or  if your instance has an ipv6 address, use one of the following commands to copy the file to the  home directory. (public dns) to transfer a file to your instance using your instance's public dns name, enter the following command.  (ipv6) alternatively, if your instance has an ipv6 address, to transfer a file using the instance's ipv6 address, enter the following command. the ipv6 address must be enclosed in square brackets (), which must be escaped (). you see a response like the following: (optional) verify that the fingerprint in the security alert matches the fingerprint that you previously obtained in . if these fingerprints don't match, someone might be attempting a "man-in-the-middle" attack. if they match, continue to the next step. enter yes. you see a response like the following: if you receive a "bash: scp: command not found" error, you must first install scp on your linux instance. for some operating systems, this is located in the  package. for amazon linux variants, such as the amazon ecs-optimized ami, use the following command to install scp: to transfer files in the other direction (from your amazon ec2 instance to your local computer), reverse the order of the host parameters. for example, to transfer the  file from your ec2 instance back to the home directory on your local computer as , use of the following commands on your local computer. (public dns) to transfer a file to your instance using your instance's public dns name, enter the following command.  (ipv6) alternatively, if your instance has an ipv6 address, to transfer a file using the instance's ipv6 address, enter the following command. the ipv6 address must be enclosed in square brackets (), which must be escaped (). 
the following information can help you troubleshoot issues with connecting to your instance. for additional help with windows instances, see  in the amazon ec2 user guide for windows instances. topics we recommend that you begin troubleshooting by checking some common causes for issues connecting to your instance. verify the user name for your instanceyou can connect to your instance using the user name for your user account or the default user name for the ami that you used to launch your instance.   get the user name for your user account. for more information about how to create a user account, see . get the default user name for the ami that you used to launch your instance:for amazon linux 2 or the amazon linux ami, the user name is .for a centos ami, the user name is .for a debian ami, the user name is .for a fedora ami, the user name is  or .for a rhel ami, the user name is  or .for a suse ami, the user name is  or .for an ubuntu ami, the user name is .otherwise, if  and  don't work, check with the ami provider.verify that your security group rules allow trafficmake sure your security group rules allow inbound traffic from your public ipv4 address on the proper port. for steps to verify, see  verify that your instance is readyafter you launch an instance, it can take a few minutes for the instance to be ready so that you can connect to it. check your instance to make sure it is running and has passed its status checks.   open the amazon ec2 console at . in the navigation pane, choose instances, and then select your instance. verify that your instance state is running and your status checks have passed.  verify the general prerequisites for connecting to your instancefor more information, see . if you try to connect to your instance and get an error message  or , try the following: check your security group rules. you need a security group rule that allows inbound traffic from your public ipv4 address on the proper port. open the amazon ec2 console at . in the navigation pane, choose instances, and then select your instance. in the description tab at the bottom of the console page, next to security groups, select view inbound rules to display the list of rules that are in effect for the selected instance. for linux instances: when you select view inbound rules, a window will appear that displays the port(s) to which traffic is allowed. verify that there is a rule that allows traffic from your computer to port 22 (ssh). for windows instances: when you select view inbound rules, a window will appear that displays the port(s) to which traffic is allowed. verify that there is a rule that allows traffic from your computer to port 3389 (rdp). each time you restart your instance, a new ip address (and host name) will be assigned. if your security group has a rule that allows inbound traffic from a single ip address, this address may not be static if your computer is on a corporate network or if you are connecting through an internet service provider (isp). instead, specify the range of ip addresses used by client computers. if your security group does not have a rule that allows inbound traffic as described in the previous step, add a rule to your security group. for more information, see . for more information about security group rules, see  in the amazon vpc user guide. check the route table for the subnet. you need a route that sends all traffic destined outside the vpc to the internet gateway for the vpc. open the amazon ec2 console at . in the navigation pane, choose instances, and then select your instance. in the description tab, write down the values of vpc id and subnet id. open the amazon vpc console at . in the navigation pane, choose internet gateways. verify that there is an internet gateway attached to your vpc. otherwise, choose create internet gateway to create an internet gateway. select the internet gateway, and then choose attach to vpc and follow the directions to attach it to your vpc. in the navigation pane, choose subnets, and then select your subnet. on the route table tab, verify that there is a route with  as the destination and the internet gateway for your vpc as the target. if you're connecting to your instance using its ipv6 address, verify that there is a route for all ipv6 traffic () that points to the internet gateway. otherwise, do the following: choose the id of the route table (rtb-xxxxxxxx) to navigate to the route table. on the routes tab, choose edit routes. choose add route, use  as the destination and the internet gateway as the target. for ipv6, choose add route, use  as the destination and the internet gateway as the target. choose save routes. check the network access control list (acl) for the subnet. the network acls must allow inbound and outbound traffic from your local ip address on the proper port. the default network acl allows all inbound and outbound traffic. open the amazon vpc console at . in the navigation pane, choose subnets and select your subnet. on the description tab, find network acl, and choose its id (acl-xxxxxxxx). select the network acl. for inbound rules, verify that the rules allow traffic from your computer. otherwise, delete or modify the rule that is blocking traffic from your computer. for outbound rules, verify that the rules allow traffic to your computer. otherwise, delete or modify the rule that is blocking traffic to your computer. if your computer is on a corporate network, ask your network administrator whether the internal firewall allows inbound and outbound traffic from your computer on port 22 (for linux instances) or port 3389 (for windows instances). if you have a firewall on your computer, verify that it allows inbound and outbound traffic from your computer on port 22 (for linux instances) or port 3389 (for windows instances). check that your instance has a public ipv4 address. if not, you can associate an elastic ip address with your instance. for more information, see . check the cpu load on your instance; the server may be overloaded. aws automatically provides data such as amazon cloudwatch metrics and instance status, which you can use to see how much cpu load is on your instance and, if necessary, adjust how your loads are handled. for more information, see . if your load is variable, you can automatically scale your instances up or down using  and . if your load is steadily growing, you can move to a larger instance type. for more information, see . to connect to your instance using an ipv6 address, check the following:  your subnet must be associated with a route table that has a route for ipv6 traffic () to an internet gateway. your security group rules must allow inbound traffic from your local ipv6 address on the proper port (22 for linux and 3389 for windows).your network acl rules must allow inbound and outbound ipv6 traffic.if you launched your instance from an older ami, it may not be configured for dhcpv6 (ipv6 addresses are not automatically recognized on the network interface). for more information, see  in the amazon vpc user guide.your local computer must have an ipv6 address, and must be configured to use ipv6. if you try to connect to your instance and get the error message, , the file in which the private key is stored is incorrectly configured. if the private key file ends in , it might still be incorrectly configured. a possible cause for an incorrectly configured private key file is a missing certificate. if the private key file is incorrectly configured, follow these steps to resolve the error create a new key pair. for more information, see . add the new key pair to your instance. for more information, see . connect to your instance using the new key pair. if you use ssh to connect to your instance use  to get triple verbose debugging information while connecting: the following sample output demonstrates what you might see if you were trying to connect to your instance with a key that was not recognized by the server: if you use putty to connect to your instance verify that your private key (.pem) file has been converted to the format recognized by putty (.ppk). for more information about converting your private key, see . notein puttygen, load your private key file and select save private key rather than generate. verify that you are connecting with the appropriate user name for your ami. enter the user name in the host name box in the putty configuration window.for amazon linux 2 or the amazon linux ami, the user name is .for a centos ami, the user name is .for a debian ami, the user name is .for a fedora ami, the user name is  or .for a rhel ami, the user name is  or .for a suse ami, the user name is  or .for an ubuntu ami, the user name is .otherwise, if  and  don't work, check with the ami provider.verify that you have an inbound security group rule to allow inbound traffic to the appropriate port. for more information, see . if you connect to your instance using ssh and get any of the following errors, , , , or , verify that you are connecting with the appropriate user name for your ami and that you have specified the proper private key ( file for your instance. the appropriate user names are as follows: for amazon linux 2 or the amazon linux ami, the user name is .for a centos ami, the user name is .for a debian ami, the user name is .for a fedora ami, the user name is  or .for a rhel ami, the user name is  or .for a suse ami, the user name is  or .for an ubuntu ami, the user name is .otherwise, if  and  don't work, check with the ami provider.for example, to use an ssh client to connect to an amazon linux instance, use the following command: confirm that you are using the private key file that corresponds to the key pair that you selected when you launched the instance. open the amazon ec2 console at . select your instance. in the description tab, verify the value of key pair name. if you did not specify a key pair when you launched the instance, you can terminate the instance and launch a new instance, ensuring that you specify a key pair. if this is an instance that you have been using but you no longer have the  file for your key pair, you can replace the key pair with a new one. for more information, see . if you generated your own key pair, ensure that your key generator is set up to create rsa keys. dsa keys are not accepted. if you get a  error and none of the above applies (for example, you were able to connect previously), the permissions on the home directory of your instance may have been changed. permissions for  must be limited to the owner only. to verify the permissions on your instance stop your instance and detach the root volume. for more information, see  and . launch a temporary instance in the same availability zone as your current instance (use a similar or the same ami as you used for your current instance), and attach the root volume to the temporary instance. for more information, see . connect to the temporary instance, create a mount point, and mount the volume that you attached. for more information, see . from the temporary instance, check the permissions of the  directory of the attached volume. if necessary, adjust the permissions as follows: unmount the volume, detach it from the temporary instance, and re-attach it to the original instance. ensure that you specify the correct device name for the root volume; for example, . start your instance. if you no longer require the temporary instance, you can terminate it. your private key file must be protected from read and write operations from any other users. if your private key can be read or written to by anyone but you, then ssh ignores your key and you see the following warning message below. if you see a similar message when you try to log in to your instance, examine the first line of the error message to verify that you are using the correct public key for your instance. the above example uses the private key  with file permissions of , which allow anyone to read or write to this file. this permission level is very insecure, and so ssh ignores this key. to fix the error, execute the following command, substituting the path for your private key file. if you use a third-party tool, such as ssh-keygen, to create an rsa key pair, it generates the private key in the openssh key format. when you connect to your instance, if you use the private key in the openssh format to decrypt the password, you'll get the error . to resolve the error, the private key must be in the pem format. use the following command to create the private key in the pem format: if you use putty to connect to your instance and get either of the following errors, error: server refused our key or error: no supported authentication methods available, verify that you are connecting with the appropriate user name for your ami. type the user name in user name in the putty configuration window. the appropriate user names are as follows: for amazon linux 2 or the amazon linux ami, the user name is .for a centos ami, the user name is .for a debian ami, the user name is .for a fedora ami, the user name is  or .for a rhel ami, the user name is  or .for a suse ami, the user name is  or .for an ubuntu ami, the user name is .otherwise, if  and  don't work, check with the ami provider.you should also verify that your private key (.pem) file has been correctly converted to the format recognized by putty (.ppk). for more information about converting your private key, see . the  command is a type of icmp traffic — if you are unable to ping your instance, ensure that your inbound security group rules allow icmp traffic for the  message from all sources, or from the computer or instance from which you are issuing the command. if you are unable to issue a  command from your instance, ensure that your outbound security group rules allow icmp traffic for the  message to all destinations, or to the host that you are attempting to ping.  commands can also be blocked by a firewall or time out due to network latency or hardware issues. you should consult your local network or system administrator for help with further troubleshooting. if you are connecting to your instance with putty and you receive the error "server unexpectedly closed network connection," verify that you have enabled keepalives on the connection page of the putty configuration to avoid being disconnected. some servers disconnect clients when they do not receive any data within a specified period of time. set the seconds between keepalives to 59 seconds.  if you still experience issues after enabling keepalives, try to disable nagle's algorithm on the connection page of the putty configuration.  
you can create a security group and add rules that reflect the role of the instance that's associated with the security group. for example, an instance that's configured as a web server needs security group rules that allow inbound http and https access. likewise, a database instance needs rules that allow access for the type of database, such as access over port 3306 for mysql. the following are examples of the kinds of rules that you can add to security groups for specific kinds of access. topics the following inbound rules allow http and https access from any ip address. if your vpc is enabled for ipv6, you can add rules to control inbound http and https traffic from ipv6 addresses. the following inbound rules are examples of rules you might add for database access, depending on what type of database you're running on your instance. for more information about amazon rds instances, see the . for the source ip, specify one of the following: a specific ip address or range of ip addresses (in cidr block notation) in your local networka security group id for a group of instances that access the databaseyou can optionally restrict outbound traffic from your database servers. for example, you might want to allow access to the internet for software updates, but restrict all other kinds of traffic. you must first remove the default outbound rule that allows all outbound traffic. to connect to your instance, your security group must have inbound rules that allow ssh access (for linux instances) or rdp access (for windows instances). to allow instances that are associated with the same security group to communicate with each other, you must explicitly add rules for this.  the following table describes the inbound rule for a security group that enables associated instances to communicate with each other. the rule allows all types of traffic. the  command is a type of icmp traffic. to ping your instance, you must add the following inbound icmp rule. to use the  command to ping the ipv6 address for your instance, you must add the following inbound icmpv6 rule. if you've set up your ec2 instance as a dns server, you must ensure that tcp and udp traffic can reach your dns server over port 53.  for the source ip, specify one of the following: an ip address or range of ip addresses (in cidr block notation) in a networkthe id of a security group for the set of instances in your network that require access to the dns serverif you're using an amazon efs file system with your amazon ec2 instances, the security group that you associate with your amazon efs mount targets must allow traffic over the nfs protocol.  to mount an amazon efs file system on your amazon ec2 instance, you must connect to your instance. therefore, the security group associated with your instance must have rules that allow inbound ssh from your local computer or local network. if you're using a load balancer, the security group associated with your load balancer must have rules that allow communication with your instances or targets. |  |  | inbound |  | --- | | protocol type | protocol number | port | source ip | notes |  | tcp | 6 | the listener port |  for an internet-facing load-balancer: 0.0.0.0/0 (all ipv4 addresses) for an internal load-balancer: the ipv4 cidr block of the vpc  | allow inbound traffic on the load balancer listener port. |  | outbound |  | --- | | protocol type | protocol number | port | destination ip | notes |  | tcp | 6 | the instance listener port | the id of the instance security group | allow outbound traffic to instances on the instance listener port. |  | tcp | 6 | the health check port | the id of the instance security group | allow outbound traffic to instances on the health check port. |  the security group rules for your instances must allow the load balancer to communicate with your instances on both the listener port and the health check port. |  |  | inbound |  | --- | | protocol type | protocol number | port | source ip | notes |  | tcp | 6 | the instance listener port |  the id of the load balancer security group  | allow traffic from the load balancer on the instance listener port. |  | tcp | 6 | the health check port | the id of the load balancer security group | allow traffic from the load balancer on the health check port. |  for more information, see  in the user guide for classic load balancers, and  in the user guide for application load balancers. you can update the inbound or outbound rules for your vpc security groups to reference security groups in the peered vpc. doing so allows traffic to flow to and from instances that are associated with the referenced security group in the peered vpc. for more information about how to configure security groups for vpc peering, see . 
*copyright &copy; 2020 amazon web services, inc. and/or its affiliates. all rights reserved.* amazon's trademarks and trade dress may not be used in       connection with any product or service that is not amazon's,       in any manner that is likely to cause confusion among customers,       or in any manner that disparages or discredits amazon. all other       trademarks not owned by amazon are the property of their respective      owners, who may or may not be affiliated with, connected to, or       sponsored by amazon. 
a burstable performance instance configured as  can sustain high cpu utilization for any period of time whenever required. the hourly instance price automatically covers all cpu usage spikes if the average cpu utilization of the instance is at or below the baseline over a rolling 24-hour period or the instance lifetime, whichever is shorter. for the vast majority of general-purpose workloads, instances configured as  provide ample performance without any additional charges. if the instance runs at higher cpu utilization for a prolonged period, it can do so for a flat additional rate per vcpu-hour. for information about instance pricing, see  and the section for t2/t3 unlimited mode pricing on the . importantif you use a  or  instance under the  offer and use it in  mode, charges may apply if your average utilization over a rolling 24-hour period exceeds the  of the instance.t3 instances launch as  by default. if you launch t3 spot instances as  and plan to use them immediately and for a short duration, with no idle time for accruing cpu credits, you will incur charges for surplus credits. if the average cpu usage over a 24-hour period exceeds the baseline, you will also incur charges for surplus credits. we recommend that you launch your t3 spot instances in  mode to avoid paying higher costs. for more information, see  and . contents 
your application might need to determine whether it is running on an ec2 instance. for information about identifying windows instances, see  in the amazon ec2 user guide for windows instances. for a definitive and cryptographically verified method of identifying an ec2 instance, check the instance identity document, including its signature. these documents are available on every ec2 instance at the local, non-routable address . for more information, see . you can get the system uuid and look for the presence of the characters "ec2" or "ec2" in the beginning octet of the uuid. this method to determine whether a system is an ec2 instance is quick but potentially inaccurate because there is a small chance that a system that is not an ec2 instance could have a uuid that starts with these characters. furthermore, for ec2 instances that are not using amazon linux, the distribution's implementation of smbios might represent the uuid in little-endian format, therefore the "ec2" characters do not appear at the beginning of the uuid. example : get the uuid from the hypervisorif  exists, you can use the following command:   in the following example output, the uuid starts with "ec2", which indicates that the system is probably an ec2 instance.   example : get the uuid from dmi (hvm instances only)on hvm instances only, you can use the desktop management interface (dmi).you can use the  tool to return the uuid. on amazon linux, use the following command to install the  tool if it's not already installed on your instance:   then run the following command:   alternatively, use the following command:   in the following example output, the uuid starts with "ec2", which indicates that the system is probably an ec2 instance.   in the following example output, the uuid is represented in little-endian format.   on nitro instances, the following command can be used:   this returns the instance id, which is unique to ec2 instances:   
when you attach a volume to your instance, you include a device name for the volume. this device name is used by amazon ec2. the block device driver for the instance assigns the actual volume name when mounting the volume, and the name assigned can be different from the name that amazon ec2 uses. the number of volumes that your instance can support is determined by the operating system. for more information, see . topics for information about device names on windows instances, see  in the amazon ec2 user guide for windows instances. there are two types of virtualization available for linux instances: paravirtual (pv) and hardware virtual machine (hvm). the virtualization type of an instance is determined by the ami used to launch the instance. all instance types support hvm amis. some previous generation instance types support pv amis. be sure to note the virtualization type of your ami because the recommended and available device names that you can use depend on the virtualization type of your instance. for more information, see . the following table lists the available device names that you can specify in a block device mapping or when attaching an ebs volume. * the device names that you specify for nvme ebs volumes in a block device mapping are renamed using nvme device names (). the block device driver can assign nvme device names in a different order than you specified for the volumes in the block device mapping. ** nvme instance store volumes are automatically enumerated and assigned an nvme device name. for more information about instance store volumes, see . for more information about nvme ebs volumes (nitro-based instances), including how to identify the ebs device, see . keep the following in mind when selecting a device name: although you can attach your ebs volumes using the device names used to attach instance store volumes, we strongly recommend that you don't because the behavior can be unpredictable.the number of nvme instance store volumes for an instance depends on the size of the instance. nvme instance store volumes are automatically enumerated and assigned an nvme device name ().depending on the block device driver of the kernel, the device could be attached with a different name than you specified. for example, if you specify a device name of , your device could be renamed  or . in most cases, the trailing letter remains the same. in some versions of red hat enterprise linux (and its variants, such as centos), the trailing letter could change ( could become ). in these cases, the trailing letter of each device name is incremented the same number of times. for example, if  is renamed , then  is renamed . amazon linux creates a symbolic link for the name you specified to the renamed device. other operating systems could behave differently.hvm amis do not support the use of trailing numbers on device names, except for , which is reserved for the root device, and . while using  is possible, we do not recommend using this device mapping with hvm instances.when using pv amis, you cannot attach volumes that share the same device letters both with and without trailing digits. for example, if you attach a volume as  and another volume as , only  is visible to the instance. to use trailing digits in device names, you must use trailing digits on all device names that share the same base letters (such as , , ).some custom kernels might have restrictions that limit use to  or . if you're having trouble using  or , try switching to  or .
if you purchase a reserved instance and you already have a running instance that matches the specifications of the reserved instance, the billing benefit is immediately applied. you do not have to restart your instances. if you do not have an eligible running instance, launch an instance and ensure that you match the same criteria that you specified for your reserved instance. for more information, see .  reserved instances apply to usage in the same manner, irrespective of the offering type (standard or convertible), and are automatically applied to running on-demand instances with matching attributes. reserved instances assigned to a specific availability zone provide the reserved instance discount to matching instance usage in that availability zone. for example, if you purchase two  default tenancy linux/unix standard reserved instances in availability zone us-east-1a, then up to two  default tenancy linux/unix instances running in the availability zone us-east-1a can benefit from the reserved instance discount. the attributes (tenancy, platform, availability zone, instance type, and instance size) of the running instances must match that of the reserved instances. regional reserved instances are purchased for a region and provide availability zone flexibility. the reserved instance discount applies to instance usage in any availability zone in that region.  regional reserved instances also provide instance size flexibility where the reserved instance discount applies to instance usage within the instance family, regardless of size. limitations for instance size flexibility instance size flexibility does not apply to the following reserved instances: reserved instances that are purchased for a specific availability zone (zonal reserved instances)reserved instances with dedicated tenancyreserved instances for windows server, windows server with sql standard, windows server with sql server enterprise, windows server with sql server web, rhel, and suse linux enterprise serverreserved instances for g4 instancesinstance size flexibility is determined by the normalization factor of the instance size. the discount applies either fully or partially to running instances of the same instance family, depending on the instance size of the reservation, in any availability zone in the region. the only attributes that must be matched are the instance family, tenancy, and platform.  instance size flexibility is applied from the smallest to the largest instance size within the instance family based on the normalization factor.  the following table lists the different sizes within an instance family, and the corresponding normalization factor per hour. this scale is used to apply the discounted rate of reserved instances to the normalized usage of the instance family. for example, a  instance has a normalization factor of 2. if you purchase a  default tenancy amazon linux/unix reserved instance in the us east (n. virginia) and you have two running  instances in your account in that region, the billing benefit is applied in full to both instances.   or, if you have one  instance running in your account in the us east (n. virginia) region, the billing benefit is applied to 50% of the usage of the instance.  the normalization factor is also applied when modifying reserved instances. for more information, see . instance size flexibility also applies to bare metal instances within the instance family. if you have regional amazon linux/unix reserved instances with shared tenancy on bare metal instances, you can benefit from the reserved instance savings within the same instance family. the opposite is also true: if you have regional amazon linux/unix reserved instances with shared tenancy on instances in the same family as a bare metal instance, you can benefit from the reserved instance savings on the bare metal instance.  a bare metal instance is the same size as the largest instance within the same instance family. for example, an  is the same size as an , so they have the same normalization factor. notethe  instance sizes do not have a single normalization factor. they vary based on the specific instance family. for example, an  instance has a normalization factor of 128. if you purchase an  default tenancy amazon linux/unix reserved instance in the us east (n. virginia), the billing benefit can apply as follows: if you have one running  in your account in that region, the billing benefit is applied in full to the  instance ( normalization factor = 128).or, if you have two running  instances in your account in that region, the billing benefit is applied in full to both  instances ( normalization factor = 64).or, if you have four running  instances in your account in that region, the billing benefit is applied in full to all four  instances ( normalization factor = 32).the opposite is also true. for example, if you purchase two  default tenancy amazon linux/unix reserved instances in the us east (n. virginia), and you have one running  instance in that region, the billing benefit is applied in full to the  instance. the following scenarios cover the ways in which reserved instances are applied. example scenario 1: reserved instances in a single accountyou are running the following on-demand instances in account a:   4 x  linux, default tenancy instances in availability zone us-east-1a2 x  amazon linux, default tenancy instances in availability zone us-east-1b1 x  amazon linux, default tenancy instances in availability zone us-east-1c you purchase the following reserved instances in account a:  4 x  linux, default tenancy reserved instances in availability zone us-east-1a (capacity is reserved)4 x  amazon linux, default tenancy reserved instances in region us-east-11 x  amazon linux, default tenancy reserved instances in region us-east-1 the reserved instance benefits are applied in the following way:  the discount and capacity reservation of the four  zonal reserved instances is used by the four  instances because the attributes (instance size, region, platform, tenancy) between them match.the  regional reserved instances provide availability zone and instance size flexibility, because they are regional amazon linux reserved instances with default tenancy. an  is equivalent to 4 normalized units/hour. you've purchased four  regional reserved instances, and in total, they are equal to 16 normalized units/hour (4x4). account a has two  instances running, which is equivalent to 16 normalized units/hour (2x8). in this case, the four  regional reserved instances provide the billing benefit to an entire hour of usage of the two  instances. the  regional reserved instance in us-east-1 provides availability zone and instance size flexibility, because it is a regional amazon linux reserved instance with default tenancy, and applies to the  instance. a  instance is equivalent to 4 normalized units/hour and a  is equivalent to 8 normalized units/hour. in this case, the  regional reserved instance provides partial benefit to  usage. this is because the  reserved instance is equivalent to 4 normalized units/hour of usage, but the  instance requires 8 normalized units/hour. therefore, the  reserved instance billing discount applies to 50% of  usage. the remaining  usage is charged at the on-demand rate. example scenario 2: regional reserved instances in linked accountsreserved instances are first applied to usage within the purchasing account, followed by qualifying usage in any other account in the organization. for more information, see . for regional reserved instances that offer instance size flexibility, the benefit is applied from the smallest to the largest instance size within the instance family.you're running the following on-demand instances in account a (the purchasing account):   2 x  linux, default tenancy instances in availability zone us-east-1a1 x  linux, default tenancy instances in availability zone us-east-1b2 x  linux, default tenancy instances in availability zone us-east-1a1 x  linux, default tenancy instances in availability zone us-east-1b another customer is running the following on-demand instances in account b—a linked account:  2 x  linux, default tenancy instances in availability zone us-east-1a you purchase the following regional reserved instances in account a:  4 x  linux, default tenancy reserved instances in region us-east-12 x  linux, default tenancy reserved instances in region us-east-1 the regional reserved instance benefits are applied in the following way:  the discount of the four  reserved instances is used by the two  instances and the single  instance in account a (purchasing account). all three instances match the attributes (instance family, region, platform, tenancy). the discount is applied to instances in the purchasing account (account a) first, even though account b (linked account) has two  that also match the reserved instances. there is no capacity reservation because the reserved instances are regional reserved instances.the discount of the two  reserved instances applies to the two  instances, because they are a smaller instance size than the  instance. there is no capacity reservation because the reserved instances are regional reserved instances.example scenario 3: zonal reserved instances in a linked accountin general, reserved instances that are owned by an account are applied first to usage in that account. however, if there are qualifying, unused reserved instances for a specific availability zone (zonal reserved instances) in other accounts in the organization, they are applied to the account before regional reserved instances owned by the account. this is done to ensure maximum reserved instance utilization and a lower bill. for billing purposes, all the accounts in the organization are treated as one account. the following example may help explain this.you're running the following on-demand instance in account a (the purchasing account):   1 x  linux, default tenancy instance in availability zone us-east-1a a customer is running the following on-demand instance in linked account b:  1 x  linux, default tenancy instance in availability zone us-east-1b you purchase the following regional reserved instances in account a:  1 x  linux, default tenancy reserved instance in region us-east-1 a customer also purchases the following zonal reserved instances in linked account c:  1 x  linux, default tenancy reserved instances in availability zone us-east-1a the reserved instance benefits are applied in the following way:  the discount of the  zonal reserved instance owned by account c is applied to the  usage in account a.the discount of the  regional reserved instance owned by account a is applied to the  usage in account b.if the regional reserved instance owned by account a was first applied to the usage in account a, the zonal reserved instance owned by account c remains unused and usage in account b is charged at on-demand rates.for more information, see . 
the  is pre-installed and pre-configured in the ami with the following description: amazon linux 2 with .net core, powershell, mono, and mate desktop environment. the environment provides an intuitive graphical user interface for administering amazon linux 2 instances without using the command line. the interface uses graphical representations, such as icons, windows, toolbars, folders, wallpapers, and desktop widgets. built-in, gui-based tools are available to perform common tasks. for example, there are tools for adding and removing software, applying updates, organizing files, launching programs, and monitoring system health. complete the following procedure to use the mate desktop environment. to configure remote desktop protocol (rdp) connections and set up a password use the following  command to get the id of the ami for amazon linux 2 that includes mate in the ami name. the following is example output: launch an ec2 instance with the ami that you located in the previous step. configure the security group to allow for inbound tcp traffic to port 3389. for more information about configuring security groups, see . this configuration enables you to use an rdp client to connect to the instance. connect to the instance using . run the following command to set the password for . open an rdp client on the computer from which you will connect to the instance (for example, remote desktop connection on a computer running microsoft windows). enter  as the user name and enter the password that you set in the previous step. to disable the mate desktop environment on your ec2 instanceyou can turn off the gui environment at any time by running one of the following commands. to enable the mate desktop environment on your ec2 instanceto turn the gui back on, you can run one of the following commands. 
when you purchase a reserved instance, you determine the scope of the reserved instance. the scope is either regional or zonal.  regional: when you purchase a reserved instance for a region, it's referred to as a regional reserved instance.zonal: when you purchase a reserved instance for a specific availability zone, it's referred to as a zonal reserved instance.the following table highlights some key differences between regional reserved instances and zonal reserved instances: for more information and examples, see . 
the ec2rescue for linux tool can be installed on an amazon ec2 linux instance that meets the following prerequisites. prerequisites supported operating systems:amazon linux 2amazon linux 2016.09+suse linux enterprise server 12+rhel 7+ubuntu 16.04+software requirements:python 2.7.9+ or 3.2+if your system has the required python version, you can install the standard build. otherwise, you can install the bundled build, which includes a minimal copy of python. to install the standard build from a working linux instance, download the  tool: (optional) before proceeding, you can optionally verify the signature of the ec2rescue for linux installation file. for more information, see . download the sha256 hash file: verify the integrity of the tarball: unpack the tarball: verify the installation by listing out the help file: to install the bundled buildfor a link to the download and a list of limitations, see  on github. 
amazon fsx for windows file server provides fully managed windows file servers, backed by a fully–native windows file system with the features, performance, and compatibility to easily lift and shift enterprise applications to aws. amazon fsx supports a broad set of enterprise windows workloads with fully managed file storage built on microsoft windows server. amazon fsx has native support for windows file system features and for the industry-standard server message block (smb) protocol to access file storage over a network. amazon fsx is optimized for enterprise applications in the aws cloud, with native windows compatibility, enterprise performance and features, and consistent sub-millisecond latencies. with file storage on amazon fsx, the code, applications, and tools that windows developers and administrators use today can continue to work unchanged. the windows applications and workloads that are ideal for amazon fsx include business applications, home directories, web serving, content management, data analytics, software build setups, and media processing workloads. as a fully managed service, amazon fsx for windows file server eliminates the administrative overhead of setting up and provisioning file servers and storage volumes. additionally, it keeps windows software up to date, detects and addresses hardware failures, and performs backups. it also provides rich integration with other aws services, including aws directory service for microsoft active directory, amazon workspaces, aws key management service, and aws cloudtrail. for more information, see the . for pricing information, see . 
the elastic network adapter (ena) is designed to improve operating system health and reduce the chances of long-term disruption because of unexpected hardware behavior and or failures. the ena architecture keeps device or driver failures as transparent to the system as possible. this topic provides troubleshooting information for ena. if you are unable to connect to your instance, start with the  section.  if you are able to connect to your instance, you can gather diagnostic information by using the failure detection and recovery mechanisms that are covered in the later sections of this topic. topics if you lose connectivity while enabling enhanced networking, the  module might be incompatible with your instance's current running kernel. this can happen if you install the module for a specific kernel version (without dkms, or with an improperly configured dkms.conf file) and then your instance kernel is updated. if the instance kernel that is loaded at boot time does not have the  module properly installed, your instance will not recognize the network adapter and your instance becomes unreachable. if you enable enhanced networking for a pv instance or ami, this can also make your instance unreachable. if your instance becomes unreachable after enabling enhanced networking with ena, you can disable the  attribute for your instance and it will fall back to the stock network adapter. to disable enhanced networking with ena (ebs-backed instances) from your local computer, stop the instance using the amazon ec2 console or one of the following commands:  (aws cli),  (aws tools for windows powershell). if your instance is managed by aws opsworks, you should stop the instance in the aws opsworks console so that the instance state remains in sync. importantif you are using an instance store-backed instance, you can't stop the instance. instead, proceed to . from your local computer, disable the enhanced networking attribute using the following command.  (aws cli) from your local computer, start the instance using the amazon ec2 console or one of the following commands:  (aws cli),  (aws tools for windows powershell). if your instance is managed by aws opsworks, you should start the instance in the aws opsworks console so that the instance state remains in sync. (optional) connect to your instance and try reinstalling the  module with your current kernel version by following the steps in .  to disable enhanced networking with ena (instance store-backed instances)if your instance is an instance store-backed instance, create a new ami as described in . be sure to disable the enhanced networking  attribute when you register the ami.  (aws cli)  (aws tools for windows powershell) the ena device posts keep-alive events at a fixed rate (usually once every second). the ena driver implements a watchdog mechanism, which checks for the presence of these keep-alive messages. if a message or messages are present, the watchdog is rearmed, otherwise the driver concludes that the device experienced a failure and then does the following: dumps its current statistics to syslogresets the ena deviceresets the ena driver statethe above reset procedure may result in some traffic loss for a short period of time (tcp connections should be able to recover), but should not otherwise affect the user. the ena device may also indirectly request a device reset procedure, by not sending a keep-alive notification, for example, if the ena device reaches an unknown state after loading an irrecoverable configuration. below is an example of the reset procedure: the ena architecture suggests a limited usage of memory mapped i/o (mmio) read operations. mmio registers are accessed by the ena device driver only during its initialization procedure.  if the driver logs (available in dmesg output) indicate failures of read operations, this may be caused by an incompatible or incorrectly compiled driver, a busy hardware device, or hardware failure. intermittent log entries that indicate failures on read operations should not be considered an issue; the driver will retry them in this case. however, a sequence of log entries containing read failures indicate a driver or hardware problem. below is an example of driver log entry indicating a read operation failure due to a timeout: if you experience insufficient network performance or latency issues, you should retrieve the device statistics and examine them. these statistics can be obtained using ethtool, as shown below: the following command output parameters are described below: the number of times that the netdev watchdog was activated. unsupported. this value should always be zero. unsupported. this value should always be zero. the number of times that the driver did not receive the keep-alive event in the preceding 3 seconds. the number of times that the ena interface was brought up. the number of times that the ena interface was brought down. the admin queue is in an unstable state. this value should always be zero. the number of transmitted packets for queue n. the number of transmitted bytes for queue n. the number of times that queue n was full and stopped. the number of times that queue n resumed after being stopped. direct memory access error count. if this value is not 0, it indicates low system resources. the number of times the  handler called  for queue n. the number of times the  handler was scheduled for queue n. the number of transmission doorbells for queue n. the number of times skb linearization was attempted for queue n. the number of times skb linearization failed for queue n. the number of times  failed for queue n. this value should always be zero; if not, see the driver logs. the number of packets that were left uncompleted for queue n. this value should always be zero. invalid  for queue n. the valid  is zero, minus the , minus 1.  the number of received packets for queue n. the number of received bytes for queue n. the number of times the driver did not succeed in refilling the empty portion of the  queue with the buffers for queue n. if this value is not zero, it indicates low memory resources. the number of times the  queue had a bad checksum for queue n (only if  checksum offload is supported). the number of time that page allocation failed for queue n. if this value is not zero, it indicates low memory resources. the number of time that skb allocation failed for queue n. if this value is not zero, it indicates low system resources. direct memory access error count. if this value is not 0, it indicates low system resources. too many buffers per packet. if this value is not 0, it indicates usage of very small buffers. optimization: for packets smaller that this threshold, which is set by , the packet is copied directly to the stack to avoid allocation of a new page. the number of admin commands that were aborted. this usually happens during the auto-recovery procedure. the number of admin queue doorbells. the number of admin queue completions. the number of times that the driver tried to submit new admin command, but the queue was full. the number of times that the driver did not get an admin completion for a command. the ena driver writes log messages to syslog during system boot. you can examine these logs to look for errors if you are experiencing issues. below is an example of information logged by the ena driver in syslog during system boot, along with some annotations for select messages. which errors can i ignore?the following warnings that may appear in your system's error logs can be ignored for the elastic network adapter: set host attribute isn't supportedhost attributes are not supported for this device. failed to alloc buffer for rx queuethis is a recoverable error, and it indicates that there may have been a memory pressure issue when the error was thrown. feature x isn't supportedthe referenced feature is not supported by the elastic network adapter. possible values for x include:   10: rss hash function configuration is not supported for this device.12: rss indirection table configuration is not supported for this device.18: rss hash input configuration is not supported for this device.20: interrupt moderation is not supported for this device.27: the elastic network adapter driver does not support polling the ethernet capabilities from snmpd.failed to config aenqthe elastic network adapter does not support aenq configuration. trying to set unsupported aenq eventsthis error indicates an attempt to set an aenq events group that is not supported by the elastic network adapter. 
an instance with an attached gpu, such as a p3 or g4 instance, must have the appropriate nvidia driver installed. depending on the instance type, you can either download a public nvidia driver, download a driver from amazon s3 that is available only to aws customers, or use an ami with the driver pre-installed. contents the following are the main types of nvidia drivers that can be used with gpu-based instances. tesla driversthese drivers are intended primarily for compute workloads, which use gpus for computational tasks such as parallelized floating-point calculations for machine learning and fast fourier transforms for high performance computing applications. grid driversthese drivers are certified to provide optimal performance for professional visualization applications that render content such as 3d models or high-resolution videos. you can configure grid drivers to support two modes. quadro virtual workstations provide access to four 4k displays per gpu. grid vapps provide rdsh app hosting capabilities. gaming driversthese drivers contain optimizations for gaming and are updated frequently to provide performance enhancements. they support a single 4k display per gpu. nvidia control panelthe nvidia control panel is supported with grid and gaming drivers. it is not supported with tesla drivers. supported apis for tesla, grid, and gaming drivers opencl, opengl, and vulkannvidia cuda and related libraries (for example, cudnn, tensorrt, nvjpeg, and cublas)nvenc for video encoding and nvdec for video decodingthe following table summarizes the supported nvidia drivers for each gpu instance type. † using marketplace amis only use one of the following options to get the nvidia drivers required for your gpu instance. topics aws and nvidia offer different amazon machine images (ami) that come with the nvidia drivers installed. to update the driver version installed using one of these amis, you must uninstall the nvidia packages from your instance to avoid version conflicts. use this command to uninstall the nvidia packages: the cuda toolkit package has dependencies on the nvidia drivers. uninstalling the nvidia packages erases the cuda toolkit. you must reinstall the cuda toolkit after installing the nvidia driver. the options offered by aws come with the necessary license for the driver. alternatively, you can install the public drivers and bring your own license. to install a public driver, download it from the nvidia site as described here. alternatively, you can use the options offered by aws instead of the public drivers. to use a grid driver on a p3 instance, use the aws marketplace amis as described in . to use a grid driver on a g3 or g4 instance, use the aws marketplace amis, as described in option 1 or install the nvidia drivers provided by aws as described in . to download a public nvidia driverlog on to your linux instance and download the 64-bit nvidia driver appropriate for the instance type from . for product type, product series, and product, use the options in the following table. † g4 instances require driver version 418.87 or later. to install the nvidia driver on linuxfor more information about installing and configuring the driver, see the .  these downloads are available to aws customers only. by downloading, you agree to use the downloaded software only to develop amis for use with the nvidia tesla t4 or nvidia tesla m60 hardware. upon installation of the software, you are bound by the terms of the . prerequisites configure default credentials for the aws cli on your linux instance. for more information, see  in the aws command line interface user guide.iam users must have the permissions granted by the amazons3readonlyaccess policy.to install the nvidia grid driver on your linux instance connect to your linux instance. install gcc and make, if they are not already installed. update your package cache and get necessary package updates for your instance. for amazon linux, centos, and red hat enterprise linux: for ubuntu and debian: (ubuntu 16.04 and later, with the  package) upgrade the  package to receive the latest version. reboot your instance to load the latest kernel version. reconnect to your instance after it has rebooted. install the gcc compiler and the kernel headers package for the version of the kernel you are currently running. for amazon linux, centos, and red hat enterprise linux: for ubuntu and debian: [centos, red hat enterprise linux, ubuntu, debian] disable the  open source driver for nvidia graphics cards. add  to the  blacklist file. copy the following code block and paste it into a terminal. edit the  file and add the following line: rebuild the grub configuration. for centos and red hat enterprise linux: for ubuntu and debian: download the grid driver installation utility using the following command: for g3 instances: for g4 instances: multiple versions of the grid driver are stored in this bucket. you can see all of the available versions using the following command. add permissions to run the driver installation utility using the following command. run the self-install script as follows to install the grid driver that you downloaded. for example: when prompted, accept the license agreement and specify the installation options as required (you can accept the default options). reboot the instance. confirm that the driver is functional. the response for the following command lists the installed version of the nvidia driver and details about the gpus. (optional) to help take advantage of the four displays of up to 4k resolution, set up the high-performance display protocol . (optional) nvidia quadro virtual workstation mode is enabled by default. to activate grid virtual applications for rdsh application hosting capabilities, complete the grid virtual application activation steps in . these drivers are available to aws customers only. by downloading them, you agree to use the downloaded software only to develop amis for use with the nvidia tesla t4 hardware. upon installation of the software, you are bound by the terms of the . prerequisites configure default credentials for the aws cli on your linux instance. for more information, see  in the aws command line interface user guide.iam users must have the permissions granted by the amazons3readonlyaccess policy.to install the nvidia gaming driver on your linux instance connect to your linux instance. install gcc and make, if they are not already installed. update your package cache and get necessary package updates for your instance. for amazon linux, centos, and red hat enterprise linux: for ubuntu and debian: (ubuntu 16.04 and later, with the  package) upgrade the  package to receive the latest version. reboot your instance to load the latest kernel version. reconnect to your instance after it has rebooted. install the gcc compiler and the kernel headers package for the version of the kernel you are currently running. for amazon linux, centos, and red hat enterprise linux: for ubuntu and debian: [centos, red hat enterprise linux, ubuntu, debian] disable the  open source driver for nvidia graphics cards. add  to the  blacklist file. copy the following code block and paste it into a terminal. edit the  file and add the following line: rebuild the grub configuration. for centos and red hat enterprise linux: for ubuntu and debian: download the gaming driver installation utility using the following command: multiple versions of the gaming driver are stored in this bucket. you can see all of the available versions using the following command: add permissions to run the driver installation utility using the following command. run the installer using the following command: when prompted, accept the license agreement and specify the installation options as required (you can accept the default options). use the following command to create the required configuration file. use the following command to download and rename the certification file. for version 440.68 or later: for earlier versions: reboot the instance. (optional) to help take advantage of a single display of up to 4k resolution, set up the high-performance display protocol . 
burstable performance instances, which are t3, t3a, and t2 instances, are designed to provide a baseline level of cpu performance with the ability to burst to a higher level when required by your workload. burstable performance instances are well suited for a wide range of general-purpose applications. examples include microservices, low-latency interactive applications, small and medium databases, virtual desktops, development, build, and stage environments, code repositories, and product prototypes. burstable performance instances are the only instance types that use credits for cpu usage. for more information about instance pricing and additional hardware details, see  and . if your account is less than 12 months old, you can use a  instance for free (or a  instance in regions where  is unavailable) within certain usage limits. for more information, see . topics the following are the requirements for these instances: these instances are available as on-demand instances, reserved instances, dedicated instances, and spot instances, but not as scheduled instances. they are also not supported on a dedicated host. for more information, see .ensure that the instance size you choose passes the minimum memory requirements of your operating system and applications. operating systems with graphical user interfaces that consume significant memory and cpu resources (for example, windows) might require a  or larger instance size for many use cases. as the memory and cpu requirements of your workload grow over time, you can scale to larger instance sizes of the same instance type, or another instance type.for additional requirements, see .follow these best practices to get the maximum benefit from burstable performance instances. use a recommended ami – use an ami that provides the required drivers. for more information, see .turn on instance recovery – create a cloudwatch alarm that monitors an ec2 instance and automatically recovers it if it becomes impaired for any reason. for more information, see .
ec2rescue for linux is an easy-to-use, open-source tool that can be run on an amazon ec2 linux instance to diagnose and troubleshoot common issues using its library of over 100 modules. a few generalized use cases for ec2rescue for linux include gathering syslog and package manager logs, collecting resource utilization data, and diagnosing/remediating known problematic kernel parameters and common openssh issues. noteif you are using a windows instance, see . contents 
with instance status monitoring, you can quickly determine whether amazon ec2 has detected any problems that might prevent your instances from running applications. amazon ec2 performs automated checks on every running ec2 instance to identify hardware and software issues. you can view the results of these status checks to identify specific and detectable problems. the event status data augments the information that amazon ec2 already provides about the state of each instance (such as , , ) and the utilization metrics that amazon cloudwatch monitors (cpu utilization, network traffic, and disk activity). status checks are performed every minute, returning a pass or a fail status. if all checks pass, the overall status of the instance is ok. if one or more checks fail, the overall status is impaired. status checks are built into amazon ec2, so they cannot be disabled or deleted. when a status check fails, the corresponding cloudwatch metric for status checks is incremented. for more information, see . you can use these metrics to create cloudwatch alarms that are triggered based on the result of the status checks. for example, you can create an alarm to warn you if status checks fail on a specific instance. for more information, see . you can also create an amazon cloudwatch alarm that monitors an amazon ec2 instance and automatically recovers the instance if it becomes impaired due to an underlying issue. for more information, see . topics there are two types of status checks: system status checks and instance status checks. system status checksmonitor the aws systems on which your instance runs. these checks detect underlying problems with your instance that require aws involvement to repair. when a system status check fails, you can choose to wait for aws to fix the issue, or you can resolve it yourself. for instances backed by amazon ebs, you can stop and start the instance yourself, which in most cases results in the instance being migrated to a new host. for instances backed by instance store, you can terminate and replace the instance. the following are examples of problems that can cause system status checks to fail: loss of network connectivityloss of system powersoftware issues on the physical hosthardware issues on the physical host that impact network reachabilityinstance status checksmonitor the software and network configuration of your individual instance. amazon ec2 checks the health of the instance by sending an address resolution protocol (arp) request to the network interface (nic). these checks detect problems that require your involvement to repair. when an instance status check fails, you typically must address the problem yourself (for example, by rebooting the instance or by making instance configuration changes). the following are examples of problems that can cause instance status checks to fail: failed system status checksincorrect networking or startup configurationexhausted memorycorrupted file systemincompatible kernelamazon ec2 provides you with several ways to view and work with status checks. you can view status checks using the aws management console. to view status checks (console) open the amazon ec2 console at . in the navigation pane, choose instances. on the instances page, the status checks column lists the operational status of each instance. to view the status of a specific instance, select the instance, and then choose the status checks tab. if you have an instance with a failed status check and the instance has been unreachable for over 20 minutes, choose aws support to submit a request for assistance. to troubleshoot system or instance status check failures yourself, see . to review the cloudwatch metrics for status checks, select the instance, and then choose the monitoring tab. scroll until you see the graphs for the following metrics: status check failed (any)status check failed (instance)status check failed (system)you can view status checks for running instances using the  (aws cli) command. to view the status of all instances, use the following command. to get the status of all instances with an instance status of , use the following command. to get the status of a single instance, use the following command. alternatively, use the following commands:  (aws tools for windows powershell)  (amazon ec2 query api)if you have an instance with a failed status check, see . you can provide feedback if you are having problems with an instance whose status is not shown as impaired, or if you want to send aws additional details about the problems you are experiencing with an impaired instance. we use reported feedback to identify issues impacting multiple customers, but do not respond to individual account issues. providing feedback does not change the status check results that you currently see for the instance. to report instance status (console) open the amazon ec2 console at . in the navigation pane, choose instances. select the instance, choose the status checks tab, and choose submit feedback. complete the report instance status form, and then choose submit. use the  (aws cli) command to send feedback about the status of an impaired instance. alternatively, use the following commands:  (aws tools for windows powershell)  (amazon ec2 query api)you can use the  to create cloudwatch alarms to notify you when an instance has a failed status check. use the following procedure to configure an alarm that sends you a notification by email, or stops, terminates, or recovers an instance when it fails a status check. to create a status check alarm (console) open the amazon ec2 console at . in the navigation pane, choose instances. select the instance, choose the status checks tab, and choose create status check alarm. select send a notification to. choose an existing sns topic, or choose create topic to create a new one. if creating a new topic, in with these recipients, enter your email address and the addresses of any additional recipients, separated by commas. (optional) select take the action, and then select the action that you'd like to take. in whenever, select the status check that you want to be notified about. if you selected recover this instance in the previous step, select status check failed (system). in for at least, set the number of periods you want to evaluate and in consecutive periods, select the evaluation period duration before triggering the alarm and sending an email. (optional) in name of alarm, replace the default name with another name for the alarm. choose create alarm. importantif you added an email address to the list of recipients or created a new topic, amazon sns sends a subscription confirmation email message to each new address. each recipient must confirm the subscription by choosing the link contained in that message. alert notifications are sent only to confirmed addresses. if you need to make changes to an instance status alarm, you can edit it. to edit a status check alarm using the console open the amazon ec2 console at . in the navigation pane, choose instances. select the instance and choose actions, cloudwatch monitoring, add/edit alarms. in the alarm details dialog box, choose the name of the alarm. in the edit alarm dialog box, make the desired changes, and then choose save. in the following example, the alarm publishes a notification to an sns topic, , when the instance fails either the instance check or system status check for at least two consecutive periods. the cloudwatch metric used is . to create a status check alarm using the aws cli select an existing sns topic or create a new one. for more information, see  in the aws command line interface user guide. use the following  command to view the available amazon cloudwatch metrics for amazon ec2. use the following  command to create the alarm. the period is the time frame, in seconds, in which amazon cloudwatch metrics are collected. this example uses 300, which is 60 seconds multiplied by 5 minutes. the evaluation period is the number of consecutive periods for which the value of the metric must be compared to the threshold. this example uses 2. the alarm actions are the actions to perform when this alarm is triggered. this example configures the alarm to send an email using amazon sns. 
the following steps help you to get started with one of the following aws deep learning amis: deep learning ami (amazon linux 2) version 25.0 and laterdeep learning ami (amazon linux) version 25.0 and laterdeep learning ami (ubuntu 18.04) version 25.0 and laterdeep learning ami (ubuntu 16.04) version 25.0 and laterfor more information, see the . topics an efa requires a security group that allows all inbound and outbound traffic to and from the security group itself. to create an efa-enabled security group open the amazon ec2 console at . in the navigation pane, choose security groups and then choose create security group. in the create security group window, do the following: for security group name, enter a descriptive name for the security group, such as . (optional) for description, enter a brief description of the security group. for vpc, select the vpc into which you intend to launch your efa-enabled instances. choose create. select the security group that you created, and on the description tab, copy the group id. on the inbound and outbound tabs, do the following: choose edit. for type, choose all traffic. for source, choose custom. paste the security group id that you copied into the field. choose save. launch a temporary instance that you can use to install and configure the efa software components. you use this instance to create an efa-enabled ami from which you can launch your efa-enabled instances. to launch a temporary instance open the amazon ec2 console at . choose launch instance. on the choose an ami page, choose a supported aws deep learning ami version 25.0 or later. on the choose an instance type page, select  and then choose next: configure instance details. on the configure instance details page, do the following: for elastic fabric adapter, choose enable. in the network interfaces section, for device eth0, choose new network interface. choose next: add storage. on the add storage page, specify the volumes to attach to the instances, in addition to the volumes specified by the ami (such as the root device volume). then choose next: add tags. on the add tags page, specify a tag that you can use to identify the temporary instance, and then choose next: configure security group. on the configure security group page, for assign a security group, select select an existing security group. then select the security group that you created in step 1. on the review instance launch page, review the settings, and then choose launch to choose a key pair and to launch your instance. run a test to ensure that your temporary instance is properly configured for efa and nccl.  to test your efa and nccl configuration create a host file that specifies the hosts on which to run the tests. the following command creates a host file named  that includes a reference to the instance itself.run the test and specify the host file () and the number of gpus to use (). the following command runs the  test on 8 gpus on the instance itself, and specifies the following environment variables. —specifies the fabric interface provider. this must be set to .—specifies the minimum number of send credits that the sender requests from the receiver.  is the recommended value for nccl jobs using efa. the value should only be increased for message transfers that are larger than 256 mb.—enables detailed debugging output. you can also specify  to print only the nccl version at the start of the test, or  to receive only error messages.—disables tree algorithms for the test.for more information about the nccl test arguments, see the  in the official nccl-tests repository. install the machine learning applications on the temporary instance. the installation procedure varies depending on the specific machine learning application. for more information about installing software on your linux instance, see . noteyou might need to refer to your machine learning application’s documentation for installation instructions. after you have installed the required software components, you create an ami that you can reuse to launch your efa-enabled instances. to create an ami from your temporary instance open the amazon ec2 console at . in the navigation pane, choose instances. select the temporary instance that you created and choose actions, image, create image. in the create image window, do the following: for image name, enter a descriptive name for the ami. (optional) for image description, enter a brief description of the ami. choose create image and then choose close. in the navigation pane, choose amis. locate the ami you created in the list. wait for the status to transition from  to  before continuing to the next step. at this point, you no longer need the temporary instance that you launched. you can terminate the instance to stop incurring charges for it. to terminate the temporary instance open the amazon ec2 console at . in the navigation pane, choose instances. select the temporary instance that you created and then choose actions, instance state, terminate, yes, terminate. launch your efa and nccl-enabled instances into a cluster placement group using the efa-enabled ami and the efa-enabled security group that you created earlier. to launch your efa and nccl-enabled instances into a cluster placement group open the amazon ec2 console at . choose launch instance. on the choose an ami page, choose my amis, find the ami that you created earlier, and then choose select. on the choose an instance type page, select p3dn.24xlarge and then choose next: configure instance details. on the configure instance details page, do the following: for number of instances, enter the number of efa and nccl-enabled instances that you want to launch. for network and subnet, select the vpc and subnet into which to launch the instances. for placement group, select add instance to placement group. for placement group name, select add to a new placement group, and then enter a descriptive name for the placement group. then for placement group strategy, select cluster. for efa, choose enable. in the network interfaces section, for device eth0, choose new network interface. you can optionally specify a primary ipv4 address and one or more secondary ipv4 addresses. if you are launching the instance into a subnet that has an associated ipv6 cidr block, you can optionally specify a primary ipv6 address and one or more secondary ipv6 addresses. choose next: add storage. on the add storage page, specify the volumes to attach to the instances in addition to the volumes specified by the ami (such as the root device volume). then choose next: add tags. on the add tags page, specify tags for the instances, such as a user-friendly name, and then choose next: configure security group. on the configure security group page, for assign a security group, select select an existing security group, and then select the security group that you created earlier. choose review and launch. on the review instance launch page, review the settings, and then choose launch to choose a key pair and to launch your instances. to enable your applications to run across all of the instances in your cluster, you must enable passwordless ssh access from the leader node to the member nodes. the leader node is the instance from which you run your applications. the remaining instances in the cluster are the member nodes. to enable passwordless ssh between the instances in the cluster select one instance in the cluster as the leader node, and connect to it. disable  and enable  on the leader node. open  using your preferred text editor and add the following. generate an rsa key pair. the key pair is created in the  directory. change the permissions of the private key on the leader node. open  using your preferred text editor and copy the key. for each member node in the cluster, do the following: connect to the instance. open  using your preferred text editor and add the public key that you copied earlier. to test that the passwordless ssh is functioning as expected, connect to your leader node and run the following command. you should connect to the member node without being prompted for a key or password. 
when you launch a new ec2 instance, the ec2 service attempts to place the instance in such a way that all of your instances are spread out across underlying hardware to minimize correlated failures. you can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload. depending on the type of workload, you can create a placement group using one of the following placement strategies: cluster – packs instances close together inside an availability zone. this strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of hpc applications.partition – spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. this strategy is typically used by large distributed and replicated workloads, such as hadoop, cassandra, and kafka.spread – strictly places a small group of instances across distinct underlying hardware to reduce correlated failures.there is no charge for creating a placement group. topics a cluster placement group is a logical grouping of instances within a single availability zone. a cluster placement group can span peered vpcs in the same region. instances in the same cluster placement group enjoy a higher per-flow throughput limit of up to 10 gbps for tcp/ip traffic and are placed in the same high-bisection bandwidth segment of the network. the following image shows instances that are placed into a cluster placement group.  cluster placement groups are recommended for applications that benefit from low network latency, high network throughput, or both. they are also recommended when the majority of the network traffic is between the instances in the group. to provide the lowest latency and the highest packet-per-second network performance for your placement group, choose an instance type that supports enhanced networking. for more information, see . we recommend that you launch your instances in the following way: use a single launch request to launch the number of instances that you need in the placement group.use the same instance type for all instances in the placement group. if you try to add more instances to the placement group later, or if you try to launch more than one instance type in the placement group, you increase your chances of getting an insufficient capacity error. if you stop an instance in a placement group and then start it again, it still runs in the placement group. however, the start fails if there isn't enough capacity for the instance. if you receive a capacity error when launching an instance in a placement group that already has running instances, stop and start all of the instances in the placement group, and try the launch again. starting the instances may migrate them to hardware that has capacity for all of the requested instances. partition placement groups help reduce the likelihood of correlated hardware failures for your application. when using partition placement groups, amazon ec2 divides each group into logical segments called partitions. amazon ec2 ensures that each partition within a placement group has its own set of racks. each rack has its own network and power source. no two partitions within a placement group share the same racks, allowing you to isolate the impact of hardware failure within your application. the following image is a simple visual representation of a partition placement group in a single availability zone. it shows instances that are placed into a partition placement group with three partitions—partition 1, partition 2, and partition 3. each partition comprises multiple instances. the instances in a partition do not share racks with the instances in the other partitions, allowing you to contain the impact of a single hardware failure to only the associated partition.  partition placement groups can be used to deploy large distributed and replicated workloads, such as hdfs, hbase, and cassandra, across distinct racks. when you launch instances into a partition placement group, amazon ec2 tries to distribute the instances evenly across the number of partitions that you specify. you can also launch instances into a specific partition to have more control over where the instances are placed. a partition placement group can have partitions in multiple availability zones in the same region. a partition placement group can have a maximum of seven partitions per availability zone. the number of instances that can be launched into a partition placement group is limited only by the limits of your account.  in addition, partition placement groups offer visibility into the partitions — you can see which instances are in which partitions. you can share this information with topology-aware applications, such as hdfs, hbase, and cassandra. these applications use this information to make intelligent data replication decisions for increasing data availability and durability. if you start or launch an instance in a partition placement group and there is insufficient unique hardware to fulfill the request, the request fails. amazon ec2 makes more distinct hardware available over time, so you can try your request again later. a spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. the following image shows seven instances in a single availability zone that are placed into a spread placement group. the seven instances are placed on seven different racks.  spread placement groups are recommended for applications that have a small number of critical instances that should be kept separate from each other. launching instances in a spread placement group reduces the risk of simultaneous failures that might occur when instances share the same racks. spread placement groups provide access to distinct racks, and are therefore suitable for mixing instance types or launching instances over time. a spread placement group can span multiple availability zones in the same region. you can have a maximum of seven running instances per availability zone per group. if you start or launch an instance in a spread placement group and there is insufficient unique hardware to fulfill the request, the request fails. amazon ec2 makes more distinct hardware available over time, so you can try your request again later. before you use placement groups, be aware of the following rules: the name that you specify for a placement group must be unique within your aws account for the region.you can't merge placement groups.an instance can be launched in one placement group at a time; it cannot span multiple placement groups. and  provide a capacity reservation for ec2 instances in a specific availability zone. the capacity reservation can be used by instances in a placement group. however, it is not possible to explicitly reserve capacity for a placement group.instances with a tenancy of  cannot be launched in placement groups.the following rules apply to cluster placement groups: when you launch an instance into a cluster placement group, you must use one of the following instance types:general purpose: a1, m4, m5, m5a, m5ad, m5d, m5dn, m5n, m6gcompute optimized: c3, c4, c5, c5a, c5d, c5n, c6g,  memory optimized: , r3, r4, r5, r5a, r5ad, r5d, r5dn, r5n, r6g,  x1, x1e, z1dstorage optimized: d2, h1, , i2, i3, i3enaccelerated computing: f1, g2, g3, g4dn, inf1, p2, p3, p3dna cluster placement group can't span multiple availability zones.the maximum network throughput speed of traffic between two instances in a cluster placement group is limited by the slower of the two instances. for applications with high-throughput requirements, choose an instance type with network connectivity that meets your requirements.for instances that are enabled for enhanced networking, the following rules apply:instances within a cluster placement group can use up to 10 gbps for single-flow traffic. instances that are not within a cluster placement group can use up to 5 gbps for single-flow traffic.traffic to and from amazon s3 buckets within the same region over the public ip address space or through a vpc endpoint can use all available instance aggregate bandwidth.you can launch multiple instance types into a cluster placement group. however, this reduces the likelihood that the required capacity will be available for your launch to succeed. we recommend using the same instance type for all instances in a cluster placement group.network traffic to the internet and over an aws direct connect connection to on-premises resources is limited to 5 gbps.the following rules apply to partition placement groups: a partition placement group supports a maximum of seven partitions per availability zone. the number of instances that you can launch in a partition placement group is limited only by your account limits.when instances are launched into a partition placement group, amazon ec2 tries to evenly distribute the instances across all partitions. amazon ec2 doesn’t guarantee an even distribution of instances across all partitions.a partition placement group with dedicated instances can have a maximum of two partitions.partition placement groups are not supported for dedicated hosts.the following rules apply to spread placement groups: a spread placement group supports a maximum of seven running instances per availability zone. for example, in a region with three availability zones, you can run a total of 21 instances in the group (seven per zone). if you try to start an eighth instance in the same availability zone and in the same spread placement group, the instance will not launch. if you need to have more than seven instances in an availability zone, then the recommendation is to use multiple spread placement groups. using multiple spread placement groups does not provide guarantees about the spread of instances between groups, but it does ensure the spread for each group, thus limiting impact from certain classes of failures. spread placement groups are not supported for dedicated instances or dedicated hosts.you can create a placement group using one of the following methods. noteyou can tag a placement group on creation using the command line tools only. to create a placement group using the console open the amazon ec2 console at . in the navigation pane, choose placement groups, create placement group. specify a name for the group. choose the placement strategy for the group. if you choose partition, choose the number of partitions within the group. choose create group. to create a placement group using the console open the amazon ec2 console at . in the navigation pane, choose placement groups, create placement group. specify a name for the group. choose the placement strategy for the group. if you choose partition, specify the number of partitions within the group. choose create. to create a placement group using the aws cliuse the  command. the following example creates a placement group named  that uses the  placement strategy, and it applies a tag with a key of  and a value of . to create a partition placement group using the aws cliuse the  command. specify the  parameter with the value , and specify the  parameter with the desired number of partitions. in this example, the partition placement group is named  and is created with five partitions. to create a placement group using the aws tools for windows powershelluse the  command. to help categorize and manage your existing placement groups, you can tag them with custom metadata. for more information about how tags work, see . when you tag a placement group, the instances that are launched into the placement group are not automatically tagged. you need to explicitly tag the instances that are launched into the placement group. for more information, see . you can view, add, and delete tags using the new console and the command line tools. to view, add, or delete a tag for an existing placement group open the amazon ec2 console at . in the navigation pane, choose placement groups. select a placement group, and then choose actions, manage tags. the manage tags section displays any tags that are assigned to the placement group. do the following to add or remove tags: to add a tag, choose add tag, and then enter the tag key and value. you can add up to 50 tags per placement group. for more information, see .to delete a tag, choose remove next to the tag that you want to delete.choose save changes. to view placement group tagsuse the  command to view the tags for the specified resource. in the following example, you describe the tags for all of your placement groups. you can also use the  command to view the tags for a placement group by specifying its id. in the following example, you describe the tags for . you can also view the tags of a placement group by describing the placement group. use the  command to view the configuration of the specified placement group, which includes any tags that were specified for the placement group. to tag an existing placement group using the aws cliyou can use the  command to tag existing resources. in the following example, the existing placement group is tagged with key=cost-center and value=cc-123. to delete a tag from a placement group using the aws cliyou can use the  command to delete tags from existing resources. for examples, see  in the aws cli command reference. to view placement group tagsuse the  command. to describe the tags for a specific placement groupuse the  command. to tag an existing placement groupuse the  command. to delete a tag from a placement groupuse the  command. you can launch an instance into a placement group if the  using one of the following methods. to launch instances into a placement group using the console open the amazon ec2 console at . in the navigation pane, choose instances. choose launch instance. complete the wizard as directed, taking care to do the following: on the choose an instance type page, select an instance type that can be launched into a placement group.on the configure instance details page, the following fields are applicable to placement groups:for number of instances, enter the total number of instances that you need in this placement group, because you might not be able to add instances to the placement group later.for placement group, select the add instance to placement group check box. if you do not see placement group on this page, verify that you have selected an instance type that can be launched into a placement group. otherwise, this option is not available.for placement group name, you can choose to add the instances to an existing placement group or to a new placement group that you create.for placement group strategy, choose the appropriate strategy. if you choose partition, for target partition, choose auto distribution to have amazon ec2 do a best effort to distribute the instances evenly across all the partitions in the group. alternatively, specify the partition in which to launch the instances.to launch instances into a placement group using the aws cliuse the  command and specify the placement group name using the  parameter. in this example, the placement group is named . to launch instances into a specific partition of a partition placement group using the aws cliuse the  command and specify the placement group name and partition using the  parameter. in this example, the placement group is named  and the partition number is . to launch instances into a placement group using aws tools for windows powershelluse the  command and specify the placement group name using the  parameter. you can view the placement information of your instances using one of the following methods. you can also filter partition placement groups by the partition number using the aws cli. to view the placement group and partition number of an instance using the console open the amazon ec2 console at . in the navigation pane, choose instances.  select the instance and, in the details pane, inspect placement group. if the instance is not in a placement group, the field is empty. otherwise, the placement group name is displayed. if the placement group is a partition placement group, inspect partition number for the partition number for the instance. to view the partition number for an instance in a partition placement group using the aws cliuse the  command and specify the  parameter. the response contains the placement information, which includes the placement group name and the partition number for the instance. to filter instances for a specific partition placement group and partition number using the aws cliuse the  command and specify the  parameter with the  and  filters. in this example, the placement group is named  and the partition number is . the response lists all the instances that are in the specified partition within the specified placement group. the following is example output showing only the instance id, instance type, and placement information for the returned instances. you can change the placement group for an instance in any of the following ways: move an existing instance to a placement groupmove an instance from one placement group to anotherremove an instance from a placement group before you move or remove the instance, the instance must be in the  state. you can move or remove an instance using the aws cli or an aws sdk. to move an instance to a placement group using the aws cli stop the instance using the  command. use the  command and specify the name of the placement group to which to move the instance. start the instance using the  command. to move an instance to a placement group using the aws tools for windows powershell stop the instance using the  command. use the  command and specify the name of the placement group to which to move the instance. start the instance using the  command.   to remove an instance from a placement group using the aws cli stop the instance using the  command. use the  command and specify an empty string for the placement group name. start the instance using the  command. to remove an instance from a placement group using the aws tools for windows powershell stop the instance using the  command. use the  command and specify an empty string for the placement group name. start the instance using the  command. if you need to replace a placement group or no longer need one, you can delete it. you can delete a placement group using one of the following methods. importantbefore you can delete a placement group, it must contain no instances. you can  all instances that you launched into the placement group,  them to another placement group, or  them from the placement group. you can verify that an instance is in a placement group before you terminate or move it by selecting the instance in the instances screen and checking the value of placement group in the details pane. to delete a placement group using the console open the amazon ec2 console at . in the navigation pane, choose placement groups. select the placement group and choose delete. when prompted for confirmation, enter delete and then choose delete. to delete a placement group using the console open the amazon ec2 console at . in the navigation pane, choose placement groups. select the placement group and choose delete placement group. when prompted for confirmation, choose delete. to delete a placement group using the aws cli use the  command and specify the placement group name to delete the placement group. in this example, the placement group name is . to delete a placement group using the aws tools for windows powershelluse the  command to delete the placement group. 
when you create an iam policy that grants iam users permission to use ec2 resources, you can include tag information in the  element of the policy to control access based on tags. this gives you better control over which ec2 resources a user can modify, use, or delete. for example, you can create a policy that allows users to terminate an instance but denies the action if the instance has the tag . to do this, you use the  condition key to allow or deny access to the resource based on the tags that are attached to the resource.  to learn whether an amazon ec2 api action supports controlling access using the  condition key, see  in the iam user guide. note that the  actions do not support resource-level permissions, and therefore you must specify them in a separate statement without conditions.  for example iam policies, see .  noteif you allow or deny users access to resources based on tags, you must consider explicitly denying users the ability to add those tags to or remove them from the same resources. otherwise, it's possible for a user to circumvent your restrictions and gain access to a resource by modifying its tags.  
after you launch an instance, you can open the amazon ec2 console and view the monitoring graphs for an instance on the monitoring tab. each graph is based on one of the available amazon ec2 metrics. the following graphs are available: average cpu utilization (percent)average disk reads (bytes)average disk writes (bytes)maximum network in (bytes)maximum network out (bytes)summary disk read operations (count)summary disk write operations (count)summary status (any)summary status instance (count)summary status system (count)for more information about the metrics and the data they provide to the graphs, see . graph metrics using the cloudwatch consoleyou can also use the cloudwatch console to graph metric data generated by amazon ec2 and other aws services. for more information, see  in the amazon cloudwatch user guide. 
amazon ec2 provides different resources that you can use. these resources include images, instances, volumes, and snapshots. when you create your aws account, we set default quotas (also referred to as limits) on these resources on a per-region basis. for example, there is a limit on the number of instances that you can launch in a region. therefore, when you launch an instance in the us west (oregon) region, the request must not cause your usage to exceed your current instance limit in that region. the amazon ec2 console provides limit information for the resources managed by the amazon ec2 and amazon vpc consoles. you can request an increase for many of these limits. use the limit information that we provide to manage your aws infrastructure. plan to request any limit increases in advance of the time that you'll need them. for more information, see  in the amazon web services general reference. use the ec2 limits page in the amazon ec2 console to view the current limits for resources provided by amazon ec2 and amazon vpc, on a per-region basis. to view your current limits open the amazon ec2 console at . from the navigation bar, select a region. from the navigation pane, choose limits. locate the resource in the list. you can use the search fields to filter the list by resource name or resource group. the current limit column displays the current maximum for the resource for your account. use the limits page in the amazon ec2 console to request an increase in the limits for resources provided by amazon ec2 or amazon vpc, on a per-region basis. alternatively, request an increase using service quotas. for more information, see  in the service quotas user guide. to request a limit increase using the amazon ec2 console open the amazon ec2 console at . from the navigation bar, select a region. from the navigation pane, choose limits. select the resource in the list, and choose request limit increase. complete the required fields on the limit increase form. we'll respond to you using the contact method that you specified. amazon ec2 restricts traffic on port 25 of all instances by default. you can request that this restriction be removed. for more information, see  in the aws knowledge center. 
if you created your aws account before 2013-12-04, you might have support for ec2-classic in some regions. some amazon ec2 resources and features, such as enhanced networking and newer instance types, require a virtual private cloud (vpc). some resources can be shared between ec2-classic and a vpc, while some can't. for more information, see . if your account supports ec2-classic, you might have set up resources for use in ec2-classic. if you want to migrate from ec2-classic to a vpc, you must recreate those resources in your vpc. there are two ways of migrating to a vpc. you can do a full migration, or you can do an incremental migration over time. the method you choose depends on the size and complexity of your application in ec2-classic. for example, if your application consists of one or two instances running a static website, and you can afford a short period of downtime, you can do a full migration. if you have a multi-tier application with processes that cannot be interrupted, you can do an incremental migration using classiclink. this allows you to transfer functionality one component at a time until your application is running fully in your vpc. if you need to migrate a windows instance, see  in the amazon ec2 user guide for windows instances. topics complete the following tasks to fully migrate your application from ec2-classic to a vpc. topics to start using a vpc, ensure that you have one in your account. you can create one using one of these methods:  your aws account comes with a default vpc in each region, which is ready for you to use. instances that you launch are by default launched into this vpc, unless you specify otherwise. for more information about your default vpc, see . use this option if you'd prefer not to set up a vpc yourself, or if you do not need specific requirements for your vpc configuration.in your existing aws account, open the amazon vpc console and use the vpc wizard to create a new vpc. for more information, see . use this option if you want to set up a vpc quickly in your existing ec2-classic account, using one of the available configuration sets in the wizard. you'll specify this vpc each time you launch an instance. in your existing aws account, open the amazon vpc console and set up the components of a vpc according to your requirements. for more information, see . use this option if you have specific requirements for your vpc, such as a particular number of subnets. you'll specify this vpc each time you launch an instance. you cannot use the same security groups between ec2-classic and a vpc. however, if you want your instances in your vpc to have the same security group rules as your ec2-classic instances, you can use the amazon ec2 console to copy your existing ec2-classic security group rules to a new vpc security group.  importantyou can only copy security group rules to a new security group in the same aws account in the same region. if you've created a new aws account, you cannot use this method to copy your existing security group rules to your new account. you'll have to create a new security group, and add the rules yourself. for more information about creating a new security group, see . to copy your security group rules to a new security group open the amazon ec2 console at . in the navigation pane, choose security groups. select the security group that's associated with your ec2-classic instance, then choose actions and select copy to new. in the create security group dialog box, specify a name and description for your new security group. select your vpc from the vpc list.  the inbound tab is populated with the rules from your ec2-classic security group. you can modify the rules as required. in the outbound tab, a rule that allows all outbound traffic has automatically been created for you. for more information about modifying security group rules, see . noteif you've defined a rule in your ec2-classic security group that references another security group, you will not be able to use the same rule in your vpc security group. modify the rule to reference a security group in the same vpc. choose create. an ami is a template for launching your instance. you can create your own ami based on an existing ec2-classic instance, then use that ami to launch instances into your vpc. the method you use to create your ami depends on the root device type of your instance, and the operating system platform on which your instance runs. to find out the root device type of your instance, go to the instances page, select your instance, and look at the information in the root device type field in the description tab. if the value is , then your instance is ebs-backed. if the value is , then your instance is instance store-backed. you can also use the  aws cli command to find out the root device type. the following table provides options for you to create your ami based on the root device type of your instance, and the software platform. importantsome instance types support both pv and hvm virtualization, while others support only one or the other. if you plan to use your ami to launch a different instance type than your current instance type, check that the instance type supports the type of virtualization that your ami offers. if your ami supports pv virtualization, and you want to use an instance type that supports hvm virtualization, you may have to reinstall your software on a base hvm ami. for more information about pv and hvm virtualization, see . you can create an amazon ebs volume and use it to back up and store the data on your instance—like you would use a physical hard drive. amazon ebs volumes can be attached and detached from any instance in the same availability zone. you can detach a volume from your instance in ec2-classic, and attach it to a new instance that you launch into your vpc in the same availability zone.  for more information about amazon ebs volumes, see the following topics: to back up the data on your amazon ebs volume, you can take periodic snapshots of your volume. for more information, see . if you need to, you can create an amazon ebs volume from your snapshot. for more information, see . after you've created an ami, you can launch an instance into your vpc. the instance will have the same data and configurations as your existing ec2-classic instance.  you can either launch your instance into a vpc that you've created in your existing account, or into a new, vpc-only aws account. you can use the amazon ec2 launch wizard to launch an instance into your vpc. to launch an instance into your vpc open the amazon ec2 console at . on the dashboard, choose launch instance.  on the choose an amazon machine image page, select the my amis category, and select the ami you created.  on the choose an instance type page, select the type of instance, and choose next: configure instance details. on the configure instance details page, select your vpc from the network list. select the required subnet from the subnet list. configure any other details you require, then go through the next pages of the wizard until you reach the configure security group page. select select an existing group, and select the security group you created earlier. choose review and launch. review your instance details, then choose launch to specify a key pair and launch your instance. for more information about the parameters you can configure in each step of the wizard, see . to launch an instance in your new aws account, you'll first have to share the ami you created with your new account. you can then use the amazon ec2 launch wizard to launch an instance into your default vpc.  to share an ami with your new aws account open the amazon ec2 console at . switch to the account in which you created your ami. in the navigation pane, choose amis.  in the filter list, ensure owned by me is selected, then select your ami.  in the permissions tab, choose edit. enter the account number of your new aws account, choose add permission, and then choose save. to launch an instance into your default vpc open the amazon ec2 console at . switch to your new aws account. in the navigation pane, choose amis. in the filter list, select private images. select the ami that you shared from your ec2-classic account, then choose launch.  on the choose an instance type page, select the type of instance, and choose next: configure instance details. on the configure instance details page, your default vpc should be selected in the network list. configure any other details you require, then go through the next pages of the wizard until you reach the configure security group page. select select an existing group, and select the security group you created earlier. choose review and launch. review your instance details, then choose launch to specify a key pair and launch your instance. for more information about the parameters you can configure in each step of the wizard, see . in this example, you use aws to host your gardening website. to manage your website, you have three running instances in ec2-classic. instances a and b host your public-facing web application, and you use elastic load balancing to load balance the traffic between these instances. you've assigned elastic ip addresses to instances a and b so that you have static ip addresses for configuration and administration tasks on those instances. instance c holds your mysql database for your website. you've registered the domain name , and you've used route 53 to create a hosted zone with an alias record set that's associated with the dns name of your load balancer.  the first part of migrating to a vpc is deciding what kind of vpc architecture will suit your needs. in this case, you've decided on the following: one public subnet for your web servers, and one private subnet for your database server. as your website grows, you can add more web servers and database servers to your subnets. by default, instances in the private subnet cannot access the internet; however, you can enable internet access through a network address translation (nat) device in the public subnet. you may want to set up a nat device to support periodic updates and patches from the internet for your database server. you'll migrate your elastic ip addresses to a vpc, and create a load balancer in your public subnet to load balance the traffic between your web servers.  to migrate your web application to a vpc, you can follow these steps: create a vpc: in this case, you can use the vpc wizard in the amazon vpc console to create your vpc and subnets. the second wizard configuration creates a vpc with one private and one public subnet, and launches and configures a nat device in your public subnet for you. for more information, see  in the amazon vpc user guide.create amis from your instances: create an ami from one of your web servers, and a second ami from your database server. for more information, see .configure your security groups: in your ec2-classic environment, you have one security group for your web servers, and another security group for your database server. you can use the amazon ec2 console to copy the rules from each security group into new security groups for your vpc. for more information, see .  tipcreate the security groups that are referenced by other security groups first.launch an instance into your new vpc: launch replacement web servers into your public subnet, and launch your replacement database server into your private subnet. for more information, see .configure your nat device: if you are using a nat instance, you must create security group for it that allows http and https traffic from your private subnet. for more information, see . if you are using a nat gateway, traffic from your private subnet is automatically allowed.configure your database: when you created an ami from your database server in ec2-classic, all the configuration information that was stored in that instance was copied to the ami. you may have to connect to your new database server and update the configuration details; for example, if you configured your database to grant full read, write, and modification permissions to your web servers in ec2-classic, you'll have to update the configuration files to grant the same permissions to your new vpc web servers instead. configure your web servers: your web servers will have the same configuration settings as your instances in ec2-classic. for example, if you configured your web servers to use the database in ec2-classic, update your web servers' configuration settings to point to your new database instance. noteby default, instances launched into a nondefault subnet are not assigned a public ip address, unless you specify otherwise at launch. your new database server may not have a public ip address. in this case, you can update your web servers' configuration file to use your new database server's private dns name. instances in the same vpc can communicate with each other via private ip address.migrate your elastic ip addresses: disassociate your elastic ip addresses from your web servers in ec2-classic, and then migrate them to a vpc. after you've migrated them, you can associate them with your new web servers in your vpc. for more information, see .create a new load balancer: to continue using elastic load balancing to load balance the traffic to your instances, make sure you understand the various ways you can configure your load balancer in vpc. for more information, see .update your dns records: after you've set up your load balancer in your public subnet, ensure that your  domain points to your new load balancer. to do this, you'll need to update your dns records and update your alias record set in route 53. for more information about using route 53, see .shut down your ec2-classic resources: after you've verified that your web application is working from within the vpc architecture, you can shut down your ec2-classic resources to stop incurring charges for them. terminate your ec2-classic instances, and release your ec2-classic elastic ip addresses.the classiclink feature makes it easier to manage an incremental migration to a vpc. classiclink allows you to link an ec2-classic instance to a vpc in your account in the same region, allowing your new vpc resources to communicate with the ec2-classic instance using private ipv4 addresses. you can then migrate functionality to the vpc one step at a time. this topic provides some basic steps for managing an incremental migration from ec2-classic to a vpc.  for more information about classiclink, see . topics to use classiclink effectively, you must first identify the components of your application that must be migrated to the vpc, and then confirm the order in which to migrate that functionality.  for example, you have an application that relies on a presentation web server, a backend database server, and authentication logic for transactions. you may decide to start the migration process with the authentication logic, then the database server, and finally, the web server. to start using a vpc, ensure that you have one in your account. you can create one using one of these methods:  in your existing aws account, open the amazon vpc console and use the vpc wizard to create a new vpc. for more information, see . use this option if you want to set up a vpc quickly in your existing ec2-classic account, using one of the available configuration sets in the wizard. you'll specify this vpc each time you launch an instance. in your existing aws account, open the amazon vpc console and set up the components of a vpc according to your requirements. for more information, see . use this option if you have specific requirements for your vpc, such as a particular number of subnets. you'll specify this vpc each time you launch an instance. after you've created a vpc, you can enable it for classiclink. for more information about classiclink, see .  to enable a vpc for classiclink open the amazon vpc console at . in the navigation pane, choose your vpcs. select your vpc, and then select enable classiclink from the actions list. in the confirmation dialog box, choose yes, enable. an ami is a template for launching your instance. you can create your own ami based on an existing ec2-classic instance, then use that ami to launch instances into your vpc. the method you use to create your ami depends on the root device type of your instance, and the operating system platform on which your instance runs. to find out the root device type of your instance, go to the instances page, select your instance, and look at the information in the root device type field in the description tab. if the value is , then your instance is ebs-backed. if the value is , then your instance is instance store-backed. you can also use the  aws cli command to find out the root device type. the following table provides options for you to create your ami based on the root device type of your instance, and the software platform. importantsome instance types support both pv and hvm virtualization, while others support only one or the other. if you plan to use your ami to launch a different instance type than your current instance type, check that the instance type supports the type of virtualization that your ami offers. if your ami supports pv virtualization, and you want to use an instance type that supports hvm virtualization, you may have to reinstall your software on a base hvm ami. for more information about pv and hvm virtualization, see . you can create an amazon ebs volume and use it to back up and store the data on your instance—like you would use a physical hard drive. amazon ebs volumes can be attached and detached from any instance in the same availability zone. you can detach a volume from your instance in ec2-classic, and attach it to a new instance that you launch into your vpc in the same availability zone.  for more information about amazon ebs volumes, see the following topics: to back up the data on your amazon ebs volume, you can take periodic snapshots of your volume. for more information, see . if you need to, you can create an amazon ebs volume from your snapshot. for more information, see . the next step in the migration process is to launch instances into your vpc so that you can start transferring functionality to them. you can use the amis that you created in the previous step to launch instances into your vpc. the instances will have the same data and configurations as your existing ec2-classic instances.  to launch an instance into your vpc using your custom ami open the amazon ec2 console at . on the dashboard, choose launch instance.  on the choose an amazon machine image page, select the my amis category, and select the ami you created.  on the choose an instance type page, select the type of instance, and choose next: configure instance details. on the configure instance details page, select your vpc from the network list. select the required subnet from the subnet list. configure any other details you require, then go through the next pages of the wizard until you reach the configure security group page. select select an existing group, and select the security group you created earlier. choose review and launch. review your instance details, then choose launch to specify a key pair and launch your instance. for more information about the parameters you can configure in each step of the wizard, see . after you've launched your instance and it's in the  state, you can connect to it and configure it as required. after you've configured your instances and made the functionality of your application available in the vpc, you can use classiclink to enable private ip communication between your new vpc instances and your ec2-classic instances. to link an instance to a vpc open the amazon ec2 console at . in the navigation pane, choose instances.  select your ec2-classic instance, then choose actions, classiclink, and link to vpc. noteensure that your instance is in the  state. in the dialog box, select your classiclink-enabled vpc (only vpcs that are enabled for classiclink are displayed). select one or more of the vpc security groups to associate with your instance. when you are done, choose link to vpc. depending on the size of your application and the functionality that must be migrated, repeat steps 4 to 6 until you've moved all the components of your application from ec2-classic into your vpc.  after you've enabled internal communication between the ec2-classic and vpc instances, you must update your application to point to your migrated service in your vpc, instead of your service in the ec2-classic platform. the exact steps for this depend on your application’s design. generally, this includes updating your destination ip addresses to point to the ip addresses of your vpc instances instead of your ec2-classic instances. you can migrate your elastic ip addresses that you are currently using in the ec2-classic platform to a vpc. for more information, see . after you've completed this step and you've tested that the application is functioning from your vpc, you can terminate your ec2-classic instances, and disable classiclink for your vpc. you can also clean up any ec2-classic resources that you may no longer need to avoid incurring charges for them; for example, you can release elastic ip addresses, and delete the volumes that were associated with your ec2-classic instances. 
amazon virtual private cloud (amazon vpc) enables you to define a virtual network in your own logically isolated area within the aws cloud, known as a virtual private cloud (vpc). you can launch your amazon ec2 resources, such as instances, into the subnets of your vpc. your vpc closely resembles a traditional network that you might operate in your own data center, with the benefits of using scalable infrastructure from aws. you can configure your vpc; you can select its ip address range, create subnets, and configure route tables, network gateways, and security settings. you can connect instances in your vpc to the internet or to your own data center. when you create your aws account, we create a default vpc for you in each region. a default vpc is a vpc that is already configured and ready for you to use. you can launch instances into your default vpc immediately. alternatively, you can create your own nondefault vpc and configure it as you need. if you created your aws account before 2013-12-04, you might have support for the ec2-classic platform in some regions. if you created your aws account after 2013-12-04, it does not support ec2-classic, so you must launch your resources in a vpc. for more information, see . for more information about amazon vpc, see the following documentation. 
you can specify multiple private ipv4 and ipv6 addresses for your instances. the number of network interfaces and private ipv4 and ipv6 addresses that you can specify for an instance depends on the instance type. for more information, see . it can be useful to assign multiple ip addresses to an instance in your vpc to do the following: host multiple websites on a single server by using multiple ssl certificates on a single server and associating each certificate with a specific ip address.operate network appliances, such as firewalls or load balancers, that have multiple ip addresses for each network interface. redirect internal traffic to a standby instance in case your instance fails, by reassigning the secondary ip address to the standby instance. topics the following list explains how multiple ip addresses work with network interfaces: you can assign a secondary private ipv4 address to any network interface. the network interface need not be attached to the instance.you can assign multiple ipv6 addresses to a network interface that's in a subnet that has an associated ipv6 cidr block.you must choose a secondary ipv4 address from the ipv4 cidr block range of the subnet for the network interface.you must choose ipv6 addresses from the ipv6 cidr block range of the subnet for the network interface.you associate security groups with network interfaces, not individual ip addresses. therefore, each ip address you specify in a network interface is subject to the security group of its network interface.multiple ip addresses can be assigned and unassigned to network interfaces attached to running or stopped instances.secondary private ipv4 addresses that are assigned to a network interface can be reassigned to another one if you explicitly allow it.an ipv6 address cannot be reassigned to another network interface; you must first unassign the ipv6 address from the existing network interface.when assigning multiple ip addresses to a network interface using the command line tools or api, the entire operation fails if one of the ip addresses can't be assigned.primary private ipv4 addresses, secondary private ipv4 addresses, elastic ip addresses, and ipv6 addresses remain with a secondary network interface when it is detached from an instance or attached to an instance.although you can't detach the primary network interface from an instance, you can reassign the secondary private ipv4 address of the primary network interface to another network interface.the following list explains how multiple ip addresses work with elastic ip addresses (ipv4 only): each private ipv4 address can be associated with a single elastic ip address, and vice versa.when a secondary private ipv4 address is reassigned to another interface, the secondary private ipv4 address retains its association with an elastic ip address.when a secondary private ipv4 address is unassigned from an interface, an associated elastic ip address is automatically disassociated from the secondary private ipv4 address.you can assign a secondary private ipv4 address to an instance, associate an elastic ipv4 address with a secondary private ipv4 address, and unassign a secondary private ipv4 address. topics you can assign the secondary private ipv4 address to the network interface for an instance as you launch the instance, or after the instance is running. this section includes the following procedures. to assign a secondary private ipv4 address when launching an instance open the amazon ec2 console at . choose launch instance. select an ami, then choose an instance type and choose next: configure instance details. on the configure instance details page, for network, select a vpc and for subnet, select a subnet. in the network interfaces section, do the following, and then choose next: add storage: to add another network interface, choose add device. the console enables you to specify up to two network interfaces when you launch an instance. after you launch the instance, choose network interfaces in the navigation pane to add additional network interfaces. the total number of network interfaces that you can attach varies by instance type. for more information, see .  importantwhen you add a second network interface, the system can no longer auto-assign a public ipv4 address. you will not be able to connect to the instance over ipv4 unless you assign an elastic ip address to the primary network interface (eth0). you can assign the elastic ip address after you complete the launch wizard. for more information, see .for each network interface, under secondary ip addresses, choose add ip, and then enter a private ip address from the subnet range, or accept the default  value to let amazon select an address.on the next add storage page, you can specify volumes to attach to the instance besides the volumes specified by the ami (such as the root device volume), and then choose next: add tags. on the add tags page, specify tags for the instance, such as a user-friendly name, and then choose next: configure security group. on the configure security group page, select an existing security group or create a new one. choose review and launch.  on the review instance launch page, review your settings, and then choose launch to choose a key pair and launch your instance. if you're new to amazon ec2 and haven't created any key pairs, the wizard prompts you to create one. importantafter you have added a secondary private ip address to a network interface, you must connect to the instance and configure the secondary private ip address on the instance itself. for more information, see . to assign a secondary ipv4 address during launch using the command line you can use one of the following commands. for more information about these command line interfaces, see .the  option with the  command (aws cli)define  and specify the  parameter with the  command (aws tools for windows powershell).to assign a secondary private ipv4 address to a network interface open the amazon ec2 console at . in the navigation pane, choose network interfaces, and then select the network interface attached to the instance. choose actions, manage ip addresses. under ipv4 addresses, choose assign new ip. enter a specific ipv4 address that's within the subnet range for the instance, or leave the field blank to let amazon select an ip address for you. (optional) choose allow reassignment to allow the secondary private ip address to be reassigned if it is already assigned to another network interface. choose yes, update. alternatively, you can assign a secondary private ipv4 address to an instance. choose instances in the navigation pane, select the instance, and then choose actions, networking, manage ip addresses. you can configure the same information as you did in the steps above. the ip address is assigned to the primary network interface (eth0) for the instance.  to assign a secondary private ipv4 to an existing instance using the command line you can use one of the following commands. for more information about these command line interfaces, see . (aws cli) (aws tools for windows powershell)after you assign a secondary private ipv4 address to your instance, you need to configure the operating system on your instance to recognize the secondary private ip address. if you are using amazon linux, the ec2-net-utils package can take care of this step for you. it configures additional network interfaces that you attach while the instance is running, refreshes secondary ipv4 addresses during dhcp lease renewal, and updates the related routing rules. you can immediately refresh the list of interfaces by using the command  and then view the up-to-date list using . if you require manual control over your network configuration, you can remove the ec2-net-utils package. for more information, see .if you are using another linux distribution, see the documentation for your linux distribution. search for information about configuring additional network interfaces and secondary ipv4 addresses. if the instance has two or more interfaces on the same subnet, search for information about using routing rules to work around asymmetric routing.for information about configuring a windows instance, see  in the amazon ec2 user guide for windows instances. to associate an elastic ip address with a secondary private ipv4 address open the amazon ec2 console at . in the navigation pane, choose elastic ips. choose actions, and then select associate address. for network interface, select the network interface, and then select the secondary ip address from the private ip list. choose associate. to associate an elastic ip address with a secondary private ipv4 address using the command line you can use one of the following commands. for more information about these command line interfaces, see . (aws cli) (aws tools for windows powershell)to view the private ipv4 addresses assigned to a network interface open the amazon ec2 console at . in the navigation pane, choose network interfaces. select the network interface with private ip addresses to view. on the details tab in the details pane, check the primary private ipv4 ip and secondary private ipv4 ips fields for the primary private ipv4 address and any secondary private ipv4 addresses assigned to the network interface. to view the private ipv4 addresses assigned to an instance open the amazon ec2 console at . in the navigation pane, choose instances. select the instance with private ipv4 addresses to view. on the description tab in the details pane, check the private ips and secondary private ips fields for the primary private ipv4 address and any secondary private ipv4 addresses assigned to the instance through its network interface. if you no longer require a secondary private ipv4 address, you can unassign it from the instance or the network interface. when a secondary private ipv4 address is unassigned from a network interface, the elastic ip address (if it exists) is also disassociated. to unassign a secondary private ipv4 address from an instance open the amazon ec2 console at . in the navigation pane, choose instances. select an instance, choose actions, networking, manage ip addresses. under ipv4 addresses, choose unassign for the ipv4 address to unassign. choose yes, update. to unassign a secondary private ipv4 address from a network interface open the amazon ec2 console at . in the navigation pane, choose network interfaces. select the network interface, choose actions, manage ip addresses. under ipv4 addresses, choose unassign for the ipv4 address to unassign. choose yes, update. to unassign a secondary private ipv4 address using the command line you can use one of the following commands. for more information about these command line interfaces, see . (aws cli) (aws tools for windows powershell)you can assign multiple ipv6 addresses to your instance, view the ipv6 addresses assigned to your instance, and unassign ipv6 addresses from your instance. topics you can assign one or more ipv6 addresses to your instance during launch or after launch. to assign an ipv6 address to an instance, the vpc and subnet in which you launch the instance must have an associated ipv6 cidr block. for more information, see  in the amazon vpc user guide. to assign multiple ipv6 addresses during launch open the amazon ec2 console at . from the dashboard, choose launch instance. select an ami, choose an instance type, and choose next: configure instance details. ensure that you choose an instance type that support ipv6. for more information, see . on the configure instance details page, select a vpc from the network list, and a subnet from the subnet list.  in the network interfaces section, do the following, and then choose next: add storage: to assign a single ipv6 address to the primary network interface (eth0), under ipv6 ips, choose add ip. to add a secondary ipv6 address, choose add ip again. you can enter an ipv6 address from the range of the subnet, or leave the default auto-assign value to let amazon choose an ipv6 address from the subnet for you.choose add device to add another network interface and repeat the steps above to add one or more ipv6 addresses to the network interface. the console enables you to specify up to two network interfaces when you launch an instance. after you launch the instance, choose network interfaces in the navigation pane to add additional network interfaces. the total number of network interfaces that you can attach varies by instance type. for more information, see . follow the next steps in the wizard to attach volumes and tag your instance. on the configure security group page, select an existing security group or create a new one. if you want your instance to be reachable over ipv6, ensure that your security group has rules that allow access from ipv6 addresses. for more information, see . choose review and launch.  on the review instance launch page, review your settings, and then choose launch to choose a key pair and launch your instance. if you're new to amazon ec2 and haven't created any key pairs, the wizard prompts you to create one. you can use the instances screen amazon ec2 console to assign multiple ipv6 addresses to an existing instance. this assigns the ipv6 addresses to the primary network interface (eth0) for the instance. to assign a specific ipv6 address to the instance, ensure that the ipv6 address is not already assigned to another instance or network interface. to assign multiple ipv6 addresses to an existing instance open the amazon ec2 console at . in the navigation pane, choose instances. select your instance, choose actions, networking, manage ip addresses. under ipv6 addresses, choose assign new ip for each ipv6 address you want to add. you can specify an ipv6 address from the range of the subnet, or leave the auto-assign value to let amazon choose an ipv6 address for you. choose yes, update. alternatively, you can assign multiple ipv6 addresses to an existing network interface. the network interface must have been created in a subnet that has an associated ipv6 cidr block. to assign a specific ipv6 address to the network interface, ensure that the ipv6 address is not already assigned to another network interface. to assign multiple ipv6 addresses to a network interface open the amazon ec2 console at . in the navigation pane, choose network interfaces. select your network interface, choose actions, manage ip addresses. under ipv6 addresses, choose assign new ip for each ipv6 address you want to add. you can specify an ipv6 address from the range of the subnet, or leave the auto-assign value to let amazon choose an ipv6 address for you. choose yes, update. cli overview you can use one of the following commands. for more information about these command line interfaces, see . assign an ipv6 address during launch:use the  or  options with the  command (aws cli)define  and specify the  or  parameters with the  command (aws tools for windows powershell).assign an ipv6 address to a network interface: (aws cli) (aws tools for windows powershell)you can view the ipv6 addresses for an instance or for a network interface. to view the ipv6 addresses assigned to an instance open the amazon ec2 console at . in the navigation pane, choose instances. select your instance. in the details pane, review the ipv6 ips field. to view the ipv6 addresses assigned to a network interface open the amazon ec2 console at . in the navigation pane, choose network interfaces. select your network interface. in the details pane, review the ipv6 ips field. cli overview you can use one of the following commands. for more information about these command line interfaces, see . view the ipv6 addresses for an instance: (aws cli) (aws tools for windows powershell).view the ipv6 addresses for a network interface: (aws cli) (aws tools for windows powershell)you can unassign an ipv6 address from the primary network interface of an instance, or you can unassign an ipv6 address from a network interface. to unassign an ipv6 address from an instance open the amazon ec2 console at . in the navigation pane, choose instances. select your instance, choose actions, networking, manage ip addresses. under ipv6 addresses, choose unassign for the ipv6 address to unassign. choose yes, update. to unassign an ipv6 address from a network interface open the amazon ec2 console at . in the navigation pane, choose network interfaces. select your network interface, choose actions, manage ip addresses. under ipv6 addresses, choose unassign for the ipv6 address to unassign. choose save. cli overview you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell).
with step scaling policies, you specify cloudwatch alarms to trigger the scaling process. for example, if you want to scale out when cpu utilization reaches a certain level, create an alarm using the  metric provided by amazon ec2. when you create a step scaling policy, you must specify one of the following scaling adjustment types: add – increase the target capacity of the fleet by a specified number of capacity units or a specified percentage of the current capacity.remove – decrease the target capacity of the fleet by a specified number of capacity units or a specified percentage of the current capacity.set to – set the target capacity of the fleet to the specified number of capacity units.when an alarm is triggered, the automatic scaling process calculates the new target capacity using the fulfilled capacity and the scaling policy, and then updates the target capacity accordingly. for example, suppose that the target capacity and fulfilled capacity are 10 and the scaling policy adds 1. when the alarm is triggered, the automatic scaling process adds 1 to 10 to get 11, so spot fleet launches 1 instance. when a spot fleet terminates an instance because the target capacity was decreased, the instance receives a spot instance interruption notice. limitation the spot fleet request must have a request type of . automatic scaling is not supported for one-time requests or spot blocks.prerequisites consider which cloudwatch metrics are important to your application. you can create cloudwatch alarms based on metrics provided by aws or your own custom metrics.for the aws metrics that you will use in your scaling policies, enable cloudwatch metrics collection if the service that provides the metrics does not enable it by default.to create a cloudwatch alarm open the cloudwatch console at . in the navigation pane, choose alarms. choose create alarm. on the specify metric and conditions page, choose select metric.  choose ec2 spot, fleet request metrics, select a metric (for example, cpuutilization), and then choose select metric. the specify metric and conditions page appears, showing a graph and other information about the metric you selected.  for period, choose the evaluation period for the alarm, for example, 1 minute. when evaluating the alarm, each period is aggregated into one data point.  notea shorter period creates a more sensitive alarm.  for conditions, define the alarm by defining the threshold condition. for example, you can define a threshold to trigger the alarm whenever the value of the metric is greater than or equal to 80 percent. under additional configuration, for datapoints to alarm, specify how many datapoints (evaluation periods) must be in the alarm state to trigger the alarm, for example, 1 evaluation period or 2 out of 3 evaluation periods. this creates an alarm that goes to alarm state if that many consecutive periods are breaching. for more information, see  in the amazon cloudwatch user guide. for missing data treatment, choose one of the options (or leave the default of treat missing data as missing). for more information, see  in the amazon cloudwatch user guide. choose next. (optional) to receive notification of a scaling event, for notification, you can choose or create the amazon sns topic you want to use to receive notifications. otherwise, you can delete the notification now and add one later as needed. choose next. under add a description, enter a name and description for the alarm and choose next. choose create alarm. to configure a step scaling policy for your spot fleet (console) open the amazon ec2 console at . in the navigation pane, choose spot requests. select your spot fleet request and choose auto scaling. if automatic scaling is not configured, choose configure. use scale capacity between to set the minimum and maximum capacity for your fleet. automatic scaling does not scale your fleet below the minimum capacity or above the maximum capacity. initially, scaling policies contains policies named scaleup and scaledown. you can complete these policies, or choose remove policy to delete them. you can also choose add policy. to define a policy, do the following: for policy name, type a name for the policy. for policy trigger, select an existing alarm or choose create new alarm to open the amazon cloudwatch console and create an alarm. for modify capacity, select a scaling adjustment type, select a number, and select a unit. (optional) to perform step scaling, choose define steps. by default, an add policy has a lower bound of -infinity and an upper bound of the alarm threshold. by default, a remove policy has a lower bound of the alarm threshold and an upper bound of +infinity. to add another step, choose add step. (optional) to modify the default value for the cooldown period, select a number from cooldown period. choose save. to configure step scaling policies for your spot fleet using the aws cli register the spot fleet request as a scalable target using the  command. create a scaling policy using the  command. create an alarm that triggers the scaling policy using the  command. 
you can subscribe to an aws marketplace product and launch an instance from the product's ami using the amazon ec2 launch wizard. for more information about paid amis, see . to cancel your subscription after launch, you first have to terminate all instances running from it. for more information, see .  to launch an instance from the aws marketplace using the launch wizard open the amazon ec2 console at . from the amazon ec2 dashboard, choose launch instance. on the choose an amazon machine image (ami) page, choose the aws marketplace category on the left. find a suitable ami by browsing the categories, or using the search functionality. choose select to choose your product. a dialog displays an overview of the product you've selected. you can view the pricing information, as well as any other information that the vendor has provided. when you're ready, choose continue. noteyou are not charged for using the product until you have launched an instance with the ami. take note of the pricing for each supported instance type, as you will be prompted to select an instance type on the next page of the wizard. additional taxes may also apply to the product. on the choose an instance type page, select the hardware configuration and size of the instance to launch. when you're done, choose next: configure instance details. on the next pages of the wizard, you can configure your instance, add storage, and add tags. for more information about the different options you can configure, see . choose next until you reach the configure security group page.  the wizard creates a new security group according to the vendor's specifications for the product. the security group may include rules that allow all ipv4 addresses () access on ssh (port 22) on linux or rdp (port 3389) on windows. we recommend that you adjust these rules to allow only a specific address or range of addresses to access your instance over those ports. when you are ready, choose review and launch. on the review instance launch page, check the details of the ami from which you're about to launch the instance, as well as the other configuration details you set up in the wizard. when you're ready, choose launch to select or create a key pair, and launch your instance. depending on the product you've subscribed to, the instance may take a few minutes or more to launch. you are first subscribed to the product before your instance can launch. if there are any problems with your credit card details, you will be asked to update your account details. when the launch confirmation page displays, choose view instances to go to the instances page.  noteyou are charged the subscription price as long as your instance is running, even if it is idle. if your instance is stopped, you may still be charged for storage. when your instance is in the  state, you can connect to it. to do this, select your instance in the list and choose connect. follow the instructions in the dialog. for more information about connecting to your instance, see . importantcheck the vendor's usage instructions carefully, as you may need to use a specific user name to log in to the instance. for more information about accessing your subscription details, see . if the instance fails to launch or the state immediately goes to  instead of , see . to launch instances from aws marketplace products using the api or command line tools, first ensure that you are subscribed to the product. you can then launch an instance with the product's ami id using the following methods: 
with amazon ebs, you can create point-in-time snapshots of volumes, which we store for you in amazon s3. after you create a snapshot and it has finished copying to amazon s3 (when the snapshot status is ), you can copy it from one aws region to another, or within the same region. amazon s3 server-side encryption (256-bit aes) protects a snapshot's data in transit during a copy operation. the snapshot copy receives an id that is different from the id of the original snapshot. to copy multi-volume snapshots to another aws region, retrieve the snapshots using the tag you applied to the multi-volume snapshots group when you created it. then individually copy the snapshots to another region. for information about copying an amazon rds snapshot, see  in the amazon rds user guide. if you would like another account to be able to copy your snapshot, you must either modify the snapshot permissions to allow access to that account or make the snapshot public so that all aws accounts can copy it. for more information, see . for pricing information about copying snapshots across aws regions and accounts, see . note that snapshot copy operations within a single account and region do not copy any actual data and therefore are cost-free as long as the encryption status of the snapshot copy does not change. noteif you copy a snapshot to a new region, a complete (non-incremental) copy is always created, resulting in additional delay and storage costs. noteif you copy a snapshot and encrypt it to a new cmk, a complete (non-incremental) copy is always created, resulting in additional delay and storage costs. use cases geographic expansion: launch your applications in a new aws region.migration: move an application to a new region, to enable better availability and to minimize cost.disaster recovery: back up your data and logs across different geographical locations at regular intervals. in case of disaster, you can restore your applications using point-in-time backups stored in the secondary region. this minimizes data loss and recovery time.encryption: encrypt a previously unencrypted snapshot, change the key with which the snapshot is encrypted, or create a copy that you own in order to create a volume from it (for encrypted snapshots that have been shared with you).data retention and auditing requirements: copy your encrypted ebs snapshots from one aws account to another to preserve data logs or other files for auditing or data retention. using a different account helps prevent accidental snapshot deletions, and protects you if your main aws account is compromised.prerequisites you can copy any accessible snapshots that have a  status, including shared snapshots and snapshots that you have created.you can copy aws marketplace, vm import/export, and aws storage gateway snapshots, but you must verify that the snapshot is supported in the destination region.limits each account can have up to twenty concurrent snapshot copy requests to a single destination region.user-defined tags are not copied from the source snapshot to the new snapshot. you can add user-defined tags during or after the copy operation. for more information, see .snapshots created by the  action have an arbitrary volume id that should not be used for any purpose.whether a snapshot copy is incremental is determined by the most recently completed snapshot copy. when you copy a snapshot across regions or accounts, the copy is an incremental copy if the following conditions are met: the snapshot was copied to the destination region or account previously.the most recent snapshot copy still exists in the destination region or account.all copies of the snapshot in the destination region or account are either unencrypted or were encrypted using the same cmk.if the most recent snapshot copy was deleted, the next copy is a full copy, not an incremental copy. if a copy is still pending when you start a another copy, the second copy starts only after the first copy finishes. we recommend that you tag your snapshots with the volume id and creation time so that you can keep track of the most recent snapshot copy of a volume in the destination region or account. to see whether your snapshot copies are incremental, check the  cloudwatch event. when you copy a snapshot, you can encrypt the copy or you can specify a cmk different from the original one, and the resulting copied snapshot uses the new cmk. however, changing the encryption status of a snapshot during a copy operation results in a full (not incremental) copy, which might incur greater data transfer and storage charges.  to copy an encrypted snapshot shared from another aws account, you must have permissions to use the snapshot and the customer master key (cmk) that was used to encrypt the snapshot. when using an encrypted snapshot that was shared with you, we recommend that you re-encrypt the snapshot by copying it using a cmk that you own. this protects you if the original cmk is compromised, or if the owner revokes it, which could cause you to lose access to any encrypted volumes that you created using the snapshot. for more information, see . you apply encryption to ebs snapshot copies by setting the  parameter to . (the  parameter is optional if  is enabled). optionally, you can use  to specify a custom key to use to encrypt the snapshot copy. (the  parameter must also be set to , even if encryption by default is enabled.) if  is not specified, the key that is used for encryption depends on the encryption state of the source snapshot and its ownership. the following table describes the encryption outcome for each possible combination of settings. encryption outcomes: copying a snapshot * this is the default cmk used for ebs encryption for the aws account and region. by default this is a unique aws managed cmk for ebs, or you can specify a customer managed cmk. for more information, see . ** this is a customer managed cmk specified for the copy action. this cmk is used instead of the default cmk for the aws account and region. use the following procedure to copy a snapshot using the amazon ec2 console. to copy a snapshot using the console open the amazon ec2 console at . in the navigation pane, choose snapshots. select the snapshot to copy, and then choose copy from the actions list. in the copy snapshot dialog box, update the following as necessary: destination region: select the region where you want to write the copy of the snapshot.description: by default, the description includes information about the source snapshot so that you can identify a copy from the original. you can change this description as necessary.encryption: if the source snapshot is not encrypted, you can choose to encrypt the copy. if you have enabled , the encryption option is set and cannot be unset from the snapshot console. if the encryption option is set, you can choose to encrypt it to a customer managed cmk by selecting one in the field, described below. you cannot strip encryption from an encrypted snapshot. noteif you copy a snapshot and encrypt it to a new cmk, a complete (non-incremental) copy is always created, resulting in additional delay and storage costs. master key: the customer master key (cmk) to be used to encrypt this snapshot. the default key for your account is displayed initially, but you can optionally select from the master keys in your account or type/paste the arn of a key from a different account. you can create new master encryption keys in the iam console . choose copy. in the copy snapshot confirmation dialog box, choose snapshots to go to the snapshots page in the region specified, or choose close. to view the progress of the copy process, switch to the destination region, and then refresh the snapshots page. copies in progress are listed at the top of the page. to check for failureif you attempt to copy an encrypted snapshot without having permissions to use the encryption key, the operation fails silently. the error state is not displayed in the console until you refresh the page. you can also check the state of the snapshot from the command line, as in the following example. if the copy failed because of insufficient key permissions, you see the following message: "statemessage": "given key id is not accessible". when copying an encrypted snapshot, you must have  permissions on the default cmk. explicitly denying these permissions results in copy failure. for information about managing cmk keys, see . to copy a snapshot using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)
to use a dedicated host, you first allocate hosts for use in your account. you then launch instances onto the hosts by specifying host tenancy for the instance. you must select a specific host for the instance to launch on to, or you can allow it to launch on to any host that has auto-placement enabled and matches its instance type. when an instance is stopped and restarted, the host affinity setting determines whether it's restarted on the same, or a different, host. if you no longer need an on-demand host, you can stop the instances running on the host, direct them to launch on a different host, and then release the host. dedicated hosts are also integrated with aws license manager. with license manager, you can create a host resource group, which is a collection of dedicated hosts that are managed as a single entity. when creating a host resource group, you specify the host management preferences, such as auto-allocate and auto-release, for the dedicated hosts. this allows you to launch instances onto dedicated hosts without manually allocating and managing those hosts. for more information, see  in the aws license manager user guide. topics to begin using dedicated hosts, you must allocate dedicated hosts in your account using the amazon ec2 console or the command line tools. after you allocate the dedicated host, the dedicated host capacity is made available in your account immediately and you can start launching instances onto the dedicated host. support for multiple instance types on the same dedicated host is available for the following instance families: , , , , , and . other instance families support only a single instance type on the same dedicated host. you can allocate a dedicated host using the following methods. to allocate a dedicated host open the amazon ec2 console at . in the navigation pane, choose dedicated hosts and then choose allocate dedicated host. for instance family, choose the instance family for the dedicated host. specify whether the dedicated host supports multiple instance types within the selected instance family, or a specific instance type only. do one of the following. to configure the dedicated host to support multiple instance types in the selected instance family, for support multiple instance types, choose enable. enabling this allows you to launch different instance types from the same instance family onto the dedicated host. for example, if you choose the  instance family and choose this option, you can launch  and  instances onto the dedicated host.to configure the dedicated host to support a single instance type within the selected instance family, clear support multiple instance types, and then for instance type, choose the instance type to support. this allows you to launch a single instance type on the dedicated host. for example, if you choose this option and specify  as the supported instance type, you can launch only  instances onto the dedicated host.for availability zone, choose the availability zone in which to allocate the dedicated host. to allow the dedicated host to accept untargeted instance launches that match its instance type, for instance auto-placement, choose enable. for more information about auto-placement, see . to enable host recovery for the dedicated host, for host recovery, choose enable. for more information, see . for quantity, enter the number of dedicated hosts to allocate. (optional) choose add tag and enter a tag key and a tag value. choose allocate. to allocate a dedicated host open the amazon ec2 console at . in the navigation pane, choose dedicated hosts, allocate dedicated host. for instance family, choose the instance family for the dedicated host. specify whether the dedicated host supports multiple instance types within the selected instance family, or a specific instance type only. do one of the following. to configure the dedicated host to support multiple instance types in the selected instance family, select support multiple instance types. enabling this allows you to launch different instance types from the same instance family onto the dedicated host. for example, if you choose the  instance family and choose this option, you can launch  and  instances onto the dedicated host. the instance family must be powered by the nitro system.to configure the dedicated host to support a single instance type within the selected instance family, clear support multiple instance types, and then for instance type, choose the instance type to support. this allows you to launch a single instance type on the dedicated host. for example, if you choose this option and specify  as the supported instance type, you can launch only  instances onto the dedicated host.for availability zone, choose the availability zone in which to allocate the dedicated host. to allow the dedicated host to accept untargeted instance launches that match its instance type, for instance auto-placement, choose enable. for more information about auto-placement, see . to enable host recovery for the dedicated host, for host recovery choose enable. for more information, see . for quantity, enter the number of dedicated hosts to allocate. (optional) choose add tag and enter a tag key and a tag value. choose allocate host. to allocate a dedicated hostuse the  aws cli command. the following command allocates a dedicated host that supports multiple instance types from the  instance family in  availability zone. the host also has host recovery enabled and it has auto-placement disabled. the following command allocates a dedicated host that supports untargeted  instance launches in the  availability zone, enables host recovery, and applies a tag with a key of  and a value of . to allocate a dedicated hostuse the  aws tools for windows powershell command. the following command allocates a dedicated host that supports multiple instance types from the  instance family in  availability zone. the host also has host recovery enabled and it has auto-placement disabled. the following commands allocate a dedicated host that supports untargeted  instance launches in the  availability zone, enable host recovery, and apply a tag with a key of  and a value of . the  parameter used to tag a dedicated host on creation requires an object that specifies the type of resource to be tagged, the tag key, and the tag value. the following commands create the required object. the following command allocates the dedicated host and applies the tag specified in the  object. after you have allocated a dedicated host, you can launch instances onto it. you can't launch instances with  tenancy if you do not have active dedicated hosts with enough available capacity for the instance type that you are launching. notethe instances launched onto dedicated hosts can only be launched in a vpc. for more information, see . before you launch your instances, take note of the limitations. for more information, see . you can launch an instance onto a dedicated host using the following methods. to launch an instance onto a specific dedicated host from the dedicated hosts page open the amazon ec2 console at . choose dedicated hosts in the navigation pane. on the dedicated hosts page, select a host and choose actions, launch instance(s) onto host. select an ami from the list. sql server, suse, and rhel amis provided by amazon ec2 can't be used with dedicated hosts. on the choose an instance type page, select the instance type to launch and then choose next: configure instance details.  if the dedicated host supports a single instance type only, the supported instance type is selected by default and can't be changed. if the dedicated host supports multiple instance types, you must select an instance type within the supported instance family based on the available instance capacity of the dedicated host. we recommend that you launch the larger instance sizes first, and then fill the remaining instance capacity with the smaller instance sizes as needed. on the configure instance details page, configure the instance settings to suit your needs, and then for affinity, choose one of the following options: off—the instance launches onto the specified host, but it is not guaranteed to restart on the same dedicated host if stopped.host—if stopped, the instance always restarts on this specific host.for more information about affinity, see . the tenancy and host options are pre-configured based on the host that you selected. choose review and launch. on the review instance launch page, choose launch. when prompted, select an existing key pair or create a new one, and then choose launch instances. to launch an instance onto a dedicated host using the launch instance wizard open the amazon ec2 console at . in the navigation pane, choose instances, launch instance. select an ami from the list. sql server, suse, and rhel amis provided by amazon ec2 can't be used with dedicated hosts. select the type of instance to launch and choose next: configure instance details. on the configure instance details page, configure the instance settings to suit your needs, and then configure the following settings, which are specific to a dedicated host: tenancy—choose dedicated host - launch this instance on a dedicated host.host—choose either use auto-placement to launch the instance on any dedicated host that has auto-placement enabled, or select a specific dedicated host in the list. the list displays only dedicated hosts that support the selected instance type.affinity—choose one of the following options:off—the instance launches onto the specified host, but it is not guaranteed to restart on it if stopped.host—if stopped, the instance always restarts on the specified host.for more information, see . if you are unable to see these settings, check that you have selected a vpc in the network menu. choose review and launch. on the review instance launch page, choose launch. when prompted, select an existing key pair or create a new one, and then choose launch instances. to launch an instance onto a dedicated hostuse the  aws cli command and specify the instance affinity, tenancy, and host in the  request parameter. to launch an instance onto a dedicated hostuse the  aws tools for windows powershell command and specify the instance affinity, tenancy, and host in the  request parameter. when you launch an instance into a host resource group that has a dedicated host with available instance capacity, amazon ec2 launches the instance onto that host. if the host resource group does not have a host with available instance capacity, amazon ec2 automatically allocates a new host in the host resource group, and then launches the instance onto that host. for more information, see  in the aws license manager user guide. requirements and limits you must associate a core- or socket-based license configuration with the ami.you can't use sql server, suse, or rhel amis provided by amazon ec2 with dedicated hosts.you can't target a specific host by choosing a host id, and you can't enable instance affinity when launching an instance into a host resource group.you can launch an instance into a host resource group using the following methods. to launch an instance into a host resource group open the amazon ec2 console at . in the navigation pane, choose instances, launch instance. select an ami. select the type of instance to launch and choose next: configure instance details. on the configure instance details page, configure the instance settings to suit your needs, and then do the following: for tenancy, choose dedicated host. for host resource group, choose launch instance into a host resource group. for host resource group name, choose the host resource group in which to launch the instance. choose review and launch. on the review instance launch page, choose launch. when prompted, select an existing key pair or create a new one, and then choose launch instances. to launch an instance into a host resource groupuse the  aws cli command, and in the  request parameter, omit the tenancy option and specify the host resource group arn. to launch an instance into a host resource groupuse the  aws tools for windows powershell command, and in the  request parameter, omit the tenancy option and specify the host resource group arn. placement control for dedicated hosts happens on both the instance level and host level. auto-placement is configured at the host level. it allows you to manage whether instances that you launch are launched onto a specific host, or onto any available host that has matching configurations. when the auto-placement of a dedicated host is disabled, it only accepts host tenancy instance launches that specify its unique host id. this is the default setting for new dedicated hosts. when the auto-placement of a dedicated host is enabled, it accepts any untargeted instance launches that match its instance type configuration. when launching an instance, you need to configure its tenancy. launching an instance onto a dedicated host without providing a specific  enables it to launch on any dedicated host that has auto-placement enabled and that matches its instance type. host affinity is configured at the instance level. it establishes a launch relationship between an instance and a dedicated host. when affinity is set to , an instance launched onto a specific host always restarts on the same host if stopped. this applies to both targeted and untargeted launches. when affinity is set to , and you stop and restart the instance, it can be restarted on any available host. however, it tries to launch back onto the last dedicated host on which it ran (on a best-effort basis). you can modify the auto-placement settings of a dedicated host after you have allocated it to your aws account, using one of the following methods. to modify the auto-placement of a dedicated host open the amazon ec2 console at . in the navigation pane, choose dedicated hosts. select a host and choose actions, modify host. for instance auto-placement, choose enable to enable auto-placement, or clear enable to disable auto-placement. for more information, see . choose save. to modify the auto-placement of a dedicated host open the amazon ec2 console at . choose dedicated hosts in the navigation pane. on the dedicated hosts page, select a host and choose actions, modify auto-placement. on the modify auto-placement window, for allow instance auto-placement, choose yes to enable auto-placement, or choose no to disable auto-placement. for more information, see . choose save. to modify the auto-placement of a dedicated hostuse the  aws cli command. the following example enables auto-placement for the specified dedicated host. to modify the auto-placement of a dedicated hostuse the  aws tools for windows powershell command. the following example enables auto-placement for the specified dedicated host. support for multiple instance types on the same dedicated host is available for the following instance families: , , , , , and . other instance families support only a single instance type on the same dedicated host. you can allocate a dedicated host using the following methods. you can modify a dedicated host to change the instance types that it supports. if it currently supports a single instance type, you can modify it to support multiple instance types within that instance family. similarly, if it currently supports multiple instance types, you can modify it to support a specific instance type only. to modify a dedicated host to support multiple instance types, you must first stop all running instances on the host. the modification takes approximately 10 minutes to complete. the dedicated host transitions to the  state while the modification is in progress. you can't start stopped instances or launch new instances on the dedicated host while it is in the  state. to modify a dedicated host that supports multiple instance types to support only a single instance type, the host must either have no running instances, or the running instances must be of the instance type that you want the host to support. for example, to modify a host that supports multiple instance types in the  instance family to support only  instances, the dedicated host must either have no running instances, or it must have only  instances running on it. you can modify the supported instance types using one of the following methods. to modify the supported instance types for a dedicated host open the amazon ec2 console at . in the navigation pane, choose dedicated host. select the dedicated host to modify and choose actions, modify host. do one of the following, depending on the current configuration of the dedicated host: if the dedicated host currently supports a specific instance type, support multiple instance types is not enabled, and instance type lists the supported instance type. to modify the host to support multiple types in the current instance family, for support multiple instance types, choose enable. you must first stop all instances running on the host before modifying it to support multiple instance types. if the dedicated host currently supports multiple instance types in an instance family, enabled is selected for support multiple instance types. to modify the host to support a specific instance type, for support multiple instance types, clear enable, and then for instance type, select the specific instance type to support. you can't change the instance family supported by the dedicated host. choose save. to modify the supported instance types for a dedicated host open the amazon ec2 console at . in the navigation pane, choose dedicated host. select the dedicated host to modify and choose actions, modify supported instance types. do one of the following, depending on the current configuration of the dedicated host: if the dedicated host currently supports a specific instance type, no is selected for support multiple instance types. to modify the host to support multiple types in the current instance family, for support multiple instance types, select yes. you must first stop all instances running on the host before modifying it to support multiple instance types. if the dedicated host currently supports multiple instance types in an instance family, yes is selected for support multiple instance types, and instance family displays the supported instance family. to modify the host to support a specific instance type, for support multiple instance types, select no, and then for instance type, select the specific instance type to support. you can't change the instance family supported by the dedicated host. choose save. to modify the supported instance types for a dedicated hostuse the  aws cli command. the following command modifies a dedicated host to support multiple instance types within the  instance family. the following command modifies a dedicated host to support  instances only. to modify the supported instance types for a dedicated hostuse the  aws tools for windows powershell command. the following command modifies a dedicated host to support multiple instance types within the  instance family. the following command modifies a dedicated host to support  instances only. you can change the tenancy of an instance from  to , or from  to , after you have launched it. you can also modify the affinity between the instance and the host. to modify either instance tenancy or affinity, the instance must be in the  state. you can modify an instance's tenancy and affinity using the following methods. to modify instance tenancy or affinity open the amazon ec2 console at . choose instances, and select the instance to modify. choose actions, instance state, and stop. open the context (right-click) menu on the instance and choose instance settings, modify instance placement. on the modify instance placement page, configure the following: tenancy—choose one of the following:run a dedicated hardware instance—launches the instance as a dedicated instance. for more information, see .launch the instance on a dedicated host—launches the instance onto a dedicated host with configurable affinity.affinity—choose one of the following:this instance can run on any one of my hosts—the instance launches onto any available dedicated host in your account that supports its instance type.this instance can only run on the selected host—the instance is only able to run on the dedicated host selected for target host.target host—select the dedicated host that the instance must run on. if no target host is listed, you might not have available, compatible dedicated hosts in your account.for more information, see . choose save. to modify instance tenancy or affinityuse the  aws cli command. the following example changes the specified instance's affinity from  to , and specifies the dedicated host that the instance has affinity with. to modify instance tenancy or affinityuse the  aws tools for windows powershell command. the following example changes the specified instance's affinity from  to , and specifies the dedicated host that the instance has affinity with. you can view details about a dedicated host and the individual instances on it using the following methods. to view the details of a dedicated host open the amazon ec2 console at . in the navigation pane, choose dedicated hosts. on the dedicated hosts page, select a host. for information about the host, choose details. available vcpus indicates the vcpus that are available on the dedicated host for new instance launches. for example, a dedicated host that supports multiple instance types within the  instance family, and that has no instances running on it, has 72 available vcpus. this means that you can launch different combinations of instance types onto the dedicated host to consume the 72 available vcpus. for information about instances running on the host, choose running instances. to view the details of a dedicated host open the amazon ec2 console at . in the navigation pane, choose dedicated hosts. on the dedicated hosts page, select a host. for information about the host, choose description. available vcpus indicates the vcpus that are available on the dedicated host for new instance launches. for example, a dedicated host that supports multiple instance types within the  instance family, and that has no instances running on it, has 72 available vcpus. this means that you can launch different combinations of instance types onto the dedicated host to consume the 72 available vcpus. for information about instances running on the host, choose instances. to view the capacity of a dedicated hostuse the  aws cli command. the following example uses the  (aws cli) command to view the available instance capacity for a dedicated host that supports multiple instance types within the  instance family. the dedicated host already has two  instances and four  instances running on it. to view the instance capacity of a dedicated hostuse the  aws tools for windows powershell command. you can assign custom tags to your existing dedicated hosts to categorize them in different ways, for example, by purpose, owner, or environment. this helps you to quickly find a specific dedicated host based on the custom tags that you assigned. dedicated host tags can also be used for cost allocation tracking. you can also apply tags to dedicated hosts at the time of creation. for more information, see . you can tag a dedicated host using the following methods. to tag a dedicated host open the amazon ec2 console at . in the navigation pane, choose dedicated hosts.  select the dedicated host to tag, and then choose actions, manage tags. in the manage tags screen, choose add tag, and then specify the key and value for the tag. (optional) choose add tag to add additional tags to the dedicated host. choose save changes. to tag a dedicated host open the amazon ec2 console at . in the navigation pane, choose dedicated hosts.  select the dedicated host to tag, and then choose tags.  choose add/edit tags. in the add/edit tags dialog box, choose create tag, and then specify the key and value for the tag. (optional) choose create tag to add additional tags to the dedicated host. choose save. to tag a dedicated hostuse the  aws cli command. the following command tags the specified dedicated host with . to tag a dedicated hostuse the  aws tools for windows powershell command. the  command needs a  object, which specifies the key and value pair to be used for the dedicated host tag. the following commands create a  object named , with a key and value pair of  and  respectively. the following command tags the specified dedicated host with the  object. amazon ec2 constantly monitors the state of your dedicated hosts. updates are communicated on the amazon ec2 console. you can view information about a dedicated host using the following methods. to view the state of a dedicated host open the amazon ec2 console at . in the navigation pane, choose dedicated hosts. locate the dedicated host in the list and review the value in the state column. to view the state of a dedicated hostuse the  aws cli command and then review the  property in the  response element. to view the state of a dedicated hostuse the  aws tools for windows powershell command and then review the  property in the  response element. the following table explains the possible dedicated host states. any running instances on the dedicated host must be stopped before you can release the host. these instances can be migrated to other dedicated hosts in your account so that you can continue to use them. these steps apply only to on-demand dedicated hosts. you can release a dedicated host using the following methods. to release a dedicated host open the amazon ec2 console at . in the navigation pane, choose dedicated hosts. on the dedicated hosts page, select the dedicated host to release. choose actions, release host. to confirm, choose release. to release a dedicated host open the amazon ec2 console at . choose dedicated hosts in the navigation pane. on the dedicated hosts page, select the dedicated host to release. choose actions, release hosts. choose release to confirm. to release a dedicated hostuse the  aws cli command. to release a dedicated hostuse the  aws tools for windows powershell command. after you release a dedicated host, you can't reuse the same host or host id again, and you are no longer charged on-demand billing rates for it. the state of the dedicated host is changed to , and you are not able to launch any instances onto that host. noteif you have recently released dedicated hosts, it can take some time for them to stop counting towards your limit. during this time, you might experience  errors when trying to allocate new dedicated hosts. if this is the case, try allocating new hosts again after a few minutes. the instances that were stopped are still available for use and are listed on the instances page. they retain their  tenancy setting. you can purchase reservations using the following methods: to purchase reservations open the amazon ec2 console at . choose dedicated hosts, dedicated host reservations, purchase dedicated host reservation. on the purchase dedicated host reservation screen, you can search for available offerings using the default settings, or you can specify custom values for the following: host instance family—the options listed correspond with the dedicated hosts in your account that are not already assigned to a reservation.availability zone—the availability zone of the dedicated hosts in your account that aren't already assigned to a reservation.payment option—the payment option for the offering.term—the term of the reservation, which can be one or three years.choose find offering and select an offering that matches your requirements. choose the dedicated hosts to associate with the reservation, and then choose review. review your order and choose order. to purchase reservations use the  aws cli command to list the available offerings that match your needs. the following example lists the offerings that support instances in the  instance family and have a one-year term. notethe term is specified in seconds. a one-year term includes 31,536,000 seconds, and a three-year term includes 94,608,000 seconds. the command returns a list of offerings that match your criteria. note the  of the offering to purchase.  use the  aws cli command to purchase the offering and provide the  noted in the previous step. the following example purchases the specified reservation and associates it with a specific dedicated host that is already allocated in the aws account, and it applies a tag with a key of  and a value of . to purchase reservations use the  aws tools for windows powershell command to list the available offerings that match your needs. the following examples list the offerings that support instances in the  instance family and have a one-year term. notethe term is specified in seconds. a one-year term includes 31,536,000 seconds, and a three-year term includes 94,608,000 seconds. the command returns a list of offerings that match your criteria. note the  of the offering to purchase.  use the  aws tools for windows powershell command to purchase the offering and provide the  noted in the previous step. the following example purchases the specified reservation and associates it with a specific dedicated host that is already allocated in the aws account. you can view information about the dedicated hosts that are associated with your reservation, including: the term of the reservationthe payment optionthe start and end datesyou can view details of your dedicated host reservations using the following methods. to view the details of a dedicated host reservation open the amazon ec2 console at . choose dedicated hosts in the navigation pane. on the dedicated hosts page, choose dedicated host reservations, and then select the reservation from the list provided. choose details for information about the reservation. choose hosts for information about the dedicated hosts with which the reservation is associated. to view the details of a dedicated host reservationuse the  aws cli command. to view the details of a dedicated host reservationuse the  aws tools for windows powershell command. you can assign custom tags to your dedicated host reservations to categorize them in different ways, for example, by purpose, owner, or environment. this helps you to quickly find a specific dedicated host reservation based on the custom tags that you assigned. you can tag a dedicated host reservation using the command line tools only. to tag a dedicated host reservationuse the  aws cli command. to tag a dedicated host reservationuse the  aws tools for windows powershell command. the  command needs a  parameter, which specifies the key and value pair to be used for the dedicated host reservation tag. the following commands create the  parameter. 
to disable ec2 instance connect, connect to your instance and uninstall the  package that you installed on the os. if the  configuration matches what it was set to when you installed ec2 instance connect, uninstalling  also removes the  configuration. if you modified the  configuration after installing ec2 instance connect, you must update it manually. you can uninstall ec2 instance connect on amazon linux 2 2.0.20190618 or later, where ec2 instance connect is preconfigured. to uninstall ec2 instance connect on an instance launched with amazon linux 2 connect to your instance using ssh. specify the ssh key pair you used for your instance when you launched it and the default user name for the amazon linux 2 ami, which is . for example, the following ssh command connects to the instance with the public dns name , using the key pair . uninstall the  package using the yum command. to uninstall ec2 instance connect on an instance launched with an ubuntu ami connect to your instance using ssh. specify the ssh key pair you used for your instance when you launched it and the default user name for the ubuntu ami, which is . for example, the following ssh command connects to the instance with the public dns name , using the key pair . uninstall the  package using the apt-get command. 
as a managed service, amazon ec2 is protected by the aws global network security procedures that are described in the  whitepaper. you use aws published api calls to access amazon ec2 through the network. clients must support transport layer security (tls) 1.0 or later. we recommend tls 1.2 or later. clients must also support cipher suites with perfect forward secrecy (pfs) such as ephemeral diffie-hellman (dhe) or elliptic curve ephemeral diffie-hellman (ecdhe). most modern systems such as java 7 and later support these modes. additionally, requests must be signed using an access key id and a secret access key that is associated with an iam principal. or you can use the  (aws sts) to generate temporary security credentials to sign requests. a virtual private cloud (vpc) is a virtual network in your own logically isolated area in the aws cloud. use separate vpcs to isolate infrastructure by workload or organizational entity. a subnet is a range of ip addresses in a vpc. when you launch an instance, you launch it into a subnet in your vpc. use subnets to isolate the tiers of your application (for example, web, application, and database) within a single vpc. use private subnets for your instances if they should not be accessed directly from the internet. to call the amazon ec2 api from your vpc without sending traffic over the public internet, use aws privatelink. different ec2 instances on the same physical host are isolated from each other as though they are on separate physical hosts. the hypervisor isolates cpu and memory, and the instances are provided virtualized disks instead of access to the raw disk devices. when you stop or terminate an instance, the memory allocated to it is scrubbed (set to zero) by the hypervisor before it is allocated to a new instance, and every block of storage is reset. this ensures that your data is not unintentionally exposed to another instance. network mac addresses are dynamically assigned to instances by the aws network infrastructure. ip addresses are either dynamically assigned to instances by the aws network infrastructure, or assigned by an ec2 administrator through authenticated api requests. the aws network allows instances to send traffic only from the mac and ip addresses assigned to them. otherwise, the traffic is dropped. by default, an instance cannot receive traffic that is not specifically addressed to it. if you need to run network address translation (nat), routing, or firewall services on your instance, you can disable source/destination checking for the network interface. consider the following options for controlling network traffic to your ec2 instances: restrict access to your instances using . for example, you can allow traffic only from the address ranges for your corporate network.use private subnets for your instances if they should not be accessed directly from the internet. use a bastion host or nat gateway for internet access from an instance in a private subnet.use aws virtual private network or aws direct connect to establish private connections from your remote networks to your vpcs. for more information, see .use  to monitor the traffic that reaches your instances.use  to check for unintended network accessibility from your instances.use  to connect to your instances using secure shell (ssh) without the need to share and manage ssh keys.use  to access your instances remotely instead of opening inbound ssh ports and managing ssh keys.use  to automate common administrative tasks instead of opening inbound ssh ports and managing ssh keys.in addition to restricting network access to each amazon ec2 instance, amazon vpc supports implementing additional network security controls like in-line gateways, proxy servers, and various network monitoring options. for more information, see the  whitepaper. 
after you launch your instance, you can connect to it and use it the way that you'd use a computer sitting in front of you. the following instructions explain how to connect to your instance using putty, a free ssh client for windows. if you receive an error while attempting to connect to your instance, see . before you connect to your linux instance using putty, complete the following prerequisites. verify that the instance is readyafter you launch an instance, it can take a few minutes for the instance to be ready so that you can connect to it. check that your instance has passed its status checks. you can view this information in the status checks column on the instances page. verify the general prerequisites for connecting to your instancefor more information, see . install putty on your local computerdownload and install putty from the . if you already have an older version of putty installed, we recommend that you download the latest version. be sure to install the entire suite. convert your private key using puttygenlocate the private key (.pem file) for the key pair that you specified when you launched the instance. convert the .pem file to a .ppk file for use with putty. for more information, follow the steps in the next section. putty does not natively support the private key format for ssh keys. putty provides a tool named puttygen, which converts keys to the required format for putty. you must convert your private key (.pem file) into this format (.ppk file) as follows in order to connect to your instance using putty. to convert your private key from the start menu, choose all programs, putty, puttygen. under type of key to generate, choose rsa. if you're using an older version of puttygen, choose ssh-2 rsa. choose load. by default, puttygen displays only files with the extension . to locate your  file, choose the option to display files of all types. select your  file for the key pair that you specified when you launched your instance and choose open. puttygen displays a notice that the  file was successfully imported. choose ok. to save the key in the format that putty can use, choose save private key. puttygen displays a warning about saving the key without a passphrase. choose yes. notea passphrase on a private key is an extra layer of protection. even if your private key is discovered, it can't be used without the passphrase. the downside to using a passphrase is that it makes automation harder because human intervention is needed to log on to an instance, or to copy files to an instance. specify the same name for the key that you used for the key pair (for example, ) and choose save. putty automatically adds the  file extension.  your private key is now in the correct format for use with putty. you can now connect to your instance using putty's ssh client. use the following procedure to connect to your linux instance using putty. you need the  file that you created for your private key. for more information, see  in the preceding section. if you receive an error while attempting to connect to your instance, see . to connect to your instance using putty start putty (from the start menu, choose all programs, putty, putty). in the category pane, choose session and complete the following fields: in the host name box, do one of the following: (public dns) to connect using your instance's public dns name, enter my-instance-user-name@my-instance-public-dns-name.(ipv6) alternatively, if your instance has an ipv6 address, to connect using your instance's ipv6 address, enter my-instance-user-name@my-instance-ipv6-address.for information about how to get the user name for your instance, and the public dns name or ipv6 address of your instance, see . ensure that the port value is 22. under connection type, select ssh. (optional) you can configure putty to automatically send 'keepalive' data at regular intervals to keep the session active. this is useful to avoid disconnecting from your instance due to session inactivity. in the category pane, choose connection, and then enter the required interval in the seconds between keepalives field. for example, if your session disconnects after 10 minutes of inactivity, enter 180 to configure putty to send keepalive data every 3 minutes. in the category pane, expand connection, expand ssh, and then choose auth. complete the following:  choose browse. select the  file that you generated for your key pair and choose open. (optional) if you plan to start this session again later, you can save the session information for future use. under category, choose session, enter a name for the session in saved sessions, and then choose save. choose open. if this is the first time you have connected to this instance, putty displays a security alert dialog box that asks whether you trust the host to which you are connecting. (optional) verify that the fingerprint in the security alert dialog box matches the fingerprint that you previously obtained in . if these fingerprints don't match, someone might be attempting a "man-in-the-middle" attack. if they match, continue to the next step. choose yes. a window opens and you are connected to your instance. noteif you specified a passphrase when you converted your private key to putty's format, you must provide that passphrase when you log in to the instance. if you receive an error while attempting to connect to your instance, see . the putty secure copy client (pscp) is a command line tool that you can use to transfer files between your windows computer and your linux instance. if you prefer a graphical user interface (gui), you can use an open source gui tool named winscp. for more information, see . to use pscp, you need the private key you generated in . you also need the public dns name of your linux instance, or the ipv6 address if your instance has one. the following example transfers the file  from the c:\ drive on a windows computer to the  home directory on an amazon linux instance. to transfer a file, use one of the following commands. (public dns) to transfer a file using your instance's public dns name, enter the following command. (ipv6) alternatively, if your instance has an ipv6 address, to transfer a file using your instance's ipv6 address, enter the following command. the ipv6 address must be enclosed in square brackets (). winscp is a gui-based file manager for windows that allows you to upload and transfer files to a remote computer using the sftp, scp, ftp, and ftps protocols. winscp allows you to drag and drop files from your windows computer to your linux instance or synchronize entire directory structures between the two systems. to use winscp, you need the private key that you generated in . you also need the public dns name of your linux instance. download and install winscp from . for most users, the default installation options are ok. start winscp. at the winscp login screen, for host name, enter one of the following: (public dns or ipv4 address) to log in using your instance's public dns name or public ipv4 address, enter the public dns name or public ipv4 address for your instance.(ipv6) alternatively, if your instance has an ipv6 address, to log in using your instance's ipv6 address, enter the ipv6 address for your instance.for user name, enter the default user name for your ami. for amazon linux 2 or the amazon linux ami, the user name is .for a centos ami, the user name is .for a debian ami, the user name is .for a fedora ami, the user name is  or .for a rhel ami, the user name is  or .for a suse ami, the user name is  or .for an ubuntu ami, the user name is .otherwise, if  and  don't work, check with the ami provider.specify the private key for your instance. for private key, enter the path to your private key, or choose the "..." button to browse for the file. to open the advanced site settings, for newer versions of winscp, choose advanced. to find the private key file setting, under ssh, choose authentication. here is a screenshot from winscp version 5.9.4: winscp requires a putty private key file (). you can convert a  security key file to the  format using puttygen. for more information, see . (optional) in the left panel, choose directories. for remote directory, enter the path for the directory to which to add files. to open the advanced site settings for newer versions of winscp, choose advanced. to find the remote directory setting, under environment, choose directories. choose login. to add the host fingerprint to the host cache, choose yes. after the connection is established, in the connection window your linux instance is on the right and your local machine is on the left. you can drag and drop files directly into the remote file system from your local machine. for more information on winscp, see the project documentation at . if you receive a "cannot execute scp to start transfer" error, you must first install scp on your linux instance. for some operating systems, this is located in the  package. for amazon linux variants, such as the amazon ecs-optimized ami, use the following command to install scp. 
general purpose instances provide a balance of compute, memory, and networking resources, and can be used for a wide range of workloads. a1 instances these instances are ideally suited for scale-out workloads that are supported by the arm ecosystem. these instances are well-suited for the following: web serverscontainerized microservicesbare metal instances, such as , provide your applications with direct access to physical resources of the host server, such as processors and memory. for more information, see  and . m5 and m5a instances these instances provide an ideal cloud infrastructure, offering a balance of compute, memory, and networking resources for a broad range of applications that are deployed in the cloud. they are well-suited for the following: small and midsize databasesdata processing tasks that require additional memorycaching fleetsbackend servers for sap, microsoft sharepoint, cluster computing, and other enterprise applicationsfor more information, see . bare metal instances, such as , provide your applications with direct access to physical resources of the host server, such as processors and memory. these instances are well suited for the following: workloads that require access to low-level hardware features (for example, intel vt) that are not available or fully supported in virtualized environmentsapplications that require a non-virtualized environment for licensing or supportm6g instances these instances are powered by aws graviton2 processors and deliver balanced compute, memory, and networking for a broad range a general purpose workloads. they are well suited for the following: application serversmicroservicesgaming serversmidsize data storescaching fleetsbare metal instances, such as , provide your applications with direct access to physical resources of the host server, such as processors and memory. for more information, see . t2, t3, and t3a instances these instances provide a baseline level of cpu performance with the ability to burst to a higher level when required by your workload. an unlimited instance can sustain high cpu performance for any period of time whenever required. for more information, see . these instances are well-suited for the following: websites and web applicationscode repositoriesdevelopment, build, test, and staging environmentsmicroservicesfor more information, see  and . topics the following is a summary of the hardware specifications for general purpose instances. for more information about the hardware specifications for each amazon ec2 instance type, see . for more information about specifying cpu options, see . ebs-optimized instances enable you to get consistently high performance for your ebs volumes by eliminating contention between amazon ebs i/o and other network traffic from your instance. some general purpose instances are ebs-optimized by default at no additional cost. for more information, see . some general purpose instance types provide the ability to control processor c-states and p-states on linux. c-states control the sleep levels that a core can enter when it is inactive, while p-states control the desired performance (in cpu frequency) from a core. for more information, see . you can enable enhanced networking on supported instance types to provide lower latencies, lower network jitter, and higher packet-per-second (pps) performance. most applications do not consistently need a high level of network performance, but can benefit from access to increased bandwidth when they send or receive data. for more information, see . the following is a summary of network performance for general purpose instances that support enhanced networking. † these instances use a network i/o credit mechanism to allocate network bandwidth to instances based on average bandwidth utilization. they accrue credits when their bandwidth is below their baseline bandwidth, and can use these credits when they perform network data transfers. for more information, open a support case and ask about baseline bandwidth for the specific instance types that you are interested in. if you use a linux ami with kernel version 4.4 or later and use all the ssd-based instance store volumes available to your instance, you get the iops (4,096 byte block size) performance listed in the following table (at queue depth saturation). otherwise, you get lower iops performance. * for these instances, you can get up to the specified performance. as you fill the ssd-based instance store volumes for your instance, the number of write iops that you can achieve decreases. this is due to the extra work the ssd controller must do to find available space, rewrite existing data, and erase unused space so that it can be rewritten. this process of garbage collection results in internal write amplification to the ssd, expressed as the ratio of ssd write operations to user write operations. this decrease in performance is even larger if the write operations are not in multiples of 4,096 bytes or not aligned to a 4,096-byte boundary. if you write a smaller amount of bytes or bytes that are not aligned, the ssd controller must read the surrounding data and store the result in a new location. this pattern results in significantly increased write amplification, increased latency, and dramatically reduced i/o performance. ssd controllers can use several strategies to reduce the impact of write amplification. one such strategy is to reserve space in the ssd instance storage so that the controller can more efficiently manage the space available for write operations. this is called over-provisioning. the ssd-based instance store volumes provided to an instance don't have any space reserved for over-provisioning. to reduce write amplification, we recommend that you leave 10% of the volume unpartitioned so that the ssd controller can use it for over-provisioning. this decreases the storage that you can use, but increases performance even if the disk is close to full capacity. for instance store volumes that support trim, you can use the trim command to notify the ssd controller whenever you no longer need data that you've written. this provides the controller with more free space, which can reduce write amplification and increase performance. for more information, see . the following is a summary of features for general purpose instances: * the root device volume must be an amazon ebs volume. for more information, see the following: m5, m5d, and t3 instances feature a 3.1 ghz intel xeon platinum 8000 series processor from either the first generation (skylake-sp) or second generation (cascade lake).m5a, m5ad, and t3a instances feature a 2.5 ghz amd epyc 7000 series processor.a1 instances feature a 2.3 ghz aws graviton processor based on 64-bit arm architecture.m6g instances feature an aws graviton2 processor based on 64-bit arm architecture.m4, m5, m5a, m5ad, m5d,  and larger, and  and larger, and  and larger instance types require 64-bit hvm amis. they have high-memory, and require a 64-bit operating system to take advantage of that capacity. hvm amis provide superior performance in comparison to paravirtual (pv) amis on high-memory instance types. in addition, you must use an hvm ami to take advantage of enhanced networking.instances built on the nitro system have the following requirements:  must be installed must be installedthe following linux amis meet these requirements: amazon linux 2amazon linux ami 2018.03ubuntu 14.04 (with  kernel) or laterred hat enterprise linux 7.4 or latersuse linux enterprise server 12 sp2 or latercentos 7.4.1708 or laterfreebsd 11.1 or laterdebian gnu/linux 9 or laterinstances with an aws graviton processors have the following requirements: use an ami for the 64-bit arm architecture.support booting through uefi with acpi tables and support acpi hot-plug of pci devices.the following linux amis meet these requirements: amazon linux 2 (64-bit arm)ubuntu 16.04 or later (64-bit arm)red hat enterprise linux 8.0 or later (64-bit arm)suse linux enterprise server 15 or later (64-bit arm)instances built on the nitro system instances support a maximum of 28 attachments, including network interfaces, ebs volumes, and nvme instance store volumes. for more information, see .launching a bare metal instance boots the underlying server, which includes verifying all hardware and firmware components. this means that it can take 20 minutes from the time the instance enters the running state until it becomes available over the network.to attach or detach ebs volumes or secondary network interfaces from a bare metal instance requires pcie native hotplug support. amazon linux 2 and the latest versions of the amazon linux ami support pcie native hotplug, but earlier versions do not. you must enable the following linux kernel configuration options: bare metal instances use a pci-based serial device rather than an i/o port-based serial device. the upstream linux kernel and the latest amazon linux amis support this device. bare metal instances also provide an acpi spcr table to enable the system to automatically use the pci-based serial device. the latest windows amis automatically use the pci-based serial device.instances built on the nitro system should have system-logind or acpid installed to support clean shutdown through api requests.there is a limit on the total number of instances that you can launch in a region, and there are additional limits on some instance types. for more information, see  in the amazon ec2 faq.
amazon s3 is a repository for internet data. amazon s3 provides access to reliable, fast, and inexpensive data storage infrastructure. it is designed to make web-scale computing easier by enabling you to store and retrieve any amount of data, at any time, from within amazon ec2 or anywhere on the web. amazon s3 stores data objects redundantly on multiple devices across multiple facilities and allows concurrent read or write access to these data objects by many separate clients or application threads. you can use the redundant data stored in amazon s3 to recover quickly and reliably from instance or application failures.  amazon ec2 uses amazon s3 for storing amazon machine images (amis). you use amis for launching ec2 instances. in case of instance failure, you can use the stored ami to immediately launch another instance, thereby allowing for fast recovery and business continuity. amazon ec2 also uses amazon s3 to store snapshots (backup copies) of the data volumes. you can use snapshots for recovering data quickly and reliably in case of application or system failures. you can also use snapshots as a baseline to create multiple new data volumes, expand the size of an existing data volume, or move data volumes across multiple availability zones, thereby making your data usage highly scalable. for more information about using data volumes and snapshots, see . objects are the fundamental entities stored in amazon s3. every object stored in amazon s3 is contained in a bucket. buckets organize the amazon s3 namespace at the highest level and identify the account responsible for that storage. amazon s3 buckets are similar to internet domain names. objects stored in the buckets have a unique key value and are retrieved using a url. for example, if an object with a key value  is stored in the  bucket, then it is addressable using the url .  for more information about the features of amazon s3, see the . given the benefits of amazon s3 for storage, you may decide to use this service to store files and data sets for use with ec2 instances. there are several ways to move data to and from amazon s3 to your instances. in addition to the examples discussed below, there are a variety of tools that people have written that you can use to access your data in amazon s3 from your computer or your instance. some of the common ones are discussed in the aws forums. if you have permission, you can copy a file to or from amazon s3 and your instance using one of the following methods. get or wgetthe wget utility is an http and ftp client that allows you to download public objects from amazon s3. it is installed by default in amazon linux and most other distributions, and available for download on windows. to download an amazon s3 object, use the following command, substituting the url of the object to download. this method requires that the object you request is public; if the object is not public, you receive an "error 403: forbidden" message. if you receive this error, open the amazon s3 console and change the permissions of the object to public. for more information, see the . aws command line interfacethe aws command line interface (aws cli) is a unified tool to manage your aws services. the aws cli enables users to authenticate themselves and download restricted items from amazon s3 and also to upload items. for more information, such as how to install and configure the tools, see the . the aws s3 cp command is similar to the unix cp command. you can copy files from amazon s3 to your instance, copy files from your instance to amazon s3, and copy files from one amazon s3 location to another. use the following command to copy an object from amazon s3 to your instance. use the following command to copy an object from your instance back into amazon s3. the aws s3 sync command can synchronize an entire amazon s3 bucket to a local directory location. this can be helpful for downloading a data set and keeping the local copy up-to-date with the remote set. if you have the proper permissions on the amazon s3 bucket, you can push your local directory back up to the cloud when you are finished by reversing the source and destination locations in the command. use the following command to download an entire amazon s3 bucket to a local directory on your instance. amazon s3 apiif you are a developer, you can use an api to access data in amazon s3. for more information, see the . you can use this api and its examples to help develop your application and integrate it with other apis and sdks, such as the boto python interface. 
compute optimized instances are ideal for compute-bound applications that benefit from high-performance processors. c5 and c5n instances these instances are well suited for the following: batch processing workloadsmedia transcodinghigh-performance web servershigh-performance computing (hpc)scientific modelingdedicated gaming servers and ad serving enginesmachine learning inference and other compute-intensive applicationsbare metal instances, such as , provide your applications with direct access to physical resources of the host server, such as processors and memory. for more information, see . c6g instances these instances are powered by aws graviton2 processors and are ideal for running advanced, compute-intensive workloads, such as the following: high-performance computing (hpc)batch processingad servingvideo encodinggaming serversscientific modelingdistributed analyticscpu-based machine learning inferencebare metal instances, such as , provide your applications with direct access to physical resources of the host server, such as processors and memory. for more information, see . topics the following is a summary of the hardware specifications for compute optimized instances. for more information about the hardware specifications for each amazon ec2 instance type, see . for more information about specifying cpu options, see . ebs-optimized instances enable you to get consistently high performance for your ebs volumes by eliminating contention between amazon ebs i/o and other network traffic from your instance. some compute optimized instances are ebs-optimized by default at no additional cost. for more information, see . some compute optimized instance types provide the ability to control processor c-states and p-states on linux. c-states control the sleep levels that a core can enter when it is inactive, while p-states control the desired performance (in cpu frequency) from a core. for more information, see . you can enable enhanced networking on supported instance types to provide lower latencies, lower network jitter, and higher packet-per-second (pps) performance. most applications do not consistently need a high level of network performance, but can benefit from access to increased bandwidth when they send or receive data. for more information, see . the following is a summary of network performance for compute optimized instances that support enhanced networking. † these instances use a network i/o credit mechanism to allocate network bandwidth to instances based on average bandwidth utilization. they accrue credits when their bandwidth is below their baseline bandwidth, and can use these credits when they perform network data transfers. for more information, open a support case and ask about baseline bandwidth for the specific instance types that you are interested in. if you use a linux ami with kernel version 4.4 or later and use all the ssd-based instance store volumes available to your instance, you get the iops (4,096 byte block size) performance listed in the following table (at queue depth saturation). otherwise, you get lower iops performance. * for these instances, you can get up to the specified performance. as you fill the ssd-based instance store volumes for your instance, the number of write iops that you can achieve decreases. this is due to the extra work the ssd controller must do to find available space, rewrite existing data, and erase unused space so that it can be rewritten. this process of garbage collection results in internal write amplification to the ssd, expressed as the ratio of ssd write operations to user write operations. this decrease in performance is even larger if the write operations are not in multiples of 4,096 bytes or not aligned to a 4,096-byte boundary. if you write a smaller amount of bytes or bytes that are not aligned, the ssd controller must read the surrounding data and store the result in a new location. this pattern results in significantly increased write amplification, increased latency, and dramatically reduced i/o performance. ssd controllers can use several strategies to reduce the impact of write amplification. one such strategy is to reserve space in the ssd instance storage so that the controller can more efficiently manage the space available for write operations. this is called over-provisioning. the ssd-based instance store volumes provided to an instance don't have any space reserved for over-provisioning. to reduce write amplification, we recommend that you leave 10% of the volume unpartitioned so that the ssd controller can use it for over-provisioning. this decreases the storage that you can use, but increases performance even if the disk is close to full capacity. for instance store volumes that support trim, you can use the trim command to notify the ssd controller whenever you no longer need data that you've written. this provides the controller with more free space, which can reduce write amplification and increase performance. for more information, see . the following is a summary of features for compute optimized instances: * the root device volume must be an amazon ebs volume. for more information, see the following: c5 and c5d instances feature a 3.1 ghz intel xeon platinum 8000 series processor from either the first generation (skylake-sp) or second generation (cascade lake).c5a instances feature a second-generation amd epyc processor (rome) running at frequencies as high as 3.3. ghz.c6g instances feature an aws graviton2 processor based on 64-bit arm architecture.c4 instances and instances based on the  require 64-bit ebs-backed hvm amis. they have high-memory and require a 64-bit operating system to take advantage of that capacity. hvm amis provide superior performance in comparison to paravirtual (pv) amis on high-memory instance types. in addition, you must use an hvm ami to take advantage of enhanced networking.instances built on the nitro system have the following requirements:  must be installed must be installedthe following linux amis meet these requirements: amazon linux 2amazon linux ami 2018.03ubuntu 14.04 (with  kernel) or laterred hat enterprise linux 7.4 or latersuse linux enterprise server 12 sp2 or latercentos 7.4.1708 or laterfreebsd 11.1 or laterdebian gnu/linux 9 or laterinstances with an aws graviton processors have the following requirements: use an ami for the 64-bit arm architecture.support booting through uefi with acpi tables and support acpi hot-plug of pci devices.the following amis meet these requirements: amazon linux 2 (64-bit arm)ubuntu 16.04 or later (64-bit arm)red hat enterprise linux 8.0 or later (64-bit arm)suse linux enterprise server 15 or later (64-bit arm)instances built on the nitro system instances support a maximum of 28 attachments, including network interfaces, ebs volumes, and nvme instance store volumes. for more information, see .launching a bare metal instance boots the underlying server, which includes verifying all hardware and firmware components. this means that it can take 20 minutes from the time the instance enters the running state until it becomes available over the network.to attach or detach ebs volumes or secondary network interfaces from a bare metal instance requires pcie native hotplug support. amazon linux 2 and the latest versions of the amazon linux ami support pcie native hotplug, but earlier versions do not. you must enable the following linux kernel configuration options: bare metal instances use a pci-based serial device rather than an i/o port-based serial device. the upstream linux kernel and the latest amazon linux amis support this device. bare metal instances also provide an acpi spcr table to enable the system to automatically use the pci-based serial device. the latest windows amis automatically use the pci-based serial device.instances built on the nitro system should have acpid installed to support clean shutdown through api requests.there is a limit on the total number of instances that you can launch in a region, and there are additional limits on some instance types. for more information, see  in the amazon ec2 faq.
third-party auditors assess the security and compliance of amazon ec2 as part of multiple aws compliance programs. these include soc, pci, fedramp, hipaa, and others. for a list of aws services in scope of specific compliance programs, see . for general information, see . you can download third-party audit reports using aws artifact. for more information, see . your compliance responsibility when using amazon ec2 is determined by the sensitivity of your data, your company's compliance objectives, and applicable laws and regulations. aws provides the following resources to help with compliance:  – these deployment guides discuss architectural considerations and provide steps for deploying security- and compliance-focused baseline environments on aws. – this whitepaper describes how companies can use aws to create hipaa-compliant applications. – this collection of workbooks and guides might apply to your industry and location. in the aws config developer guide – aws config; assesses how well your resource configurations comply with internal practices, industry guidelines, and regulations. – this aws service provides a comprehensive view of your security state within aws that helps you check your compliance with security industry standards and best practices.
secure sockets layer/transport layer security (ssl/tls) creates an encrypted channel between a web server and web client that protects data in transit from being eavesdropped on. this tutorial explains how to add support manually for ssl/tls on an ec2 instance with amazon linux 2 and apache web server. if you plan to offer commercial-grade services, the , which is not discussed here, is a good option. for historical reasons, web encryption is often referred to simply as ssl. while web browsers still support ssl, its successor protocol tls is less vulnerable to attack. amazon linux 2 disables server-side support for all versions of ssl by default.  consider tls 1.0 to be unsafe, and both tls 1.0 and tls 1.1 are on track to be formally  by the ietf. this tutorial contains guidance based exclusively on enabling tls 1.2. (a newer tls 1.3 protocol exists in draft form, but is not yet supported on amazon linux 2.) for more information about the updated encryption standards, see  and . this tutorial refers to modern web encryption simply as tls. importantthese procedures are intended for use with amazon linux 2. we also assume that you are starting with a fresh amazon ec2 instance. if you are trying to set up a lamp web server on an instance with a different distribution, or if you are reusing an older, existing instance, some procedures in this tutorial might not work for you. for information about lamp web servers on ubuntu, see the ubuntu community documentation . for information about red hat enterprise linux, see the customer portal topic . topics before you begin this tutorial, complete the following steps: launch an ebs-backed amazon linux 2 instance. for more information, see . configure your security groups to allow your instance to accept connections on the following tcp ports:  ssh (port 22)http (port 80)https (port 443)for more information, see . install the apache web server. for step-by-step instructions, see . only the httpd package and its dependencies are needed, so you can ignore the instructions involving php and mariadb.to identify and authenticate websites, the tls public key infrastructure (pki) relies on the domain name system (dns). to use your ec2 instance to host a public website, you need to register a domain name for your web server or transfer an existing domain name to your amazon ec2 host. numerous third-party domain registration and dns hosting services are available for this, or you can use . this procedure takes you through the process of setting up tls on amazon linux 2 with a self-signed digital certificate. notea self-signed certificate is acceptable for testing but not production. if you expose your self-signed certificate to the internet, visitors to your site are greeted by security warnings.  to enable tls on a server  and confirm that apache is running. if the returned value is not "enabled," start apache and set it to start each time the system boots. to ensure that all of your software packages are up to date, perform a quick software update on your instance. this process may take a few minutes, but it is important to make sure that you have the latest security updates and bug fixes. notethe  option installs the updates without asking for confirmation. if you would like to examine the updates before installing, you can omit this option. now that your instance is current, add tls support by installing the apache module . your instance now has the following files that you use to configure your secure server and create a certificate for testing:   the configuration file for mod_ssl. it contains directives telling apache where to find encryption keys and certificates, the tls protocol versions to allow, and the encryption ciphers to accept.   a script to generate a self-signed x.509 certificate and private key for your server host. this certificate is useful for testing that apache is properly set up to use tls. because it offers no proof of identity, it should not be used in production. if used in production, it triggers warnings in web browsers. run the script to generate a self-signed dummy certificate and key for testing. this generates a new file  in the  directory. the specified file name matches the default that is assigned in the sslcertificatefile directive in .  this file contains both a self-signed certificate and the certificate's private key. apache requires the certificate and key to be in pem format, which consists of base64-encoded ascii characters framed by "begin" and "end" lines, as in the following abbreviated example. the file names and extensions are a convenience and have no effect on function. for example, you can call a certificate , , or any other file name, so long as the related directive in the  file uses the same name. notewhen you replace the default tls files with your own customized files, be sure that they are in pem format.  open the  file and comment out the following line, because the self-signed dummy certificate also contains the key. if you do not comment out this line before you complete the next step, the apache service fails to start. restart apache. notemake sure that tcp port 443 is accessible on your ec2 instance, as previously described. your apache web server should now support https (secure http) over port 443. test it by entering the ip address or fully qualified domain name of your ec2 instance into a browser url bar with the prefix https://. because you are connecting to a site with a self-signed, untrusted host certificate, your browser may display a series of security warnings. override the warnings and proceed to the site.  if the default apache test page opens, it means that you have successfully configured tls on your server. all data passing between the browser and server is now encrypted. noteto prevent site visitors from encountering warning screens, you must obtain a trusted, ca-signed certificate that not only encrypts, but also publicly authenticates you as the owner of the site.  you can use the following process to obtain a ca-signed certificate: generate a certificate signing request (csr) from a private keysubmit the csr to a certificate authority (ca)obtain a signed host certificateconfigure apache to use the certificatea self-signed tls x.509 host certificate is cryptologically identical to a ca-signed certificate. the difference is social, not mathematical. a ca promises, at a minimum, to validate a domain's ownership before issuing a certificate to an applicant. each web browser contains a list of cas trusted by the browser vendor to do this. an x.509 certificate consists primarily of a public key that corresponds to your private server key, and a signature by the ca that is cryptographically tied to the public key. when a browser connects to a web server over https, the server presents a certificate for the browser to check against its list of trusted cas. if the signer is on the list, or accessible through a chain of trust consisting of other trusted signers, the browser negotiates a fast encrypted data channel with the server and loads the page.  certificates generally cost money because of the labor involved in validating the requests, so it pays to shop around. a list of well-known cas can be found at . a few cas offer basic-level certificates free of charge. the most notable of these cas is the  project, which also supports the automation of the certificate creation and renewal process. for more information about using let's encrypt as your ca, see . underlying the host certificate is the key. as of 2019,  and  groups recommend using a minimum key (modulus) size of 2048 bits for rsa keys intended to protect documents, through 2030. the default modulus size generated by openssl in amazon linux 2 is 2048 bits, which is suitable for use in a ca-signed certificate. in the following procedure, an optional step provided for those who want a customized key, for example, one with a larger modulus or using a different encryption algorithm. these instructions for acquiring a ca-signed host certificate do not work unless you own a registered and hosted dns domain. to obtain a ca-signed certificate  and navigate to /etc/pki/tls/private/. this is the directory where you store the server's private key for tls. if you prefer to use an existing host key to generate the csr, skip to step 3. (optional) generate a new private key. here are some examples of key configurations. any of the resulting keys works with your web server, but they vary in the degree and type of security that they implement. example 1: create a default rsa host key. the resulting file, custom.key, is a 2048-bit rsa private key. example 2: create a stronger rsa key with a bigger modulus. the resulting file, custom.key, is a 4096-bit rsa private key. example 3: create a 4096-bit encrypted rsa key with password protection. the resulting file, custom.key, is a 4096-bit rsa private key encrypted with the aes-128 cipher. importantencrypting the key provides greater security, but because an encrypted key requires a password, services depending on it cannot be auto-started. each time you use this key, you must supply the password (in the preceding example, "abcde12345") over an ssh connection. example 4: create a key using a non-rsa cipher. rsa cryptography can be relatively slow because of the size of its public keys, which are based on the product of two large prime numbers. however, it is possible to create keys for tls that use non-rsa ciphers. keys based on the mathematics of elliptic curves are smaller and computationally faster when delivering an equivalent level of security. the result is a 256-bit elliptic curve private key using prime256v1, a "named curve" that openssl supports. its cryptographic strength is slightly greater than a 2048-bit rsa key, . notenot all cas provide the same level of support for elliptic-curve-based keys as for rsa keys. make sure that the new private key has highly restrictive ownership and permissions (owner=root, group=root, read/write for owner only). the commands would be as shown in the following example. the preceding commands yield the following result.  after you have created and configured a satisfactory key, you can create a csr.  create a csr using your preferred key. the following example uses custom.key. openssl opens a dialog and prompts you for the information shown in the following table. all of the fields except common name are optional for a basic, domain-validated host certificate.       finally, openssl prompts you for an optional challenge password. this password applies only to the csr and to transactions between you and your ca, so follow the ca's recommendations about this and the other optional field, optional company name. the csr challenge password has no effect on server operation.    the resulting file csr.pem contains your public key, your digital signature of your public key, and the metadata that you entered. submit the csr to a ca. this usually consists of opening your csr file in a text editor and copying the contents into a web form. at this time, you may be asked to supply one or more subject alternate names (sans) to be placed on the certificate. if www.example.com is the common name, then example.com would be a good san, and vice versa. a visitor to your site entering either of these names would see an error-free connection. if your ca web form allows it, include the common name in the list of sans. some cas include it automatically. after your request has been approved, you receive a new host certificate signed by the ca. you might also be instructed to download an intermediate certificate file that contains additional certificates needed to complete the ca's chain of trust.  noteyour ca might send you files in multiple formats intended for various purposes. for this tutorial, you should only use a certificate file in pem format, which is usually (but not always) marked with a  or  file extension. if you are uncertain which file to use, open the files with a text editor and find the one containing one or more blocks beginning with the following line.   the file should also end with the following line.   you can also test the file at the command line as shown in the following.   verify that these lines appear in the file. do not use files ending with , , or similar file extensions. place the new ca-signed certificate and any intermediate certificates in the  directory. notethere are several ways to upload your new certificate to your ec2 instance, but the most straightforward and informative way is to open a text editor (for example, vi, nano, or notepad) on both your local computer and your instance, and then copy and paste the file contents between them. you need root [sudo] permissions when performing these operations on the ec2 instance. this way, you can see immediately if there are any permission or path problems. be careful, however, not to add any additional lines while copying the contents, or to change them in any way.  from inside the  directory, check that the file ownership, group, and permission settings match the highly restrictive amazon linux 2 defaults (owner=root, group=root, read/write for owner only). the following example shows the commands to use.  these commands should yield the following result.  the permissions for the intermediate certificate file are less stringent (owner=root, group=root, owner can write, group can read, world can read). the following example shows the commands to use.  these commands should yield the following result. place the private key that you used to create the csr in the  directory.  notethere are several ways to upload your custom key to your ec2 instance, but the most straightforward and informative way is to open a text editor (for example, vi, nano, or notepad) on both your local computer and your instance, and then copy and paste the file contents between them. you need root [sudo] permissions when performing these operations on the ec2 instance. this way, you can see immediately if there are any permission or path problems. be careful, however, not to add any additional lines while copying the contents, or to change them in any way. from inside the  directory, use the following commands to verify that the file ownership, group, and permission settings match the highly restrictive amazon linux 2 defaults (owner=root, group=root, read/write for owner only). these commands should yield the following result. edit  to reflect your new certificate and key files. provide the path and file name of the ca-signed host certificate in apache's  directive: if you received an intermediate certificate file ( in this example), provide its path and file name using apache's  directive: notesome cas combine the host certificate and the intermediate certificates in a single file, making the  directive unnecessary. consult the instructions provided by your ca. provide the path and file name of the private key ( in this example) in apache's  directive: save  and restart apache. test your server by entering your domain name into a browser url bar with the prefix . your browser should load the test page over https without generating errors. after your tls is operational and exposed to the public, you should test how secure it really is. this is easy to do using online services such as , which performs a free and thorough analysis of your security setup. based on the results, you may decide to harden the default security configuration by controlling which protocols you accept, which ciphers you prefer, and which you exclude. for more information, see . importantreal-world testing is crucial to the security of your server. small configuration errors may lead to serious security breaches and loss of data. because recommended security practices change constantly in response to research and emerging threats, periodic security audits are essential to good server administration.  on the  site, enter the fully qualified domain name of your server, in the form www.example.com. after about two minutes, you receive a grade (from a to f) for your site and a detailed breakdown of the findings. the following table summarizes the report for a domain with settings identical to the default apache configuration on amazon linux 2, and with a default certbot certificate.  though the overview shows that the configuration is mostly sound, the detailed report flags several potential problems, listed here in order of severity: ✗ the rc4 cipher is supported for use by certain older browsers. a cipher is the mathematical core of an encryption algorithm. rc4, a fast cipher used to encrypt tls data-streams, is known to have several . unless you have very good reasons to support legacy browsers, you should disable this. ✗ old tls versions are supported. the configuration supports tls 1.0 (already deprecated) and tls 1.1 (on a path to deprecation). only tls 1.2 has been recommended since 2018. ✗ forward secrecy is not fully supported.  is a feature of algorithms that encrypt using temporary (ephemeral) session keys derived from the private key. this means in practice that attackers cannot decrypt https data even if they possess a web server's long-term private key. to correct and future-proof the tls configuration open the configuration file  in a text editor and comment out the following line by entering "#" at the beginning of the line. add the following directive: this directive explicitly disables ssl versions 2 and 3, as well as tls versions 1.0 and 1.1. the server now refuses to accept encrypted connections with clients using anything except tls 1.2. the verbose wording in the directive conveys more clearly, to a human reader, what the server is configured to do. notedisabling tls versions 1.0 and 1.1 in this manner blocks a small percentage of outdated web browsers from accessing your site. to modify the list of allowed ciphers in the configuration file , find the section with the sslciphersuite directive and comment out the existing line by entering "#" at the beginning of the line. specify explicit cipher suites and a cipher order that prioritizes forward secrecy and avoids insecure ciphers. the  directive used here is based on output from the , which tailors a tls configuration to the specific software running on your server. (for more information, see mozilla's useful resource .) first determine your apache and openssl versions by using the output from the following commands. for example, if the returned information is apache 2.4.34 and openssl 1.0.2, we enter this into the generator. if you choose the "modern" compatibility model, this creates an  directive that aggressively enforces security but still works for most browsers. if your software doesn't support the modern configuration, you can update your software or choose the "intermediate" configuration instead. the selected ciphers have ecdhe in their names, an abbreviation for elliptic curve diffie-hellman ephemeral . the term ephemeral indicates forward secrecy. as a by-product, these ciphers do not support rc4. we recommend that you use an explicit list of ciphers instead of relying on defaults or terse directives whose content isn't visible. copy the generated directive into . notethough shown here on several lines for readability, the directive must be on a single line when copied to , with only a colon (no spaces) between cipher names. finally, uncomment the following line by removing the "#" at the beginning of the line. this directive forces the server to prefer high-ranking ciphers, including (in this case) those that support forward secrecy. with this directive turned on, the server tries to establish a strong secure connection before falling back to allowed ciphers with lesser security. after completing both of these procedures, save the changes to  and restart apache. if you test the domain again on , you should see that the rc4 vulnerability and other warnings are gone and the summary looks something like the following. importanteach update to openssl introduces new ciphers and removes support for old ones. keep your ec2 amazon linux 2 instance up-to-date, watch for security announcements from , and be alert to reports of new security exploits in the technical press. for more information, see  in the user guide for classic load balancers. my apache webserver doesn't start unless i supply a password this is expected behavior if you installed an encrypted, password-protected, private server key. you can remove the encryption and password requirement from the key. assuming that you have a private encrypted rsa key called  in the default directory, and that the password on it is abcde12345, run the following commands on your ec2 instance to generate an unencrypted version of the key. apache should now start without prompting you for a password. i get errors when i run sudo yum install -y mod_ssl. when you are installing the required packages for ssl, you may see errors similar to the following. this typically means that your ec2 instance is not running amazon linux 2. this tutorial only supports instances freshly created from an official amazon linux 2 ami. the  certificate authority is the centerpiece of an effort by the electronic frontier foundation (eff) to encrypt the entire internet. in line with that goal, let's encrypt host certificates are designed to be created, validated, installed, and maintained with minimal human intervention. the automated aspects of certificate management are carried out by a software agent running on your web server. after you install and configure the agent, it communicates securely with let's encrypt and performs administrative tasks on apache and the key management system. this tutorial uses the free  agent because it allows you either to supply a customized encryption key as the basis for your certificates, or to allow the agent itself to create a key based on its defaults. you can also configure certbot to renew your certificates on a regular basis without human interaction, as described in . for more information, consult the certbot  and .  certbot is not officially supported on amazon linux 2, but is available for download and functions correctly when installed. we recommend that you make the following backups to protect your data and avoid inconvenience: before you begin, take a snapshot of your amazon ebs root volume. this allows you to restore the original state of your ec2 instance. for information about creating ebs snapshots, see .the procedure below requires you to edit your  file, which controls apache's operation. certbot makes its own automated changes to this and other configuration files. make a backup copy of your entire  directory in case you need to restore it.complete the following procedures before you install certbot. download the extra packages for enterprise linux (epel) 7 repository packages. these are required to supply dependencies needed by certbot.  navigate to your home directory (). download epel with the following command. install the repository packages as shown in the following command. enable epel as shown in the following command. you can confirm that epel is enabled with the following command. it should return information similar to the following. edit the main apache configuration file, . locate the "" directive and add the following lines after it, replacing the example domain names with the actual common name and subject alternative name (san). save the file and restart apache. this procedure is based on the eff documentation for installing certbot on  and on . it describes the default use of certbot, resulting in a certificate based on a 2048-bit rsa key. install certbot packages and dependencies using the following command. run certbot. at the prompt "enter email address (used for urgent renewal and security notices)," enter a contact address and press enter. agree to the let's encrypt terms of service at the prompt. enter "a" and press enter to proceed. at the authorization for eff to put you on their mailing list, enter "y" or "n" and press enter. certbot displays the common name and subject alternative name (san) that you provided in the virtualhost block. leave the input blank and press enter.  certbot displays the following output as it creates certificates and configures apache. it then prompts you about redirecting http queries to https. to allow visitors to connect to your server via unencrypted http, enter "1". if you want to accept only encrypted connections via https, enter "2". press enter to submit your choice. certbot completes the configuration of apache and reports success and other information. after you complete the installation, test and optimize the security of your server as described in .  certbot is designed to become an invisible, error-resistant part of your server system. by default, it generates host certificates with a short, 90-day expiration time. if you have not configured your system to call the command automatically, you must re-run the certbot command manually before expiration. this procedure shows how to automate certbot by setting up a cron job. to automate certbot open the  file in a text editor, such as vim or nano, using sudo. alternatively, use sudo crontab -e. add a line similar to the following and save the file. here is an explanation of each component:schedules a command to be run at 01:39 and 13:39 every day. the selected values are arbitrary, but the certbot developers suggest running the command at least twice daily. this guarantees that any certificate found to be compromised is promptly revoked and replaced.the command runs with root permissions.the command to be run. the renew subcommand causes certbot to check any previously obtained certificates and to renew those that are approaching expiration. the  flag prevents certbot from upgrading itself without your intervention. restart the cron daemon. 
you can create an amazon cloudwatch alarm that monitors an amazon ec2 instance and automatically recovers the instance if it becomes impaired due to an underlying hardware failure or a problem that requires aws involvement to repair. terminated instances cannot be recovered. a recovered instance is identical to the original instance, including the instance id, private ip addresses, elastic ip addresses, and all instance metadata. if the impaired instance is in a placement group, the recovered instance runs in the placement group. for more information about using amazon cloudwatch alarms to recover an instance, see . to troubleshoot issues with instance recovery failures, see . when the  alarm is triggered, and the recover action is initiated, you will be notified by the amazon sns topic that you selected when you created the alarm and associated the recover action. during instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is lost. when the process is complete, information is published to the sns topic you've configured for the alarm. anyone who is subscribed to this sns topic will receive an email notification that includes the status of the recovery attempt and any further instructions. you will notice an instance reboot on the recovered instance. examples of problems that cause system status checks to fail include: loss of network connectivityloss of system powersoftware issues on the physical hosthardware issues on the physical host that impact network reachabilityif your instance has a public ipv4 address, it retains the public ipv4 address after recovery. the recover action is supported only on instances with the following characteristics: uses one of the following instance types: a1, c3, c4, c5, c5a, c5n, c6g, inf1,  m3, m4, m5, m5a, m5n, m6g,  p3, r3, r4, r5, r5a, r5n, r6g,  t2, t3, t3a, x1, or x1eruns in a virtual private cloud (vpc)uses  or  instance tenancyhas only ebs volumes (do not configure instance store volumes)the following issues can cause automatic recovery of your instance to fail: temporary, insufficient capacity of replacement hardware.the instance has an attached instance store storage, which is an unsupported configuration for automatic instance recovery.there is an ongoing service health dashboard event that prevented the recovery process from successfully executing. refer to  for the latest service availability information.the instance has reached the maximum daily allowance of three recovery attempts.the automatic recovery process attempts to recover your instance for up to three separate failures per day. if the instance system status check failure persists, we recommend that you manually stop and start the instance. for more information, see . your instance may subsequently be retired if automatic recovery fails and a hardware degradation is determined to be the root cause for the original system status check failure. 
the following procedures help you install an apache web server with php and mysql support on your amazon linux instance (sometimes called a lamp web server or lamp stack). you can use this server to host a static website or deploy a dynamic php application that reads and writes information to a database. importantto set up a lamp web server on amazon linux 2, see .if you are trying to set up a lamp web server on an ubuntu or red hat enterprise linux instance, this tutorial will not work for you. for more information about other distributions, see their specific documentation. for information about lamp web servers on ubuntu, see the ubuntu community documentation  topic.  option: complete this tutorial using automationto complete this tutorial using aws systems manager automation instead of the following tasks, run the  automation document. topics prerequisitesthis tutorial assumes that you have already launched a new instance using the amazon linux ami, with a public dns name that is reachable from the internet. for more information, see . you must also have configured your security group to allow ssh (port 22), http (port 80), and https (port 443) connections. for more information about these prerequisites, see . to install and start the lamp web server with the amazon linux ami . to ensure that all of your software packages are up to date, perform a quick software update on your instance. this process may take a few minutes, but it is important to make sure that you have the latest security updates and bug fixes. the  option installs the updates without asking for confirmation. if you would like to examine the updates before installing, you can omit this option. now that your instance is current, you can install the apache web server, mysql, and php software packages.  notesome applications may not be compatible with the following recommended software environment. before installing these packages, check whether your lamp applications are compatible with them. if there is a problem, you may need to install an alternative environment. for more information, see   use the yum install command to install multiple software packages and all related dependencies at the same time. noteif you receive the error , then your instance was not launched with the amazon linux ami (perhaps you are using amazon linux 2 instead). you can view your version of amazon linux with the following command.   start the apache web server. use the chkconfig command to configure the apache web server to start at each system boot.  the chkconfig command does not provide any confirmation message when you successfully use it to enable a service. you can verify that httpd is on by running the following command: here, httpd is  in runlevels 2, 3, 4, and 5 (which is what you want to see). add a security rule to allow inbound http (port 80) connections to your instance if you have not already done so. by default, a launch-wizard-n security group was set up for your instance during initialization. this group contains a single rule to allow ssh connections.  open the amazon ec2 console at . choose instances and select your instance. under security groups, choose view inbound rules. you should see the following list of rules in your default security group: using the procedures in , add a new inbound security rule with the following values: type: httpprotocol: tcpport range: 80source: customtest your web server. in a web browser, type the public dns address (or the public ip address) of your instance. if there is no content in , you should see the apache test page. you can get the public dns for your instance using the amazon ec2 console (check the public dns column; if this column is hidden, choose show/hide columns (the gear-shaped icon) and choose public dns). if you are unable to see the apache test page, check that the security group you are using contains a rule to allow http (port 80) traffic. for information about adding an http rule to your security group, see . importantif you are not using amazon linux, you may also need to configure the firewall on your instance to allow these connections. for more information about how to configure the firewall, see the documentation for your specific distribution. notethis test page appears only when there is no content in . when you add content to the document root, your content appears at the public dns address of your instance instead of this test page. apache httpd serves files that are kept in a directory called the apache document root. the amazon linux apache document root is , which by default is owned by root. to allow the  account to manipulate files in this directory, you must modify the ownership and permissions of the directory. there are many ways to accomplish this task. in this tutorial, you add  to the  group, to give the  group ownership of the  directory and assign write permissions to the group. to set file permissions add your user (in this case, ) to the  group. log out and then log back in again to pick up the new group, and then verify your membership. log out (use the exit command or close the terminal window): to verify your membership in the  group, reconnect to your instance, and then run the following command: change the group ownership of  and its contents to the  group. to add group write permissions and to set the group id on future subdirectories, change the directory permissions of  and its subdirectories. to add group write permissions, recursively change the file permissions of  and its subdirectories: now,  (and any future members of the  group) can add, delete, and edit files in the apache document root, enabling you to add content, such as a static website or a php application. (optional) secure your web servera web server running the http protocol provides no transport security for the data that it sends or receives. when you connect to an http server using a web browser, the urls that you visit, the content of webpages that you receive, and the contents (including passwords) of any html forms that you submit are all visible to eavesdroppers anywhere along the network pathway. the best practice for securing your web server is to install support for https (http secure), which protects your data with ssl/tls encryption. for information about enabling https on your server, see . if your server is installed and running, and your file permissions are set correctly, your  account should be able to create a php file in the  directory that is available from the internet. to test your lamp web server create a php file in the apache document root. if you get a "permission denied" error when trying to run this command, try logging out and logging back in again to pick up the proper group permissions that you configured in . in a web browser, type the url of the file that you just created. this url is the public dns address of your instance followed by a forward slash and the file name. for example: you should see the php information page: if you do not see this page, verify that the  file was created properly in the previous step. you can also verify that all of the required packages were installed with the following command. the package versions in the second column do not need to match this example output. if any of the required packages are not listed in your output, install them using the sudo yum install package command. delete the  file. although this can be useful information, it should not be broadcast to the internet for security reasons. the default installation of the mysql server has several features that are great for testing and development, but they should be disabled or removed for production servers. the mysql_secure_installation command walks you through the process of setting a root password and removing the insecure features from your installation. even if you are not planning on using the mysql server, we recommend performing this procedure. to secure the database server start the mysql server. run mysql_secure_installation. when prompted, type a password for the root account. type the current root password. by default, the root account does not have a password set. press enter. type y to set a password, and type a secure password twice. for more information about creating a secure password, see . make sure to store this password in a safe place. notesetting a root password for mysql is only the most basic measure for securing your database. when you build or install a database-driven application, you typically create a database service user for that application and avoid using the root account for anything but database administration.  type y to remove the anonymous user accounts. type y to disable the remote root login. type y to remove the test database. type y to reload the privilege tables and save your changes. (optional) if you do not plan to use the mysql server right away, stop it. you can restart it when you need it again. (optional) if you want the mysql server to start at every boot, type the following command. you should now have a fully functional lamp web server. if you add content to the apache document root at , you should be able to view that content at the public dns address for your instance.  to install phpmyadmin  is a web-based database management tool that you can use to view and edit the mysql databases on your ec2 instance. follow the steps below to install and configure phpmyadmin on your amazon linux instance. importantwe do not recommend using phpmyadmin to access a lamp server unless you have enabled ssl/tls in apache; otherwise, your database administrator password and other data are transmitted insecurely across the internet. for security recommendations from the developers, see . notethe amazon linux package management system does not currently support the automatic installation of phpmyadmin in a php 7 environment. this tutorial describes how to install phpmyadmin manually. log in to your ec2 instance using ssh. install the required dependencies. restart apache. navigate to the apache document root at . select a source package for the latest phpmyadmin release from . to download the file directly to your instance, copy the link and paste it into a wget command, as in this example: create a phpmyadmin folder and extract the package into it using the following command. delete the phpmyadmin-latest-all-languages.tar.gz tarball. (optional) if the mysql server is not running, start it now. in a web browser, type the url of your phpmyadmin installation. this url is the public dns address (or the public ip address) of your instance followed by a forward slash and the name of your installation directory. for example: you should see the phpmyadmin login page: log in to your phpmyadmin installation with the  user name and the mysql root password you created earlier.  your installation must still be configured before you put it into service. to configure phpmyadmin, you can , , or combine both approaches.   for information about using phpmyadmin, see the . this section offers suggestions for resolving common problems you may encounter while setting up a new lamp server.  perform the following checks to see if your apache web server is running and accessible. is the web server running? you can verify that httpd is on by running the following command: here, httpd is  in runlevels 2, 3, 4, and 5 (which is what you want to see). if the httpd process is not running, repeat the steps described in . is the firewall correctly configured? if you are unable to see the apache test page, check that the security group you are using contains a rule to allow http (port 80) traffic. for information about adding an http rule to your security group, see . this tutorial recommends installing the most up-to-date versions of apache http server, php, and mysql. before installing an additional lamp application, check its requirements to confirm that it is compatible with your installed environment. if the latest version of php is not supported, it is possible (and entirely safe) to downgrade to an earlier supported configuration. you can also install more than one version of php in parallel, which solves certain compatibility problems with a minimum of effort. for information about configuring a preference among multiple installed php versions, see  how to downgradethe well-tested previous version of this tutorial called for the following core lamp packages: if you have already installed the latest packages as recommended at the start of this tutorial, you must first uninstall these packages and other dependencies as follows: next, install the replacement environment: if you decide later to upgrade to the recommended environment, you must first remove the customized packages and dependencies: now you can install the latest packages, as described earlier. for more information about transferring files to your instance or installing a wordpress blog on your web server, see the following documentation: for more information about the commands and software used in this tutorial, see the following webpages: apache web server: mysql database server: php programming language: the  command: the  command: for more information about registering a domain name for your web server, or transferring an existing domain name to this host, see  in the amazon route 53 developer guide. 
an amazon machine image (ami) provides the information required to launch an instance. you must specify an ami when you launch an instance. you can launch multiple instances from a single ami when you need multiple instances with the same configuration. you can use different amis to launch instances when you need instances with different configurations. an ami includes the following: one or more ebs snapshots, or, for instance-store-backed amis, a template for the root volume of the instance (for example, an operating system, an application server, and applications).launch permissions that control which aws accounts can use the ami to launch instances.a block device mapping that specifies the volumes to attach to the instance when it's launched.the following diagram summarizes the ami lifecycle. after you create and register an ami, you can use it to launch new instances. (you can also launch instances from an ami if the ami owner grants you launch permissions.) you can copy an ami within the same region or to different regions. when you no longer require an ami, you can deregister it.  you can search for an ami that meets the criteria for your instance. you can search for amis provided by aws or amis provided by the community. for more information, see  and . after you launch an instance from an ami, you can connect to it. when you are connected to an instance, you can use it just like you use any other server. for information about launching, connecting, and using your instance, see . you can launch an instance from an existing ami, customize the instance, and then save this updated configuration as a custom ami. instances launched from this new custom ami include the customizations that you made when you created the ami. the root storage device of the instance determines the process you follow to create an ami. the root volume of an instance is either an amazon ebs volume or an instance store volume. for more information about the root device volume, see . to create an amazon ebs-backed ami, see .to create an instance store-backed ami, see .to help categorize and manage your amis, you can assign custom tags to them. for more information, see . after you create an ami, you can keep it private so that only you can use it, or you can share it with a specified list of aws accounts. you can also make your custom ami public so that the community can use it. building a safe, secure, usable ami for public consumption is a fairly straightforward process, if you follow a few simple guidelines. for information about how to create and use shared amis, see . you can purchase amis from a third party, including amis that come with service contracts from organizations such as red hat. you can also create an ami and sell it to other amazon ec2 users. for more information about buying or selling amis, see . you can deregister an ami when you have finished with it. after you deregister an ami, it can't be used to launch new instances. existing instances launched from the ami are not affected. for more information, see . amazon linux 2 and the amazon linux ami are supported and maintained linux images provided by aws. the following are some of the features of amazon linux 2 and amazon linux ami: a stable, secure, and high-performance execution environment for applications running on amazon ec2.provided at no additional charge to amazon ec2 users.repository access to multiple versions of mysql, postgresql, python, ruby, tomcat, and many more common packages.updated on a regular basis to include the latest components, and these updates are also made available in the yum repositories for installation on running instances.includes packages that enable easy integration with aws services, such as the aws cli, amazon ec2 api and ami tools, the boto library for python, and the elastic load balancing tools.for more information, see . 
you can use the amazon ec2 console or the command line to find shared amis.  amis are a regional resource. therefore, when searching for a shared ami (public or private), you must search for it from within the region from which it is being shared. to make an ami available in a different region, copy the ami to the region and then share it. for more information, see . to find a shared private ami using the console open the amazon ec2 console at . in the navigation pane, choose amis.  in the first filter, choose private images. all amis that have been shared with you are listed. to granulate your search, choose the search bar and use the filter options provided in the menu. to find a shared public ami using the console open the amazon ec2 console at . in the navigation pane, choose amis. in the first filter, choose public images. to granulate your search, choose the search bar and use the filter options provided in the menu. use filters to list only the types of amis that interest you. for example, choose owner : and then choose amazon images to display only amazon's public images. use the  command (aws cli) to list amis. you can scope the list to the types of amis that interest you, as shown in the following examples. example: list all public amisthe following command lists all public amis, including any public amis that you own. example: list amis with explicit launch permissionsthe following command lists the amis for which you have explicit launch permissions. this list does not include any amis that you own. example: list amis owned by amazonthe following command lists the amis owned by amazon. amazon's public amis have an aliased owner, which appears as  in the account field. this enables you to find amis from amazon easily. other users can't alias their amis. example: list amis owned by an accountthe following command lists the amis owned by the specified aws account. example: scope amis using a filterto reduce the number of displayed amis, use a filter to list only the types of amis that interest you. for example, use the following filter to display only ebs-backed amis. before you use a shared ami, take the following steps to confirm that there are no pre-installed credentials that would allow unwanted access to your instance by a third party and no pre-configured remote logging that could transmit sensitive data to a third party. check the documentation for the linux distribution used by the ami for information about improving the security of the system. to ensure that you don't accidentally lose access to your instance, we recommend that you initiate two ssh sessions and keep the second session open until you've removed credentials that you don't recognize and confirmed that you can still log into your instance using ssh. identify and disable any unauthorized public ssh keys. the only key in the file should be the key you used to launch the ami. the following command locates  files: disable password-based authentication for the root user. open the  file and edit the  line as follows: alternatively, you can disable the ability to log into the instance as the root user: restart the sshd service. check whether there are any other user accounts that are able to log in to your instance. accounts with superuser privileges are particularly dangerous. remove or lock the password of any unknown accounts. check for open ports that you aren't using and running network services listening for incoming connections. to prevent preconfigured remote logging, you should delete the existing configuration file and restart the rsyslog service. for example: verify that all cron jobs are legitimate. if you discover a public ami that you feel presents a security risk, contact the aws security team. for more information, see the . 
amazon efs provides scalable file storage for use with amazon ec2. you can create an efs file system and configure your instances to mount the file system. you can use an efs file system as a common data source for workloads and applications running on multiple instances. for more information, see the . in this tutorial, you create an efs file system and two linux instances that can share data using the file system. importantamazon efs is not supported on windows instances. topics create a security group (for example, efs-sg) to associate with the ec2 instances and efs mount target, and add the following rules:allow inbound ssh connections to the ec2 instances from your computer (the source is the cidr block for your network).allow inbound nfs connections to the file system via the efs mount target from the ec2 instances that are associated with this security group (the source is the security group itself). for more information, see , and  in the amazon elastic file system user guide.create a key pair. you must specify a key pair when you configure your instances or you can't connect to them. for more information, see .amazon efs enables you to create a file system that multiple instances can mount and access at the same time. for more information, see  in the amazon elastic file system user guide. to create a file system open the amazon elastic file system console at . choose create file system. on the configure network access page, do the following: for vpc, select the vpc to use for your instances. for create mount targets, select all the availability zones. for each availability zone, ensure that the value for security group is the security group that you created in . choose next step. on the configure file system settings page, do the following: for the tag with key=name, type a name for the file system in value. for choose throughput mode, keep the default option, bursting. for choose performance mode, keep the default option, general purpose. choose next step. on the configure client access page, keep the default settings and choose next step. on the review and create page, choose create file system. after the file system is created, note the file system id, as you'll use it later in this tutorial. use the following procedure to launch two  instances. the user data script mounts the file system to both instances during launch and updates /etc/fstab to ensure that the file system is remounted after an instance reboot. note that t2 instances must be launched in a subnet. you can use a default vpc or a nondefault vpc. notethere are other ways that you can mount the volume (for example, on an already running instance). for more information, see  in the amazon elastic file system user guide. to launch two instances and mount an efs file system open the amazon ec2 console at . choose launch instance. on the choose an amazon machine image page, select an amazon linux ami with the hvm virtualization type. on the choose an instance type page, keep the default instance type,  and choose next: configure instance details. on the configure instance details page, do the following: for number of instances, type 2. [default vpc] if you have a default vpc, it is the default value for network. keep the default vpc and the default value for subnet to use the default subnet in the availability zone that amazon ec2 chooses for your instances. [nondefault vpc] select your vpc for network and a public subnet from subnet. [nondefault vpc] for auto-assign public ip, choose enable. otherwise, your instances do not get public ip addresses or public dns names. under advanced details, select as text, and paste the following script into user data. update file_system_id with the id of your file system. you can optionally update mount_point with a directory for your mounted file system. advance to step 6 of the wizard.on the configure security group page, choose select an existing security group and select the security group that you created in , and then choose review and launch. on the review instance launch page, choose launch. in the select an existing key pair or create a new key pair dialog box, select choose an existing key pair and choose your key pair. select the acknowledgment check box, and choose launch instances. in the navigation pane, choose instances to see the status of your instances. initially, their status is . after the status changes to , your instances are ready for use. you can connect to your instances and verify that the file system is mounted to the directory that you specified (for example, /mnt/efs). to verify that the file system is mounted connect to your instances. for more information, see . from the terminal window for each instance, run the df -t command to verify that the efs file system is mounted. note that the name of the file system, shown in the example output as efs-dns, has the following form: (optional) create a file in the file system from one instance, and then verify that you can view the file from the other instance. from the first instance, run the following command to create the file: from the second instance, run the following command to view the file: when you are finished with this tutorial, you can terminate the instances and delete the file system. to terminate the instances open the amazon ec2 console at . in the navigation pane, choose instances. select the instances to terminate. choose actions, instance state, terminate. choose yes, terminate when prompted for confirmation. to delete the file system open the amazon elastic file system console at . select the file system to delete. choose actions, delete file system. when prompted for confirmation, type the id of the file system and choose delete file system. 
connect to the linux instances that you launched and transfer files between your local computer and your instance. to connect to a windows instance, see  in the amazon ec2 user guide for windows instances. the operating system of your local computer determines the options that you have to connect from your local computer to your linux instance. if your local computer operating system is linux or macos x   if your local computer operating system is windows   
the ebs direct apis service is integrated with aws cloudtrail, a service that provides a record of actions taken by a user, role, or an aws service in the ebs direct apis. cloudtrail captures all api calls for the ebs direct apis as events. the calls captured include calls to api operations of the ebs direct apis. if you create a trail, you can enable continuous delivery of cloudtrail events to an amazon simple storage service (amazon s3) bucket, including events for the ebs direct apis. if you don't configure a trail, you can still view the most recent events in the cloudtrail console in event history. you can use the information collected by cloudtrail to determine the request that was made to the ebs direct apis, the ip address from which the request was made, who made the request, when it was made, and additional details.  for more information about cloudtrail, see the . cloudtrail is enabled on your aws account when you create the account. when supported event activity occurs in the ebs direct apis, that activity is recorded in a cloudtrail event along with other aws service events in event history. you can view, search, and download recent events in your aws account. for more information, see . for an ongoing record of events in your aws account, including events for the ebs direct apis, create a trail. a trail enables cloudtrail to deliver log files to an s3 bucket. by default, when you create a trail in the console, the trail applies to all aws regions. the trail logs events from all regions in the aws partition and delivers the log files to the s3 bucket that you specify. additionally, you can configure other aws services to further analyze and act upon the event data collected in cloudtrail logs. for more information, see the following:  and the ebs direct apis supports logging the following actions as events in cloudtrail log files: every event or log entry contains information about who generated the request. the identity information helps you determine the following:  whether the request was made with root or aws identity and access management (iam) user credentials.whether the request was made with temporary security credentials for a role or federated user.whether the request was made by another aws service.for more information, see the . a trail is a configuration that enables delivery of events as log files to an s3 bucket that you specify. cloudtrail log files contain one or more log entries. an event represents a single request from any source and includes information about the requested action, the date and time of the action, request parameters, and so on. cloudtrail log files aren't an ordered stack trace of the public api calls, so they don't appear in any specific order. the following examples show cloudtrail log entries that demonstrates the  and  actions. startsnapshot example: completesnapshot example: 
when you launch an instance, the root device volume contains the image used to boot the instance. when we introduced amazon ec2, all amis were backed by amazon ec2 instance store, which means the root device for an instance launched from the ami is an instance store volume created from a template stored in amazon s3. after we introduced amazon ebs, we introduced amis that are backed by amazon ebs. this means that the root device for an instance launched from the ami is an amazon ebs volume created from an amazon ebs snapshot. you can choose between amis backed by amazon ec2 instance store and amis backed by amazon ebs. we recommend that you use amis backed by amazon ebs, because they launch faster and use persistent storage. for more information about the device names amazon ec2 uses for your root volumes, see . topics you can launch an instance from either an instance store-backed ami or an amazon ebs-backed ami. the description of an ami includes which type of ami it is; you'll see the root device referred to in some places as either  (for amazon ebs-backed) or  (for instance store-backed). this is important because there are significant differences between what you can do with each type of ami. for more information about these differences, see . instance store-backed instancesinstances that use instance stores for the root device automatically have one or more instance store volumes available, with one volume serving as the root device volume. when an instance is launched, the image that is used to boot the instance is copied to the root volume. note that you can optionally use additional instance store volumes, depending on the instance type. any data on the instance store volumes persists as long as the instance is running, but this data is deleted when the instance is terminated (instance store-backed instances do not support the stop action) or if it fails (such as if an underlying drive has issues).  after an instance store-backed instance fails or terminates, it cannot be restored. if you plan to use amazon ec2 instance store-backed instances, we highly recommend that you distribute the data on your instance stores across multiple availability zones. you should also back up critical data from your instance store volumes to persistent storage on a regular basis. for more information, see . amazon ebs-backed instancesinstances that use amazon ebs for the root device automatically have an amazon ebs volume attached. when you launch an amazon ebs-backed instance, we create an amazon ebs volume for each amazon ebs snapshot referenced by the ami you use. you can optionally use other amazon ebs volumes or instance store volumes, depending on the instance type.  an amazon ebs-backed instance can be stopped and later restarted without affecting data stored in the attached volumes. there are various instance– and volume-related tasks you can do when an amazon ebs-backed instance is in a stopped state. for example, you can modify the properties of the instance, change its size, or update the kernel it is using, or you can attach your root volume to a different running instance for debugging or any other purpose.  if an amazon ebs-backed instance fails, you can restore your session by following one of these methods: stop and then start again (try this method first).automatically snapshot all relevant volumes and create a new ami. for more information, see .attach the volume to the new instance by following these steps: create a snapshot of the root volume. register a new ami using the snapshot. launch a new instance from the new ami. detach the remaining amazon ebs volumes from the old instance. reattach the amazon ebs volumes to the new instance. for more information, see . the ami that you specify when you launch your instance determines the type of root device volume that your instance has. to choose an amazon ebs-backed ami using the console open the amazon ec2 console. in the navigation pane, choose amis. from the filter lists, select the image type (such as public images). in the search bar choose platform to select the operating system (such as amazon linux), and root device type to select ebs images. (optional) to get additional information to help you make your choice, choose the show/hide columns icon, update the columns to display, and choose close. choose an ami and write down its ami id. to choose an instance store-backed ami using the console open the amazon ec2 console. in the navigation pane, choose amis. from the filter lists, select the image type (such as public images). in the search bar, choose platform to select the operating system (such as amazon linux), and root device type to select instance store. (optional) to get additional information to help you make your choice, choose the show/hide columns icon, update the columns to display, and choose close. choose an ami and write down its ami id. to verify the type of the root device volume of an ami using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)to determine the root device type of an instance using the console open the amazon ec2 console. in the navigation pane, choose instances, and select the instance. check the value of root device type in the description tab as follows: if the value is , this is an amazon ebs-backed instance.if the value is , this is an instance store-backed instance.to determine the root device type of an instance using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)by default, the root volume for an ami backed by amazon ebs is deleted when the instance terminates. you can change the default behavior to ensure that the volume persists after the instance terminates. to change the default behavior, set the  attribute to  using a block device mapping. topics you can configure the root volume to persist when you launch an instance using the amazon ec2 console or the command line tools. to configure the root volume to persist when you launch an instance using the console open the amazon ec2 console at . in the navigation pane, choose instances and then choose launch instance. on the choose an amazon machine image (ami) page, select the ami to use and choose select. follow the wizard to complete the choose an instance type and configure instance details pages. on the add storage page, deselect delete on termination for the root volume. complete the remaining wizard pages, and then choose launch. to configure the root volume to persist when you launch an instance using the aws cliuse the  command and include a block device mapping that sets the  attribute to . specify the following in . to configure the root volume to persist when you launch an instance using the tools for windows powershelluse the  command and include a block device mapping that sets the  attribute to . you can configure the root volume to persist for a running instance using the command line tools only.  to configure the root volume to persist for an existing instance using the aws cliuse the  command with a block device mapping that sets the  attribute to . to configure the root volume to persist for an existing instance using the aws tools for windows powershelluse the  command with a block device mapping that sets the  attribute to . you can confirm that a root volume is configured to persist using the amazon ec2 console or the command line tools. to confirm that a root volume is configured to persist using the amazon ec2 console open the amazon ec2 console at . in the navigation pane, choose instances and then select the instance. in the description tab, choose the entry for root device. if delete on termination is set to , the volume is configured to persist. to confirm that a root volume is configured to persist using the aws cliuse the  command and verify that the  attribute in the  response element is set to . to confirm that a root volume is configured to persist using the aws tools for windows powershelluse the  and verify that the  attribute in the  response element is set to . 
on-demand capacity reservations enable you to reserve capacity for your amazon ec2 instances in a specific availability zone for any duration. this gives you the ability to create and manage capacity reservations independently from the billing discounts offered by savings plans or regional reserved instances. by creating capacity reservations, you ensure that you always have access to ec2 capacity when you need it, for as long as you need it. you can create capacity reservations at any time, without entering into a one-year or three-year term commitment, and the capacity is available immediately. when you no longer need it, cancel the capacity reservation to stop incurring charges. when you create a capacity reservation, you specify: the availability zone in which to reserve the capacitythe number of instances for which to reserve capacitythe instance attributes, including the instance type, tenancy, and platform/oscapacity reservations can only be used by instances that match their attributes. by default, they are automatically used by running instances that match the attributes. if you don't have any running instances that match the attributes of the capacity reservation, it remains unused until you launch an instance with matching attributes. in addition, you can use savings plans and regional reserved instances with your capacity reservations to benefit from billing discounts. aws automatically applies your discount when the attributes of a capacity reservation match the attributes of a savings plan or  regional reserved instance. for more information, see . topics the following table highlights key differences between capacity reservations, reserved instances, and savings plans: for more information, see the following: the number of instances for which you are allowed to reserve capacity is based on your account's on-demand instance limit. you can reserve capacity for as many instances as that limit allows, minus the number of instances that are already running. before you create capacity reservations, take note of the following limitations and restrictions. active and unused capacity reservations count toward your on-demand instance limitscapacity reservations are not transferable from one aws account to another. however, you can share capacity reservations with other aws accounts. for more information, see .zonal reserved instance billing discounts do not apply to capacity reservationscapacity reservations can't be created in placement groupscapacity reservations can't be used with dedicated hostscapacity reservations can't be used with bring your own license (byol)capacity reservations can't be used with local zones
to use an ec2 fleet, you create a request that includes the total target capacity, on-demand capacity, spot capacity, one or more launch specifications for the instances, and the maximum price that you are willing to pay. the fleet request must include a launch template that defines the information that the fleet needs to launch an instance, such as an ami, instance type, subnet or availability zone, and one or more security groups. you can specify launch specification overrides for the instance type, subnet, availability zone, and maximum price you're willing to pay, and you can assign weighted capacity to each launch specification override. if your fleet includes spot instances, amazon ec2 can attempt to maintain your fleet target capacity as spot prices change. an ec2 fleet request remains active until it expires or you delete it. when you delete a fleet, you can specify whether deletion terminates the instances in that fleet. topics an ec2 fleet request can be in one of the following states:  – the ec2 fleet request is being evaluated and amazon ec2 is preparing to launch the target number of instances, which can include on-demand instances, spot instances, or both. – the ec2 fleet request has been validated and amazon ec2 is attempting to maintain the target number of running instances. the request remains in this state until it is modified or deleted. – the ec2 fleet request is being modified. the request remains in this state until the modification is fully processed or the request is deleted. only a  request type can be modified. this state does not apply to other request types. – the ec2 fleet request is deleted and does not launch additional instances. its existing instances continue to run until they are interrupted or terminated. the request remains in this state until all instances are interrupted or terminated. – the ec2 fleet request is deleted and its instances are terminating. the request remains in this state until all instances are terminated. – the ec2 fleet is deleted and has no running instances. the request is deleted two days after its instances are terminated.the following illustration represents the transitions between the ec2 fleet request states. if you exceed your fleet limits, the request is deleted immediately.  to create an ec2 fleet, the following prerequisites must be in place. a launch template includes information about the instances to launch, such as the instance type, availability zone, and the maximum price that you are willing to pay. for more information, see . the  role grants the ec2 fleet permission to request, launch, terminate, and tag instances on your behalf. amazon ec2 uses this service-linked role to complete the following actions:  – launch instances. – request spot instances. – terminate instances. – describe amazon machine images (amis) for the spot instances. – describe the status of the spot instances. – describe the subnets for spot instances. – add tags to the ec2 fleet, instances, and volumes.ensure that this role exists before you use the aws cli or an api to create an ec2 fleet. notean  ec2 fleet does not require this role. to create the role, use the iam console as follows. to create the awsserviceroleforec2fleet role for ec2 fleet open the iam console at . in the navigation pane, choose roles, and then choose create role. for select type of trusted entity, choose aws service. for choose the service that will use this role, choose ec2 - fleet, and then choose next: permissions, next: tags, and next: review. on the review page, choose create role. if you no longer need to use ec2 fleet, we recommend that you delete the awsserviceroleforec2fleet role. after this role is deleted from your account, you can create the role again if you create another fleet. for more information, see  in the iam user guide. if you specify an  or an  in your ec2 fleet and you use a customer-managed customer master key (cmk) for encryption, you must grant the awsserviceroleforec2fleet role permission to use the cmk so that amazon ec2 can launch instances on your behalf. to do this, you must add a grant to the cmk, as shown in the following procedure. when providing permissions, grants are an alternative to key policies. for more information, see  and  in the aws key management service developer guide. to grant the awsserviceroleforec2fleet role permissions to use the cmk use the  command to add a grant to the cmk and to specify the principal (the awsserviceroleforec2fleet service-linked role) that is given permission to perform the operations that the grant permits. the cmk is specified by the  parameter and the arn of the cmk. the principal is specified by the  parameter and the arn of the awsserviceroleforec2fleet service-linked role. if your iam users will create or manage an ec2 fleet, be sure to grant them the required permissions as follows. to grant an iam user permissions for ec2 fleet open the iam console at . in the navigation pane, choose policies. choose create policy. on the create policy page, choose the json tab, replace the text with the following, and choose review policy. the  grants an iam user permission to call all amazon ec2 api actions. to limit the user to specific amazon ec2 api actions, specify those actions instead. an iam user must have permission to call the  action to enumerate existing iam roles, the  action to specify the ec2 fleet role, and the  action to enumerate existing instance profiles. (optional) to enable an iam user to create roles or instance profiles using the iam console, you must also add the following actions to the policy: on the review policy page, enter a policy name and description, and choose create policy. in the navigation pane, choose users and select the user. on the permissions tab, choose add permissions. choose attach existing policies directly. select the policy that you created earlier and choose next: review. choose add permissions. ec2 fleet checks the health status of the instances in the fleet every two minutes. the health status of an instance is either  or . the fleet determines the health status of an instance using the status checks provided by amazon ec2. if the status of either the instance status check or the system status check is  for three consecutive health checks, the health status of the instance is . otherwise, the health status is . for more information, see . you can configure your ec2 fleet to replace unhealthy instances. after enabling health check replacement, an instance is replaced after its health status is reported as . the fleet could go below its target capacity for up to a few minutes while an unhealthy instance is being replaced. requirements health check replacement is supported only with ec2 fleets that maintain a target capacity, not with one-time fleets.you can configure your ec2 fleet to replace unhealthy instances only when you create it.iam users can use health check replacement only if they have permission to call the  action.to create an ec2 fleet, you need only specify the launch template, total target capacity, and whether the default purchasing option is on-demand or spot. if you do not specify a parameter, the fleet uses the default value. to view the full list of fleet configuration parameters, you can generate a json file as follows. to generate a json file with all possible ec2 fleet parameters using the command line use the  (aws cli) command and the  parameter to generate an ec2 fleet json file: the following ec2 fleet parameters are available: noteuse lowercase for all parameter values; otherwise, you get an error when amazon ec2 uses the json file to launch the ec2 fleet. allocationstrategy (for spotoptions)(optional) indicates how to allocate the spot instance target capacity across the spot instance pools specified by the ec2 fleet. valid values are , , and . the default is . specify the allocation strategy that meets your needs. for more information, see . instanceinterruptionbehavior(optional) the behavior when a spot instance is interrupted. valid values are , , and . by default, the spot service terminates spot instances when they are interrupted. if the fleet type is , you can specify that the spot service hibernates or stops spot instances when they are interrupted. instancepoolstousecountthe number of spot pools across which to allocate your target spot capacity. valid only when spot allocationstrategy is set to . ec2 fleet selects the cheapest spot pools and evenly allocates your target spot capacity across the number of spot pools that you specify. singleinstancetypeindicates that the fleet uses a single instance type to launch all spot instances in the fleet. singleavailabilityzoneindicates that the fleet launches all spot instances into a single availability zone. maxtotalpricethe maximum amount per hour for spot instances that you're willing to pay. mintargetcapacitythe minimum target capacity for spot instances in the fleet. if the minimum target capacity is not reached, the fleet launches no instances. allocationstrategy (for ondemandoptions)the order of the launch template overrides to use in fulfilling on-demand capacity. if you specify , ec2 fleet uses price to determine the order, launching the lowest price first. if you specify prioritized, ec2 fleet uses the priority that you assigned to each launch template override, launching the highest priority first. if you do not specify a value, ec2 fleet defaults to . singleinstancetypeindicates that the fleet uses a single instance type to launch all on-demand instances in the fleet. singleavailabilityzoneindicates that the fleet launches all on-demand instances into a single availability zone. maxtotalpricethe maximum amount per hour for on-demand instances that you're willing to pay. mintargetcapacitythe minimum target capacity for on-demand instances in the fleet. if the minimum target capacity is not reached, the fleet launches no instances. excesscapacityterminationpolicy(optional) indicates whether running instances should be terminated if the total target capacity of the ec2 fleet is decreased below the current size of the ec2 fleet. valid values are  and . launchtemplateidthe id of the launch template to use. you must specify either the launch template id or launch template name. the launch template must specify an amazon machine image (ami). for information about creating launch templates, see . launchtemplatenamethe name of the launch template to use. you must specify either the launch template id or launch template name. the launch template must specify an amazon machine image (ami). for more information, see . versionthe launch template version number, , or . you must specify a value, otherwise the request fails. if the value is , amazon ec2 uses the latest version of the launch template. if the value is , amazon ec2 uses the default version of the launch template. for more information, see . instancetype(optional) the instance type. if entered, this value overrides the launch template. the instance types must have the minimum hardware specifications that you need (vcpus, memory, or storage). maxprice(optional) the maximum price per unit hour that you are willing to pay for a spot instance. if entered, this value overrides the launch template. you can use the default maximum price (the on-demand price) or specify the maximum price that you are willing to pay. your spot instances are not launched if your maximum price is lower than the spot price for the instance types that you specified. subnetid(optional) the id of the subnet in which to launch the instances. if entered, this value overrides the launch template.to create a new vpc, go the amazon vpc console. when you are done, return to the json file and enter the new subnet id. availabilityzone(optional) the availability zone in which to launch the instances. the default is to let aws choose the zones for your instances. if you prefer, you can specify specific zones. if entered, this value overrides the launch template.specify one or more availability zones. if you have more than one subnet in a zone, specify the appropriate subnet. to add subnets, go to the amazon vpc console. when you are done, return to the json file and enter the new subnet id. weightedcapacity(optional) the number of units provided by the specified instance type. if entered, this value overrides the launch template. prioritythe priority for the launch template override. if allocationstrategy is set to , ec2 fleet uses priority to determine which launch template override to use first in fulfilling on-demand capacity. the highest priority is launched first. valid values are whole numbers starting at . the lower the number, the higher the priority. if no number is set, the override has the lowest priority. totaltargetcapacitythe number of instances to launch. you can choose instances or performance characteristics that are important to your application workload, such as vcpus, memory, or storage. if the request type is , you can specify a target capacity of 0 and add capacity later. ondemandtargetcapacity(optional) the number of on-demand instances to launch. this number must be less than the . spottargetcapacity(optional) the number of spot instances to launch. this number must be less than the . defaulttargetcapacitytypeif the value for  is higher than the combined values for  and , the difference is launched as the instance purchasing option specified here. valid values are  or . terminateinstanceswithexpiration(optional) by default, amazon ec2 terminates your instances when the ec2 fleet request expires. the default value is . to keep them running after your request expires, do not enter a value for this parameter. type(optional) indicates whether the ec2 fleet submits a synchronous one-time request for your desired capacity (), or an asynchronous one-time request for your desired capacity, but with no attempt maintain the capacity or to submit requests in alternative capacity pools if capacity is unavailable (), or submits an asynchronous request for your desired capacity and continues to maintain your desired capacity by replenishing interrupted spot instances (). valid values are , , and . the default value is . for more information, see . validfrom(optional) to create a request that is valid only during a specific time period, enter a start date. validuntil(optional) to create a request that is valid only during a specific time period, enter an end date. replaceunhealthyinstances(optional) to replace unhealthy instances in an ec2 fleet that is configured to  the fleet, enter . otherwise, leave this parameter empty. tagspecifications(optional) the key-value pair for tagging the ec2 fleet request on creation. the value for  must be , otherwise the fleet request fails. to tag instances at launch, specify the tags in the . for information about tagging after launch, see . when you create an ec2 fleet, you must specify a launch template that includes information about the instances to launch, such as the instance type, availability zone, and the maximum price you are willing to pay. you can create an ec2 fleet that includes multiple launch specifications that override the launch template. the launch specifications can vary by instance type, availability zone, subnet, and maximum price, and can include a different weighted capacity. when you create an ec2 fleet, use a json file to specify information about the instances to launch. for more information, see . ec2 fleets can only be created using the aws cli. to create an ec2 fleet (aws cli) use the  (aws cli) command to create an ec2 fleet.for example configuration files, see . the following is example output for a fleet of type  or . the following is example output for a fleet of type  that launched the target capacity. the following is example output for a fleet of type  that launched part of the target capacity with errors for instances that were not launched. the following is example output for a fleet of type  that launched no instances. to help categorize and manage your ec2 fleet requests, you can tag them with custom metadata. you can assign a tag to an ec2 fleet request when you create it, or afterward. when you tag a fleet request, the instances and volumes that are launched by the fleet are not automatically tagged. you need to explicitly tag the instances and volumes launched by the fleet. you can choose to assign tags to only the fleet request, or to only the instances launched by the fleet, or to only the volumes attached to the instances launched by the fleet, or to all three. notefor  fleet types, you can tag volumes that are attached to on-demand instances and spot instances. for  or  fleet types, you can only tag volumes that are attached to on-demand instances. for more information about how tags work, see . prerequisite grant the iam user the permission to tag resources. for more information, see . to grant an iam user the permission to tag resourcescreate a iam policy that includes the following: the  action. this grants the iam user permission to create tags.the  action. this grants the iam user permission to create an ec2 fleet request.for , we recommend that you specify . this allows users to tag all resource types.importantwe currently do not support resource-level permissions for the  resource. if you specify  as a resource, you will get an unauthorized exception when you try to tag the fleet. the following example illustrates how not to set the policy.    to tag a new ec2 fleet requestto tag an ec2 fleet request when you create it, specify the key-value pair in the  used to create the fleet. the value for  must be . if you specify another value, the fleet request fails. to tag instances and volumes launched by an ec2 fleetto tag instances and volumes when they are launched by the fleet, specify the tags in the  that is referenced in the ec2 fleet request. noteyou can't tag volumes attached to spot instances that are launched by a  or  fleet type. to tag an existing ec2 fleet request, instance, and volume (aws cli)use the  command to tag existing resources. the ec2 fleet launches on-demand instances when there is available capacity, and launches spot instances when your maximum price exceeds the spot price and capacity is available. the on-demand instances run until you terminate them, and the spot instances run until they are interrupted or you terminate them. the returned list of running instances is refreshed periodically and might be out of date. to monitor your ec2 fleet (aws cli)use the  command to describe your ec2 fleets. the following is example output. use the  command to describe the instances for the specified ec2 fleet. use the  command to describe the history for the specified ec2 fleet for the specified time.  you can modify an ec2 fleet that is in the  or  state. when you modify a fleet, it enters the  state. you can only modify an ec2 fleet that is of type . you cannot modify an ec2 fleet of type  or . you can modify the following parameters of an ec2 fleet:  – increase or decrease the target capacity for , , and . – whether running instances should be terminated if the total target capacity of the ec2 fleet is decreased below the current size of the fleet. valid values are  and .when you increase the target capacity, the ec2 fleet launches the additional instances according to the instance purchasing option specified for , which are either on-demand instances or spot instances. if the  is , the ec2 fleet launches the additional spot instances according to its allocation strategy. if the allocation strategy is , the fleet launches the instances from the lowest-priced spot instance pool in the request. if the allocation strategy is , the fleet distributes the instances across the pools in the request. when you decrease the target capacity, the ec2 fleet deletes any open requests that exceed the new target capacity. you can request that the fleet terminate instances until the size of the fleet reaches the new target capacity. if the allocation strategy is , the fleet terminates the instances with the highest price per unit. if the allocation strategy is , the fleet terminates instances across the pools. alternatively, you can request that ec2 fleet keep the fleet at its current size, but not replace any spot instances that are interrupted or any instances that you terminate manually. when an ec2 fleet terminates a spot instance because the target capacity was decreased, the instance receives a spot instance interruption notice. to modify an ec2 fleet (aws cli)use the  command to update the target capacity of the specified ec2 fleet. if you are decreasing the target capacity but want to keep the fleet at its current size, you can modify the previous command as follows. if you no longer require an ec2 fleet, you can delete it. after you delete a fleet, it launches no new instances. you must specify whether the ec2 fleet must terminate its instances. if you specify that the instances must be terminated when the fleet is deleted, it enters the  state. otherwise, it enters the  state, and the instances continue to run until they are interrupted or you terminate them manually. you can only delete fleets of type  and . you cannot delete an  ec2 fleet. to delete an ec2 fleet (aws cli)use the  command and the  parameter to delete the specified ec2 fleet and terminate the instances. the following is example output. you can modify the previous command using the  parameter to delete the specified ec2 fleet without terminating the instances. the following is example output. if an ec2 fleet fails to delete,  returns the id of the ec2 fleet, an error code, and an error message. the error codes include , , , and . 
the following requirements and limitations apply when you modify an amazon ebs volume. to learn more about the general requirements for ebs volumes, see . elastic volumes are supported on the following instances: all the following previous-generation instances: c1, c3, cc2, cr1, g2, i2, m1, m3, and r3if your instance type does not support elastic volumes, see . linux amis require a guid partition table (gpt) and grub 2 for boot volumes that are 2 tib (2,048 gib) or larger. many linux amis today still use the mbr partitioning scheme, which only supports boot volume sizes up to 2 tib. if your instance does not boot with a boot volume larger than 2 tib, the ami you are using may be limited to a boot volume size of less than 2 tib. non-boot volumes do not have this limitation on linux instances. for requirements affecting windows volumes, see  in the amazon ec2 user guide for windows instances. before attempting to resize a boot volume beyond 2 tib, you can determine whether the volume is using mbr or gpt partitioning by running the following command on your instance: an amazon linux instance with gpt partitioning returns the following information: a suse instance with mbr partitioning returns the following information: elastic volume operations are not supported on multi-attach enabled amazon ebs volumes.the new volume size cannot exceed the supported volume capacity. for more information, see .if the volume was attached before november 3, 2016 23:40 utc, you must initialize elastic volumes support. for more information, see .if you are using an unsupported previous-generation instance type, or if you encounter an error while attempting a volume modification, see .a  volume that is attached to an instance as a root volume cannot be modified to an  or  volume. if detached and modified to  or , it cannot be attached to an instance as the root volume.a  volume cannot be modified to an  or  volume if the requested volume size is below the minimum size for  and  volumes.in some cases, you must detach the volume or stop the instance for modification to proceed. if you encounter an error message while attempting to modify an ebs volume, or if you are modifying an ebs volume attached to a previous-generation instance type, take one of the following steps:for a non-root volume, detach the volume from the instance, apply the modifications, and then re-attach the volume.for a root (boot) volume, stop the instance, apply the modifications, and then restart the instance.after provisioning over 32,000 iops on an existing  volume, you may need to do one of the following to see the full performance improvements:detach and attach the volume.restart the instance.decreasing the size of an ebs volume is not supported. however, you can create a smaller volume and then migrate your data to it using an application-level tool such as rsync.modification time is increased if you modify a volume that has not been fully initialized. for more information see .after modifying a volume, wait at least six hours and ensure that the volume is in the  or  state before making additional modifications to the same volume.while  instances fully support volume modification, , , and  instances might not support all volume modification features.
an instance reboot is equivalent to an operating system reboot. in most cases, it takes only a few minutes to reboot your instance. when you reboot an instance, it keeps its public dns name (ipv4), private ipv4 address, ipv6 address (if applicable), and any data on its instance store volumes. rebooting an instance doesn't start a new instance billing period (with a minimum one-minute charge), unlike stopping and starting your instance. we might schedule your instance for a reboot for necessary maintenance, such as to apply updates that require a reboot. no action is required on your part; we recommend that you wait for the reboot to occur within its scheduled window. for more information, see . we recommend that you use the amazon ec2 console, a command line tool, or the amazon ec2 api to reboot your instance instead of running the operating system reboot command from your instance. if you use the amazon ec2 console, a command line tool, or the amazon ec2 api to reboot your instance, we perform a hard reboot if the instance does not cleanly shut down within four minutes. if you use aws cloudtrail, then using amazon ec2 to reboot your instance also creates an api record of when your instance was rebooted. to reboot an instance using the console open the amazon ec2 console. in the navigation pane, choose instances. select the instance and choose actions, instance state, reboot. choose yes, reboot when prompted for confirmation. the instance will remain in a "running" state.  to reboot an instance using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)
you can determine the platform details and billing information associated with an amazon machine image (ami) before you launch an on-demand instance or spot instance, or purchase a reserved instance. for spot instances, you can use the platform details to confirm that the ami is supported for spot instances. when purchasing a reserved instance, you can make sure that, for platform, you select the correct value that maps to platform details on the ami. by knowing the billing information before launching an instance or purchasing a reserved instance, you reduce the chance of erroneously launching instances from incorrect amis and incurring unplanned costs. for more information about instance pricing, see . topics the following fields provide billing information associated with an ami: platform details the platform details associated with the billing code of the ami. for example, . usage operationthe operation of the amazon ec2 instance and the billing code that is associated with the ami. for example, . usage operation corresponds to the  column on your aws cost and usage report (cur) and in the . for the list of usage operation codes, see  in the following section. you can view these fields on the instances or amis page in the amazon ec2 console, or in the response that is returned by the  command. the following table lists the platform details and usage operation values that can be displayed on the instances or amis page in the amazon ec2 console, or in the response that is returned by the  command. * if two software licenses are associated with an ami, the platform details field shows both. ** if you are running spot instances, the  on your aws cost and usage report might be different from the usage operation value that is listed here. for example, if  displays , it means that amazon ec2 is running red hat enterprise linux spot instance-hour in us east (virginia) in vpc zone #6. you can view the platform details and usage operation values associated with an ami from the ami or from the instance. you can view these values in the amazon ec2 console or by using the aws cli. to view the platform details and usage operation associated with an ami (console) open the amazon ec2 console at . in the navigation pane, choose amis, and then select an ami. on the details tab, check the values for platform details and usage operation. to view the platform details and usage operation associated with an ami (aws cli)use the  command. the following example output shows the  and  fields. in this example, the ami-0123456789example platform is  and the usage operation and billing code is . to view the platform details and usage operation associated with an ami (console) open the amazon ec2 console at . in the navigation pane, choose instances, and then select an instance. on the details tab, check the values for platform details and usage operation. to view the platform details and usage operation associated with an ami (console) after you have launched an instance, you can find the billing information by inspecting the  field in the instance metadata. for more information, see . alternatively, you can use the  command to obtain the ami id for the instance, and then use the  command, as described in the preceding procedure, to obtain the billing information from the  and  fields in the response. to ensure that you're not incurring unplanned costs, you can confirm that the billing information for an instance in your aws cost and usage report (cur) matches the billing information associated with the ami that you used to launch the instance. to confirm the billing information, find the instance id in your cur and check the corresponding value in the  column. the value should match the value for usage operation associated with the ami. for example, the ami, , has the following billing information: platform details =  and usage operation = . if you launched an instance using this ami, you can find the instance id in your cur and check the corresponding value in the  column. in this example. the value should be . 
session manager is a fully managed aws systems manager capability that lets you manage your amazon ec2 instances through an interactive one-click browser-based shell or through the aws cli. you can use session manager to start a session with an instance in your account. after the session is started, you can run bash commands as you would through any other connection type. for more information about session manager, see  in the aws systems manager user guide. before attempting to connect to an instance using session manager, ensure that the necessary setup steps have been completed. for more information and instructions, see . to connect to a linux instance using session manager using the amazon ec2 console open the amazon ec2 console at . in the navigation pane, choose instances. select the instance and choose connect. for connection method, choose session manager. choose connect. troubleshootingif you receive an error that you’re not authorized to perform one or more systems manager actions (), then you must update your policies to allow you to start sessions from the amazon ec2 console. for more information, see  in the aws systems manager user guide. 
a key pair, consisting of a private key and a public key, is a set of security credentials that you use to prove your identity when connecting to an instance. amazon ec2 stores the public key, and you store the private key. you use the private key, instead of a password, to securely access your instances. anyone who possesses your private keys can connect to your instances, so it's important that you store your private keys in a secure place. when you launch an instance, you are . if you plan to connect to the instance using ssh, you must specify a key pair. you can choose an existing key pair or create a new one. when your instance boots for the first time, the content of the public key that you specified at launch is placed on your linux instance in an entry within . when you connect to your linux instance using ssh, to log in you must specify the private key that corresponds to the public key content. for more information about connecting to your instance, see . for more information about key pairs and windows instances, see  in the amazon ec2 user guide for windows instances because amazon ec2 doesn't keep a copy of your private key, there is no way to recover a private key if you lose it. however, there can still be a way to connect to instances for which you've lost the private key. for more information, see . the keys that amazon ec2 uses are 2048-bit ssh-2 rsa keys. you can have up to 5,000 key pairs per region. topics you can use amazon ec2 to create a new key pair, or you can import an existing key pair. topics you can create a key pair using one of the following methods.  to create your key pair open the amazon ec2 console at . in the navigation pane, under network & security, choose key pairs. choose create key pair. for name, enter a descriptive name for the key pair. amazon ec2 associates the public key with the name that you specify as the key name. a key name can include up to 255 ascii characters. it can’t include leading or trailing spaces. for file format, choose the format in which to save the private key. to save the private key in a format that can be used with openssh, choose pem. to save the private key in a format that can be used with putty, choose ppk. choose create key pair. the private key file is automatically downloaded by your browser. the base file name is the name you specified as the name of your key pair, and the file name extension is determined by the file format you chose. save the private key file in a safe place. importantthis is the only chance for you to save the private key file. if you will use an ssh client on a macos or linux computer to connect to your linux instance, use the following command to set the permissions of your private key file so that only you can read it. if you do not set these permissions, then you cannot connect to your instance using this key pair. for more information, see . to create your key pair open the amazon ec2 console at . in the navigation pane, under network & security, choose key pairs. choose create key pair. for key pair name, enter a descriptive name for the key pair, and then choose create. a key name can include up to 255 ascii characters. it can’t include leading or trailing spaces. the private key file is automatically downloaded by your browser. the base file name is the name you specified as the name of your key pair, and the file name extension is . save the private key file in a safe place. importantthis is the only chance for you to save the private key file. if you will use an ssh client on a macos or linux computer to connect to your linux instance, use the following command to set the permissions of your private key file so that only you can read it. if you do not set these permissions, then you cannot connect to your instance using this key pair. for more information, see . to create your key pair use the  aws cli command as follows to generate the key and save it to a  file. if you will use an ssh client on a macos or linux computer to connect to your linux instance, use the following command to set the permissions of your private key file so that only you can read it. if you do not set these permissions, then you cannot connect to your instance using this key pair. for more information, see . to create your key pairuse the  aws tools for windows powershell command as follows to generate the key and save it to a  file. instead of using amazon ec2 to create your key pair, you can create an rsa key pair using a third-party tool and then import the public key to amazon ec2. for example, you can use ssh-keygen (a tool provided with the standard openssh installation) to create a key pair. alternatively, java, ruby, python, and many other programming languages provide standard libraries that you can use to create an rsa key pair. requirements the following formats are supported: openssh public key format (the format in ). if you connect using ssh while using the ec2 instance connect api, the ssh2 format is also supported.base64 encoded der formatssh public key file format as specified in ssh private key file format must be pem (for example, use  to convert the openssh key into the pem format)create an rsa key. amazon ec2 does not accept dsa keys.the supported lengths are 1024, 2048, and 4096. if you connect using ssh while using the ec2 instance connect api, the supported lengths are 2048 and 4096.to create a key pair using a third-party tool generate a key pair with a third-party tool of your choice. save the public key to a local file. for example,  (linux) or   (windows). the file name extension for this file is not important. save the private key to a different local file that has the  extension. for example,  (linux) or  (windows). save the private key file in a safe place. you'll need to provide the name of your key pair when you launch an instance and the corresponding private key each time you connect to the instance. after you have created the key pair, use one of the following methods to import your key pair to amazon ec2. to import the public key open the amazon ec2 console at . in the navigation pane, choose key pairs. choose import key pair. for name, enter a descriptive name for the key pair. the name can include up to 255 ascii characters. it can’t include leading or trailing spaces. either choose browse to navigate to and select your public key, or paste the contents of your public key into the public key contents field. choose import key pair. verify that the key pair you imported appears in the list of key pairs. to import the public key open the amazon ec2 console at . in the navigation pane, under network & security, choose key pairs. choose import key pair.  in the import key pair dialog box, choose browse, and select the public key file that you saved previously. enter a name for the key pair in the key pair name field, and choose import. the name can include up to 255 ascii characters. it can’t include leading or trailing spaces. verify that the key pair you imported appears in the list of key pairs. to import the public keyuse the  aws cli command. to verify that the key pair was imported successfullyuse the  aws cli command. to import the public keyuse the  aws tools for windows powershell command. to verify that the key pair was imported successfullyuse the  aws tools for windows powershell command. to help categorize and manage your existing key pairs, you can tag them with custom metadata. for more information about how tags work, see . you can view, add, and delete tags using the new console and the command line tools. to view, add, or delete a tag for an existing key pair open the amazon ec2 console at . in the navigation pane, choose key pairs. select a key pair, and then choose actions, manage tags. the manage tags section displays any tags that are assigned to the key pair. to add a tag, choose add tag, and then enter the tag key and value. you can add up to 50 tags per key pair. for more information, see .to delete a tag, choose remove next to the tag that you want to delete.choose save changes. to view key pair tagsuse the  aws cli command. in the following example, you describe the tags for all of your key pairs. to describe the tags for a specific key pairuse the  aws cli command. to tag an existing key pairuse the  aws cli command. in the following example, the existing key pair is tagged with  and . to delete a tag from a key pairuse the  aws cli command. for examples, see  in the aws cli command reference. to view key pair tagsuse the  command. to describe the tags for a specific key pairuse the  command. to tag an existing key pairuse the  command. to delete a tag from a key pairuse the  command. on your local linux or macos computer, you can use the ssh-keygen command to retrieve the public key for your key pair. specify the path where you downloaded your private key (the  file). the command returns the public key, as shown in the following example. if the command fails, run the following command to ensure that you've changed the permissions on your key pair file so that only you can view it. the public key that you specified when you launched an instance is also available to you through its instance metadata. to view the public key that you specified when launching the instance, use the following command from your instance: the following is an example output. the following is an example output. if you change the key pair that you use to connect to the instance, we don't update the instance metadata to show the new public key. instead, the instance metadata continues to show the public key for the key pair that you specified when you launched the instance. for more information, see . alternatively, on a linux instance, the public key content is placed in an entry within . you can open this file in an editor. the following is an example entry for the key pair named my-key-pair. it consists of the public key followed by the name of the key pair. on the key pairs page in the amazon ec2 console, the fingerprint column displays the fingerprints generated from your key pairs. aws calculates the fingerprint differently depending on whether the key pair was generated by aws or a third-party tool. if you created the key pair using aws, the fingerprint is calculated using an sha-1 hash function. if you created the key pair with a third-party tool and uploaded the public key to aws, or if you generated a new public key from an existing aws-created private key and uploaded it to aws, the fingerprint is calculated using an md5 hash function. you can use the ssh2 fingerprint that's displayed on the key pairs page to verify that the private key you have on your local machine matches the public key stored in aws. from the computer where you downloaded the private key file, generate an ssh2 fingerprint from the private key file. the output should match the fingerprint that's displayed in the console. if you created your key pair using aws, you can use the openssl tools to generate a fingerprint as shown in the following example. if you created a key pair using a third-party tool and uploaded the public key to aws, you can use the openssl tools to generate the fingerprint as shown in the following example. if you created an openssh key pair using openssh 7.8 or later and uploaded the public key to aws, you can use ssh-keygen to generate the fingerprint as shown in the following example. you can change the key pair that is used to access the default system account of your instance. for example, if a user in your organization requires access to the system user account using a separate key pair, you can add that key pair to your instance. or, if someone has a copy of the  file and you want to prevent them from connecting to your instance (for example, if they've left your organization), you can replace the key pair with a new one. to add or replace a key pair, you must be able to connect to your instance. if you've lost your existing private key or you launched your instance without a key pair, you won't be able connect to your instance and therefore won't be able to add or replace a key pair. if you've lost your existing private key, you might be able to retrieve it. for more information, see . if you launched your instance without a key pair, you won't be able to connect to the instance unless you chose an ami that is configured to allow users another way to log in. notethese procedures are for modifying the key pair for the default user account, such as . for more information about adding user accounts to your instance, see . to add or replace a key pair create a new key pair using  or a . retrieve the public key from your new key pair. for more information, see . connect to your instance using your existing private key file. using a text editor of your choice, open the  file on the instance. paste the public key information from your new key pair underneath the existing public key information. save the file. disconnect from your instance, and test that you can connect to your instance using the new private key file. (optional) if you're replacing an existing key pair, connect to your instance and delete the public key information for the original key pair from the  file. noteif you're using an auto scaling group (for example, in an elastic beanstalk environment), ensure that the key pair you're replacing is not specified in your launch configuration. amazon ec2 auto scaling launches a replacement instance if it detects an unhealthy instance; however, the instance launch fails if the key pair cannot be found.  when you delete a key pair, you are only deleting the amazon ec2 copy of the public key. deleting a key pair doesn't affect the private key on your computer or the public key on any instances that already launched using that key pair. you can't launch a new instance using a deleted key pair, but you can continue to connect to any instances that you launched using a deleted key pair, as long as you still have the private key () file. if you're using an auto scaling group (for example, in an elastic beanstalk environment), ensure that the key pair you're deleting is not specified in your launch configuration. amazon ec2 auto scaling launches a replacement instance if it detects an unhealthy instance; however, the instance launch fails if the key pair cannot be found.  you can delete a key pair using one of the following methods. to delete your key pair open the amazon ec2 console at . in the navigation pane, choose key pairs. select the key pair to delete and choose delete. in the confirmation field, enter  and then choose delete. to delete your key pair open the amazon ec2 console at . in the navigation pane, under network & security, choose key pairs. select the key pair and choose delete. when prompted, choose yes. to delete your key pairuse the  aws cli command. to delete your key pairuse the  aws tools for windows powershell command. if you create a linux ami from an instance, and then use the ami to launch a new instance in a different region or account, the new instance includes the public key from the original instance. this enables you to connect to the new instance using the same private key file as your original instance. you can remove this public key from your instance by removing its entry from the  file using a text editor of your choice. for more information about managing users on your instance and providing remote access using a specific key pair, see . 
cloud security at aws is the highest priority. as an aws customer, you benefit from a data center and network architecture that are built to meet the requirements of the most security-sensitive organizations. security is a shared responsibility between aws and you. the  describes this as security of the cloud and security in the cloud: security of the cloud – aws is responsible for protecting the infrastructure that runs aws services in the aws cloud. aws also provides you with services that you can use securely. third-party auditors regularly test and verify the effectiveness of our security as part of the . to learn about the compliance programs that apply to amazon ec2, see .security in the cloud – your responsibility is determined by the aws service that you use. you are also responsible for other factors including the sensitivity of your data, your company’s requirements, and applicable laws and regulations.this documentation helps you understand how to apply the shared responsibility model when using amazon ec2. it shows you how to configure amazon ec2 to meet your security and compliance objectives. you also learn how to use other aws services that help you to monitor and secure your amazon ec2 resources. topics 
storage optimized instances are designed for workloads that require high, sequential read and write access to very large data sets on local storage. they are optimized to deliver tens of thousands of low-latency, random i/o operations per second (iops) to applications. d2 instances these instances are well suited for the following: massive parallel processing (mpp) data warehousemapreduce and hadoop distributed computinglog or data processing applicationsh1 instances these instances are well suited for the following: data-intensive workloads such as mapreduce and distributed file systemsapplications requiring sequential access to large amounts of data on direct-attached instance storageapplications that require high-throughput access to large quantities of datai3 and i3en instances these instances are well suited for the following: high frequency online transaction processing (oltp) systemsrelational databasesnosql databasescache for in-memory databases (for example, redis)data warehousing applicationsdistributed file systemsbare metal instances provide your applications with direct access to physical resources of the host server, such as processors and memory. these instances are well suited for the following: workloads that require access to low-level hardware features (for example, intel vt) that are not available or fully supported in virtualized environmentsapplications that require a non-virtualized environment for licensing or supportfor more information, see . topics the primary data storage for d2 instances is hdd instance store volumes. the primary data storage for i3 and i3en instances is non-volatile memory express (nvme) ssd instance store volumes. instance store volumes persist only for the life of the instance. when you stop or terminate an instance, the applications and data in its instance store volumes are erased. we recommend that you regularly back up or replicate important data in your instance store volumes. for more information, see  and . the following is a summary of the hardware specifications for storage optimized instances. for more information about the hardware specifications for each amazon ec2 instance type, see . for more information about specifying cpu options, see . to ensure the best disk throughput performance from your instance on linux, we recommend that you use the most recent version of amazon linux 2 or the amazon linux ami. for instances with nvme instance store volumes, you must use a linux ami with kernel version 4.4 or later. otherwise, your instance will not achieve the maximum iops performance available. d2 instances provide the best disk performance when you use a linux kernel that supports persistent grants, an extension to the xen block ring protocol that significantly improves disk throughput and scalability. for more information about persistent grants, see  in the xen project blog. ebs-optimized instances enable you to get consistently high performance for your ebs volumes by eliminating contention between amazon ebs i/o and other network traffic from your instance. some storage optimized instances are ebs-optimized by default at no additional cost. for more information, see . some storage optimized instance types provide the ability to control processor c-states and p-states on linux. c-states control the sleep levels that a core can enter when it is inactive, while p-states control the desired performance (in cpu frequency) from a core. for more information, see . you can enable enhanced networking on supported instance types to provide lower latencies, lower network jitter, and higher packet-per-second (pps) performance. most applications do not consistently need a high level of network performance, but can benefit from access to increased bandwidth when they send or receive data. for more information, see . the following is a summary of network performance for storage optimized instances that support enhanced networking. † these instances use a network i/o credit mechanism to allocate network bandwidth to instances based on average bandwidth utilization. they accrue credits when their bandwidth is below their baseline bandwidth, and can use these credits when they perform network data transfers. for more information, open a support case and ask about baseline bandwidth for the specific instance types that you are interested in. if you use a linux ami with kernel version 4.4 or later and use all the ssd-based instance store volumes available to your instance, you get the iops (4,096 byte block size) performance listed in the following table (at queue depth saturation). otherwise, you get lower iops performance. * for these instances, you can get up to the specified performance. as you fill your ssd-based instance store volumes, the i/o performance that you get decreases. this is due to the extra work that the ssd controller must do to find available space, rewrite existing data, and erase unused space so that it can be rewritten. this process of garbage collection results in internal write amplification to the ssd, expressed as the ratio of ssd write operations to user write operations. this decrease in performance is even larger if the write operations are not in multiples of 4,096 bytes or not aligned to a 4,096-byte boundary. if you write a smaller amount of bytes or bytes that are not aligned, the ssd controller must read the surrounding data and store the result in a new location. this pattern results in significantly increased write amplification, increased latency, and dramatically reduced i/o performance. ssd controllers can use several strategies to reduce the impact of write amplification. one such strategy is to reserve space in the ssd instance storage so that the controller can more efficiently manage the space available for write operations. this is called over-provisioning. the ssd-based instance store volumes provided to an instance don't have any space reserved for over-provisioning. to reduce write amplification, we recommend that you leave 10% of the volume unpartitioned so that the ssd controller can use it for over-provisioning. this decreases the storage that you can use, but increases performance even if the disk is close to full capacity. for instance store volumes that support trim, you can use the trim command to notify the ssd controller whenever you no longer need data that you've written. this provides the controller with more free space, which can reduce write amplification and increase performance. for more information, see . the following is a summary of features for storage optimized instances: * the root device volume must be an amazon ebs volume. for more information, see the following: the  instance type provides 36 vcpus, which might cause launch issues in some linux operating systems that have a vcpu limit of 32. we strongly recommend that you use the latest amis when you launch  instances. the following linux amis support launching  instances with 36 vcpus: amazon linux 2 (hvm)amazon linux ami 2018.03 (hvm)ubuntu server 14.04 lts (hvm) or laterred hat enterprise linux 7.1 (hvm)suse linux enterprise server 12 (hvm)if you must use a different ami for your application, and your  instance launch does not complete successfully (for example, if your instance status changes to  during launch with a  state transition reason), modify your instance as described in the following procedure to support more than 32 vcpus so that you can use the  instance type. to update an instance to support more than 32 vcpus launch a d2 instance using your ami, choosing any d2 instance type other than . update the kernel to the latest version by following your operating system-specific instructions. for example, for rhel 6, use the following command: stop the instance. (optional) create an ami from the instance that you can use to launch any additional  instances that you need in the future. change the instance type of your stopped instance to  (choose actions, instance settings, change instance type, and then follow the directions). start the instance. if the instance launches properly, you are done. if the instance still does not boot properly, proceed to the next step. (optional) if the instance still does not boot properly, the kernel on your instance may not support more than 32 vcpus. however, you may be able to boot the instance if you limit the vcpus. change the instance type of your stopped instance to any d2 instance type other than  (choose actions, instance settings, change instance type, and then follow the directions). add the  option to your boot kernel parameters by following your operating system-specific instructions. for example, for rhel 6, edit the  file and add the following option to the most recent and active  entry: stop the instance. (optional) create an ami from the instance that you can use to launch any additional  instances that you need in the future. change the instance type of your stopped instance to  (choose actions, instance settings, change instance type, and then follow the directions). start the instance. you must launch storage optimized instances using an hvm ami. for more information, see .instances built on the  have the following requirements:  must be installed must be installedthe following linux amis meet these requirements: amazon linux 2amazon linux ami 2018.03ubuntu 14.04 (with  kernel) or laterred hat enterprise linux 7.4 or latersuse linux enterprise server 12 sp2 or latercentos 7.4.1708 or laterfreebsd 11.1 or laterdebian gnu/linux 9 or laterlaunching a bare metal instance boots the underlying server, which includes verifying all hardware and firmware components. this means that it can take 20 minutes from the time the instance enters the running state until it becomes available over the network.to attach or detach ebs volumes or secondary network interfaces from a bare metal instance requires pcie native hotplug support. amazon linux 2 and the latest versions of the amazon linux ami support pcie native hotplug, but earlier versions do not. you must enable the following linux kernel configuration options: bare metal instances use a pci-based serial device rather than an i/o port-based serial device. the upstream linux kernel and the latest amazon linux amis support this device. bare metal instances also provide an acpi spcr table to enable the system to automatically use the pci-based serial device. the latest windows amis automatically use the pci-based serial device.with freebsd amis, bare metal instances take nearly an hour to boot and i/o to the local nvme storage does not complete. as a workaround, add the following line to  and reboot: the  instance type has 36 vcpus, which might cause launch issues in some linux operating systems that have a vcpu limit of 32. for more information, see .there is a limit on the total number of instances that you can launch in a region, and there are additional limits on some instance types. for more information, see  in the amazon ec2 faq.
your security credentials identify you to services in aws and grant you unlimited use of your aws resources, such as your amazon ec2 resources. you can use features of amazon ec2 and aws identity and access management (iam) to allow other users, services, and applications to use your amazon ec2 resources without sharing your security credentials. you can use iam to control how other users use resources in your aws account, and you can use security groups to control access to your amazon ec2 instances. you can choose to allow full use or limited use of your amazon ec2 resources. topics a security group acts as a firewall that controls the traffic allowed to reach one or more instances. when you launch an instance, you assign it one or more security groups. you add rules to each security group that control traffic for the instance. you can modify the rules for a security group at any time; the new rules are automatically applied to all instances to which the security group is assigned.  for more information, see . your organization might have multiple aws accounts. amazon ec2 enables you to specify additional aws accounts that can use your amazon machine images (amis) and amazon ebs snapshots. these permissions work at the aws account level only; you can't restrict permissions for specific users within the specified aws account. all users in the aws account that you've specified can use the ami or snapshot. each ami has a  attribute that controls which aws accounts can access the ami. for more information, see . each amazon ebs snapshot has a  attribute that controls which aws accounts can use the snapshot. for more information, see . iam enables you to do the following: create users and groups under your aws accountassign unique security credentials to each user under your aws accountcontrol each user's permissions to perform tasks using aws resourcesallow the users in another aws account to share your aws resourcescreate roles for your aws account and define the users or services that can assume themuse existing identities for your enterprise to grant permissions to perform tasks using aws resourcesby using iam with amazon ec2, you can control whether users in your organization can perform a task using specific amazon ec2 api actions and whether they can use specific aws resources. this topic helps you answer the following questions: how do i create groups and users in iam?how do i create a policy?what iam policies do i need to carry out tasks in amazon ec2?how do i grant permissions to perform actions in amazon ec2?how do i grant permissions to perform actions on specific resources in amazon ec2?to create an iam group open the iam console at . in the navigation pane, choose groups and then choose create new group.  for group name, enter a name for your group, and then choose next step. on the attach policy page, select an aws managed policy and then choose next step. for example, for amazon ec2, one of the following aws managed policies might meet your needs: poweruseraccessreadonlyaccessamazonec2fullaccessamazonec2readonlyaccesschoose create group. your new group is listed under group name. to create an iam user, add the user to your group, and create a password for the user in the navigation pane, choose users, add user. for user name, enter a user name. for access type, select both programmatic access and aws management console access. for console password, choose one of the following: autogenerated password. each user gets a randomly generated password that meets the current password policy in effect (if any). you can view or download the passwords when you get to the final page.custom password. each user is assigned the password that you enter in the box.choose next: permissions. on the set permissions page, choose add user to group. select the check box next to the group that you created earlier and choose next: review. choose create user. to view the users' access keys (access key ids and secret access keys), choose show next to each password and secret access key to see. to save the access keys, choose download .csv and then save the file to a safe location. importantyou cannot retrieve the secret access key after you complete this step; if you misplace it you must create a new one. choose close. give each user his or her credentials (access keys and password); this enables them to use services based on the permissions you specified for the iam group.  for more information about iam, see the following: 
you can access instance metadata from a running instance using one of the following methods: instance metadata service version 1 (imdsv1) – a request/response methodinstance metadata service version 2 (imdsv2) – a session-oriented methodby default, you can use either imdsv1 or imdsv2, or both. the instance metadata service distinguishes between imdsv1 and imdsv2 requests based on whether, for any given request, either the  or  headers, which are unique to imdsv2, are present in that request. you can configure the instance metadata service on each instance such that local code or users must use imdsv2. when you specify that imdsv2 must be used, imdsv1 no longer works. for more information, see . imdsv2 uses session-oriented requests. with session-oriented requests, you create a session token that defines the session duration, which can be a minimum of one second and a maximum of six hours. during the specified duration, you can use the same session token for subsequent requests. after the specified duration expires, you must create a new session token to use for future requests. the following example uses a linux shell script and imdsv2 to retrieve the top-level instance metadata items. the example command:  creates a session token lasting six hours (21,600 seconds) using the  requeststores the session token header in a variable named requests the top-level metadata items using the tokenafter you've created a token, you can reuse it until it expires. in the following example command, which gets the id of the ami used to launch the instance, the token that is stored in  in the previous example is reused. when you use imdsv2 to request instance metadata, the request must include the following: use a  request to initiate a session to the instance metadata service. the  request returns a token that must be included in subsequent  requests to the instance metadata service. the token is required to access metadata using imdsv2. include the token in all  requests to the instance metadata service. when token usage is set to , requests without a valid token or with an expired token receive a  http error code. for information about changing the token usage requirement, see  in the aws cli command reference. the token is an instance-specific key. the token is not valid on other ec2 instances and will be rejected if you attempt to use it outside of the instance on which it was generated.the  request must include a header that specifies the time to live (ttl) for the token, in seconds, up to a maximum of six hours (21,600 seconds). the token represents a logical session. the ttl specifies the length of time that the token is valid and, therefore, the duration of the session.after a token expires, to continue accessing instance metadata, you must create a new session using another .you can choose to reuse a token or create a new token with every request. for a small number of requests, it might be easier to generate and immediately use a token each time you need to access the instance metadata service. but for efficiency, you can specify a longer duration for the token and reuse it rather than having to write a  request every time you need to request instance metadata. there is no practical limit on the number of concurrent tokens, each representing its own session. imdsv2 is, however, still constrained by normal instance metadata service connection and throttling limits. for more information, see .http  and  methods are allowed in imdsv2 instance metadata requests.  requests are rejected if they contain an x-forwarded-for header. by default, the response to  requests has a response hop limit (time to live) of  at the ip protocol level. you can adjust the hop limit using the  command if you need to make it larger. for example, you might need a larger hop limit for backward compatibility with container services running on the instance. for more information, see  in the aws cli command reference. use of instance metadata service version 2 (imdsv2) is optional. instance metadata service version 1 (imdsv1) will continue to be supported indefinitely. if you choose to migrate to using imdsv2, we recommend that you use the following tools and transition path.  tools for helping with the transition to imdsv2 if your software uses imdsv1, use the following tools to help reconfigure your software to use imdsv2. aws software: the latest versions of the aws sdks and clis support imdsv2. to use imdsv2, make sure that your ec2 instances have the latest versions of the aws sdks and clis. for information about updating the cli, see  in the aws command line interface user guide.cloudwatch: imdsv2 uses token-backed sessions, while imdsv1 does not. the  cloudwatch metric tracks the number of calls to the instance metadata service that are using imdsv1. by tracking this metric to zero, you can determine if and when all of your software has been upgraded to use imdsv2. for more information, see .updates to ec2 apis and clis: for existing instances, you can use the  cli command (or the  api) to require the use of imdsv2. for new instances, you can use the  cli command (or the  api) and the  parameter to launch new instances that require the use of imdsv2. to require the use of imdsv2 on all new instances launched by auto scaling groups, your auto scaling groups must use launch templates. when you , configure the  parameters to require the use imdsv2. for auto scaling groups that use launch configurations, . after you replace a launch configuration with a launch template, the auto scaling group launches new instances using the new launch template, but existing instances are not affected. use the  cli command (or the  api) to require the use of imdsv2 on the existing instances, or terminate the instances and the auto scaling group will launch new replacement instances with the instance metadata options settings that are defined in the launch template. iam policies and scps: you can use an iam condition to enforce that iam users can't launch an instance unless it uses imdsv2. you can also use iam conditions to enforce that iam users can't modify running instances to re-enable imdsv1, and to enforce that the instance metadata service is available on the instance. the , , and  iam condition keys can be used to control the use of the  and the  api and corresponding cli. if a policy is created, and a parameter in the api call does not match the state specified in the policy using the condition key, the api or cli call fails with an  response. these condition keys can be used either in iam policies or aws organizations service control policies (scps). furthermore, you can choose an additional layer of protection to enforce the change from imdsv1 to imdsv2. at the access management layer with respect to the apis called via ec2 role credentials, you can use a new condition key in either iam policies or aws organizations service control policies (scps). specifically, by using the policy condition key  with a value of  in your iam policies, api calls made with ec2 role credentials obtained from imdsv1 will receive an  response. the same thing can be achieved more broadly with that condition required by an scp. this ensures that credentials delivered via imdsv1 cannot actually be used to call apis because any api calls not matching the specified condition will receive an  error. for example iam policies, see . for more information, see  in the aws organizations user guide. recommended path to requiring imdsv2 access using the above tools, we recommend that you follow this path for transitioning to imdsv2: update the sdks, clis, and your software that use role credentials on their ec2 instances to imdsv2-compatible versions. for information about updating the cli, see  in the aws command line interface user guide. then, change your software that directly accesses instance metadata (in other words, that does not use an sdk) using the imdsv2 requests.  track your transition progress by using the cloudwatch metric . this metric shows the number of calls to the instance metadata service that are using imdsv1 on your instances. for more information, see .  everything is ready on all instances when the cloudwatch metric  records zero imdsv1 usage. at this stage, you can do the following: for existing instances: you can require imdsv2 use through the  command. you can make these changes on running instances; you do not need to restart your instances. for new instances: when launching a new instance, you can use the  command to specify that only imdsv2 is to be used.specifying instance metadata options is available only through the api or aws cli; it is currently not available via the aws management console. for more information, see . the , , and  iam condition keys can be used to control the use of the  and the  api and corresponding cli. if a policy is created, and a parameter in the api call does not match the state specified in the policy using the condition key, the api or cli call fails with an  response. for example iam policies, see . instance metadata options allow you to configure new or existing instances to do the following: require the use of imdsv2 when requesting instance metadataspecify the  response hop limitturn off access to instance metadatayou can also use iam condition keys in an iam policy or scp to do the following: allow an instance to launch only if it's configured to require the use of imdsv2restrict the number of allowed hopsturn off access to instance metadatato configure the instance metadata options on new or existing instances, you use the aws sdk or cli. for more information, see  and  in the aws cli command reference. noteyou should proceed cautiously and conduct careful testing before making any changes. take note of the following:if you enforce the use of imdsv2, applications or agents that use imdsv1 for instance metadata access will break. if you turn off all access to instance metadata, applications or agents that rely on instance metadata access to function will break. topics you can require the use imdsv2 on an instance when you launch it. you can also create an iam policy that prevents users from launching new instances unless they require imdsv2 on the new instance. to require the use of imdsv2 on a new instancethe following  example launches a  instance with  set to . when you specify a value for , you must also set  to . because the secure token header is set to  for metadata retrieval requests, this opts in the instance to require using imdsv2 when requesting instance metadata. to enforce the use of imdsv2 on all new instancesto ensure that iam users can only launch instances that require the use of imdsv2 when requesting instance metadata, you can specify that the condition to require imdsv2 must be met before an instance can be launched. for the example iam policy, see . to turn off access to instance metadatato ensure that access to your instance metadata is turned off, regardless of which version of the instance metadata service you are using, launch the instance with  set to . you can turn on access later on using the  command. you can require the use imdsv2 on an existing instance. you can also change the put response hop limit and turn off access to instance metadata on an existing instance. you can also create an iam policy that prevents users from modifying the instance metadata options on an existing instance. to require the use of imdsv2 on an existing instancefor existing instances, you can opt in to require that imdsv2 is used when requesting instance metadata. use the  cli command and set the  parameter to . notewhen specifying a value for , you must also set  to . to change the put response hop limit on an existing instancefor existing instances, you can change the settings of the  response hop limit. use the  cli command and set the  parameter to the required number of hops. in the following example, the hop limit is set to . note that when specifying a value for , you must also set  to . to turn off access to instance metadata on an existing instancefor existing instances, you can turn off access to your instance metadata by disabling the http endpoint of the instance metadata service, regardless of which version of the instance metadata service you are using. you can reverse this change at any time by enabling the http endpoint. use the  cli command and set the  parameter to . to control the use of modify-instance-metadata-optionsto control which iam users can modify the instance metadata options on an existing instance, you can specify a policy that prevents all users other than users with a specified role to use the  api. for the example iam policy, see . 
the following procedures help you install an apache web server with php and  (a community-developed fork of mysql) support on your amazon linux 2 instance (sometimes called a lamp web server or lamp stack). you can use this server to host a static website or deploy a dynamic php application that reads and writes information to a database. importantto set up a lamp web server on amazon linux ami, see .if you are trying to set up a lamp web server on an ubuntu or red hat enterprise linux instance, this tutorial will not work for you. for more information about other distributions, see their specific documentation. for information about lamp web servers on ubuntu, see the ubuntu community documentation  topic.  option: complete this tutorial using automationto complete this tutorial using aws systems manager automation instead of the following tasks, run the  automation document. topics prerequisitesthis tutorial assumes that you have already launched a new instance using amazon linux 2, with a public dns name that is reachable from the internet. for more information, see . you must also have configured your security group to allow ssh (port 22), http (port 80), and https (port 443) connections. for more information about these prerequisites, see . notethe following procedure installs the latest php version available on amazon linux 2, currently php 7.2. if you plan to use php applications other than those described in this tutorial, you should check their compatibility with php 7.2. to prepare the lamp server . to ensure that all of your software packages are up to date, perform a quick software update on your instance. this process may take a few minutes, but it is important to make sure that you have the latest security updates and bug fixes. the  option installs the updates without asking for confirmation. if you would like to examine the updates before installing, you can omit this option. install the  and  amazon linux extras repositories to get the latest versions of the lamp mariadb and php packages for amazon linux 2. noteif you receive an error stating , then your instance was not launched with an amazon linux 2 ami (perhaps you are using the amazon linux ami instead). you can view your version of amazon linux with the following command.   to set up a lamp web server on amazon linux ami , see . now that your instance is current, you can install the apache web server, mariadb, and php software packages.  use the yum install command to install multiple software packages and all related dependencies at the same time. noteyou can view the current versions of these packages with the following command:   start the apache web server. use the systemctl command to configure the apache web server to start at each system boot.  you can verify that httpd is on by running the following command: add a security rule to allow inbound http (port 80) connections to your instance if you have not already done so. by default, a launch-wizard-n security group was set up for your instance during initialization. this group contains a single rule to allow ssh connections.  open the amazon ec2 console at . choose instances and select your instance. under security groups, choose view inbound rules. you should see the following list of rules in your default security group: using the procedures in , add a new inbound security rule with the following values: type: httpprotocol: tcpport range: 80source: customtest your web server. in a web browser, type the public dns address (or the public ip address) of your instance. if there is no content in , you should see the apache test page. you can get the public dns for your instance using the amazon ec2 console (check the public dns column; if this column is hidden, choose show/hide columns (the gear-shaped icon) and choose public dns). if you are unable to see the apache test page, check that the security group you are using contains a rule to allow http (port 80) traffic. for information about adding an http rule to your security group, see . importantif you are not using amazon linux, you may also need to configure the firewall on your instance to allow these connections. for more information about how to configure the firewall, see the documentation for your specific distribution. apache httpd serves files that are kept in a directory called the apache document root. the amazon linux apache document root is , which by default is owned by root. to allow the  account to manipulate files in this directory, you must modify the ownership and permissions of the directory. there are many ways to accomplish this task. in this tutorial, you add  to the  group, to give the  group ownership of the  directory and assign write permissions to the group. to set file permissions add your user (in this case, ) to the  group. log out and then log back in again to pick up the new group, and then verify your membership. log out (use the exit command or close the terminal window): to verify your membership in the  group, reconnect to your instance, and then run the following command: change the group ownership of  and its contents to the  group. to add group write permissions and to set the group id on future subdirectories, change the directory permissions of  and its subdirectories. to add group write permissions, recursively change the file permissions of  and its subdirectories: now,  (and any future members of the  group) can add, delete, and edit files in the apache document root, enabling you to add content, such as a static website or a php application. to secure your web server (optional)a web server running the http protocol provides no transport security for the data that it sends or receives. when you connect to an http server using a web browser, the urls that you visit, the content of webpages that you receive, and the contents (including passwords) of any html forms that you submit are all visible to eavesdroppers anywhere along the network pathway. the best practice for securing your web server is to install support for https (http secure), which protects your data with ssl/tls encryption. for information about enabling https on your server, see . if your server is installed and running, and your file permissions are set correctly, your  account should be able to create a php file in the  directory that is available from the internet. to test your lamp server create a php file in the apache document root. if you get a "permission denied" error when trying to run this command, try logging out and logging back in again to pick up the proper group permissions that you configured in . in a web browser, type the url of the file that you just created. this url is the public dns address of your instance followed by a forward slash and the file name. for example: you should see the php information page: noteif you do not see this page, verify that the  file was created properly in the previous step. you can also verify that all of the required packages were installed with the following command.   if any of the required packages are not listed in your output, install them with the sudo yum install package command. also verify that the  and  extras are enabled in the output of the amazon-linux-extras command. delete the  file. although this can be useful information, it should not be broadcast to the internet for security reasons. you should now have a fully functional lamp web server. if you add content to the apache document root at , you should be able to view that content at the public dns address for your instance.  the default installation of the mariadb server has several features that are great for testing and development, but they should be disabled or removed for production servers. the mysql_secure_installation command walks you through the process of setting a root password and removing the insecure features from your installation. even if you are not planning on using the mariadb server, we recommend performing this procedure. to secure the mariadb server start the mariadb server. run mysql_secure_installation. when prompted, type a password for the root account. type the current root password. by default, the root account does not have a password set. press enter. type y to set a password, and type a secure password twice. for more information about creating a secure password, see . make sure to store this password in a safe place. notesetting a root password for mariadb is only the most basic measure for securing your database. when you build or install a database-driven application, you typically create a database service user for that application and avoid using the root account for anything but database administration.  type y to remove the anonymous user accounts. type y to disable the remote root login. type y to remove the test database. type y to reload the privilege tables and save your changes. (optional) if you do not plan to use the mariadb server right away, stop it. you can restart it when you need it again. (optional) if you want the mariadb server to start at every boot, type the following command.  is a web-based database management tool that you can use to view and edit the mysql databases on your ec2 instance. follow the steps below to install and configure  on your amazon linux instance. importantwe do not recommend using  to access a lamp server unless you have enabled ssl/tls in apache; otherwise, your database administrator password and other data are transmitted insecurely across the internet. for security recommendations from the developers, see . for general information about securing a web server on an ec2 instance, see . to install phpmyadmin install the required dependencies. restart apache. restart . navigate to the apache document root at . select a source package for the latest phpmyadmin release from . to download the file directly to your instance, copy the link and paste it into a wget command, as in this example: create a  folder and extract the package into it with the following command. delete the phpmyadmin-latest-all-languages.tar.gz tarball. (optional) if the mysql server is not running, start it now. in a web browser, type the url of your phpmyadmin installation. this url is the public dns address (or the public ip address) of your instance followed by a forward slash and the name of your installation directory. for example: you should see the phpmyadmin login page: log in to your phpmyadmin installation with the  user name and the mysql root password you created earlier.  your installation must still be configured before you put it into service. to configure phpmyadmin, you can , , or combine both approaches.   for information about using phpmyadmin, see the . this section offers suggestions for resolving common problems you may encounter while setting up a new lamp server.  perform the following checks to see if your apache web server is running and accessible. is the web server running? you can verify that httpd is on by running the following command: if the httpd process is not running, repeat the steps described in . is the firewall correctly configured? if you are unable to see the apache test page, check that the security group you are using contains a rule to allow http (port 80) traffic. for information about adding an http rule to your security group, see . for more information about transferring files to your instance or installing a wordpress blog on your web server, see the following documentation: for more information about the commands and software used in this tutorial, see the following webpages: apache web server: mariadb database server: php programming language: the  command: the  command: for more information about registering a domain name for your web server, or transferring an existing domain name to this host, see  in the amazon route 53 developer guide. 
you can monitor your instances using amazon cloudwatch, which collects and processes raw data from amazon ec2 into readable, near real-time metrics. these statistics are recorded for a period of 15 months, so that you can access historical information and gain a better perspective on how your web application or service is performing. by default, amazon ec2 sends metric data to cloudwatch in 5-minute periods. to send metric data for your instance to cloudwatch in 1-minute periods, you can enable detailed monitoring on the instance. for more information, see . the amazon ec2 console displays a series of graphs based on the raw data from amazon cloudwatch. depending on your needs, you might prefer to get data for your instances from amazon cloudwatch instead of the graphs in the console. for more information about amazon cloudwatch, see the . topics 
topics get the id of the instance. you can get the id of your instance using the amazon ec2 console (from the instance id column). if you prefer, you can use the  (aws cli) or  (aws tools for windows powershell) command. get the public dns name of the instance. you can get the public dns for your instance using the amazon ec2 console. check the public dns (ipv4) column. if this column is hidden, choose the show/hide icon and select public dns (ipv4). if you prefer, you can use the  (aws cli) or  (aws tools for windows powershell) command. (ipv6 only) get the ipv6 address of the instance. if you've assigned an ipv6 address to your instance, you can optionally connect to the instance using its ipv6 address instead of a public ipv4 address or public ipv4 dns hostname. your local computer must have an ipv6 address and must be configured to use ipv6. you can get the ipv6 address of your instance using the amazon ec2 console. check the ipv6 ips field. if you prefer, you can use the  (aws cli) or  (aws tools for windows powershell) command. for more information about ipv6, see . get the user name for your instance. you can connect to your instance using the user name for your user account or the default user name for the ami that you used to launch your instance. get the user name for your user account. for more information about how to create a user account, see . get the default user name for the ami that you used to launch your instance:for amazon linux 2 or the amazon linux ami, the user name is .for a centos ami, the user name is .for a debian ami, the user name is .for a fedora ami, the user name is  or .for a rhel ami, the user name is  or .for a suse ami, the user name is  or .for an ubuntu ami, the user name is .otherwise, if  and  don't work, check with the ami provider.enable inbound ssh traffic from your ip address to your instance. ensure that the security group associated with your instance allows incoming ssh traffic from your ip address. the default security group for the vpc does not allow incoming ssh traffic by default. the security group created by the launch instance wizard enables ssh traffic by default. for more information, see . locate the private key get the fully-qualified path to the location on your computer of the  file for the key pair that you specified when you launched the instance. for more information about how you created your key pair, see . set the permissions of your private key if you will use an ssh client on a macos or linux computer to connect to your linux instance, use the following command to set the permissions of your private key file so that only you can read it. if you do not set these permissions, then you cannot connect to your instance using this key pair. for more information, see . to protect yourself from man-in-the-middle attacks, you can verify the rsa key fingerprint when you connect to your instance. verifying the fingerprint is useful if you've launched your instance from a public ami from a third party. first you get the instance fingerprint. then, when you connect to the instance, you are prompted to verify the fingerprint. you can compare the fingerprint you obtained with the fingerprint displayed for verification. if these fingerprints don't match, someone might be attempting a "man-in-the-middle" attack. if they match, you can confidently connect to your instance. prerequisites for getting the instance fingerprint: to get the instance fingerprint, you must use the aws cli. for information about installing the aws cli, see  in the aws command line interface user guide.the instance must not be in the  state. the fingerprint is available only after the first boot of the instance is complete.to get the instance fingerprint on your local computer (not on the instance), use the  (aws cli) command as follows to obtain the fingerprint: here is an example of what you should look for in the output. the exact output can vary by the operating system, ami version, and whether you had aws create the key. 
there are several gpu setting optimizations that you can perform to achieve the best performance on g3, g4, p2, p3, and p3dn instances. by default, the nvidia driver uses an autoboost feature, which varies the gpu clock speeds. by disabling the autoboost feature and setting the gpu clock speeds to their maximum frequency, you can consistently achieve the maximum performance with your gpu instances. the following procedure helps you to configure the gpu settings to be persistent, disable the autoboost feature, and set the gpu clock speeds to their maximum frequency. to optimize gpu settings configure the gpu settings to be persistent. this command can take several minutes to run. disable the autoboost feature for all gpus on the instance. notegpus on p3, p3dn, and g4 instances do not support autoboost. set all gpu clock speeds to their maximum frequency. use the memory and graphics clock speeds specified in the following commands. notesome versions of the nvidia driver do not allow setting application clock speed and throw a  error, which you can ignore. g3 instances: g4 instances: p2 instances: p3 and p3dn instances: 
complete the tasks in this section to get set up for launching an amazon ec2 instance for the first time:     when you are finished, you will be ready for the  tutorial. when you sign up for amazon web services (aws), your aws account is automatically signed up for all services in aws, including amazon ec2. you are charged only for the services that you use. with amazon ec2, you pay only for what you use. if you are a new aws customer, you can get started with amazon ec2 for free. for more information, see . if you have an aws account already, skip to the next task. if you don't have an aws account, use the following procedure to create one. to create an aws account open . follow the online instructions. part of the sign-up procedure involves receiving a phone call and entering a verification code on the phone keypad. aws uses public-key cryptography to secure the login information for your instance. a linux instance has no password; you use a key pair to log in to your instance securely. you specify the name of the key pair when you launch your instance, then provide the private key when you log in using ssh.  if you haven't created a key pair already, you can create one using the amazon ec2 console. note that if you plan to launch instances in multiple regions, you'll need to create a key pair in each region. for more information about regions, see . you can create a key pair using one of the following methods.  to create your key pair open the amazon ec2 console at . in the navigation pane, choose key pairs. choose create key pair. for name, enter a descriptive name for the key pair. amazon ec2 associates the public key with the name that you specify as the key name. a key name can include up to 255 ascii characters. it can’t include leading or trailing spaces. for file format, choose the format in which to save the private key. to save the private key in a format that can be used with openssh, choose pem. to save the private key in a format that can be used with putty, choose ppk. choose create key pair. the private key file is automatically downloaded by your browser. the base file name is the name you specified as the name of your key pair, and the file name extension is determined by the file format you chose. save the private key file in a safe place. importantthis is the only chance for you to save the private key file. if you will use an ssh client on a macos or linux computer to connect to your linux instance, use the following command to set the permissions of your private key file so that only you can read it. if you do not set these permissions, then you cannot connect to your instance using this key pair. for more information, see . to create your key pair open the amazon ec2 console at . in the navigation pane, under network & security, choose key pairs. notethe navigation pane is on the left side of the amazon ec2 console. if you do not see the pane, it might be minimized; choose the arrow to expand the pane.  choose create key pair. for key pair name, enter a name for the new key pair, and then choose create. the name can include up to 255 ascii characters. it can’t include leading or trailing spaces. the private key file is automatically downloaded by your browser. the base file name is the name you specified as the name of your key pair, and the file name extension is . save the private key file in a safe place. importantthis is the only chance for you to save the private key file. if you will use an ssh client on a macos or linux computer to connect to your linux instance, use the following command to set the permissions of your private key file so that only you can read it. if you do not set these permissions, then you cannot connect to your instance using this key pair. for more information, see . for more information, see . security groups act as a firewall for associated instances, controlling both inbound and outbound traffic at the instance level. you must add rules to a security group that enable you to connect to your instance from your ip address using ssh. you can also add rules that allow inbound and outbound http and https access from anywhere. note that if you plan to launch instances in multiple regions, you'll need to create a security group in each region. for more information about regions, see . prerequisitesyou'll need the public ipv4 address of your local computer. the security group editor in the amazon ec2 console can automatically detect the public ipv4 address for you. alternatively, you can use the search phrase "what is my ip address" in an internet browser, or use the following service: . if you are connecting through an internet service provider (isp) or from behind a firewall without a static ip address, you need to find out the range of ip addresses used by client computers. you can create a custom security group using one of the following methods. to create a security group with least privilege open the amazon ec2 console at . from the navigation bar, select a region for the security group. security groups are specific to a region, so you should select the same region in which you created your key pair. in the navigation pane, choose security groups. choose create security group. in the basic details section, do the following: enter a name for the new security group and a description. use a name that is easy for you to remember, such as your user name, followed by _sg_, plus the region name. for example, me_sg_uswest2. in the vpc list, select your default vpc for the region. in the inbound rules section, create the following rules (choose add rule for each new rule): choose http from the type list, and make sure that source is set to anywhere ().choose https from the type list, and make sure that source is set to anywhere ().choose ssh from the type list. in the source box, choose my ip to automatically populate the field with the public ipv4 address of your local computer. alternatively, choose custom and specify the public ipv4 address of your computer or network in cidr notation. to specify an individual ip address in cidr notation, add the routing suffix , for example, . if your company allocates addresses from a range, specify the entire range, such as . warningfor security reasons, we don't recommend that you allow ssh access from all ipv4 addresses () to your instance, except for testing purposes and only for a short time.choose create security group. to create a security group with least privilege open the amazon ec2 console at . in the navigation pane, choose security groups. choose create security group. enter a name for the new security group and a description. use a name that is easy for you to remember, such as your user name, followed by _sg_, plus the region name. for example, me_sg_uswest2. in the vpc list, select your default vpc for the region. on the inbound tab, create the following rules (choose add rule for each new rule): choose http from the type list, and make sure that source is set to anywhere ().choose https from the type list, and make sure that source is set to anywhere ().choose ssh from the type list. in the source box, choose my ip to automatically populate the field with the public ipv4 address of your local computer. alternatively, choose custom and specify the public ipv4 address of your computer or network in cidr notation. to specify an individual ip address in cidr notation, add the routing suffix , for example, . if your company allocates addresses from a range, specify the entire range, such as . warningfor security reasons, we don't recommend that you allow ssh access from all ipv4 addresses () to your instance, except for testing purposes and only for a short time.choose create. to create a security group with least privilege use one of the following commands:  (aws cli) (aws tools for windows powershell)for more information, see . 
a paid ami is an ami that you can purchase from a developer. amazon ec2 integrates with aws marketplace, enabling developers to charge other amazon ec2 users for the use of their amis or to provide support for instances.  the aws marketplace is an online store where you can buy software that runs on aws, including amis that you can use to launch your ec2 instance. the aws marketplace amis are organized into categories, such as developer tools, to enable you to find products to suit your requirements. for more information about aws marketplace, see the  site. launching an instance from a paid ami is the same as launching an instance from any other ami. no additional parameters are required. the instance is charged according to the rates set by the owner of the ami, as well as the standard usage fees for the related web services, for example, the hourly rate for running an m1.small instance type in amazon ec2. additional taxes might also apply. the owner of the paid ami can confirm whether a specific instance was launched using that paid ami.  importantamazon devpay is no longer accepting new sellers or products. aws marketplace is now the single, unified e-commerce platform for selling software and services through aws. for information about how to deploy and sell software from aws marketplace, see . aws marketplace supports amis backed by amazon ebs. topics you can sell your ami using aws marketplace. aws marketplace offers an organized shopping experience. additionally, aws marketplace also supports aws features such as amazon ebs-backed amis, reserved instances, and spot instances. for information about how to sell your ami on aws marketplace, see .  there are several ways that you can find amis that are available for you to purchase. for example, you can use , the amazon ec2 console, or the command line. alternatively, a developer might let you know about a paid ami themselves. to find a paid ami using the console open the amazon ec2 console at . in the navigation pane, choose amis.  choose public images for the first filter. in the search bar, choose owner, then aws marketplace. if you know the product code, choose product code, then type the product code. to find a paid ami using aws marketplace open . enter the name of the operating system in the search box, and click go. to scope the results further, use one of the categories or filters. each product is labeled with its product type: either  or . you can find a paid ami using the following  command (aws cli). this command returns numerous details that describe each ami, including the product code for a paid ami. the output from  includes an entry for the product code like the following: if you know the product code, you can filter the results by product code. this example returns the most recent ami with the specified product code. you must sign up for (purchase) a paid ami before you can launch an instance using the ami. typically a seller of a paid ami presents you with information about the ami, including its price and a link where you can buy it. when you click the link, you're first asked to log into aws, and then you can purchase the ami. you can purchase a paid ami by using the amazon ec2 launch wizard. for more information, see . to use the aws marketplace, you must have an aws account. to launch instances from aws marketplace products, you must be signed up to use the amazon ec2 service, and you must be subscribed to the product from which to launch the instance. there are two ways to subscribe to products in the aws marketplace: aws marketplace website: you can launch preconfigured software quickly with the 1-click deployment feature.amazon ec2 launch wizard: you can search for an ami and launch an instance directly from the wizard. for more information, see .you can retrieve the aws marketplace product code for your instance using its instance metadata. for more information about retrieving metadata, see . to retrieve a product code, use the following command: if the instance has a product code, amazon ec2 returns it. amazon ec2 also enables developers to offer support for software (or derived amis). developers can create support products that you can sign up to use. during sign-up for the support product, the developer gives you a product code, which you must then associate with your own ami. this enables the developer to confirm that your instance is eligible for support. it also ensures that when you run instances of the product, you are charged according to the terms for the product specified by the developer.  importantyou can't use a support product with reserved instances. you always pay the price that's specified by the seller of the support product. to associate a product code with your ami, use one of the following commands, where ami_id is the id of the ami and product_code is the product code:  (aws cli)  (aws tools for windows powershell) after you set the product code attribute, it cannot be changed or removed. at the end of each month, you receive an email with the amount your credit card has been charged for using any paid or supported amis during the month. this bill is separate from your regular amazon ec2 bill. for more information, see .  on the aws marketplace website, you can check your subscription details, view the vendor's usage instructions, manage your subscriptions, and more. to check your subscription details log in to the . choose your marketplace account. choose manage your software subscriptions.  all your current subscriptions are listed. choose  usage instructions to view specific instructions for using the product, for example, a user name for connecting to your running instance.  to cancel an aws marketplace subscription ensure that you have terminated any instances running from the subscription. open the amazon ec2 console at . in the navigation pane, choose instances. select the instance, and choose actions, instance state, terminate. choose yes, terminate when prompted for confirmation. log in to the , and choose your marketplace account, then manage your software subscriptions. choose cancel subscription. you are prompted to confirm your cancellation.  noteafter you've canceled your subscription, you are no longer able to launch any instances from that ami. to use that ami again, you need to resubscribe to it, either on the aws marketplace website, or through the launch wizard in the amazon ec2 console. 
this tutorial helps you to launch an efa and mpi-enabled instance cluster for hpc workloads. in this tutorial, you will perform the following steps: topics an efa requires a security group that allows all inbound and outbound traffic to and from the security group itself. to create an efa-enabled security group open the amazon ec2 console at . in the navigation pane, choose security groups and then choose create security group. in the create security group window, do the following: for security group name, enter a descriptive name for the security group, such as . (optional) for description, enter a brief description of the security group. for vpc, select the vpc into which you intend to launch your efa-enabled instances. choose create. select the security group that you created, and on the description tab, copy the group id. on the inbound and outbound tabs, do the following: choose edit. for type, choose all traffic. for source, choose custom. paste the security group id that you copied into the field. choose save. launch a temporary instance that you can use to install and configure the efa software components. you use this instance to create an efa-enabled ami from which you can launch your efa-enabled instances. to launch a temporary instance open the amazon ec2 console at . choose launch instance. on the choose an ami page, choose select for one of the following supported amis: amazon linux, amazon linux 2, rhel 7.6, rhel 7.7, rhel 7.8, centos 7, ubuntu 16.04, and ubuntu 18.04. on the choose an instance type page, select one of the following supported instance types and then choose next: configure instance details: , , , , , , , , , , and . on the configure instance details page, do the following: for elastic fabric adapter, choose enable. in the network interfaces section, for device eth0, choose new network interface. choose next: add storage. on the add storage page, specify the volumes to attach to the instances in addition to the volumes that are specified by the ami (such as the root device volume). then choose next: add tags. on the add tags page, specify a tag that you can use to identify the temporary instance, and then choose next: configure security group. on the configure security group page, for assign a security group, select select an existing security group, and then select the security group that you created in step 1. on the review instance launch page, review the settings, and then choose launch to choose a key pair and to launch your instance. install the efa-enabled kernel, efa drivers, libfabric, and open mpi stack that is required to support efa on your temporary instance. the steps differ depending on whether you intend to use efa with open mpi or with intel mpi. to install the efa software connect to the instance you launched. for more information, see . to ensure that all of your software packages are up to date, perform a quick software update on your instance. this process may take a few minutes. amazon linux, amazon linux 2, rhel 7.6/7.7/7.8, centos 7 ubuntu 16.04 and ubuntu 18.04 download the efa software installation files. to download the latest stable version, use the following command. you can also get the latest version by replacing the version number with  in the preceding command. the software installation files are packaged into a compressed  file. extract the files from the compressed  file and navigate into the extracted directory. install the efa software. if you intend to use efa with open mpi, you must install the efa software with libfabric and open mpi, and you must skip step 5: (optional) install intel mpi. to install the efa software with libfabric and open mpi, run the following command. libfabric is installed in the  directory, while open mpi is installed in the  directory. if you intend to use efa with intel mpi only, you can install the efa software without libfabric and open mpi. in this case, intel mpi uses its embedded libfabric. if you choose to do this, you must complete step 5: (optional) install intel mpi.  to install the efa software without libfabric and open mpi, run the following command. log out of the instance and then log back in. confirm that the efa software components were successfully installed. the command should return information about the libfabric efa interfaces. the following example shows the command output. to improve your hpc application's performance, libfabric uses the instance's local memory for interprocess communications when the processes are running on the same instance.  the shared memory feature uses cross memory attach (cma), which is not supported with ptrace protection. if you are using a linux distribution that has ptrace protection enabled by default, such as ubuntu, you must disable it. if your linux distribution does not have ptrace protection enabled by default, skip this step. to disable ptrace protectiondo one of the following: to temporarily disable ptrace protection for testing purposes, run the following command. to permanently disable ptrace protection, add  to  and reboot the instance.importantif you intend to use open mpi, skip this step. perform this step only if you intend to use intel mpi. intel mpi requires an additional installation and environment variable configuration. ensure that the user performing the following steps has sudo permissions. to install intel mpi to download the intel mpi installation files, see the . you must register before you can download the installation files. after you have registered, do the following: for product, choose intel mpi library for linux. for version, choose 2019 update 7, and then choose full product. the installation files are packaged into a compressed  file. extract the files from the compressed  file and navigate into the extracted directory. open  using your preferred text editor. in line 10, change  to . save the changes and close the file. run the installation script. intel mpi is installed in the  directory by default. add the intel mpi environment variables to the corresponding shell startup scripts to ensure that they are set each time that the instance starts. do one of the following depending on your shell. for bash, add the following environment variable to  and . for csh and tcsh, add the following environment variable to . log out of the instance and then log back in. run the following command to confirm that intel mpi was successfully installed. ensure that the returned path includes the  subdirectory. noteif you no longer want to use intel mpi, remove the environment variables from the shell startup scripts. install the hpc application on the temporary instance. the installation procedure varies depending on the specific hpc application. for more information about installing software on your linux instance, see . noteyou might need to refer to your hpc application’s documentation for installation instructions. after you have installed the required software components, you create an ami that you can reuse to launch your efa-enabled instances. to create an ami from your temporary instance open the amazon ec2 console at . in the navigation pane, choose instances. select the temporary instance that you created and choose actions, image, create image. in the create image window, do the following: for image name, enter a descriptive name for the ami. (optional) for image description, enter a brief description of the ami. choose create image and then choose close. in the navigation pane, choose amis. locate the ami you created in the list. wait for the status to transition from  to  before continuing to the next step. launch your efa-enabled instances into a cluster placement group using the efa-enabled ami that you created in step 7, and the efa-enabled security group that you created in step 1. noteit is not an absolute requirement to launch your efa-enabled instances into a cluster placement group. however, we do recommend running your efa-enabled instances in a cluster placement group as it launches the instances into a low-latency group in a single availability zone. to launch your efa-enabled instances into a cluster placement group open the amazon ec2 console at . choose launch instance. on the choose an ami page, choose my amis, find the ami that you created in step 7, and then choose select. on the choose an instance type page, select one of the following supported instance types and then choose next: configure instance details: , , , , , , , , , , and . on the configure instance details page, do the following: for number of instances, enter the number of efa-enabled instances that you want to launch. for network and subnet, select the vpc and subnet into which to launch the instances. for placement group, select add instance to placement group. for placement group name, select add to a new placement group, enter a descriptive name for the placement group, and then for placement group strategy, select cluster. for efa, choose enable. in the network interfaces section, for device eth0, choose new network interface. you can optionally specify a primary ipv4 address and one or more secondary ipv4 addresses. if you're launching the instance into a subnet that has an associated ipv6 cidr block, you can optionally specify a primary ipv6 address and one or more secondary ipv6 addresses. choose next: add storage. on the add storage page, specify the volumes to attach to the instances in addition to the volumes specified by the ami (such as the root device volume), and then choose next: add tags. on the add tags page, specify tags for the instances, such as a user-friendly name, and then choose next: configure security group. on the configure security group page, for assign a security group, select select an existing security group, and then select the security group that you created in step 1. choose review and launch. on the review instance launch page, review the settings, and then choose launch to choose a key pair and to launch your instances. at this point, you no longer need the temporary instance that you launched. you can terminate the instance to stop incurring charges for it. to terminate the temporary instance open the amazon ec2 console at . in the navigation pane, choose instances. select the temporary instance that you created and then choose actions, instance state, terminate, yes, terminate. to enable your applications to run across all of the instances in your cluster, you must enable passwordless ssh access from the leader node to the member nodes. the leader node is the instance from which you run your applications. the remaining instances in the cluster are the member nodes. to enable passwordless ssh between the instances in the cluster select one instance in the cluster as the leader node, and connect to it. disable  and enable  on the leader node. open  using your preferred text editor and add the following. generate an rsa key pair. the key pair is created in the  directory. change the permissions of the private key on the leader node. open  using your preferred text editor and copy the key. for each member node in the cluster, do the following: connect to the instance. open  using your preferred text editor and add the public key that you copied earlier. to test that the passwordless ssh is functioning as expected, connect to your leader node and run the following command. you should connect to the member node without being prompted for a key or password. 
this list of practices will help you get the maximum benefit from amazon ec2. security manage access to aws resources and apis using identity federation, iam users, and iam roles. establish credential management policies and procedures for creating, distributing, rotating, and revoking aws access credentials. for more information, see  in the iam user guide.implement the least permissive rules for your security group. for more information, see .regularly patch, update, and secure the operating system and applications on your instance. for more information about updating amazon linux 2 or the amazon linux ami, see  in the amazon ec2 user guide for linux instances.storage understand the implications of the root device type for data persistence, backup, and recovery. for more information, see .use separate amazon ebs volumes for the operating system versus your data. ensure that the volume with your data persists after instance termination. for more information, see .use the instance store available for your instance to store temporary data. remember that the data stored in instance store is deleted when you stop or terminate your instance. if you use instance store for database storage, ensure that you have a cluster with a replication factor that ensures fault tolerance.encrypt ebs volumes and snapshots. for more information, see .resource management use instance metadata and custom resource tags to track and identify your aws resources. for more information, see  and .view your current limits for amazon ec2. plan to request any limit increases in advance of the time that you'll need them. for more information, see .backup and recovery regularly back up your ebs volumes using , and create an  from your instance to save the configuration as a template for launching future instances.deploy critical components of your application across multiple availability zones, and replicate your data appropriately.design your applications to handle dynamic ip addressing when your instance restarts. for more information, see .monitor and respond to events. for more information, see .ensure that you are prepared to handle failover. for a basic solution, you can manually attach a network interface or elastic ip address to a replacement instance. for more information, see . for an automated solution, you can use amazon ec2 auto scaling. for more information, see the .regularly test the process of recovering your instances and amazon ebs volumes if they fail.
the following tutorials show you how to perform common tasks using ec2 instances running linux. for videos, see . topics 
using amazon cloudwatch alarm actions, you can create alarms that automatically stop, terminate, reboot, or recover your instances. you can use the stop or terminate actions to help you save money when you no longer need an instance to be running. you can use the reboot and recover actions to automatically reboot those instances or recover them onto new hardware if a system impairment occurs. the  service-linked role enables aws to perform alarm actions on your behalf. the first time you create an alarm in the aws management console, the iam cli, or the iam api, cloudwatch creates the service-linked role for you. there are a number of scenarios in which you might want to automatically stop or terminate your instance. for example, you might have instances dedicated to batch payroll processing jobs or scientific computing tasks that run for a period of time and then complete their work. rather than letting those instances sit idle (and accrue charges), you can stop or terminate them, which can help you to save money. the main difference between using the stop and the terminate alarm actions is that you can easily restart a stopped instance if you need to run it again later, and you can keep the same instance id and root volume. however, you cannot restart a terminated instance. instead, you must launch a new instance. you can add the stop, terminate, reboot, or recover actions to any alarm that is set on an amazon ec2 per-instance metric, including basic and detailed monitoring metrics provided by amazon cloudwatch (in the  namespace), as well as any custom metrics that include the  dimension, as long as its value refers to a valid running amazon ec2 instance. console supportyou can create alarms using the amazon ec2 console or the cloudwatch console. the procedures in this documentation use the amazon ec2 console. for procedures that use the cloudwatch console, see  in the amazon cloudwatch user guide. permissionsif you are an aws identity and access management (iam) user, you must have the following permissions to create or modify an alarm: , , , and  – for all alarms with amazon ec2 actions and  – for all alarms on amazon ec2 instance status metrics – for alarms with stop actions – for alarms with terminate actionsno specific permissions are needed for alarms with recover actions.if you have read/write permissions for amazon cloudwatch but not for amazon ec2, you can still create an alarm but the stop or terminate actions won't be performed on the amazon ec2 instance. however, if you are later granted permission to use the associated amazon ec2 apis, the alarm actions you created earlier are performed. for more information about iam permissions, see  in the iam user guide. topics you can create an alarm that stops an amazon ec2 instance when a certain threshold has been met. for example, you may run development or test instances and occasionally forget to shut them off. you can create an alarm that is triggered when the average cpu utilization percentage has been lower than 10 percent for 24 hours, signaling that it is idle and no longer in use. you can adjust the threshold, duration, and period to suit your needs, plus you can add an amazon simple notification service (amazon sns) notification so that you receive an email when the alarm is triggered. instances that use an amazon ebs volume as the root device can be stopped or terminated, whereas instances that use the instance store as the root device can only be terminated. to create an alarm to stop an idle instance (amazon ec2 console) open the amazon ec2 console at . in the navigation pane, choose instances. select the instance. on the monitoring tab, choose create alarm. in the create alarm dialog box, do the following: to receive an email when the alarm is triggered, for send a notification to, choose an existing amazon sns topic, or choose create topic to create a new one. to create a new topic, for send a notification to, enter a name for the topic, and then for with these recipients, enter the email addresses of the recipients (separated by commas). after you create the alarm, you will receive a subscription confirmation email that you must accept before you can get notifications for this topic. choose take the action, stop this instance. for whenever, choose the statistic you want to use and then choose the metric. in this example, choose average and cpu utilization. for is, specify the metric threshold. in this example, enter 10 percent. for for at least, specify the evaluation period for the alarm. in this example, enter 24 consecutive period(s) of 1 hour. to change the name of the alarm, for name of alarm, enter a new name. alarm names must contain only ascii characters. if you don't enter a name for the alarm, amazon cloudwatch automatically creates one for you. noteyou can adjust the alarm configuration based on your own requirements before creating the alarm, or you can edit them later. this includes the metric, threshold, duration, action, and notification settings. however, after you create an alarm, you cannot edit its name later. choose create alarm. you can create an alarm that terminates an ec2 instance automatically when a certain threshold has been met (as long as termination protection is not enabled for the instance). for example, you might want to terminate an instance when it has completed its work, and you don’t need the instance again. if you might want to use the instance later, you should stop the instance instead of terminating it. for information on enabling and disabling termination protection for an instance, see . to create an alarm to terminate an idle instance (amazon ec2 console) open the amazon ec2 console at . in the navigation pane, choose instances. select the instance. on the monitoring tab, choose create alarm. in the create alarm dialog box, do the following: to receive an email when the alarm is triggered, for send a notification to, choose an existing amazon sns topic, or choose create topic to create a new one. to create a new topic, for send a notification to, enter a name for the topic, and then for with these recipients, enter the email addresses of the recipients (separated by commas). after you create the alarm, you will receive a subscription confirmation email that you must accept before you can get notifications for this topic. choose take the action, terminate this instance. for whenever, choose a statistic and then choose the metric. in this example, choose average and cpu utilization. for is, specify the metric threshold. in this example, enter 10 percent. for for at least, specify the evaluation period for the alarm. in this example, enter 24 consecutive period(s) of 1 hour. to change the name of the alarm, for name of alarm, enter a new name. alarm names must contain only ascii characters. if you don't enter a name for the alarm, amazon cloudwatch automatically creates one for you. noteyou can adjust the alarm configuration based on your own requirements before creating the alarm, or you can edit them later. this includes the metric, threshold, duration, action, and notification settings. however, after you create an alarm, you cannot edit its name later. choose create alarm. you can create an amazon cloudwatch alarm that monitors an amazon ec2 instance and automatically reboots the instance. the reboot alarm action is recommended for instance health check failures (as opposed to the recover alarm action, which is suited for system health check failures). an instance reboot is equivalent to an operating system reboot. in most cases, it takes only a few minutes to reboot your instance. when you reboot an instance, it remains on the same physical host, so your instance keeps its public dns name, private ip address, and any data on its instance store volumes. rebooting an instance doesn't start a new instance billing period (with a minimum one-minute charge), unlike stopping and restarting your instance. for more information, see  in the amazon ec2 user guide for linux instances. importantto avoid a race condition between the reboot and recover actions, avoid setting the same number of evaluation periods for a reboot alarm and a recover alarm. we recommend that you set reboot alarms to three evaluation periods of one minute each. for more information, see  in the amazon cloudwatch user guide. to create an alarm to reboot an instance (amazon ec2 console) open the amazon ec2 console at . in the navigation pane, choose instances. select the instance. on the monitoring tab, choose create alarm. in the create alarm dialog box, do the following: to receive an email when the alarm is triggered, for send a notification to, choose an existing amazon sns topic, or choose create topic to create a new one. to create a new topic, for send a notification to, enter a name for the topic, and for with these recipients, enter the email addresses of the recipients (separated by commas). after you create the alarm, you will receive a subscription confirmation email that you must accept before you can get notifications for this topic. select take the action, reboot this instance. for whenever, choose status check failed (instance). for for at least, specify the evaluation period for the alarm. in this example, enter 3 consecutive period(s) of 1 minute. to change the name of the alarm, for name of alarm, enter a new name. alarm names must contain only ascii characters. if you don't enter a name for the alarm, amazon cloudwatch automatically creates one for you. choose create alarm. you can create an amazon cloudwatch alarm that monitors an amazon ec2 instance. if the instance becomes impaired due to an underlying hardware failure or a problem that requires aws involvement to repair, you can automatically recover the instance. terminated instances cannot be recovered. a recovered instance is identical to the original instance, including the instance id, private ip addresses, elastic ip addresses, and all instance metadata. cloudwatch prevents you from adding a recovery action to an alarm that is on an instance which does not support recovery actions. when the  alarm is triggered, and the recover action is initiated, you are notified by the amazon sns topic that you chose when you created the alarm and associated the recover action. during instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is lost. when the process is complete, information is published to the sns topic you've configured for the alarm. anyone who is subscribed to this sns topic receives an email notification that includes the status of the recovery attempt and any further instructions. you notice an instance reboot on the recovered instance. the recover action can be used only with , not with . the following problems can cause system status checks to fail: loss of network connectivityloss of system powersoftware issues on the physical hosthardware issues on the physical host that impact network reachabilitythe recover action is supported only on instances with the following characteristics: use one of the following instance types: a1, c3, c4, c5, c5a, c5n, c6g, inf1,  m3, m4, m5, m5a, m5n, m6g,  p3, r3, r4, r5, r5a, r5n, r6g,  t2, t3, t3a, x1, or x1euse  or  instance tenancyuse ebs volumes only (do not configure instance store volumes). for more information, see .if your instance has a public ip address, it retains the public ip address after recovery. importantto avoid a race condition between the reboot and recover actions, avoid setting the same number of evaluation periods for a reboot alarm and a recover alarm. we recommend that you set recover alarms to two evaluation periods of one minute each. for more information, see  in the amazon cloudwatch user guide. to create an alarm to recover an instance (amazon ec2 console) open the amazon ec2 console at . in the navigation pane, choose instances. select the instance. on the monitoring tab, choose create alarm. in the create alarm dialog box, do the following: to receive an email when the alarm is triggered, for send a notification to, choose an existing amazon sns topic, or choose create topic to create a new one. to create a new topic, for send a notification to, enter a name for the topic, and for with these recipients, enter the email addresses of the recipients (separated by commas). after you create the alarm, you will receive a subscription confirmation email that you must accept before you can get email for this topic. noteusers must subscribe to the specified sns topic to receive email notifications when the alarm is triggered. the aws account root user always receives email notifications when automatic instance recovery actions occur, even if an sns topic is not specified. the aws account root user always receives email notifications when automatic instance recovery actions occur, even if it is not subscribed to the specified sns topic. select take the action, recover this instance. for whenever, choose status check failed (system). for for at least, specify the evaluation period for the alarm. in this example, enter 2 consecutive period(s) of 1 minute. to change the name of the alarm, for name of alarm, enter a new name. alarm names must contain only ascii characters. if you don't enter a name for the alarm, amazon cloudwatch automatically creates one for you. choose create alarm. you can view alarm and action history in the amazon cloudwatch console. amazon cloudwatch keeps the last two weeks' worth of alarm and action history. to view the history of triggered alarms and actions (cloudwatch console) open the cloudwatch console at . in the navigation pane, choose alarms. select an alarm. the details tab shows the most recent state transition along with the time and metric values. choose the history tab to view the most recent history entries. you can use the amazon ec2 console to create alarm actions that stop or terminate an amazon ec2 instance when certain conditions are met. in the following screen capture of the console page where you set the alarm actions, we've numbered the settings. we've also numbered the settings in the scenarios that follow, to help you create the appropriate actions.  create an alarm that stops an instance used for software development or testing when it has been idle for at least an hour. create an alarm that stops an instance and sends an email when the instance has been idle for 24 hours. create an alarm that sends email when an instance exceeds 10 gb of outbound network traffic per day. create an alarm that stops an instance and send a text message (sms) if outbound traffic exceeds 1 gb per hour. create an alarm that stops an instance when memory utilization reaches or exceeds 90%, so that application logs can be retrieved for troubleshooting. notethe memoryutilization metric is a custom metric. in order to use the memoryutilization metric, you must install the perl scripts for linux instances. for more information, see . create an alarm that stops an instance that fails three consecutive status checks (performed at 5-minute intervals). create an alarm that terminates an instance that runs batch jobs when it is no longer sending results data. 
traditional amazon ec2 instance types provide fixed cpu utilization, while burstable performance instances provide a baseline level of cpu utilization with the ability to burst cpu utilization above the baseline level. the baseline utilization and ability to burst are governed by cpu credits. a cpu credit provides for 100% utilization of a full cpu core for one minute. other combinations of number of vcpus, utilization, and time can also equate to one cpu credit. for example, one cpu credit is equal to one vcpu running at 50% utilization for two minutes, or two vcpus running at 25% utilization for two minutes. contents each burstable performance instance continuously earns (at a millisecond-level resolution) a set rate of cpu credits per hour, depending on the instance size. the accounting process for whether credits are accrued or spent also happens at a millisecond-level resolution, so you don't have to worry about overspending cpu credits; a short burst of cpu uses a small fraction of a cpu credit. if a burstable performance instance uses fewer cpu resources than is required for baseline utilization (such as when it is idle), the unspent cpu credits are accrued in the cpu credit balance. if a burstable performance instance needs to burst above the baseline utilization level, it spends the accrued credits. the more credits that a burstable performance instance has accrued, the more time it can burst beyond its baseline when more cpu utilization is needed. the following table lists the burstable performance instance types, the rate at which cpu credits are earned per hour, the maximum number of earned cpu credits that an instance can accrue, the number of vcpus per instance, and the baseline utilization as a percentage of a full core (using a single vcpu). the number of cpu credits earned per hour is determined by the instance size. for example, a  earns six credits per hour, while a  earns 24 credits per hour. the preceding table lists the credit earn rate for all instances. while earned credits never expire on a running instance, there is a limit to the number of earned credits that an instance can accrue. the limit is determined by the cpu credit balance limit. after the limit is reached, any new credits that are earned are discarded, as indicated by the following image. the full bucket indicates the cpu credit balance limit, and the spillover indicates the newly earned credits that exceed the limit.  the cpu credit balance limit differs for each instance size. for example, a  instance can accrue a maximum of 288 earned cpu credits in the cpu credit balance. the preceding table lists the maximum number of earned credits that each instance can accrue. notet2 standard instances also earn launch credits. launch credits do not count towards the cpu credit balance limit. if a t2 instance has not spent its launch credits, and remains idle over a 24-hour period while accruing earned credits, its cpu credit balance appears as over the limit. for more information, see .t3 and t3a instances do not earn launch credits. these instances launch as  by default, and therefore can burst immediately upon start without any launch credits. cpu credits on a running instance do not expire. for t3 and t3a, the cpu credit balance persists for seven days after an instance stops and the credits are lost thereafter. if you start the instance within seven days, no credits are lost. for t2, the cpu credit balance does not persist between instance stops and starts. if you stop a t2 instance, the instance loses all its accrued credits. for more information, see  in the . the baseline utilization is the level at which the cpu can be utilized for a net credit balance of zero, when the number cpu credits being earned matches the number of cpu credits being used. baseline utilization is also known as the baseline. baseline utilization is expressed as a percentage of vcpu utilization, which is calculated as follows:  for example, a  instance, with 2 vcpus, earns 6 credits per hour, resulting in a baseline utilization of 5% , which is calculated as follows:  a  instance, with 4 vcpus, earns 96 credits per hour, resulting in a baseline utilization of 40% (). 
a burstable performance instance configured as  is suited to workloads with an average cpu utilization that is consistently below the baseline cpu utilization of the instance. to burst above the baseline, the instance spends credits that it has accrued in its cpu credit balance. if the instance is running low on accrued credits, cpu utilization is gradually lowered to the baseline level, so that the instance does not experience a sharp performance drop-off when its accrued cpu credit balance is depleted. for more information, see . contents 
amazon ec2 provides the following purchasing options to enable you to optimize your costs based on your needs: on-demand instances – pay, by the second, for the instances that you launch.savings plans – reduce your amazon ec2 costs by making a commitment to a consistent amount of usage, in usd per hour, for a term of 1 or 3 years.reserved instances – reduce your amazon ec2 costs by making a commitment to a consistent instance configuration, including instance type and region, for a term of 1 or 3 years.scheduled instances – purchase instances that are always available on the specified recurring schedule, for a one-year term.spot instances – request unused ec2 instances, which can reduce your amazon ec2 costs significantly.dedicated hosts – pay for a physical host that is fully dedicated to running your instances, and bring your existing per-socket, per-core, or per-vm software licenses to reduce costs.dedicated instances – pay, by the hour, for instances that run on single-tenant hardware.capacity reservations – reserve capacity for your ec2 instances in a specific availability zone for any duration.if you require a capacity reservation, purchase reserved instances or capacity reservations for a specific availability zone, or purchase scheduled instances. spot instances are a cost-effective choice if you can be flexible about when your applications run and if they can be interrupted. dedicated hosts or dedicated instances can help you address compliance requirements and reduce costs by using your existing server-bound software licenses. for more information, see . for more information about savings plans, see the . topics the lifecycle of an instance starts when it is launched and ends when it is terminated. the purchasing option that you choose affects the lifecycle of the instance. for example, an on-demand instance runs when you launch it and ends when you terminate it. a spot instance runs as long as capacity is available and your maximum price is higher than the spot price. you can launch a scheduled instance during its scheduled time period; amazon ec2 launches the instances and then terminates them three minutes before the time period ends. use the following procedure to determine the lifecycle of an instance. to determine the instance lifecycle using the console open the amazon ec2 console at . in the navigation pane, choose instances. select the instance. on the description tab, find tenancy. if the value is , the instance is running on a dedicated host. if the value is , the instance is a dedicated instance. on the description tab, find lifecycle. if the value is , the instance is a spot instance. if the value is , the instance is a scheduled instance. if the value is , the instance is either an on-demand instance or a reserved instance. (optional) if you have purchased a reserved instance and want to verify that it is being applied, you can check the usage reports for amazon ec2. for more information, see . to determine the instance lifecycle using the aws cliuse the following  command: if the instance is running on a dedicated host, the output contains the following information: if the instance is a dedicated instance, the output contains the following information: if the instance is a spot instance, the output contains the following information: if the instance is a scheduled instance, the output contains the following information: otherwise, the output does not contain . 
amazon ec2 instances support multithreading, which enables multiple threads to run concurrently on a single cpu core. each thread is represented as a virtual cpu (vcpu) on the instance. an instance has a default number of cpu cores, which varies according to instance type. for example, an  instance type has two cpu cores and two threads per core by default—four vcpus in total. noteeach vcpu is a thread of a cpu core, except for t2 instances. in most cases, there is an amazon ec2 instance type that has a combination of memory and number of vcpus to suit your workloads. however, you can specify the following cpu options to optimize your instance for specific workloads or business needs: number of cpu cores: you can customize the number of cpu cores for the instance. you might do this to potentially optimize the licensing costs of your software with an instance that has sufficient amounts of ram for memory-intensive workloads but fewer cpu cores.threads per core: you can disable multithreading by specifying a single thread per cpu core. you might do this for certain workloads, such as high performance computing (hpc) workloads.you can specify these cpu options during instance launch. there is no additional or reduced charge for specifying cpu options. you're charged the same as instances that are launched with default cpu options. topics to specify the cpu options for your instance, be aware of the following rules: cpu options can only be specified during instance launch and cannot be modified after launch.when you launch an instance, you must specify both the number of cpu cores and threads per core in the request. for example requests, see .the number of vcpus for the instance is the number of cpu cores multiplied by the threads per core. to specify a custom number of vcpus, you must specify a valid number of cpu cores and threads per core for the instance type. you cannot exceed the default number of vcpus for the instance. for more information, see .to disable multithreading, specify one thread per core.when you  of an existing instance, the cpu options automatically change to the default cpu options for the new instance type.the specified cpu options persist after you stop, start, or reboot an instance.the following tables list the instance types that support specifying cpu options. for each type, the table shows the default and supported number of cpu cores and threads per core. accelerated computing instances   compute optimized instances   general purpose instances   memory optimized instances   storage optimized instances   you can specify cpu options during instance launch. the following examples are for an  instance type, which has the following : default cpu cores: 8default threads per core: 2default vcpus: 16 (8 * 2)valid number of cpu cores: 1, 2, 3, 4, 5, 6, 7, 8valid number of threads per core: 1, 2to disable multithreading, specify one thread per core. to disable multithreading during instance launch (console) follow the  procedure. on the configure instance details page, for cpu options, choose specify cpu options. for core count, choose the number of required cpu cores. in this example, to specify the default cpu core count for an  instance, choose . to disable multithreading, for threads per core, choose 1. continue as prompted by the wizard. when you've finished reviewing your options on the review instance launch page, choose launch. for more information, see . to disable multithreading during instance launch (aws cli)use the  aws cli command and specify a value of  for  for the  parameter. for , specify the number of cpu cores. in this example, to specify the default cpu core count for an  instance, specify a value of . you can customize the number of cpu cores and threads per core for the instance. to specify a custom number of vcpus during instance launch (console) the following example launches an  instance with six vcpus. follow the  procedure. on the configure instance details page, for cpu options, choose specify cpu options. to get six vcpus, specify three cpu cores and two threads per core, as follows: for core count, choose 3.for threads per core, choose 2.continue as prompted by the wizard. when you've finished reviewing your options on the review instance launch page, choose launch. for more information, see . to specify a custom number of vcpus during instance launch (aws cli)the following example launches an  instance with six vcpus. use the  aws cli command and specify the number of cpu cores and number of threads in the  parameter. you can specify three cpu cores and two threads per core to get six vcpus. alternatively, specify six cpu cores and one thread per core (disable multithreading) to get six vcpus: you can view the cpu options for an existing instance in the amazon ec2 console or by describing the instance using the aws cli. to view the cpu options for an instance (console) open the amazon ec2 console at . in the left navigation pane, choose instances, and select the instance. choose description and view the number of vcpus field. to view the core count and threads per core, choose the number of vcpus field value. to view the cpu options for an instance (aws cli)use the  command. in the output that's returned, the  field indicates the number of cores for the instance. the  field indicates the number of threads per core. alternatively, connect to your instance and use a tool such as lscpu to view the cpu information for your instance. you can use aws config to record, assess, audit, and evaluate configuration changes for instances, including terminated instances. for more information, see  in the aws config developer guide. 
aws can schedule events for your instances, such as a reboot, stop/start, or retirement. these events do not occur frequently. if one of your instances will be affected by a scheduled event, aws sends an email to the email address that's associated with your aws account prior to the scheduled event. the email provides details about the event, including the start and end date. depending on the event, you might be able to take action to control the timing of the event. to update the contact information for your account so that you can be sure to be notified about scheduled events, go to the  page. topics amazon ec2 supports the following types of events for your instances, where the event occurs at a scheduled time: instance stop: at the scheduled time, the instance is stopped. when you start it again, it's migrated to a new host. applies only to instances backed by amazon ebs.instance retirement: at the scheduled time, the instance is stopped if it is backed by amazon ebs, or terminated if it is backed by instance store.instance reboot: at the scheduled time, the instance is rebooted.system reboot: at the scheduled time, the host for the instance is rebooted.system maintenance: at the scheduled time, the instance might be temporarily affected by network maintenance or power maintenance.in addition to receiving notification of scheduled events in email, you can check for scheduled events using one of the following methods. to view scheduled events for your instances using the console open the amazon ec2 console at . you can view scheduled events in the following screens: in the navigation pane, choose events. any resources with an associated event are displayed. you can filter by resource id, resource type, availability zone, event status, or event type.alternatively, in the navigation pane, choose ec2 dashboard. any resources with an associated event are displayed under scheduled events.some events are also shown for affected resources. for example, in the navigation pane, choose instances and select an instance. if the instance has an associated instance stop or instance retirement event, it is displayed in the lower pane.to view scheduled events for your instances using the console open the amazon ec2 console at . you can view scheduled events in the following screens: in the navigation pane, choose events. any resources with an associated event are displayed. you can filter by resource type, or by specific event types. you can select the resource to view details.alternatively, in the navigation pane, choose ec2 dashboard. any resources with an associated event are displayed under scheduled events.some events are also shown for affected resources. for example, in the navigation pane, choose instances and select an instance. if the instance has an associated instance stop or instance retirement event, it is displayed in the lower pane.to view scheduled events for your instances using the aws cliuse the  command. the following example output shows a reboot event. the following example output shows an instance retirement event. to view scheduled events for your instances using the aws tools for windows powershelluse the following  command. the following example output shows an instance retirement event. to view scheduled events for your instances using instance metadatayou can retrieve information about active maintenance events for your instances from the  using instance metadata service version 2 or instance metadata service version 1. imdsv2 imdsv1 the following is example output with information about a scheduled system reboot event, in json format.  to view event history about completed or canceled events for your instances using instance metadatayou can retrieve information about completed or canceled events for your instances from  using instance metadata service version 2 or instance metadata service version 1. imdsv2 imdsv1 the following is example output with information about a system reboot event that was canceled, and a system reboot event that was completed, in json format. you can customize scheduled event notifications to include tags in the email notification. this makes it easier to identify the affected resource (instances or dedicated hosts) and to prioritize actions for the upcoming event. when you customize event notifications to include tags, you can choose to include: all of the tags that are associated with the affected resourceonly specific tags that are associated with the affected resourcefor example, suppose that you assign , , , and  tags to all of your instances. you can choose to include all of the tags in event notifications. alternatively, if you'd like to see only the  and  tags in event notifications, then you can choose to include only those tags. after you select the tags to include, the event notifications will include the resource id (instance id or dedicated host id) and the tag key and value pairs that are associated with the affected resource. topics the tags that you choose to include apply to all resources (instances and dedicated hosts) in the selected region. to customize event notifications in other regions, first select the required region and then perform the following steps. you can include tags in event notifications using one of the following methods. to include tags in event notifications open the amazon ec2 console at . in the navigation pane, choose events. choose actions, manage event notifications. select include resource tags in event notifications. do one of the following, depending on the tags that you want to include in event notifications: to include all of the tags associated with the affected instance or dedicated host, select include all resource tags.to manually select the tags to include, select choose the tags to include, and then for choose the tags to include, enter the tag key and press enter.choose save. to include all tags in event notificationsuse the  aws cli command and set the  parameter to . to include specific tags in event notificationsuse the  aws cli command and specify the tags to include using the  parameter. you can remove tags from event notifications using one of the following methods. to remove tags from event notifications open the amazon ec2 console at . in the navigation pane, choose events. choose actions, manage event notifications. do one of the following, depending on the tag that you want to remove from event notifications. to remove all tags from event notifications, clear include resource tags in event notifications.to remove specific tags from event notifications, choose remove (x) for the tags listed below the choose the tags to include field.choose save. to remove all tags from event notificationsuse the  aws cli command and set the  parameter to . to remove specific tags from event notificationsuse the  aws cli command and specify the tags to remove using the  parameter. you can view the tags that are to be included in event notifications using one of the following methods. to view the tags that are to be included in event notifications open the amazon ec2 console at . in the navigation pane, choose events. choose actions, manage event notifications. to view the tags that are to be included in event notificationsuse the  aws cli command. when aws detects irreparable failure of the underlying host for your instance, it schedules the instance to stop or terminate, depending on the type of root device for the instance. if the root device is an ebs volume, the instance is scheduled to stop. if the root device is an instance store volume, the instance is scheduled to terminate. for more information, see . importantany data stored on instance store volumes is lost when an instance is stopped or terminated. this includes instance store volumes that are attached to an instance that has an ebs volume as the root device. be sure to save data from your instance store volumes that you might need later before the instance is stopped or terminated. actions for instances backed by amazon ebsyou can wait for the instance to stop as scheduled. alternatively, you can stop and start the instance yourself, which migrates it to a new host. for more information about stopping your instance, in addition to information about the changes to your instance configuration when it's stopped, see . you can automate an immediate stop and start in response to a scheduled instance stop event. for more information, see  in the aws health user guide. actions for instances backed by instance storewe recommend that you launch a replacement instance from your most recent ami and migrate all necessary data to the replacement instance before the instance is scheduled to terminate. then, you can terminate the original instance, or wait for it to terminate as scheduled. when aws must perform tasks such as installing updates or maintaining the underlying host, it can schedule the instance or the underlying host for a reboot. you can  so that your instance is rebooted at a specific date and time that suits you. if you stop your linked , it is automatically unlinked from the vpc and the vpc security groups are no longer associated with the instance. you can link your instance to the vpc again after you've restarted it. you can view whether a reboot event is an instance reboot or a system reboot using one of the following methods. to view the type of scheduled reboot event using the console open the amazon ec2 console at . in the navigation pane, choose events. choose resource type: instance from the filter list. for each instance, view the value in the event type column. the value is either system-reboot or instance-reboot. to view the type of scheduled reboot event using the console open the amazon ec2 console at . in the navigation pane, choose events. choose instance resources from the filter list. for each instance, view the value in the event type column. the value is either system-reboot or instance-reboot. to view the type of scheduled reboot event using the aws cliuse the  command. for scheduled reboot events, the value for  is either  or . the following example output shows a  event.  actions for instance rebootyou can wait for the instance reboot to occur within its scheduled maintenance window,  the instance reboot to a date and time that suits you, or  the instance yourself at a time that is convenient for you. after your instance is rebooted, the scheduled event is cleared and the event's description is updated. the pending maintenance to the underlying host is completed, and you can begin using your instance again after it has fully booted.  actions for system rebootit is not possible for you to reboot the system yourself. you can wait for the system reboot to occur during its scheduled maintenance window, or you can  the system reboot to a date and time that suits you. a system reboot typically completes in a matter of minutes. after the system reboot has occurred, the instance retains its ip address and dns name, and any data on local instance store volumes is preserved. after the system reboot is complete, the scheduled event for the instance is cleared, and you can verify that the software on your instance is operating as expected. alternatively, if it is necessary to maintain the instance at a different time and you can't reschedule the system reboot, then you can stop and start an amazon ebs-backed instance, which migrates it to a new host. however, the data on the local instance store volumes is not preserved. you can also automate an immediate instance stop and start in response to a scheduled system reboot event. for more information, see  in the aws health user guide. for an instance store-backed instance, if you can't reschedule the system reboot, then you can launch a replacement instance from your most recent ami, migrate all necessary data to the replacement instance before the scheduled maintenance window, and then terminate the original instance. when aws must maintain the underlying host for an instance, it schedules the instance for maintenance. there are two types of maintenance events: network maintenance and power maintenance. during network maintenance, scheduled instances lose network connectivity for a brief period of time. normal network connectivity to your instance is restored after maintenance is complete. during power maintenance, scheduled instances are taken offline for a brief period, and then rebooted. when a reboot is performed, all of your instance's configuration settings are retained. after your instance has rebooted (this normally takes a few minutes), verify that your application is working as expected. at this point, your instance should no longer have a scheduled event associated with it, or if it does, the description of the scheduled event begins with [completed]. it sometimes takes up to 1 hour for the instance status description to refresh. completed maintenance events are displayed on the amazon ec2 console dashboard for up to a week. actions for instances backed by amazon ebsyou can wait for the maintenance to occur as scheduled. alternatively, you can stop and start the instance, which migrates it to a new host. for more information about stopping your instance, in addition to information about the changes to your instance configuration when it's stopped, see . you can automate an immediate stop and start in response to a scheduled maintenance event. for more information, see  in the aws health user guide. actions for instances backed by instance storeyou can wait for the maintenance to occur as scheduled. alternatively, if you want to maintain normal operation during a scheduled maintenance window, you can launch a replacement instance from your most recent ami, migrate all necessary data to the replacement instance before the scheduled maintenance window, and then terminate the original instance. you can reschedule an event so that it occurs at a specific date and time that suits you. only events that have a deadline date can be rescheduled. there are other . you can reschedule an event using one of the following methods. to reschedule an event using the console open the amazon ec2 console at . in the navigation pane, choose events. choose resource type: instance from the filter list. select one or more instances, and then choose actions, schedule event. only events that have an event deadline date, indicated by a value for deadline, can be rescheduled. if one of the selected events does not have a deadline date, actions, schedule event is disabled. for new start time, enter a new date and time for the event. the new date and time must occur before the event deadline. choose save. it might take 1-2 minutes for the updated event start time to be reflected in the console. to reschedule an event using the console open the amazon ec2 console at . in the navigation pane, choose events. choose instance resources from the filter list. select one or more instances, and then choose actions, schedule event. only events that have an event deadline date, indicated by a value for event deadline, can be rescheduled.  for event start time, enter a new date and time for the event. the new date and time must occur before the event deadline. choose schedule event. it might take 1-2 minutes for the updated event start time to be reflected in the console. to reschedule an event using the aws cli only events that have an event deadline date, indicated by a value for , can be rescheduled. use the  command to view the  parameter value. the following example output shows a  event that can be rescheduled because  contains a value. to reschedule the event, use the  command. specify the new event start time using the  parameter. the new event start time must fall before the . it might take 1-2 minutes before the  command returns the updated  parameter value. only events with an event deadline date can be rescheduled. the event can be rescheduled up to the event deadline date. the deadline column in the console and the  field in the aws cli indicate if the event has a deadline date.only events that have not yet started can be rescheduled. the start time column in the console and the  field in the aws cli indicate the event start time. events that are scheduled to start in the next 5 minutes cannot be rescheduled.the new event start time must be at least 60 minutes from the current time.if you reschedule multiple events using the console, the event deadline date is determined by the event with the earliest event deadline date. 
automatic scaling is the ability to increase or decrease the target capacity of your spot fleet automatically based on demand. a spot fleet can either launch instances (scale out) or terminate instances (scale in), within the range that you choose, in response to one or more scaling policies. spot fleet supports the following types of automatic scaling:  – increase or decrease  the current capacity of the fleet based on a target value for a specific metric. this is similar to the way that your thermostat maintains the temperature of your home—you select temperature and the thermostat does the rest. – increase or decrease the current capacity of the fleet based on a set of scaling adjustments, known as step adjustments, that vary based on the size of the alarm breach. – increase or decrease the current capacity of the fleet based on the date and time.if you are using , keep in mind that spot fleet can exceed the target capacity as needed. fulfilled capacity can be a floating-point number but target capacity must be an integer, so spot fleet rounds up to the next integer. you must take these behaviors into account when you look at the outcome of a scaling policy when an alarm is triggered. for example, suppose that the target capacity is 30, the fulfilled capacity is 30.1, and the scaling policy subtracts 1. when the alarm is triggered, the automatic scaling process subtracts 1 from 30.1 to get 29.1 and then rounds it up to 30, so no scaling action is taken. as another example, suppose that you selected instance weights of 2, 4, and 8, and a target capacity of 10, but no weight 2 instances were available so spot fleet provisioned instances of weights 4 and 8 for a fulfilled capacity of 12. if the scaling policy decreases target capacity by 20% and an alarm is triggered, the automatic scaling process subtracts 12*0.2 from 12 to get 9.6 and then rounds it up to 10, so no scaling action is taken. the scaling policies that you create for spot fleet support a cooldown period. this is the number of seconds after a scaling activity completes where previous trigger-related scaling activities can influence future scaling events. for scale-out policies, while the cooldown period is in effect, the capacity that has been added by the previous scale-out event that initiated the cooldown is calculated as part of the desired capacity for the next scale out. the intention is to continuously (but not excessively) scale out. for scale in policies, the cooldown period is used to block subsequent scale in requests until it has expired. the intention is to scale in conservatively to protect your application's availability. however, if another alarm triggers a scale-out policy during the cooldown period after a scale-in, automatic scaling scales out your scalable target immediately. we recommend that you scale based on instance metrics with a 1-minute frequency because that ensures a faster response to utilization changes. scaling on metrics with a 5-minute frequency can result in slower response time and scaling on stale metric data. to send metric data for your instances to cloudwatch in 1-minute periods, you must specifically enable detailed monitoring. for more information, see  and . for more information about configuring scaling for spot fleet, see the following resources:  section of the aws cli command referenceautomatic scaling for spot fleet is made possible by a combination of the amazon ec2, amazon cloudwatch, and application auto scaling apis. spot fleet requests are created with amazon ec2, alarms are created with cloudwatch, and scaling policies are created with application auto scaling.  in addition to the  and amazon ec2, the iam user that accesses fleet scaling settings must have the appropriate permissions for the services that support dynamic scaling. iam users must have permissions to use the actions shown in the following example policy.  you can also create your own iam policies that allow more fine-grained permissions for calls to the application auto scaling api. for more information, see  in the application auto scaling user guide. the application auto scaling service also needs permission to describe your spot fleet and cloudwatch alarms, and permissions to modify your spot fleet target capacity on your behalf. if you enable automatic scaling for your spot fleet, it creates a service-linked role named . this service-linked role grants application auto scaling permission to describe the alarms for your policies, to monitor the current capacity of the fleet, and to modify the capacity of the fleet. the original managed spot fleet role for application auto scaling was , but it is no longer required. the service-linked role is the default role for application auto scaling. for more information, see  in the application auto scaling user guide. 
aws provides a free reporting tool called cost explorer that enables you to analyze the cost and usage of your ec2 instances and the usage of your reserved instances. you can view data up to the last 13 months, and forecast how much you are likely to spend for the next three months. you can use cost explorer to see patterns in how much you spend on aws resources over time, identify areas that need further inquiry, and see trends that you can use to understand your costs. you also can specify time ranges for the data, and view time data by day or by month. here's an example of some of the questions that you can answer when using cost explorer: how much am i spending on instances of each instance type?how many instance hours are being used by a particular department?how is my instance usage distributed across availability zones?how is my instance usage distributed across aws accounts?how well am i using my reserved instances?are my reserved instances helping me save money?for more information about working with reports in cost explorer, including saving reports, see . 
in some situations, you may find that a volume other than the volume attached to  or  has become the root volume of your instance. this can happen when you have attached the root volume of another instance, or a volume created from the snapshot of a root volume, to an instance with an existing root volume. this is due to how the initial ramdisk in linux works. it chooses the volume defined as  in the , and in some distributions, this is determined by the label attached to the volume partition. specifically, you find that your  looks something like the following:  if you check the label of both volumes, you see that they both contain the  label:  in this example, you could end up having  become the root device that your instance boots to after the initial ramdisk runs, instead of the  volume from which you had intended to boot. to solve this, use the same e2label command to change the label of the attached volume that you do not want to boot from. in some cases, specifying a uuid in  can resolve this. however, if both volumes come from the same snapshot, or the secondary is created from a snapshot of the primary volume, they share a uuid. to change the label of an attached ext4 volume use the e2label command to change the label of the volume to something other than . verify that the volume has the new label. to change the label of an attached xfs volume use the xfs_admin command to change the label of the volume to something other than . after changing the volume label as shown, you should be able to reboot the instance and have the proper volume selected by the initial ramdisk when the instance boots. importantif you intend to detach the volume with the new label and return it to another instance to use as the root volume, you must perform the above procedure again and change the volume label back to its original value. otherwise, the other instance does not boot because the ramdisk is unable to find the volume with the label . 
the following table describes important additions to the amazon ec2 documentation starting in 2019. we also update the documentation frequently to address the feedback that you send us. the following table describes important additions to the amazon ec2 documentation in 2018 and earlier years. 
the following procedures will help you install, configure, and secure a wordpress blog on your amazon linux instance. this tutorial is a good introduction to using amazon ec2 in that you have full control over a web server that hosts your wordpress blog, which is not typical with a traditional hosting service. you are responsible for updating the software packages and maintaining security patches for your server. for a more automated wordpress installation that does not require direct interaction with the web server configuration, the aws cloudformation service provides a wordpress template that can also get you started quickly. for more information, see  in the aws cloudformation user guide. if you'd prefer to host your wordpress blog on a windows instance, see  in the amazon ec2 user guide for windows instances. if you need a high-availability solution with a decoupled database, see  in the aws elastic beanstalk developer guide. importantthese procedures are intended for use with amazon linux. for more information about other distributions, see their specific documentation. many steps in this tutorial do not work on ubuntu instances. for help installing wordpress on an ubuntu instance, see  in the ubuntu documentation. option: complete this tutorial using automationto complete this tutorial using aws systems manager automation instead of the following tasks, run one of the following automation documents:  (amazon linux) or  (amazon linux 2). this tutorial assumes that you have launched an amazon linux instance with a functional web server with php and database (either mysql or mariadb) support by following all of the steps in  for amazon linux ami or  for amazon linux 2. this tutorial also has steps for configuring a security group to allow  and  traffic, as well as several steps to ensure that file permissions are set properly for your web server. for information about adding rules to your security group, see . we strongly recommend that you associate an elastic ip address (eip) to the instance you are using to host a wordpress blog. this prevents the public dns address for your instance from changing and breaking your installation. if you own a domain name and you want to use it for your blog, you can update the dns record for the domain name to point to your eip address (for help with this, contact your domain name registrar). you can have one eip address associated with a running instance at no charge. for more information, see . if you don't already have a domain name for your blog, you can register a domain name with route 53 and associate your instance's eip address with your domain name. for more information, see  in the amazon route 53 developer guide. connect to your instance, and download the wordpress installation package. to download and unzip the wordpress installation package download the latest wordpress installation package with the wget command. the following command should always download the latest release. unzip and unarchive the installation package. the installation folder is unzipped to a folder called .  to create a database user and database for your wordpress installation your wordpress installation needs to store information, such as blog posts and user comments, in a database. this procedure helps you create your blog's database and a user that is authorized to read and save information to it.  start the database server. amazon linux 2 amazon linux ami log in to the database server as the  user. enter your database  password when prompted; this may be different than your  system password, or it may even be empty if you have not secured your database server. if you have not secured your database server yet, it is important that you do so. for more information, see  (amazon linux 2) or  (amazon linux ami). create a user and password for your mysql database. your wordpress installation uses these values to communicate with your mysql database. enter the following command, substituting a unique user name and password. make sure that you create a strong password for your user. do not use the single quote character ( ' ) in your password, because this will break the preceding command. for more information about creating a secure password, go to . do not reuse an existing password, and make sure to store this password in a safe place. create your database. give your database a descriptive, meaningful name, such as . notethe punctuation marks surrounding the database name in the command below are called backticks. the backtick (tab` key on a standard keyboard. backticks are not always required, but they allow you to use otherwise illegal characters, such as hyphens, in database names. grant full privileges for your database to the wordpress user that you created earlier. flush the database privileges to pick up all of your changes. exit the  client. to create and edit the wp-config.php file the wordpress installation folder contains a sample configuration file called . in this procedure, you copy this file and edit it to fit your specific configuration. copy the  file to a file called . this creates a new configuration file and keeps the original sample file intact as a backup. edit the  file with your favorite text editor (such as nano or vim) and enter values for your installation. if you do not have a favorite text editor,  is suitable for beginners. find the line that defines  and change  to the database name that you created in  of . find the line that defines  and change  to the database user that you created in  of . find the line that defines  and change  to the strong password that you created in  of . find the section called . these  and  values provide a layer of encryption to the browser cookies that wordpress users store on their local machines. basically, adding long, random values here makes your site more secure. visit  to randomly generate a set of key values that you can copy and paste into your  file. to paste text into a putty terminal, place the cursor where you want to paste the text and right-click your mouse inside the putty terminal. for more information about security keys, go to . notethe values below are for example purposes only; do not use these values for your installation. save the file and exit your text editor. to install your wordpress files under the apache document root now that you've unzipped the installation folder, created a mysql database and user, and customized the wordpress configuration file, you are ready to copy your installation files to your web server document root so you can run the installation script that completes your installation. the location of these files depends on whether you want your wordpress blog to be available at the actual root of your web server (for example, ) or in a subdirectory or folder under the root (for example, ). if you want wordpress to run at your document root, copy the contents of the wordpress installation directory (but not the directory itself) as follows:  if you want wordpress to run in an alternative directory under the document root, first create that directory, and then copy the files to it. in this example, wordpress will run from the directory : importantfor security purposes, if you are not moving on to the next procedure immediately, stop the apache web server () now. after you move your installation under the apache document root, the wordpress installation script is unprotected and an attacker could gain access to your blog if the apache web server were running. to stop the apache web server, enter the command sudo service httpd stop. if you are moving on to the next procedure, you do not need to stop the apache web server. to allow wordpress to use permalinks wordpress permalinks need to use apache  files to work properly, but this is not enabled by default on amazon linux. use this procedure to allow all overrides in the apache document root. open the  file with your favorite text editor (such as nano or vim). if you do not have a favorite text editor,  is suitable for beginners. find the section that starts with . change the  line in the above section to read . notethere are multiple  lines in this file; be sure you change the line in the  section. save the file and exit your text editor. to install the php graphics drawing library on amazon linux 2the gd library for php enables you to modify images. install this library if you need to crop the header image for your blog. the version of phpmyadmin that you install might require a specific minimum version of this library (for example, version 7.2). use the following command to install the php graphics drawing library on amazon linux 2. for example, if you installed php7.2 from amazon-linux-extras as part of installing the lamp stack, this command installs version 7.2 of the php graphics drawing library. to verify the installed version, use the following command: the following is example output: to install the php graphics drawing library on the amazon linux amithe gd library for php enables you to modify images. install this library if you need to crop the header image for your blog. the version of phpmyadmin that you install might require a specific minimum version of this library (for example, version 7.2). to verify which versions are available, use the following command: the following is an example line from the output for the php graphics drawing library (version 7.2): use the following command to install a specific version of the php graphics drawing library (for example, version 7.2) on the amazon linux ami: to fix file permissions for the apache web server some of the available features in wordpress require write access to the apache document root (such as uploading media though the administration screens). if you have not already done so, apply the following group memberships and permissions (as described in greater detail in the ). grant file ownership of  and its contents to the  user. grant group ownership of  and its contents to the  group. change the directory permissions of  and its subdirectories to add group write permissions and to set the group id on future subdirectories. recursively change the file permissions of  and its subdirectories to add group write permissions. restart the apache web server to pick up the new group and permissions. amazon linux 2 amazon linux ami to run the wordpress installation script with amazon linux 2 you are ready to install wordpress. the commands that you use depend on the operating system. the commands in this procedure are for use with amazon linux 2. use the procedure that follows this one with amazon linux ami. use the systemctl command to ensure that the  and database services start at every system boot. verify that the database server is running. if the database service is not running, start it. verify that your apache web server () is running. if the  service is not running, start it. in a web browser, type the url of your wordpress blog (either the public dns address for your instance, or that address followed by the  folder). you should see the wordpress installation script. provide the information required by the wordpress installation. choose install wordpress to complete the installation. for more information, see  on the wordpress website. to run the wordpress installation script with amazon linux ami use the chkconfig command to ensure that the  and database services start at every system boot. verify that the database server is running. if the database service is not running, start it. verify that your apache web server () is running. if the  service is not running, start it. in a web browser, type the url of your wordpress blog (either the public dns address for your instance, or that address followed by the  folder). you should see the wordpress installation script. provide the information required by the wordpress installation. choose install wordpress to complete the installation. for more information, see  on the wordpress website. after you have tested your wordpress blog, consider updating its configuration. use a custom domain nameif you have a domain name associated with your ec2 instance's eip address, you can configure your blog to use that name instead of the ec2 public dns address. for more information, see . configure your blogyou can configure your blog to use different  and  to offer a more personalized experience for your readers. however, sometimes the installation process can backfire, causing you to lose your entire blog. we strongly recommend that you create a backup amazon machine image (ami) of your instance before attempting to install any themes or plugins so you can restore your blog if anything goes wrong during installation. for more information, see . increase capacityif your wordpress blog becomes popular and you need more compute power or storage, consider the following steps: expand the storage space on your instance. for more information, see .move your mysql database to  to take advantage of the service's ability to scale easily.migrate to a larger instance type. for more information, see .add additional instances. for more information, see .learn more about wordpressfor information about wordpress, see the wordpress codex help documentation at . for more information about troubleshooting your installation, go to . for information about making your wordpress blog more secure, go to . for information about keeping your wordpress blog up-to-date, go to . your wordpress installation is automatically configured using the public dns address for your ec2 instance. if you stop and restart the instance, the public dns address changes (unless it is associated with an elastic ip address) and your blog will not work anymore because it references resources at an address that no longer exists (or is assigned to another ec2 instance). a more detailed description of the problem and several possible solutions are outlined in . if this has happened to your wordpress installation, you may be able to recover your blog with the procedure below, which uses the wp-cli command line interface for wordpress. to change your wordpress site url with the wp-cli connect to your ec2 instance with ssh.  note the old site url and the new site url for your instance. the old site url is likely the public dns name for your ec2 instance when you installed wordpress. the new site url is the current public dns name for your ec2 instance. if you are not sure of your old site url, you can use curl to find it with the following command. you should see references to your old public dns name in the output, which will look like this (old site url in red): download the wp-cli with the following command. search and replace the old site url in your wordpress installation with the following command. substitute the old and new site urls for your ec2 instance and the path to your wordpress installation (usually  or ). in a web browser, enter the new site url of your wordpress blog to verify that the site is working properly again. if it is not, see  and  for more information. 
when you launch an instance, it is assigned a hostname that is a form of the private, internal ipv4 address. a typical amazon ec2 private dns name looks something like this: , where the name consists of the internal domain, the service (in this case, ), the region, and a form of the private ipv4 address. part of this hostname is displayed at the shell prompt when you log into your instance (for example, ). each time you stop and restart your amazon ec2 instance (unless you are using an elastic ip address), the public ipv4 address changes, and so does your public dns name, system hostname, and shell prompt. importantthis information applies to amazon linux. for information about other distributions, see their specific documentation. if you have a public dns name registered for the ip address of your instance (such as ), you can set the system hostname so your instance identifies itself as a part of that domain. this also changes the shell prompt so that it displays the first portion of this name instead of the hostname supplied by aws (for example, ). if you do not have a public dns name registered, you can still change the hostname, but the process is a little different. to change the system hostname to a public dns name follow this procedure if you already have a public dns name registered. for amazon linux 2: use the hostnamectl command to set your hostname to reflect the fully qualified domain name (such as webserver.mydomain.com). for amazon linux ami: on your instance, open the  configuration file in your favorite text editor and change the  entry to reflect the fully qualified domain name (such as webserver.mydomain.com). reboot the instance to pick up the new hostname. alternatively, you can reboot using the amazon ec2 console (on the instances page, choose actions, instance state, reboot). log into your instance and verify that the hostname has been updated. your prompt should show the new hostname (up to the first ".") and the hostname command should show the fully-qualified domain name. to change the system hostname without a public dns name for amazon linux 2: use the hostnamectl command to set your hostname to reflect the desired system hostname (such as webserver). for amazon linux ami: on your instance, open the  configuration file in your favorite text editor and change the  entry to reflect the desired system hostname (such as webserver). open the  file in your favorite text editor and change the entry beginning with 127.0.0.1 to match the example below, substituting your own hostname. reboot the instance to pick up the new hostname. alternatively, you can reboot using the amazon ec2 console (on the instances page, choose actions, instance state, reboot). log into your instance and verify that the hostname has been updated. your prompt should show the new hostname (up to the first ".") and the hostname command should show the fully-qualified domain name. if you do not want to modify the hostname for your instance, but you would like to have a more useful system name (such as webserver) displayed than the private name supplied by aws (for example, ), you can edit the shell prompt configuration files to display your system nickname instead of the hostname. to change the shell prompt to a host nickname create a file in  that sets the environment variable called  to the value you want in the shell prompt. for example, to set the system nickname to webserver, run the following command. open the  (red hat) or  (debian/ubuntu) file in your favorite text editor (such as vim or nano). you need to use sudo with the editor command because  and  are owned by . edit the file and change the shell prompt variable () to display your nickname instead of the hostname. find the following line that sets the shell prompt in  or  (several surrounding lines are shown below for context; look for the line that starts with ): change the  (the symbol for ) in that line to the value of the  variable. (optional) to set the title on shell windows to the new nickname, complete the following steps. create a file named . make the file executable using the following command. open the  file in your favorite text editor (such as vim or nano). you need to use sudo with the editor command because  is owned by . add the following line to the file. log out and then log back in to pick up the new nickname value. the procedures on this page are intended for use with amazon linux only. for more information about other linux distributions, see their specific documentation and the following articles: 
by modifying the permissions of a snapshot, you can share it with the aws accounts that you specify. users that you have authorized can use the snapshots you share as the basis for creating their own ebs volumes, while your original snapshot remains unaffected. if you choose, you can make your unencrypted snapshots available publicly to all aws users. you can't make your encrypted snapshots available publicly.  when you share an encrypted snapshot, you must also share the customer managed cmk used to encrypt the snapshot. you can apply cross-account permissions to a customer managed cmk either when it is created or at a later time. importantwhen you share a snapshot, you are giving others access to all of the data on the snapshot. share snapshots only with people with whom you want to share all of your snapshot data. the following considerations apply to sharing snapshots: snapshots are constrained to the region in which they were created. to share a snapshot with another region, copy the snapshot to that region. for more information, see .if your snapshot uses the longer resource id format, you can only share it with another account that also supports longer ids. for more information, see .aws prevents you from sharing snapshots that were encrypted with your default cmk. snapshots that you intend to share must instead be encrypted with a customer managed cmk. for more information, see  in the aws key management service developer guide.users of your shared cmk who are accessing encrypted snapshots must be granted permissions to perform the following actions on the key: , , , and . for more information, see  in the aws key management service developer guide.to share a snapshot using the console open the amazon ec2 console at . choose snapshots in the navigation pane. select the snapshot and then choose actions, modify permissions. make the snapshot public or share it with specific aws accounts as follows: to make the snapshot public, choose public. this option is not valid for encrypted snapshots or snapshots with an aws marketplace product code. to share the snapshot with one or more aws accounts, choose private, enter the aws account id (without hyphens) in aws account number, and choose add permission. repeat for any additional aws accounts.choose save. to use an unencrypted snapshot that was privately shared with you open the amazon ec2 console at . choose snapshots in the navigation pane. choose the private snapshots filter. locate the snapshot by id or description. you can use this snapshot as you would any other; for example, you can create a volume from the snapshot or copy the snapshot to a different region. to share an encrypted snapshot using the console open the aws kms console at . to change the aws region, use the region selector in the upper-right corner of the page. choose customer managed keys in the navigation pane. in the alias column, choose the alias (text link) of the customer managed key that you used to encrypt the snapshot. the key details open in a new page. in the key policy section, you see either the policy view or the default view. the policy view displays the key policy document. the default view displays sections for key administrators, key deletion, key use, and other aws accounts. the default view displays if you created the policy in the console and have not customized it. if the default view is not available, you'll need to manually edit the policy in the policy view. for more information, see  in the aws key management service developer guide. use either the policy view or the default view, depending on which view you can access, to add one or more aws account ids to the policy, as follows: (policy view) choose edit. add one or more aws account ids to the following statements:  and . choose save changes. in the following example, the aws account id  is added to the policy. (default view) scroll down to other aws accounts. choose add other aws accounts and enter the aws account id as prompted. to add another account, choose add another aws account and enter the aws account id. when you have added all aws accounts, choose save changes.open the amazon ec2 console at . choose snapshots in the navigation pane. select the snapshot and then choose actions, modify permissions. for each aws account, enter the aws account id in aws account number and choose add permission. when you have added all aws accounts, choose save. to use an encrypted snapshot that was shared with you open the amazon ec2 console at . choose snapshots in the navigation pane. choose the private snapshots filter. optionally add the encrypted filter. locate the snapshot by id or description. select the snapshot and choose actions, copy. (optional) select a destination region. the copy of the snapshot is encrypted by the key displayed in master key. by default, the selected key is your account's default cmk. to select a customer managed cmk, click inside the input box to see a list of available keys. choose copy. the permissions for a snapshot are specified using the  attribute of the snapshot. to make a snapshot public, set the group to . to share a snapshot with a specific aws account, set the user to the id of the aws account. to modify snapshot permissions using the command line use one of the following commands:  (aws cli) (aws tools for windows powershell)to view snapshot permissions using the command line use one of the following commands:  (aws cli) (aws tools for windows powershell)for more information about these command line interfaces, see . 
an instance is a virtual server in the aws cloud. you launch an instance from an amazon machine image (ami). the ami provides the operating system, application server, and applications for your instance. when you sign up for aws, you can get started with amazon ec2 for free using the . you can use the free tier to launch and use a  instance for free for 12 months (in regions where  is unavailable, you can use a  instance under the free tier). if you launch an instance that is not within the free tier, you incur the standard amazon ec2 usage fees for the instance. for more information, see the . you can launch an instance using the following methods. when you launch your instance, you can launch your instance in a subnet that is associated with one of the following resources: an availability zone - this option is the default.a local zone - to launch an instance in a local zone, you must opt in to the feature. for more information, see .an outpost - to launch an instance in an outpost, you must create an outpost. for information about how to create an outpost, see  in the aws outposts user guide.after you launch your instance, you can connect to it and use it. to begin, the instance state is . when the instance state is , the instance has started booting. there might be a short time before you can connect to the instance. note that bare metal instance types might take longer to launch. for more information about bare metal instances, see . the instance receives a public dns name that you can use to contact the instance from the internet. the instance also receives a private dns name that other instances within the same vpc can use to contact the instance. for more information about connecting to your instance, see . when you are finished with an instance, be sure to terminate it. for more information, see . 
reserved instances provide you with significant savings on your amazon ec2 costs compared to on-demand instance pricing. reserved instances are not physical instances, but rather a billing discount applied to the use of on-demand instances in your account. these on-demand instances must match certain attributes, such as instance type and region, in order to benefit from the billing discount. savings plans also offer significant savings on your amazon ec2 costs compared to on-demand instance pricing. with savings plans, you make a commitment to a consistent usage amount, measured in usd per hour. this provides you with the flexibility to use the instance configurations that best meet your needs and continue to save money, instead of making a commitment to a specific instance configuration. for more information, see the . the following diagram shows a basic overview of purchasing and using reserved instances.  in this scenario, you have a running on-demand instance (t2) in your account, for which you're currently paying on-demand rates. you purchase a reserved instance that matches the attributes of your running instance, and the billing benefit is immediately applied. next, you purchase a reserved instance for a c4 instance. you do not have any running instances in your account that match the attributes of this reserved instance. in the final step, you launch an instance that matches the attributes of the c4 reserved instance, and the billing benefit is immediately applied. the reserved instance pricing is determined by the following key variables. a reserved instance has four instance attributes that determine its price.  instance type: for example, . this is composed of the instance family (for example, ) and the instance size (for example, ).region: the region in which the reserved instance is purchased.tenancy: whether your instance runs on shared (default) or single-tenant (dedicated) hardware. for more information, see . platform: the operating system; for example, windows or linux/unix. for more information, see .you can purchase a reserved instance for a one-year or three-year commitment, with the three-year commitment offering a bigger discount. one-year: a year is defined as 31536000 seconds (365 days). three-year: three years is defined as 94608000 seconds (1095 days).reserved instances do not renew automatically; when they expire, you can continue using the ec2 instance without interruption, but you are charged on-demand rates. in the above example, when the reserved instances that cover the t2 and c4 instances expire, you go back to paying the on-demand rates until you terminate the instances or purchase new reserved instances that match the instance attributes. the following payment options are available for reserved instances: all upfront: full payment is made at the start of the term, with no other costs or additional hourly charges incurred for the remainder of the term, regardless of hours used.partial upfront: a portion of the cost must be paid upfront and the remaining hours in the term are billed at a discounted hourly rate, regardless of whether the reserved instance is being used.no upfront: you are billed a discounted hourly rate for every hour within the term, regardless of whether the reserved instance is being used. no upfront payment is required. noteno upfront reserved instances are based on a contractual obligation to pay monthly for the entire term of the reservation. for this reason, a successful billing history is required before you can purchase no upfront reserved instances.generally speaking, you can save more money making a higher upfront payment for reserved instances. you can also find reserved instances offered by third-party sellers at lower prices and shorter term lengths on the reserved instance marketplace. for more information, see .  if your computing needs change, you may be able to modify or exchange your reserved instance, depending on the offering class. standard: these provide the most significant discount, but can only be modified.convertible: these provide a lower discount than standard reserved instances, but can be exchanged for another convertible reserved instance with different instance attributes. convertible reserved instances can also be modified.for more information, see . after you purchase a reserved instance, you cannot cancel your purchase. however, you may be able to , , or  your reserved instance if your needs change. for more information, see the . there is a limit to the number of reserved instances that you can purchase per month. for each region you can purchase 20  reserved instances per month plus an additional 20  reserved instances per month for each availability zone. for example, in a region with three availability zones, the limit is 80 reserved instances per month: 20 regional reserved instances for the region plus 20 zonal reserved instances for each of the three availability zones (20x3=60). a regional reserved instance applies a discount to a running on-demand instance. the default on-demand instance limit is 20. you cannot exceed your running on-demand instance limit by purchasing regional reserved instances. for example, if you already have 20 running on-demand instances, and you purchase 20 regional reserved instances, the 20 regional reserved instances are used to apply a discount to the 20 running on-demand instances. if you purchase more regional reserved instances, you will not be able to launch more instances because you have reached your on-demand instance limit. before purchasing regional reserved instances, make sure your on-demand instance limit matches or exceeds the number of regional reserved instances you intend to own. if required, make sure you request an increase to your on-demand instance limit before purchasing more regional reserved instances. a zonal reserved instance—a reserved instance that is purchased for a specific availability zone— provides capacity reservation as well as a discount. you can exceed your running on-demand instance limit by purchasing zonal reserved instances. for example, if you already have 20 running on-demand instances, and you purchase 20 zonal reserved instances, you can launch a further 20 on-demand instances that match the specifications of your zonal reserved instances, giving you a total of 40 running instances. the amazon ec2 console provides limit information. for more information, see . 
amazon ec2 spot instances are spare ec2 compute capacity in the aws cloud that are available to you at savings of up to 90% off compared to on-demand prices. the only difference between on-demand instances and spot instances is that spot instances can be interrupted by amazon ec2, with two minutes of notification, when amazon ec2 needs the capacity back. spot instances are recommended for stateless, fault-tolerant, flexible applications. for example, spot instances work well for big data, containerized workloads, ci/cd, stateless web servers, high performance computing (hpc), and rendering workloads. while running, spot instances are exactly the same as on-demand instances. however, spot does not guarantee that you can keep your running instances long enough to finish your workloads. spot also does not guarantee that you can get immediate availability of the instances that you are looking for, or that you can always get the aggregate capacity that you requested. moreover, spot instance interruptions and capacity can change over time because spot instance availability varies based on supply and demand, and past performance isn’t a guarantee of future results. spot instances are not suitable for workloads that are inflexible, stateful, fault-intolerant, or tightly coupled between instance nodes. it's also not recommended for workloads that are intolerant of occasional periods when the target capacity is not completely available. we strongly warn against using spot instances for these workloads or attempting to fail-over to on-demand instances to handle interruptions. regardless of whether you're an experienced spot user or new to spot instances, if you are currently experiencing issues with spot instance interruptions or availability, we recommend that you follow these best practices to have the best experience using the spot service. topics the best way for you to gracefully handle spot instance interruptions is to architect your application to be fault-tolerant. to accomplish this, you can take advantage of spot instance interruption notices. a spot instance interruption notice is a warning that is issued two minutes before amazon ec2 interrupts a spot instance. we recommend that you create a rule in  that captures the interruption notification, and then triggers a checkpoint for the progress of your workload or gracefully handles the interruption. for a detailed example that walks you through how to create and use event rules, see . if your workload is "time-flexible," you can also configure your spot instances to be stopped or hibernated when they are interrupted. amazon ec2 automatically stops or hibernates your spot instances on interruption, and automatically resumes the instances when we have available capacity. for more information, see . a spot instance pool is a set of unused ec2 instances with the same instance type (for example, m5.large) and availability zone (for example, us-east-1a). you should be flexible about which instance types you request and in which availability zones you can deploy your workload. this gives spot a better chance to find and allocate your required amount of compute capacity. for example, don't just ask for  if you'd be willing to use larges from the c4, m5, and m4 families. depending on your specific needs, you can evaluate which instance types you can be flexible across to fulfill your compute requirements. if a workload can be vertically scaled, you should include larger instance types (more vcpus and memory) in your requests. if you can only scale horizontally, you should include older generation instance types because they are less in demand from on-demand customers. a good rule of thumb is to be flexible across at least 10 instance types for each workload. in addition, make sure that all availability zones are configured for use in your vpc and selected for your workload. spot enables you to think in terms of aggregate capacity—in units that include vcpus, memory, storage, or network throughput—rather than thinking in terms of individual instances. auto scaling groups and spot fleet enable you to launch and maintain a target capacity, and to automatically request resources to replace any that are disrupted or manually terminated. when you configure an auto scaling group or a spot fleet, you need only specify the instance types and target capacity based on your application needs. for more information, see  in the amazon ec2 auto scaling user guide and  in this user guide. allocation strategies in auto scaling groups help you to provision your target capacity without the need to manually look for the spot instance pools with spare capacity. we recommend using the  strategy because this strategy automatically provisions instances from the most-available spot instance pools. you can also take advantage of the  allocation strategy in spot fleet. because your spot instance capacity is sourced from pools with optimal capacity, this decreases the possibility that your spot instances are reclaimed. for more information about allocation strategies, see  in the amazon ec2 auto scaling user guide and  in this user guide. other aws services integrate with spot to reduce overall compute costs without the need to manage the individual instances or fleets. we recommend that you consider the following solutions for your applicable workloads: amazon emr, amazon ecs, aws batch, amazon eks, amazon sagemaker, aws elastic beanstalk, and amazon gamelift. to learn more about spot best practices with these services, see the . 
to purchase a reserved instance, search for reserved instance offerings from aws and third-party sellers, adjusting your search parameters until you find the exact match that you're looking for. when you search for reserved instances to buy, you receive a quote on the cost of the returned offerings. when you proceed with the purchase, aws automatically places a limit price on the purchase price. the total cost of your reserved instances won't exceed the amount that you were quoted. if the price rises or changes for any reason, the purchase is not completed. if, at the time of purchase, there are offerings similar to your choice but at a lower price, aws sells you the offerings at the lower price. before you confirm your purchase, review the details of the reserved instance that you plan to buy, and make sure that all the parameters are accurate. after you purchase a reserved instance (either from a third-party seller in the reserved instance marketplace or from aws), you cannot cancel your purchase. noteto purchase and modify reserved instances, ensure that your iam user account has the appropriate permissions, such as the ability to describe availability zones. for information, see  and . topics amazon ec2 supports the following linux platforms for reserved instances: linux/unixlinux with sql server standardlinux with sql server weblinux with sql server enterprisesuse linuxred hat enterprise linuxwhen you purchase a reserved instance, you must choose an offering for a platform that represents the operating system for your instance. for suse linux and rhel distributions, you must choose offerings for those specific platforms, i.e., for the suse linux or red hat enterprise linux platforms.for all other linux distributions (including ubuntu), choose an offering for the linux/unix platform.if you bring your existing rhel subscription, you must choose an offering for the linux/unix platform, not an offering for the red hat enterprise linux platform.importantif you purchase a reserved instance to apply to an on-demand instance that was launched from an ami with a billing product code, make sure that the reserved instance has the matching billing product code. if you purchase a reserved instance without the matching billing product code, the reserved instance will not be applied to the on-demand instance. for more information about how to obtain the ami billing code, see . if you plan to purchase a reserved instance to apply to an on-demand instance that was launched from an aws marketplace ami, first check the  field of the ami. the  field indicates which reserved instance to purchase. the platform details of the ami must match the platform of the reserved instance, otherwise the reserved instance will not be applied to the on-demand instance. for information about how to view the platform details of the ami, see . for information about the supported platforms for windows, see  in the amazon ec2 user guide for windows instances. by default, when you purchase a reserved instance, it is executed immediately. alternatively, you can queue your purchases for a future date and time. for example, you can queue a purchase for around the time that an existing reserved instance expires. this can help you ensure that you have uninterrupted coverage. you can queue purchases for regional reserved instances, but not zonal reserved instances or reserved instances from other sellers. you can queue a purchase up to three years in advance. on the scheduled date and time, the purchase is executed using the default payment method. after the payment is successful, the billing benefit is applied. you can view your queued purchases in the amazon ec2 console. the status of a queued purchase is queued. you can cancel a queued purchase any time before its scheduled time. for details, see . you can buy standard reserved instances in a specific availability zone and get a capacity reservation. alternatively, you can forego the capacity reservation and purchase a regional standard reserved instance. to buy standard reserved instances using the console open the amazon ec2 console at . in the navigation pane, choose reserved instances, and then choose purchase reserved instances. for offering class, choose standard to display standard reserved instances. to purchase a capacity reservation, choose only show offerings that reserve capacity in the top-right corner of the purchase screen. to purchase a regional reserved instance, leave the check box unselected. select other configurations as needed and choose search. to purchase a standard reserved instance from the reserved instance marketplace, look for 3rd party in the seller column in the search results. the term column displays non-standard terms. select the reserved instances to purchase, enter the quantity, and choose add to cart. to see a summary of the reserved instances that you selected, choose view cart. if order on is now, the purchase is completed immediately. to queue a purchase, choose now and select a date. you can select a different date for each eligible offering in the cart. the purchase is queued until 00:00, in the time zone of your browser, on the selected date.  to complete the order, choose order. if, at the time of placing the order, there are offerings similar to your choice but with a lower price, aws sells you the offerings at the lower price. the status of your order is listed in the state column. when your order is complete, the state value changes from  to . when the reserved instance is , it is ready to use. noteif the status goes to , aws may not have received your payment.  to buy a standard reserved instance using the aws cli find available reserved instances using the  command. specify  for the  parameter to return only standard reserved instances. you can apply additional parameters to narrow your results. for example, if you want to purchase a regional  reserved instance with a default tenancy for  for a 1-year term only: to find reserved instances on the reserved instance marketplace only, use the  filter and do not specify a duration in the request, as the term may be shorter than a 1– or 3-year term. when you find a reserved instance that meets your needs, take note of the offering id. for example: use the  command to buy your reserved instance. you must specify the reserved instance offering id you obtained the previous step and you must specify the number of instances for the reservation. by default, the purchase is completed immediately. alternatively, to queue the purchase, add the following parameter to the previous call. use the  command to get the status of your reserved instance. alternatively, use the following aws tools for windows powershell commands: after the purchase is complete, if you already have a running instance that matches the specifications of the reserved instance, the billing benefit is immediately applied. you do not have to restart your instances. if you do not have a suitable running instance, launch an instance and ensure that you match the same criteria that you specified for your reserved instance. for more information, see .  for examples of how reserved instances are applied to your running instances, see . you can buy convertible reserved instances in a specific availability zone and get a capacity reservation. alternatively, you can forego the capacity reservation and purchase a regional convertible reserved instance. to buy convertible reserved instances using the console open the amazon ec2 console at . in the navigation pane, choose reserved instances, and then choose purchase reserved instances. for offering class, choose convertible to display convertible reserved instances. to purchase a capacity reservation, choose only show offerings that reserve capacity in the top-right corner of the purchase screen. to purchase a regional reserved instance, leave the check box unselected. select other configurations as needed and choose search. select the convertible reserved instances to purchase, enter the quantity, and choose add to cart. to see a summary of your selection, choose view cart. if order on is now, the purchase is completed immediately. to queue a purchase, choose now and select a date. you can select a different date for each eligible offering in the cart. the purchase is queued until 00:00, in the time zone of your browser, on the selected date.  to complete the order, choose order. if, at the time of placing the order, there are offerings similar to your choice but with a lower price, aws sells you the offerings at the lower price. the status of your order is listed in the state column. when your order is complete, the state value changes from  to . when the reserved instance is , it is ready to use. noteif the status goes to , aws may not have received your payment.  to buy a convertible reserved instance using the aws cli find available reserved instances using the  command. specify  for the  parameter to return only convertible reserved instances. you can apply additional parameters to narrow your results; for example, if you want to purchase a regional  reserved instance with a default tenancy for : when you find a reserved instance that meets your needs, take note of the offering id. for example: use the  command to buy your reserved instance. you must specify the reserved instance offering id you obtained the previous step and you must specify the number of instances for the reservation. by default, the purchase is completed immediately. alternatively, to queue the purchase, add the following parameter to the previous call. use the  command to get the status of your reserved instance. alternatively, use the following aws tools for windows powershell commands: if you already have a running instance that matches the specifications of the reserved instance, the billing benefit is immediately applied. you do not have to restart your instances. if you do not have a suitable running instance, launch an instance and ensure that you match the same criteria that you specified for your reserved instance. for more information, see .  for examples of how reserved instances are applied to your running instances, see . you can view the reserved instances you've purchased using the amazon ec2 console, or a command line tool. to view your reserved instances in the console open the amazon ec2 console at . in the navigation pane, choose reserved instances. your active and retired reserved instances are listed. the state column displays the state.  if you are a seller in the reserved instance marketplace the my listings tab displays the status of a reservation that's listed in the . for more information, see . to view your reserved instances using the command line  (aws cli) (tools for windows powershell)you can queue a purchase up to three years in advance. you can cancel a queued purchase any time before its scheduled time. to cancel a queued purchase open the amazon ec2 console at . in the navigation pane, choose reserved instances. select one or more reserved instances. choose actions, delete queued reserved instances. when prompted for confirmation, choose yes, delete. to cancel a queued purchase using the command line  (aws cli) (tools for windows powershell)you can renew a reserved instance before it is scheduled to expire. renewing a reserved instance queues the purchase of a reserved instance with the same configuration until the current reserved instance expires. to renew an reserved instance using a queued purchase open the amazon ec2 console at . in the navigation pane, choose reserved instances. select one or more reserved instances. choose actions, renew reserved instances. to complete the order, choose order. reserved instances are automatically applied to running on-demand instances provided that the specifications match. if you have no running on-demand instances that match the specifications of your reserved instance, the reserved instance is unused until you launch an instance with the required specifications.  if you're launching an instance to take advantage of the billing benefit of a reserved instance, ensure that you specify the following information during launch: platform: you must choose an amazon machine image (ami) that matches the platform (product description) of your reserved instance. for example, if you specified , you can launch an instance from an amazon linux ami or an ubuntu ami.instance type: specify the same instance type as your reserved instance; for example, .availability zone: if you purchased a reserved instance for a specific availability zone, you must launch the instance into the same availability zone. if you purchased a regional reserved instance, you can launch your instance into any availability zone.tenancy: the tenancy of your instance must match the tenancy of the reserved instance; for example,  or . for more information, see .for more information, see . for examples of how reserved instances are applied to your running instances, see . you can use amazon ec2 auto scaling or other aws services to launch the on-demand instances that use your reserved instance benefits. for more information, see the . 
you can test the performance of amazon ebs volumes by simulating i/o workloads. the process is as follows: launch an ebs-optimized instance. create new ebs volumes. attach the volumes to your ebs-optimized instance. configure and mount the block device. install a tool to benchmark i/o performance. benchmark the i/o performance of your volumes. delete your volumes and terminate your instance so that you don't continue to incur charges. importantsome of the procedures result in the destruction of existing data on the ebs volumes you benchmark. the benchmarking procedures are intended for use on volumes specially created for testing purposes, not production volumes. to get optimal performance from ebs volumes, we recommend that you use an ebs-optimized instance. ebs-optimized instances deliver dedicated throughput between amazon ec2 and amazon ebs, with instance. ebs-optimized instances deliver dedicated bandwidth between amazon ec2 and amazon ebs, with specifications depending on the instance type. for more information, see . to create an ebs-optimized instance, choose launch as an ebs-optimized instance when launching the instance using the amazon ec2 console, or specify --ebs-optimized when using the command line. be sure that you launch a current-generation instance that supports this option. for more information, see . to create an  volume, choose provisioned iops ssd when creating the volume using the amazon ec2 console, or, at the command line, specify --type io1 --iops n where n is an integer between 100 and 64,000. for more detailed ebs-volume specifications, see . for information about creating an ebs volume, see . for information about attaching a volume to an instance, see . for the example tests, we recommend that you create a raid array with 6 volumes, which offers a high level of performance. because you are charged by gigabytes provisioned (and the number of provisioned iops for  volumes), not the number of volumes, there is no additional cost for creating multiple, smaller volumes and using them to create a stripe set. if you're using oracle orion to benchmark your volumes, it can simulate striping the same way that oracle asm does, so we recommend that you let orion do the striping. if you are using a different benchmarking tool, you need to stripe the volumes yourself. to create a six-volume stripe set on amazon linux, use a command such as the following: for this example, the file system is xfs. use the file system that meets your requirements. use the following command to install xfs file system support: then, use these commands to create, mount, and assign ownership to the xfs file system: to create an  volume, choose throughput optimized hdd when creating the volume using the amazon ec2 console, or specify --type  when using the command line. to create an  volume, choose cold hdd when creating the volume using the amazon ec2 console, or specify --type  when using the command line. for information about creating ebs volumes, see . for information about attaching these volumes to your instance, see . aws provides a json template for use with aws cloudformation that simplifies this setup procedure. access the  and save it as a json file. aws cloudformation allows you to configure your own ssh keys and offers an easier way to set up a performance test environment to evaluate  volumes. the template creates a current-generation instance and a 2 tib  volume, and attaches the volume to the instance at .  to create an hdd volume using the template open the aws cloudformation console at . choose create stack. choose upload a template to amazon s3 and select the json template you previously obtained. give your stack a name like “ebs-perf-testing”, and select an instance type (the default is r3.8xlarge) and ssh key. choose next twice, and then choose create stack. after the status for your new stack moves from create_in_progress to complete, choose outputs to get the public dns entry for your new instance, which will have a 2 tib  volume attached to it. connect using ssh to your new stack as user ec2-user, with the hostname obtained from the dns entry in the previous step.  proceed to . the following table lists some of the possible tools you can use to benchmark the performance of ebs volumes. these benchmarking tools support a wide variety of test parameters. you should use commands that approximate the workloads your volumes will support. these commands provided below are intended as examples to help you get started. choosing the best volume queue length based on your workload and volume type. to determine the optimal queue length for your workload on ssd-backed volumes, we recommend that you target a queue length of 1 for every 1000 iops available (baseline for  volumes and the provisioned amount for  volumes). then you can monitor your application performance and tune that value based on your application requirements. increasing the queue length is beneficial until you achieve the provisioned iops, throughput or optimal system queue length value, which is currently set to 32. for example, a volume with 3,000 provisioned iops should target a queue length of 3. you should experiment with tuning these values up or down to see what performs best for your application. to determine the optimal queue length for your workload on hdd-backed volumes, we recommend that you target a queue length of at least 4 while performing 1mib sequential i/os. then you can monitor your application performance and tune that value based on your application requirements. for example, a 2 tib  volume with burst throughput of 500 mib/s and iops of 500 should target a queue length of 4, 8, or 16 while performing 1,024 kib, 512 kib, or 256 kib sequential i/os respectively. you should experiment with tuning these values value up or down to see what performs best for your application. before you run benchmarking, you should disable processor c-states. temporarily idle cores in a supported cpu can enter a c-state to save power. when the core is called on to resume processing, a certain amount of time passes until the core is again fully operational. this latency can interfere with processor benchmarking routines. for more information about c-states and which ec2 instance types support them, see . you can disable c-states on amazon linux, rhel, and centos as follows: get the number of c-states. disable the c-states from c1 to cn. ideally, the cores should be in state c0. the following procedures describe benchmarking commands for various ebs volume types.  run the following commands on an ebs-optimized instance with attached ebs volumes. if the ebs volumes were created from snapshots, be sure to initialize them before benchmarking. for more information, see . when you are finished testing your volumes, see the following topics for help cleaning up:  and . run fio on the stripe set that you created. the following command performs 16 kb random write operations. the following command performs 16 kb random read operations. for more information about interpreting the results, see this tutorial: . run fio on your  or  volume. noteprior to running these tests, set buffered i/o on your instance as described in .  the following command performs 1 mib sequential read operations against an attached  block device (e.g., ): the following command performs 1 mib sequential write operations against an attached  block device: some workloads perform a mix of sequential reads and sequential writes to different parts of the block device. to benchmark such a workload, we recommend that you use separate, simultaneous fio jobs for reads and writes, and use the fio  option to target different block device locations for each job.  running this workload is a bit more complicated than a sequential-write or sequential-read workload. use a text editor to create a fio job file, called  in this example, that contains the following: then run the following command: for more information about interpreting the results, see this tutorial: . multiple fio jobs for direct i/o, even though using sequential read or write operations, can result in lower than expected throughput for  and  volumes. we recommend that you use one direct i/o job and use the  parameter to control the number of concurrent i/o operations. 
this topic explains how to verify the instance identity document using the base64-encoded signature and the aws rsa public certificate. importantto validate the instance identity document using the base64-encoded signature, you must request the aws rsa public certificate from .  to validate the instance identity document using the base64-encoded signature and the aws rsa public certificate connect to the instance. retrieve the base64-encoded signature from the instance metadata, convert it to binary, and add it to a file named . use one of the following commands depending on the imds version used by the instance. retrieve the plaintext instance identity document from the instance metadata and add it to a file named . use one of the following commands depending on the imds version used by the instance.add the aws rsa public certificate that you received from aws support to a file named . extract the public key from the certificate that you received from aws support and save it to a file named . use openssl dgst command to verify the instance identity document. if the signature is valid, the  message appears. if the signature cannot be verified, contact aws support. 
amazon ec2 is hosted in multiple locations world-wide. these locations are composed of regions, availability zones, and local zones. each region is a separate geographic area. each region has multiple, isolated locations known as availability zones. local zones provide you the ability to place resources, such as compute and storage, in multiple locations closer to your end users. resources aren't replicated across regions unless you specifically choose to do so. amazon operates state-of-the-art, highly available data centers. although rare, failures can occur that affect the availability of instances that are in the same location. if you host all of your instances in a single location that is affected by a failure, none of your instances would be available. topics each region is completely independent. each availability zone is isolated, but the availability zones in a region are connected through low-latency links. a local zone is an aws infrastructure deployment that places select services closer to your end users. a local zone is an extension of a region that is in a different location from your region. it provides a high-bandwidth backbone to the aws infrastructure and is ideal for latency-sensitive applications, for example machine learning. the following diagram illustrates the relationship between regions, availability zones, and local zones.  amazon ec2 resources are one of the following: global, or tied to a region, an availability zone, or a local zone. for more information, see . topics each amazon ec2 region is designed to be isolated from the other amazon ec2 regions. this achieves the greatest possible fault tolerance and stability. when you view your resources, you see only the resources that are tied to the region that you specified. this is because regions are isolated from each other, and we don't automatically replicate resources across regions. when you launch an instance, you must select an ami that's in the same region. if the ami is in another region, you can copy the ami to the region you're using. for more information, see . note that there is a charge for data transfer between regions. for more information, see . an availability zone (az) is one or more discrete data centers with redundant power, networking, and connectivity in an aws region. availability zones allow you to operate production applications and databases that are more highly available, fault tolerant, and scalable than would be possible from a single data center. all availability zones in an aws region are interconnected with high-bandwidth, low-latency networking, over fully redundant, dedicated metro fiber providing high-throughput, low-latency networking between availability zones. all traffic between availability zones is encrypted. the network performance is sufficient to accomplish synchronous replication between availability zones. availability zones make it easier to partition applications for high availability. if an application is partitioned across availability zones, the application is better isolated and protected from issues such as power outages, lightning strikes, tornadoes, earthquakes, and more. availability zones are physically separated by a meaningful distance from any other availability zone, although all are within 100 km (60 miles) of each other. when you launch an instance, you can select an availability zone or let us choose one for you. if you distribute your instances across multiple availability zones and one instance fails, you can design your application so that an instance in another availability zone can handle requests. you can also use elastic ip addresses to mask the failure of an instance in one availability zone by rapidly remapping the address to an instance in another availability zone. for more information, see .  an availability zone is represented by a region code followed by a letter identifier; for example, . to ensure that resources are distributed across the availability zones for a region, we independently map availability zones to names for each aws account. for example, the availability zone  for your aws account might not be the same location as  for another aws account. to coordinate availability zones across accounts, you must use the az id, which is a unique and consistent identifier for an availability zone. for example,  is an az id for the  region and it has the same location in every aws account. viewing az ids enables you to determine the location of resources in one account relative to the resources in another account. for example, if you share a subnet in the availability zone with the az id  with another account, this subnet is available to that account in the availability zone whose az id is also . the az id for each vpc and subnet is displayed in the amazon vpc console. for more information, see  in the amazon vpc user guide. as availability zones grow over time, our ability to expand them can become constrained. if this happens, we might restrict you from launching an instance in a constrained availability zone unless you already have an instance in that availability zone. eventually, we might also remove the constrained availability zone from the list of availability zones for new accounts. therefore, your account might have a different number of available availability zones in a region than another account.  you can list the availability zones that are available to your account. for more information, see . a local zone is an extension of an aws region in geographic proximity to your users. when you launch an instance, you can select a subnet in a local zone. local zones have their own connections to the internet and support aws direct connect, so resources created in a local zone can serve local users with very low-latency communications. for more information, see . a local zone is represented by a region code followed by an identifier that indicates the location, for example, . to use a local zone, you must first enable it. for more information, see . next, create a subnet in the local zone. finally, launch any of the following resources in the local zone subnet, so that your applications are closer to your end users: amazon ec2 instancesamazon ebs volumesamazon fsx file serversapplication load balancerdedicated hostslocal zones are not available in every region. for information about the regions that support local zones, see . you can list the local zones that are available to your account. for more information, see . a network border group is a unique set of availability zones or local zones from where aws advertises ip addresses. you can allocate the following resources from a network border group: elastic ipv4 addresses that amazon providesipv6 amazon-provided vpc addressesa network border group limits the addresses to the group. ip addresses cannot move between network border groups. your account determines the regions that are available to you. for example: an aws account provides multiple regions so that you can launch amazon ec2 instances in locations that meet your requirements. for example, you might want to launch instances in europe to be closer to your european customers or to meet legal requirements.an aws govcloud (us-west) account provides access to the aws govcloud (us-west) region only. for more information, see .an amazon aws (china) account provides access to the beijing and ningxia regions only. for more information, see .the following table lists the regions provided by an aws account. you can't describe or access additional regions from an aws account, such as aws govcloud (us-west) or the china regions. to use a region introduced after march 20, 2019, you must enable the region. for more information, see  in the aws general reference. for more information, see . the number and mapping of availability zones per region may vary between aws accounts. to get a list of the availability zones that are available to your account, you can use the amazon ec2 console or the command line interface. for more information, see . when you work with an instance using the command line interface or api actions, you must specify its regional endpoint. for more information about the regions and endpoints for amazon ec2, see  in the amazon web services general reference. for more information about endpoints and protocols in aws govcloud (us-west), see  in the aws govcloud (us) user guide. you can use the amazon ec2 console or the command line interface to determine which regions, availability zones, and local zones are available for your account. for more information about these command line interfaces, see . to find your regions, availability zones, and local zones using the console open the amazon ec2 console at . from the navigation bar, view the options in the region selector. on the navigation pane, choose ec2 dashboard. the availability zones and local zones are listed under service health, availability zone status. to find your regions, availability zones, and local zones using the aws cli use the  command as follows to describe the regions that are enabled for your account. to describe all regions, including any regions that are disabled for your account, add the  option as follows. use the  command as follows to describe the availability zones and local zones within the specified region. use the  command as follows to describe the availability zones and local zones regardless of the opt-in status. to find your regions, availability zones, and local zones using the aws tools for windows powershell use the  command as follows to describe the regions for your account. use the  command as follows to describe the availability zones within the specified region. every time you create an amazon ec2 resource, you can specify the region for the resource. you can specify the region for a resource using the aws management console or the command line. notesome aws resources might not be available in all regions, availability zones, and local zones. ensure that you can create the resources that you need in the desired regions or availability zones before launching an instance in a specific availability zone. to specify the region for a resource using the console open the amazon ec2 console at . use the region selector in the navigation bar. to specify the default region using the command line you can set the value of an environment variable to the desired regional endpoint (for example, ):  (aws cli) (aws tools for windows powershell)alternatively, you can use the  (aws cli) or  (aws tools for windows powershell) command line option with each individual command. for example, . for more information about the endpoints for amazon ec2, see . before you specify a local zone for a resource or service, you must enable the zone. you can enable local zones using the aws management console or the aws cli. notewe enable all availability zones by default and you cannot disable them. some aws resources might not be available in all regions. make sure that you can create the resources that you need in the desired regions or local zones before launching an instance in a specific local zone. to enable local zones using the console open the amazon ec2 console at . use the region selector in the navigation bar, and select your region. on the navigation pane, choose ec2 dashboard. under account attributes (in the upper right-hand corner of the page), settings, choose zones. notethis option is only available if new ec2 experience is enabled in the console. under local zone groups, turn on each local zone that you want to enable. in the enable confirmation dialog box, enter enable, and then choose ok. to enable local zones using the aws cli use the  command.you must contact  to disable a local zone. importantremove all resources before you disable a local zone. any resources that are left in the local zone incur charges. after you remove the resources, create a case with  with the title disable zone group.  when you launch an instance, select a region that puts your instances closer to specific customers, or meets the legal or other requirements that you have. by launching your instances in separate availability zones, you can protect your applications from the failure of a single location.  by launching an instance in a local zone, you can run latency-sensitive applications close to your end users while having the benefits of the aws infrastructure. when you launch an instance, you can optionally specify an availability zone or local zone in the region that you are using. if you do not specify an availability zone or local zone, we select an availability zone for you. when you launch your initial instances, we recommend that you accept the default availability zone, because this enables us to select the best availability zone for you based on system health and available capacity. if you launch additional instances, specify an availability zone only if your new instances must be close to, or separated from, your running instances. if necessary, you can migrate an instance from one availability zone to another. for example, let's say you are trying to modify the instance type of your instance and we can't launch an instance of the new instance type in the current availability zone. in this case, you can migrate the instance to an availability zone where we are able to launch an instance of that instance type. the migration process involves: creating an ami from the original instancelaunching an instance in the new availability zoneupdating the configuration of the new instance, as shown in the following procedureto migrate an instance to another availability zone create an ami from the instance. the procedure depends on your operating system and the type of root device volume for the instance. for more information, see the documentation that corresponds to your operating system and root device volume: if you need to preserve the private ipv4 address of the instance, you must delete the subnet in the current availability zone and then create a subnet in the new availability zone with the same ipv4 address range as the original subnet. note that you must terminate all instances in a subnet before you can delete it. therefore, you should create amis from all of the instances in your subnet so that you can move all instances from the current subnet to the new subnet. launch an instance from the ami that you just created, specifying the new availability zone or subnet. you can use the same instance type as the original instance, or select a new instance type. for more information, see . if the original instance has an associated elastic ip address, associate it with the new instance. for more information, see . if the original instance is a reserved instance, change the availability zone for your reservation. (if you also changed the instance type, you can also change the instance type for your reservation.) for more information, see . (optional) terminate the original instance. for more information, see . 
use the following guidelines to reduce the attack surface and improve the reliability of the amis you create. importantno list of security guidelines can be exhaustive. build your shared amis carefully and take time to consider where you might expose sensitive data. topics if you are building amis for aws marketplace, see  for guidelines, policies and best practices. for additional information about sharing amis safely, see the following articles:   for amis backed by instance store, we recommend that your amis download and upgrade the amazon ec2 ami creation tools before you use them. this ensures that new amis based on your shared amis have the latest ami tools.  for , install the  package and add the ami tools to your path with the following command. for the ,  package is already installed by default. upgrade the ami tools with the following command: for other distributions, make sure you have the latest ami tools. using a fixed root password for a public ami is a security risk that can quickly become known. even relying on users to change the password after the first login opens a small window of opportunity for potential abuse.  to solve this problem, disable password-based remote logins for the root user. to disable password-based remote logins for root open the  file with a text editor and locate the following line: change the line to: the location of this configuration file might differ for your distribution, or if you are not running openssh. if this is the case, consult the relevant documentation.  when you work with shared amis, a best practice is to disable direct root logins. to do this, log into your running instance and issue the following command: notethis command does not impact the use of .  if you plan to share an ami derived from a public ami, remove the existing ssh host key pairs located in . this forces ssh to generate new unique ssh key pairs when someone launches an instance using your ami, improving security and reducing the likelihood of "man-in-the-middle" attacks.  remove all of the following key files that are present on your system. ssh_host_dsa_key ssh_host_dsa_key.pub ssh_host_key ssh_host_key.pub ssh_host_rsa_key ssh_host_rsa_key.pub ssh_host_ecdsa_keyssh_host_ecdsa_key.pubssh_host_ed25519_keyssh_host_ed25519_key.pubyou can securely remove all of these files with the following command. warningsecure deletion utilities such as shred may not remove all copies of a file from your storage media. hidden copies of files may be created by journalling file systems (including amazon linux default ext4), snapshots, backups, raid, and temporary caching. for more information see the shred . importantif you forget to remove the existing ssh host key pairs from your public ami, our routine auditing process notifies you and all customers running instances of your ami of the potential security risk. after a short grace period, we mark the ami private.  after configuring the ami to prevent logging in using a password, you must make sure users can log in using another mechanism.  amazon ec2 allows users to specify a public-private key pair name when launching an instance. when a valid key pair name is provided to the  api call (or through the command line api tools), the public key (the portion of the key pair that amazon ec2 retains on the server after a call to  or ) is made available to the instance through an http query against the instance metadata.  to log in through ssh, your ami must retrieve the key value at boot and append it to  (or the equivalent for any other user account on the ami). users can launch instances of your ami with a key pair and log in without requiring a root password.  many distributions, including amazon linux and ubuntu, use the  package to inject public key credentials for a configured user. if your distribution does not support , you can add the following code to a system start-up script (such as ) to pull in the public key you specified at launch for the root user.  this can be applied to any user account; you do not need to restrict it to .  noterebundling an instance based on this ami includes the key with which it was launched. to prevent the key's inclusion, you must clear out (or delete) the  file or exclude this file from rebundling.  disabling sshd dns checks slightly weakens your sshd security. however, if dns resolution fails, ssh logins still work. if you do not disable sshd checks, dns resolution failures prevent all logins.  to disable sshd dns checks open the  file with a text editor and locate the following line: change the line to:  notethe location of this configuration file can differ for your distribution or if you are not running openssh. if this is the case, consult the relevant documentation.  currently, there is no easy way to know who provided a shared ami, because each ami is represented by an account id.  we recommend that you post a description of your ami, and the ami id, in the . this provides a convenient central location for users who are interested in trying new shared amis. we recommend against storing sensitive data or software on any ami that you share. users who launch a shared ami might be able to rebundle it and register it as their own. follow these guidelines to help you to avoid some easily overlooked security risks:  we recommend using the  option on  to skip any directories and subdirectories that contain secret information that you would not like to include in your bundle. in particular, exclude all user-owned ssh public/private key pairs and ssh  files when bundling the image. the amazon public amis store these in  for the root account, and  for regular user accounts. for more information, see .always delete the shell history before bundling. if you attempt more than one bundle upload in the same ami, the shell history contains your secret access key. the following example should be the last command executed before bundling from within the instance. warningthe limitations of shred described in the warning above apply here as well.be aware that bash writes the history of the current session to the disk on exit. if you log out of your instance after deleting , and then log back in, you will find that  has been re-created and contains all of the commands executed during your previous session.other programs besides bash also write histories to disk, use caution and remove or exclude unnecessary dot-files and dot-directories. bundling a running instance requires your private key and x.509 certificate. put these and other credentials in a location that is not bundled (such as the instance store).
scaling based on a schedule enables you to scale your application in response to predictable changes in demand. to use scheduled scaling, you create scheduled actions, which tell spot fleet to perform scaling activities at specific times. when you create a scheduled action, you specify the spot fleet, when the scaling activity should occur, minimum capacity, and maximum capacity. you can create scheduled actions that scale one time only or that scale on a recurring schedule. limits the spot fleet request must have a request type of . automatic scaling is not supported for one-time requests or spot blocks.to create a one-time scheduled action open the amazon ec2 console at . in the navigation pane, choose spot requests. select your spot fleet request and choose scheduled scaling. choose create scheduled action. for name, specify a name for the scheduled action. enter a value for minimum capacity, maximum capacity, or both. for recurrence, choose once. (optional) choose a date and time for start time, end time, or both. choose submit. to scale on a recurring schedule open the amazon ec2 console at . in the navigation pane, choose spot requests. select your spot fleet request and choose scheduled scaling. for recurrence, choose one of the predefined schedules (for example, every day), or choose custom and type a cron expression. for more information about the cron expressions supported by scheduled scaling, see  in the amazon cloudwatch events user guide. (optional) choose a date and time for start time, end time, or both. choose submit. to edit a scheduled action open the amazon ec2 console at . in the navigation pane, choose spot requests. select your spot fleet request and choose scheduled scaling. select the scheduled action and choose actions, edit. make the needed changes and choose submit. to delete a scheduled action open the amazon ec2 console at . in the navigation pane, choose spot requests. select your spot fleet request and choose scheduled scaling. select the scheduled action and choose actions, delete. when prompted for confirmation, choose delete. to manage scheduled scaling using the aws cli use the following commands: 
amazon ec2 provides enhanced networking capabilities through the intel 82599 vf interface, which uses the intel  driver. topics to prepare for enhanced networking using the intel 82599 vf interface, set up your instance as follows: select from the following supported instance types: c3, c4, d2, i2, m4 (excluding m4.16xlarge), and r3.launch the instance from an hvm ami using linux kernel version of 2.6.32 or later. the latest amazon linux hvm amis have the modules required for enhanced networking installed and have the required attributes set. therefore, if you launch an amazon ebs–backed, enhanced networking–supported instance using a current amazon linux hvm ami, enhanced networking is already enabled for your instance. warningenhanced networking is supported only for hvm instances. enabling enhanced networking with a pv instance can make it unreachable. setting this attribute without the proper module or module version can also make your instance unreachable.ensure that the instance has internet connectivity.install and configure the  or the  on any computer you choose, preferably your local desktop or laptop. for more information, see . enhanced networking cannot be managed from the amazon ec2 console.if you have important data on the instance that you want to preserve, you should back that data up now by creating an ami from your instance. updating kernels and kernel modules, as well as enabling the  attribute, might render incompatible instances or operating systems unreachable. if you have a recent backup, your data will still be retained if this happens.enhanced networking with the intel 82599 vf interface is enabled if the  module is installed on your instance and the  attribute is set.  instance attribute (sriovnetsupport)to check whether an instance has the enhanced networking  attribute set, use one of the following commands:  (aws cli)  (aws tools for windows powershell) if the attribute isn't set,  is empty. if the attribute is set, the value is simple, as shown in the following example output. image attribute (sriovnetsupport)to check whether an ami already has the enhanced networking  attribute set, use one of the following commands:  (aws cli)  (aws tools for windows powershell) if the attribute isn't set,  is empty. if the attribute is set, the value is simple. network interface driveruse the following command to verify that the module is being used on a particular interface, substituting the interface name that you want to check. if you are using a single interface (default), this is . if the operating system supports , this could be a name like . in the following example, the  module is not loaded, because the listed driver is . in this example, the  module is loaded. this instance has enhanced networking properly configured. the latest amazon linux hvm amis have the  module required for enhanced networking installed and have the required  attribute set. therefore, if you launch an instance type using a current amazon linux hvm ami, enhanced networking is already enabled for your instance. for more information, see . if you launched your instance using an older amazon linux ami and it does not have enhanced networking enabled already, use the following procedure to enable enhanced networking. warningthere is no way to disable the enhanced networking attribute after you've enabled it. to enable enhanced networking connect to your instance. from the instance, run the following command to update your instance with the newest kernel and kernel modules, including : from your local computer, reboot your instance using the amazon ec2 console or one of the following commands:  (aws cli),  (aws tools for windows powershell). connect to your instance again and verify that the  module is installed and at the minimum recommended version using the modinfo ixgbevf command from . [ebs-backed instance] from your local computer, stop the instance using the amazon ec2 console or one of the following commands:  (aws cli),  (aws tools for windows powershell). if your instance is managed by aws opsworks, you should stop the instance in the aws opsworks console so that the instance state remains in sync. [instance store-backed instance] you can't stop the instance to modify the attribute. instead, proceed to this procedure: . from your local computer, enable the enhanced networking attribute using one of the following commands:  (aws cli)  (aws tools for windows powershell) (optional) create an ami from the instance, as described in  . the ami inherits the enhanced networking attribute from the instance. therefore, you can use this ami to launch another instance with enhanced networking enabled by default. from your local computer, start the instance using the amazon ec2 console or one of the following commands:  (aws cli),  (aws tools for windows powershell). if your instance is managed by aws opsworks, you should start the instance in the aws opsworks console so that the instance state remains in sync. connect to your instance and verify that the  module is installed and loaded on your network interface using the ethtool -i ethn command from . to enable enhanced networking (instance store-backed instances) follow the previous procedure until the step where you stop the instance. create a new ami as described in , making sure to enable the enhanced networking attribute when you register the ami.  (aws cli)  (aws tools for windows powershell) before you begin,  on your instance. the quick start ubuntu hvm amis include the necessary drivers for enhanced networking. if you have a version of  earlier than 2.16.4, you can install the  kernel package to get the latest enhanced networking drivers. the following procedure provides the general steps for compiling the  module on an ubuntu instance. to install the linux-aws kernel package connect to your instance. update the package cache and packages. importantif during the update process, you are prompted to install , use  to install , and then choose to keep the current version of . before you begin,  on your instance. the latest quick start hvm amis include the necessary drivers for enhanced networking, therefore you do not need to perform additional steps.  the following procedure provides the general steps if you need to enable enhanced networking with the intel 82599 vf interface on a linux distribution other than amazon linux or ubuntu. for more information, such as detailed syntax for commands, file locations, or package and tool support, see the specific documentation for your linux distribution. to enable enhanced networking on linux connect to your instance. download the source for the  module on your instance from sourceforge at . versions of  earlier than 2.16.4, including version 2.14.2, do not build properly on some linux distributions, including certain versions of ubuntu. compile and install the  module on your instance. warningif you compile the  module for your current kernel and then upgrade your kernel without rebuilding the driver for the new kernel, your system might revert to the distribution-specific  module at the next reboot. this could make your system unreachable if the distribution-specific version is incompatible with enhanced networking. run the sudo depmod command to update module dependencies. update  on your instance to ensure that the new module loads at boot time. determine if your system uses predictable network interface names by default. systems that use systemd or udev versions 197 or greater can rename ethernet devices and they do not guarantee that a single network interface will be named . this behavior can cause problems connecting to your instance. for more information and to see other configuration options, see  on the freedesktop.org website. you can check the systemd or udev versions on rpm-based systems with the following command: in the above red hat enterprise linux 7 example, the systemd version is 208, so predictable network interface names must be disabled. disable predictable network interface names by adding the  option to the  line in . rebuild the grub configuration file. [ebs-backed instance] from your local computer, stop the instance using the amazon ec2 console or one of the following commands:  (aws cli),  (aws tools for windows powershell). if your instance is managed by aws opsworks, you should stop the instance in the aws opsworks console so that the instance state remains in sync. [instance store-backed instance] you can't stop the instance to modify the attribute. instead, proceed to this procedure: . from your local computer, enable the enhanced networking attribute using one of the following commands:  (aws cli)  (aws tools for windows powershell) (optional) create an ami from the instance, as described in  . the ami inherits the enhanced networking attribute from the instance. therefore, you can use this ami to launch another instance with enhanced networking enabled by default. importantif your instance operating system contains an  file, you must delete it before creating the ami. this file contains the mac address for the ethernet adapter of the original instance. if another instance boots with this file, the operating system will be unable to find the device and  might fail, causing boot issues. this file is regenerated at the next boot cycle, and any instances launched from the ami create their own version of the file. from your local computer, start the instance using the amazon ec2 console or one of the following commands:  (aws cli),  (aws tools for windows powershell). if your instance is managed by aws opsworks, you should start the instance in the aws opsworks console so that the instance state remains in sync. (optional) connect to your instance and verify that the module is installed. to enable enhanced networking (instance store–backed instances) follow the previous procedure until the step where you stop the instance. create a new ami as described in , making sure to enable the enhanced networking attribute when you register the ami.  (aws cli)  (aws tools for windows powershell) if you lose connectivity while enabling enhanced networking, the  module might be incompatible with the kernel. try installing the version of the  module included with the distribution of linux for your instance. if you enable enhanced networking for a pv instance or ami, this can make your instance unreachable. for more information, see . 
if you're new to amazon ec2, see the following topics to get started: before you launch a production environment, you need to answer the following questions. q. what instance type best meets my needs?amazon ec2 provides different instance types to enable you to choose the cpu, memory, storage, and networking capacity that you need to run your applications. for more information, see . q. what purchasing option best meets my needs?amazon ec2 supports on-demand instances (the default), spot instances, and reserved instances. for more information, see . q. which type of root volume meets my needs?each instance is backed by amazon ebs or backed by instance store. select an ami based on which type of root volume you need. for more information, see . q. can i remotely manage a fleet of ec2 instances and machines in my hybrid environment?aws systems manager enables you to remotely and securely manage the configuration of your amazon ec2 instances, and your on-premises instances and virtual machines (vms) in hybrid environments, including vms from other cloud providers. for more information, see the . 
after you no longer need an amazon ebs snapshot of a volume, you can delete it. deleting a snapshot has no effect on the volume. deleting a volume has no effect on the snapshots made from it. if you make periodic snapshots of a volume, the snapshots are incremental. this means that only the blocks on the device that have changed after your last snapshot are saved in the new snapshot. even though snapshots are saved incrementally, the snapshot deletion process is designed so that you need to retain only the most recent snapshot in order to create volumes. data that was present on a volume, held in an earlier snapshot or series of snapshots, that is subsequently deleted from that volume at a later time, is still considered unique data of the earlier snapshots. this unique data is not deleted from the sequence of snapshots unless all snapshots that reference the unique data are deleted.  when you delete a snapshot, only the data referenced exclusively by that snapshot is removed. unique data will not be deleted unless all of the snapshots that reference that data are deleted. deleting previous snapshots of a volume does not affect your ability to create volumes from later snapshots of that volume. deleting a snapshot might not reduce your organization's data storage costs. other snapshots might reference that snapshot's data, and referenced data is always preserved. if you delete a snapshot containing data being used by a later snapshot, costs associated with the referenced data are allocated to the later snapshot. for more information about how snapshots store data, see  and the following example. in the following diagram, volume 1 is shown at three points in time. a snapshot has captured each of the first two states, and in the third, a snapshot has been deleted.  in state 1, the volume has 10 gib of data. because snap a is the first snapshot taken of the volume, the entire 10 gib of data must be copied.in state 2, the volume still contains 10 gib of data, but 4 gib have changed. snap b needs to copy and store only the 4 gib that changed after snap a was taken. the other 6 gib of unchanged data, which are already copied and stored in snap a, are referenced by snap b rather than (again) copied. this is indicated by the dashed arrow.in state 3, the volume has not changed since state 2, but snapshot a has been deleted. the 6 gib of data stored in snapshot a that were referenced by snapshot b have now been moved to snapshot b, as shown by the heavy arrow. as a result, you are still charged for storing 10 gib of data; 6 gib of unchanged data preserved from snap a and 4 gib of changed data from snap b.deleting a snapshot with some of its data referenced by another snapshot  the following considerations apply to deleting snapshots: you can't delete a snapshot of the root device of an ebs volume used by a registered ami. you must first deregister the ami before you can delete the snapshot. for more information, see .you can't delete a snapshot that is managed by the aws backup service using amazon ec2. instead, use aws backup to delete the corresponding recovery points in the backup vault.you can create, retain, and delete snapshots manually, or you can use amazon data lifecycle manager to manage your snapshots for you. for more information, see .although you can delete a snapshot that is still in progress, the snapshot must complete before the deletion takes effect. this might take a long time. if you are also at your concurrent snapshot limit (five snapshots in progress), and you attempt to take an additional snapshot, you might get a  error.use the following procedure to delete a snapshot. to delete a snapshot using the console open the amazon ec2 console at . choose snapshots in the navigation pane.  select a snapshot and then choose delete from the actions list. choose yes, delete. to delete a snapshot using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)to delete multi-volume snapshots, retrieve all of the snapshots for your multi-volume group using the tag you applied to the group when you created the snapshots. then, delete the snapshots individually. you will not be prevented from deleting individual snapshots in the multi-volume snapshots group. 
when working with instance user data, keep the following in mind: user data must be base64-encoded. the amazon ec2 console can perform the base64-encoding for you or accept base64-encoded input.user data is limited to 16 kb, in raw form, before it is base64-encoded. the size of a string of length n after base64-encoding is ceil(n/3)*4.user data must be base64-decoded when you retrieve it. if you retrieve the data using instance metadata or the console, it's decoded for you automatically.user data is treated as opaque data: what you give is what you get back. it is up to the instance to be able to interpret it.if you stop an instance, modify its user data, and start the instance, the updated user data is not executed when you start the instance.you can specify user data when you launch an instance. for more information, see  and . you can modify user data for an instance in the stopped state if the root volume is an ebs volume. for more information, see . to retrieve user data from within a running instance, use the following uri. a request for user data returns the data as it is (content type ).  this example returns user data that was provided as comma-separated text. this example returns user data that was provided as a script. to retrieve user data for an instance from your own computer, see  
to achieve the maximum network performance on instances with enhanced networking, you may need to modify the default operating system configuration. we recommend the following configuration changes for applications that require high network performance. in addition to these operating system optimizations, you should also consider the maximum transmission unit (mtu) of your network traffic, and adjust according to your workload and network architecture. for more information, see . aws regularly measures average round trip latencies between instances launched in a cluster placement group of 50us and tail latencies of 200us at the 99.9 percentile. if your applications require consistently low latencies, we recommend using the latest version of the ena drivers on fixed performance nitro-based instances. these procedures were written for amazon linux 2 and amazon linux ami. however, they may also work for other linux distributions with kernel version 3.9 or newer. for more information, see your system-specific documentation. to optimize your amazon linux instance for enhanced networking check the clock source for your instance: if the clock source is , complete the following substeps. otherwise, skip to . edit the grub configuration and add  and  to the kernel boot options. for amazon linux 2, edit the  file and add these options to the  line, as shown below: for amazon linux ami, edit the  file and add these options to the  line, as shown below: (amazon linux 2 only) rebuild your grub configuration file to pick up these changes: if your instance type is listed as supported on , prevent the system from using deeper c-states to ensure low-latency system performance. for more information, see . edit the grub configuration and add  to the kernel boot options. for amazon linux 2, edit the  file and add this option to the  line, as shown below: for amazon linux ami, edit the  file and add this option to the  line, as shown below: (amazon linux 2 only) rebuild your grub configuration file to pick up these changes: ensure that your reserved kernel memory is sufficient to sustain a high rate of packet buffer allocations (the default value may be too small). open (as  or with sudo) the  file with the editor of your choice. add the  line to the file with the reserved kernel memory value (in kilobytes) for your instance type. as a rule of thumb, you should set this value to between 1-3% of available system memory, and adjust this value up or down to meet the needs of your application. apply this configuration with the following command: verify that the setting was applied with the following command: reboot your instance to load the new configuration: (optional) manually distribute packet receive interrupts so that they are associated with different cpus that all belong to the same numa node. use this carefully, however, because irqbalancer is disabled globally. notethe configuration change in this step does not survive a reboot. create a file called  and paste the following code block into it: run the script with the following command: (optional) if the vcpus that handle receive irqs are overloaded, or if your application network processing is demanding on cpu, you can offload part of the network processing to other cores with receive packet steering (rps). ensure that cores used for rps belong to the same numa node to avoid inter-numa node locks. for example, to use cores 8-15 for packet processing, use the following command. notethe configuration change in this step does not survive a reboot. (optional) if possible, keep all processing on the same numa node. install numactl: when you run your network processing program, bind it to a single numa node. for example, the following command binds the shell script, , to numa node 0: if you have hyperthreading enabled, you can configure your application to only use a single hardware thread per cpu core. you can view which cpu cores map to a numa node with the lscpu command: output: you can view which hardware threads belong to a physical cpu with the following command: output: in this example, threads 0 and 32 map to cpu 0. to avoid running on threads 32-47 (which are actually hardware threads of the same cpus as 0-15, use the following command: use multiple elastic network interfaces for different classes of traffic. for example, if you are running a web server that uses a backend database, use one elastic network interfaces for the web server front end, and another for the database connection. 
after you launch your instance, you can connect to it and use it the way that you'd use a computer sitting in front of you. the following instructions explain how to connect to your instance using a linux distribution on the windows subsystem for linux (wsl). wsl is a free download and enables you to run native linux command line tools directly on windows, alongside your traditional windows desktop, without the overhead of a virtual machine.  by installing wsl, you can use a native linux environment to connect to your linux ec2 instances instead of using putty or puttygen. the linux environment makes it easier to connect to your linux instances because it comes with a native ssh client that you can use to connect to your linux instances and change the permissions of the .pem key file. the amazon ec2 console provides the ssh command for connecting to the linux instance, and you can get verbose output from the ssh command for troubleshooting. for more information, see the . noteafter you've installed the wsl, all the prerequisites and steps are the same as those described in , and the experience is just like using native linux. if you receive an error while attempting to connect to your instance, see . contents before you connect to your linux instance, complete the following prerequisites. verify that the instance is readyafter you launch an instance, it can take a few minutes for the instance to be ready so that you can connect to it. check that your instance has passed its status checks. you can view this information in the status checks column on the instances page. verify the general prerequisites for connecting to your instancefor more information, see . install the windows subsystem for linux (wsl) and a linux distribution on your local computerinstall the wsl and a linux distribution using the instructions in the . the example in the instructions installs the ubuntu distribution of linux, but you can install any distribution. you are prompted to restart your computer for the changes to take effect. copy the private key from windows to wslin a wsl terminal window, copy the  file (for the key pair that you specified when you launched the instance) from windows to wsl. note the fully-qualified path to the  file on wsl to use when connecting to your instance. for information about how to specify the path to your windows hard drive, see    use the following procedure to connect to your linux instance using the windows subsystem for linux (wsl). if you receive an error while attempting to connect to your instance, see . to connect to your instance using ssh in a terminal window, use the ssh command to connect to the instance. you specify the path and file name of the private key (), the user name for your instance, and the public dns name or ipv6 address for your instance. for more information about how to find the private key, the user name for your instance, and the dns name or ipv6 address for an instance, see  and . to connect to your instance, use one of the following commands. (public dns) to connect using your instance's public dns name, enter the following command. (ipv6) alternatively, if your instance has an ipv6 address, you can connect to the instance using its ipv6 address. specify the ssh command with the path to the private key (.pem) file, the appropriate user name, and the ipv6 address. you see a response like the following: (optional) verify that the fingerprint in the security alert matches the fingerprint that you previously obtained in . if these fingerprints don't match, someone might be attempting a "man-in-the-middle" attack. if they match, continue to the next step. enter . you see a response like the following: one way to transfer files between your local computer and a linux instance is to use the secure copy protocol (scp). this section describes how to transfer files with scp. the procedure is similar to the procedure for connecting to an instance with ssh.  prerequisites verify the general prerequisites for transferring files to your instance. the general prerequisites for transferring files to an instance are the same as the general prerequisites for connecting to an instance. for more information, see . install an scp client most linux, unix, and apple computers include an scp client by default. if yours doesn't, the openssh project provides a free implementation of the full suite of ssh tools, including an scp client. for more information, see . the following procedure steps you through using scp to transfer a file. if you've already connected to the instance with ssh and have verified its fingerprints, you can start with the step that contains the scp command (step 4). to use scp to transfer a file transfer a file to your instance using the instance's public dns name. for example, if the name of the private key file is , the file to transfer is , the user name is , and the public dns name of the instance is  or the ipv6 address is , use one the following commands to copy the file to the  home directory. (public dns) to transfer a file using your instance's public dns name, enter the following command. (ipv6) alternatively, if your instance has an ipv6 address, you can transfer a file using the instance's ipv6 address. the ipv6 address must be enclosed in square brackets (), which must be escaped (). you see a response like the following: (optional) verify that the fingerprint in the security alert matches the fingerprint that you previously obtained in . if these fingerprints don't match, someone might be attempting a "man-in-the-middle" attack. if they match, continue to the next step. enter yes. you see a response like the following: if you receive a "bash: scp: command not found" error, you must first install scp on your linux instance. for some operating systems, this is located in the  package. for amazon linux variants, such as the amazon ecs-optimized ami, use the following command to install scp: to transfer files in the other direction (from your amazon ec2 instance to your local computer), reverse the order of the host parameters. for example, to transfer the  file from your ec2 instance back to the home directory on your local computer as , use one of the following commands on your local computer. (public dns) to transfer a file using your instance's public dns name, enter the following command. (ipv6) alternatively, if your instance has an ipv6 address, to transfer files in the other direction using the instance's ipv6 address, enter the following command. for information about uninstalling windows subsystem for linux, see . 
the size of an amazon ebs volume is constrained by the physics and arithmetic of block data storage, as well as by the implementation decisions of operating system (os) and file system designers. aws imposes additional limits on volume size to safeguard the reliability of its services. the following sections describe the most important factors that limit the usable size of an ebs volume and offer recommendations for configuring your ebs volumes. topics the following table summarizes the theoretical and implemented storage capacities for the most commonly used file systems on amazon ebs, assuming a 4,096 byte block size. *  and  **  amazon ebs abstracts the massively distributed storage of a data center into virtual hard disk drives. to an operating system installed on an ec2 instance, an attached ebs volume appears to be a physical hard disk drive containing 512-byte disk sectors. the os manages the allocation of data blocks (or clusters) onto those virtual sectors through its storage management utilities. the allocation is in conformity with a volume partitioning scheme, such as master boot record (mbr) or guid partition table (gpt), and within the capabilities of the installed file system (ext4, ntfs, and so on).  ebs is not aware of the data contained in its virtual disk sectors; it only ensures the integrity of the sectors. this means that aws actions and os actions are independent of each other. when you are selecting a volume size, be aware of the capabilities and limits of both, as in the following cases:  ebs currently supports a maximum volume size of 16 tib. this means that you can create an ebs volume as large as 16 tib, but whether the os recognizes all of that capacity depends on its own design characteristics and on how the volume is partitioned.linux boot volumes may use either the mbr or gpt partitioning scheme. mbr supports boot volumes up to 2047 gib (2 tib - 1 gib). gpt with grub 2 supports boot volumes 2 tib or larger. if your linux ami uses mbr, your boot volume is limited to 2047 gib, but your non-boot volumes do not have this limit. for more information, see .among other impacts, the partitioning scheme determines how many logical data blocks can be uniquely addressed in a single volume. for more information, see . the common partitioning schemes in use are master boot record (mbr) and guid partition table (gpt). the important differences between these schemes can be summarized as follows. mbr uses a 32-bit data structure to store block addresses. this means that each data block is mapped with one of 232 possible integers. the maximum addressable size of a volume is given by: the block size for mbr volumes is conventionally limited to 512 bytes. therefore: engineering workarounds to increase this 2-tib limit for mbr volumes have not met with widespread industry adoption. consequently, linux and windows never detect an mbr volume as being larger than 2 tib even if aws shows its size to be larger.  gpt uses a 64-bit data structure to store block addresses. this means that each data block is mapped with one of 264 possible integers. the maximum addressable size of a volume is given by: the block size for gpt volumes is commonly 4,096 bytes. therefore: real-world computer systems don't support anything close to this theoretical maximum. implemented file-system size is currently limited to 50 tib for ext4 and 256 tib for ntfs—both of which exceed the 16-tib limit imposed by aws. data storage on a modern hard drive is managed through logical block addressing, an abstraction layer that allows the operating system to read and write data in logical blocks without knowing much about the underlying hardware. the os relies on the storage device to map the blocks to its physical sectors. ebs advertises 512-byte sectors to the operating system, which reads and writes data to disk using data blocks that are a multiple of the sector size.  the industry default size for logical data blocks is currently 4,096 bytes (4 kib). because certain workloads benefit from a smaller or larger block size, file systems support non-default block sizes that can be specified during formatting. scenarios in which non-default block sizes should be used are outside the scope of this topic, but the choice of block size has consequences for the storage capacity of the volume. the following table shows storage capacity as a function of block size: the ebs-imposed limit on volume size (16 tib) is currently equal to the maximum size enabled by 4-kib data blocks. 
you specify the ebs volumes and instance store volumes for your instance using a block device mapping. each entry in a block device mapping includes a device name and the volume that it maps to. the default block device mapping is specified by the ami you use. alternatively, you can specify a block device mapping for the instance when you launch it. all the nvme instance store volumes supported by an instance type are automatically enumerated and assigned a device name on instance launch; including them in the block device mapping for the ami or the instance has no effect. for more information, see . a block device mapping always specifies the root volume for the instance. the root volume is either an amazon ebs volume or an instance store volume. for more information, see . the root volume is mounted automatically. for instances with an instance store volume for the root volume, the size of this volume varies by ami, but the maximum size is 10 gb. you can use a block device mapping to specify additional ebs volumes when you launch your instance, or you can attach additional ebs volumes after your instance is running. for more information, see . you can specify the instance store volumes for your instance only when you launch it. you can't attach instance store volumes to an instance after you've launched it. if you change the instance type, an instance store will not be attached to the new instance type. for more information, see . the number and size of available instance store volumes for your instance varies by instance type. some instance types do not support instance store volumes. if the number of instance store volumes in a block device mapping exceeds the number of instance store volumes available to an instance, the additional volumes are ignored. for more information about the instance store volumes support by each instance type, see . if the instance type you choose for your instance supports non-nvme instance store volumes, you must add them to the block device mapping for the instance when you launch it. nvme instance store volumes are available by default. after you launch an instance, you must ensure that the instance store volumes for your instance are formatted and mounted before you can use them. the root volume of an instance store-backed instance is mounted automatically. topics you can create an ami with a block device mapping that includes instance store volumes. if you launch an instance with an instance type that supports instance store volumes and an ami that specifies instance store volumes in its block device mapping, the instance includes these instance store volumes. if the number of instance store volumes in the block device mapping exceeds the number of instance store volumes available to the instance, the additional instance store volumes are ignored. considerations for m3 instances, specify instance store volumes in the block device mapping of the instance, not the ami. amazon ec2 might ignore instance store volumes that are specified only in the block device mapping of the ami.when you launch an instance, you can omit non-nvme instance store volumes specified in the ami block device mapping or add instance store volumes.to add instance store volumes to an amazon ebs-backed ami using the console open the amazon ec2 console at . in the navigation pane, choose instances and select the instance. choose actions, image, create image. in the create image dialog box, type a meaningful name and description for your image. for each instance store volume to add, choose add new volume, from volume type select an instance store volume, and from device select a device name. (for more information, see .) the number of available instance store volumes depends on the instance type. for instances with nvme instance store volumes, the device mapping of these volumes depends on the order in which the operating system enumerates the volumes. choose create image. to add instance store volumes to an ami using the command line you can use one of the following commands. for more information about these command line interfaces, see .  or  (aws cli) and  (aws tools for windows powershell)when you launch an instance, the default block device mapping is provided by the specified ami. if you need additional instance store volumes, you must add them to the instance as you launch it. you can also omit devices specified in the ami block device mapping. considerations for m3 instances, you might receive instance store volumes even if you do not specify them in the block device mapping for the instance.for hs1 instances, no matter how many instance store volumes you specify in the block device mapping of an ami, the block device mapping for an instance launched from the ami automatically includes the maximum number of supported instance store volumes. you must explicitly remove the instance store volumes that you don't want from the block device mapping for the instance before you launch it.to update the block device mapping for an instance using the console open the amazon ec2 console. from the dashboard, choose launch instance. in step 1: choose an amazon machine image (ami), select the ami to use and choose select. follow the wizard to complete step 1: choose an amazon machine image (ami), step 2: choose an instance type, and step 3: configure instance details. in step 4: add storage, modify the existing entries as needed. for each instance store volume to add, choose add new volume, from volume type select an instance store volume, and from device select a device name. the number of available instance store volumes depends on the instance type. complete the wizard and launch the instance. (optional) to view the instance store volumes available on your instance, run the lsblk command. to update the block device mapping for an instance using the command line you can use one of the following options commands with the corresponding command. for more information about these command line interfaces, see .  with  (aws cli) with  (aws tools for windows powershell)after you launch an instance, the instance store volumes are available to the instance, but you can't access them until they are mounted. for linux instances, the instance type determines which instance store volumes are mounted for you and which are available for you to mount yourself. for windows instances, the ec2config service mounts the instance store volumes for an instance. the block device driver for the instance assigns the actual volume name when mounting the volume, and the name assigned can be different than the name that amazon ec2 recommends. many instance store volumes are pre-formatted with the ext3 file system. ssd-based instance store volumes that support trim instruction are not pre-formatted with any file system. however, you can format volumes with the file system of your choice after you launch your instance. for more information, see . for windows instances, the ec2config service reformats the instance store volumes with the ntfs file system. you can confirm that the instance store devices are available from within the instance itself using instance metadata. for more information, see . for windows instances, you can also view the instance store volumes using windows disk management. for more information, see . for linux instances, you can view and mount the instance store volumes as described in the following procedure. to make an instance store volume available on linux connect to the instance using an ssh client. use the  command to view the volumes that are formatted and mounted. use the  to view any volumes that were mapped at launch but not formatted and mounted. to format and mount an instance store volume that was mapped only, do the following: create a file system on the device using the  command. create a directory on which to mount the device using the  command. mount the device on the newly created directory using the  command. 
amazon linux is provided by amazon web services (aws). it is designed to provide a stable, secure, and high-performance execution environment for applications running on amazon ec2. it also includes packages that enable easy integration with aws, including launch configuration tools and many popular aws libraries and tools. aws provides ongoing security and maintenance updates for all instances running amazon linux. many applications developed on centos (and similar distributions) run on amazon linux. topics aws provides amazon linux 2 and the amazon linux ami. if you are migrating from another linux distribution to amazon linux, we recommend that you migrate to amazon linux 2. the last version of the amazon linux ami, 2018.03, reaches the end of standard support on december 31, 2020. for more information, see the following blog post: . if you are currently using the amazon linux ami, we recommend that you migrate to amazon linux 2. to migrate to amazon linux 2, launch an instance or create a virtual machine using the current amazon linux 2 image. install your applications, plus any required packages. test your application, and make any changes required for it to run on amazon linux 2. for more information, see  and . for amazon linux docker container images, see  on docker hub. amazon linux does not allow remote root ssh by default. also, password authentication is disabled to prevent brute-force password attacks. to enable ssh logins to an amazon linux instance, you must provide your key pair to the instance at launch. you must also set the security group used to launch your instance to allow ssh access. by default, the only account that can log in remotely using ssh is ec2-user; this account also has sudo privileges. if you enable remote root login, be aware that it is less secure than relying on key pairs and a secondary user. each image contains a unique  file that identifies it. this file contains the following information about the image: , ,  — values from the build recipe that amazon used to construct the image. — a unique, random hex value generated during image creation. — the utc time of image creation, in yyyymmddhhmmss format,  — the name and id of the build recipe amazon used to construct the image.amazon linux contains an  file that specifies the current release that is installed. this file is updated using yum and is part of the  rpm. amazon linux also contains a machine-readable version of  that follows the cpe specification; see . the following is an example of  for the current version of amazon linux 2: the following is an example of  for the current version of amazon linux 2: the following is an example of  for amazon linux 2: the following is an example of  for the current amazon linux ami: the following is an example of  for the current amazon linux ami: the following command line tools for aws integration and usage are included in the amazon linux ami, or in the default repositories for amazon linux 2. for the complete list of packages in the amazon linux ami, see . aws-amitools-ec2aws-apitools-asaws-apitools-cfnaws-apitools-elbaws-apitools-monaws-cfn-bootstrapaws-cliamazon linux 2 and the minimal versions of amazon linux ( and ) do not always contain all of these packages; however, you can install them from the default repositories using the following command: for instances launched using iam roles, a simple script has been included to prepare , , , , and product-specific environment variables after a credential file has been installed to simplify the configuration of these tools. also, to allow the installation of multiple versions of the api and ami tools, we have placed symbolic links to the desired versions of these tools in , as described here: symbolic links to  directories in each of the installed tools directories. products are installed in directories of the form name-version and a symbolic link name that is attached to the most recently installed version. used by  to set product-specific environment variables, such as . amazon linux 2 and the amazon linux ami are designed to be used with online package repositories hosted in each amazon ec2 aws region. these repositories provide ongoing updates to packages in amazon linux 2 and the amazon linux ami, as well as access to hundreds of additional common open-source server applications. the repositories are available in all regions and are accessed using yum update tools. hosting repositories in each region enables us to deploy updates quickly and without any data transfer charges. amazon linux 2 and the amazon linux ami are updated regularly with security and feature enhancements. if you do not need to preserve data or customizations for your instances, you can simply launch new instances using the current ami. if you need to preserve data or customizations for your instances, you can maintain those instances through the amazon linux package repositories. these repositories contain all the updated packages. you can choose to apply these updates to your running instances. older versions of the ami and update packages continue to be available for use, even as new versions are released. importantyour instance must have access to the internet in order to access the repository. to install packages, use the following command: for the amazon linux ami, access to the extra packages for enterprise linux (epel) repository is configured, but it is not enabled by default. amazon linux 2 is not configured to use the epel repository. epel provides third-party packages in addition to those that are in the repositories. the third-party packages are not supported by aws. you can enable the epel repository with the following commands: for amazon linux 2: for the amazon linux ami: if you find that amazon linux does not contain an application you need, you can simply install the application directly on your amazon linux instance. amazon linux uses rpms and yum for package management, and that is likely the simplest way to install new applications. you should always check to see if an application is available in our central amazon linux repository first, because many applications are available there. these applications can easily be added to your amazon linux instance. to upload your applications onto a running amazon linux instance, use scp or sftp and then configure the application by logging on to your instance. your applications can also be uploaded during the instance launch by using the package_setup action from the built-in cloud-init package. for more information, see .  security updates are provided using the package repositories as well as updated ami security alerts are published in the . for more information about aws security policies or to report a security problem, go to the . amazon linux is configured to download and install critical or important security updates at launch time. we recommend that you make the necessary updates for your use case after launch. for example, you may want to apply all updates (not just security updates) at launch, or evaluate each update and apply only the ones applicable to your system. this is controlled using the following cloud-init setting: . the following snippet of cloud-init configuration shows how you can change the settings in the user data text you pass to your instance initialization: the possible values for  are as follows: apply outstanding critical security updates. apply outstanding critical and important security updates. apply outstanding critical, important, and medium security updates. apply all outstanding security updates, including low-severity security updates. apply outstanding critical or important updates that amazon marks as security updates. apply updates that amazon marks as bug fixes. bug fixes are a larger set of updates, which include security updates and fixes for various other minor bugs. apply all applicable available updates, regardless of their classification. do not apply any updates to the instance on startup. the default setting for  is security. that is, if you don't specify a different value in your user data, by default, amazon linux performs the security upgrades at launch for any packages installed at that time. amazon linux also notifies you of any updates to the installed packages by listing the number of available updates upon login using the  file. to install these updates, you need to run sudo yum upgrade on the instance.  with amazon linux, amis are treated as snapshots in time, with a repository and update structure that always gives you the latest packages when you run yum update -y. the repository structure is configured to deliver a continuous flow of updates that enable you to roll from one version of amazon linux to the next. for example, if you launch an instance from an older version of the amazon linux ami (such as 2017.09 or earlier) and run yum update -y, you end up with the latest packages. you can disable rolling updates by enabling the lock-on-launch feature. the lock-on-launch feature locks your instance to receive updates only from the specified release of the ami. for example, you can launch a 2017.09 ami and have it receive only the updates that were released prior to the 2018.03 ami, until you are ready to migrate to the 2018.03 ami. importantif you lock to a version of the repositories that is not the latest, you do not receive further updates. to receive a continuous flow of updates, you must use the latest ami, or consistently update your ami with the repositories pointed to latest. to enable lock-on-launch in new instances, launch it with the following user data passed to cloud-init: to lock existing instances to their current ami version edit . comment out . to clear the cache, run yum clean all. with amazon linux 2, you can use the extras library to install application and software updates on your instances. these software updates are known as topics. you can install a specific version of a topic or omit the version information to use the most recent version. to list the available topics, use the following command: to enable a topic and install the latest version of its package to ensure freshness, use the following command: to enable topics and install specific versions of their packages to ensure stability, use the following command: to remove a package installed from a topic, use the following command: notethis command does not remove packages that were installed as dependencies of the extra. to disable a topic and make the packages inaccessible to the yum package manager, use the following command: importantthis command is intended for advanced users. improper usage of this command could cause package compatibility conflicts. you can view the source of packages you have installed on your instance for reference purposes by using tools provided in amazon linux. source packages are available for all of the packages included in amazon linux and the online package repository. simply determine the package name for the source package you want to install and use the yumdownloader --source command to view source within your running instance. for example: the source rpm can be unpacked, and, for reference, you can view the source tree using standard rpm tools. after you finish debugging, the package is available for use. the cloud-init package is an open-source application built by canonical that is used to bootstrap linux images in a cloud computing environment, such as amazon ec2. amazon linux contains a customized version of cloud-init. it enables you to specify actions that should happen to your instance at boot time. you can pass desired actions to cloud-init through the user data fields when launching an instance. this means you can use common amis for many use cases and configure them dynamically at startup. amazon linux also uses cloud-init to perform initial configuration of the ec2-user account. for more information, see the . amazon linux uses the cloud-init actions found in  and . you can create your own cloud-init action files in . all files in this directory are read by cloud-init. they are read in lexical order, and later files overwrite values in earlier files. the cloud-init package performs these (and other) common configuration tasks for instances at boot: set the default locale.set the hostname.parse and handle user data.generate host private ssh keys.add a user's public ssh keys to  for easy login and administration.prepare the repositories for package management.handle package actions defined in user data.execute user scripts found in user data.mount instance store volumes, if applicable. by default, the  instance store volume is mounted at  if it is present and contains a valid file system; otherwise, it is not mounted.by default, any swap volumes associated with the instance are mounted (only for  and  instance types).you can override the default instance store volume mount with the following cloud-init directive: for more control over mounts, see  in the cloud-init documentation. instance store volumes that support trim are not formatted when an instance launches, so you must partition and format them before you can mount them. for more information, see . you can use the  module to partition and format your instance store volumes at boot. for more information, see  in the cloud-init documentation.the cloud-init package supports user-data handling of a variety of formats: gzipif user-data is gzip compressed, cloud-init decompresses the data and handles it appropriately.mime multipartusing a mime multipart file, you can specify more than one type of data. for example, you could specify both a user-data script and a cloud-config type. each part of the multipart file can be handled by cloud-init if it is one of the supported formats.base64 decodingif user-data is base64-encoded, cloud-init determines if it can understand the decoded data as one of the supported types. if it understands the decoded data, it decodes the data and handles it appropriately. if not, it returns the base64 data intact.user-data scriptbegins with  or .the script is executed by  during the first boot cycle. this occurs late in the boot process (after the initial configuration actions are performed).include filebegins with  or .this content is an include file. the file contains a list of urls, one per line. each of the urls is read, and their content passed through this same set of rules. the content read from the url can be gzipped, mime-multi-part, or plaintext.cloud config databegins with  or .this content is cloud-config data. for a commented example of supported configuration formats, see the examples.upstart jobbegins with  or .this content is stored in a file in , and upstart consumes the content as per other upstart jobs.cloud boothookbegins with  or .this content is boothook data. it is stored in a file under  and then executed immediately.this is the earliest "hook" available. there is no mechanism provided for running it only one time. the boothook must take care of this itself. it is provided with the instance id in the environment variable . use this variable to provide a once-per-instance set of boothook data.to be notified when new amis are released, you can subscribe using amazon sns. to subscribe to amazon linux notifications open the amazon sns console at . in the navigation bar, change the region to us east (n. virginia), if necessary. you must select the region in which the sns notification that you are subscribing to was created. in the navigation pane, choose subscriptions, create subscription. for the create subscription dialog box, do the following: [amazon linux 2] for topic arn, copy and paste the following amazon resource name (arn): arn:aws:sns:us-east-1:137112412989:amazon-linux-2-ami-updates. [amazon linux] for topic arn, copy and paste the following amazon resource name (arn): arn:aws:sns:us-east-1:137112412989:amazon-linux-ami-updates. for protocol, choose email. for endpoint, enter an email address that you can use to receive the notifications. choose create subscription. you receive a confirmation email with the subject line "aws notification - subscription confirmation". open the email and choose confirm subscription to complete your subscription. whenever amis are released, we send notifications to the subscribers of the corresponding topic. to stop receiving these notifications, use the following procedure to unsubscribe. to unsubscribe from amazon linux notifications open the amazon sns console at . in the navigation bar, change the region to us east (n. virginia), if necessary. you must use the region in which the sns notification was created. in the navigation pane, choose subscriptions, select the subscription, and choose actions, delete subscriptions. when prompted for confirmation, choose delete. amazon linux ami sns message formatthe schema for the sns message is as follows.  
by default, amazon linux instances launch with two repositories enabled:  and . while there are many packages available in these repositories that are updated by amazon web services, there may be a package that you wish to install that is contained in another repository. importantthis information applies to amazon linux. for information about other distributions, see their specific documentation. to install a package from a different repository with yum, you need to add the repository information to the  file or to its own  file in the  directory. you can do this manually, but most yum repositories provide their own  file at their repository url. to determine what yum repositories are already installed list the installed yum repositories with the following command: the resulting output lists the installed repositories and reports the status of each. enabled repositories display the number of packages they contain. to add a yum repository to  find the location of the  file. this will vary depending on the repository you are adding. in this example, the  file is at . add the repository with the yum-config-manager command. after you install a repository, you must enable it as described in the next procedure. to enable a yum repository in  use the yum-config-manager command with the  flag. the following command enables the extra packages for enterprise linux (epel) repository from the fedora project. by default, this repository is present in  on amazon linux ami instances, but it is not enabled. noteto enable the epel repository on amazon linux 2, use the following command:   for information on enabling the epel repository on other distributions, such as red hat and centos, see the epel documentation at . 
the reserved instance marketplace is a platform that supports the sale of third-party and aws customers' unused standard reserved instances, which vary in term lengths and pricing options. for example, you may want to sell reserved instances after moving instances to a new aws region, changing to a new instance type, ending projects before the term expiration, when your business needs change, or if you have unneeded capacity. if you want to sell your unused reserved instances on the reserved instance marketplace, you must meet certain eligibility criteria. topics as soon as you list your reserved instances in the reserved instance marketplace, they are available for potential buyers to find. all reserved instances are grouped according to the duration of the term remaining and the hourly price. to fulfill a buyer's request, aws first sells the reserved instance with the lowest upfront price in the specified grouping. then, we sell the reserved instance with the next lowest price, until the buyer's entire order is fulfilled. aws then processes the transactions and transfers ownership of the reserved instances to the buyer.  you own your reserved instance until it's sold. after the sale, you've given up the capacity reservation and the discounted recurring fees. if you continue to use your instance, aws charges you the on-demand price starting from the time that your reserved instance was sold. topics before you can sell your unused reservations, you must register as a seller in the reserved instance marketplace. for information, see . the following limitations and restrictions apply when selling reserved instances: only amazon ec2 standard reserved instances can be sold in the reserved instance marketplace. amazon ec2 convertible reserved instances cannot be sold. reserved instances for other aws services, such as amazon rds and amazon elasticache, cannot be sold.there must be at least one month remaining in the term of the standard reserved instance.you cannot sell a standard reserved instance in a region that is .the minimum price allowed in the reserved instance marketplace is $0.00.you can sell no upfront, partial upfront, or all upfront reserved instances in the reserved instance marketplace. if there is an upfront payment on a reserved instance, it can be sold only after aws has received the upfront payment and the reservation has been active (you've owned it) for at least 30 days.you cannot modify your listing in the reserved instance marketplace directly. however, you can change your listing by first canceling it and then creating another listing with new parameters. for information, see . you can also modify your reserved instances before listing them. for information, see .aws charges a service fee of 12 percent of the total upfront price of each standard reserved instance you sell in the reserved instance marketplace. the upfront price is the price the seller is charging for the standard reserved instance.when you register as a seller, the bank you specify must have a us address. for more information, see  in the aws marketplace seller guide.amazon internet services private limited (aispl) customers can't sell reserved instances in the reserved instance marketplace even if they have a us bank account. for more information, see noteonly the aws account root user can register an account as a seller. to sell in the reserved instance marketplace, you must first register as a seller. during registration, you provide the following information: bank information—aws must have your bank information in order to disburse funds collected when you sell your reservations. the bank you specify must have a us address. for more information, see .tax information—all sellers are required to complete a tax information interview to determine any necessary tax reporting obligations. for more information, see .after aws receives your completed seller registration, you receive an email confirming your registration and informing you that you can get started selling in the reserved instance marketplace. aws must have your bank information in order to disburse funds collected when you sell your reserved instance. the bank you specify must have a us address. for more information, see  in the aws marketplace seller guide. to register a default bank account for disbursements open the  page and sign in using your aws credentials. on the manage bank account page, provide the following information about the bank through to receive payment: bank account holder namerouting numberaccount numberbank account type noteif you are using a corporate bank account, you are prompted to send the information about the bank account via fax (1-206-765-3424).after registration, the bank account provided is set as the default, pending verification with the bank. it can take up to two weeks to verify a new bank account, during which time you can't receive disbursements. for an established account, it usually takes about two days for disbursements to complete. to change the default bank account for disbursement on the  page, sign in with the account that you used when you registered. on the manage bank account page, add a new bank account or modify the default bank account as needed. your sale of reserved instances might be subject to a transaction-based tax, such as sales tax or value-added tax. you should check with your business's tax, legal, finance, or accounting department to determine if transaction-based taxes are applicable. you are responsible for collecting and sending the transaction-based taxes to the appropriate tax authority. as part of the seller registration process, you must complete a tax interview in the . the interview collects your tax information and populates an irs form w-9, w-8ben, or w-8ben-e, which is used to determine any necessary tax reporting obligations.  the tax information you enter as part of the tax interview might differ depending on whether you operate as an individual or business, and whether you or your business are a us or non-us person or entity. as you fill out the tax interview, keep in mind the following: information provided by aws, including the information in this topic, does not constitute tax, legal, or other professional advice. to find out how the irs reporting requirements might affect your business, or if you have other questions, contact your tax, legal, or other professional advisor.to fulfill the irs reporting requirements as efficiently as possible, answer all questions and enter all information requested during the interview.check your answers. avoid misspellings or entering incorrect tax identification numbers. they can result in an invalidated tax form. based on your tax interview responses and irs reporting thresholds, amazon may file form 1099-k. amazon mails a copy of your form 1099-k on or before january 31 in the year following the year that your tax account reaches the threshold levels. for example, if your account reaches the threshold in 2018, your form 1099-k is mailed on or before january 31, 2019. for more information about irs requirements and form 1099-k, see the  website. the upfront fee is the only fee that you can specify for the reserved instance that you're selling. the upfront fee is the one-time fee that the buyer pays when they purchase a reserved instance. the following are important limits to note: you can sell up to $50,000 in reserved instances per year. to sell more, complete the  form.the minimum price is $0. the minimum allowed price in the reserved instance marketplace is $0.00.you cannot modify your listing directly. however, you can change your listing by first canceling it and then creating another listing with new parameters. you can cancel your listing at any time, as long as it's in the  state. you cannot cancel the listing if it's already matched or being processed for a sale. if some of the instances in your listing are matched and you cancel the listing, only the remaining unmatched instances are removed from the listing. because the value of reserved instances decreases over time, by default, aws can set prices to decrease in equal increments month over month. however, you can set different upfront prices based on when your reservation sells. for example, if your reserved instance has nine months of its term remaining, you can specify the amount that you would accept if a customer were to purchase that reserved instance with nine months remaining. you could set another price with five months remaining, and yet another price with one month remaining. as a registered seller, you can choose to sell one or more of your reserved instances. you can choose to sell all of them in one listing or in portions. in addition, you can list reserved instances with any configuration of instance type, platform, and scope. the console determines a suggested price. it checks for offerings that match your reserved instance and matches the one with the lowest price. otherwise, it calculates a suggested price based on the cost of the reserved instance for its remaining time. if the calculated value is less than $1.01, the suggested price is $1.01. if you cancel your listing and a portion of that listing has already been sold, the cancellation is not effective on the portion that has been sold. only the unsold portion of the listing is no longer available in the reserved instance marketplace. to list a reserved instance in the reserved instance marketplace using the aws management console open the amazon ec2 console at . in the navigation pane, choose reserved instances. select the reserved instances to list, and choose sell reserved instances. on the configure your reserved instance listing page, set the number of instances to sell and the upfront price for the remaining term in the relevant columns. see how the value of your reservation changes over the remainder of the term by selecting the arrow next to the months remaining column. if you are an advanced user and you want to customize the pricing, you can enter different values for the subsequent months. to return to the default linear price drop, choose reset. choose continue when you are finished configuring your listing. confirm the details of your listing, on the confirm your reserved instance listing page and if you're satisfied, choose list reserved instance. to view your listings in the console open the amazon ec2 console at . in the navigation pane, choose reserved instances. select the reserved instance that you've listed and choose my listings. to manage reserved instances in the reserved instance marketplace using the aws cli get a list of your reserved instances by using the  command. note the id of the reserved instance you want to list and call . you must specify the id of the reserved instance, the number of instances, and the pricing schedule. to view your listing, use the  command. to cancel your listing, use the  command. listing state on the my listings tab of the reserved instances page displays the current status of your listings: the information displayed by listing state is about the status of your listing in the reserved instance marketplace. it is different from the status information that is displayed by the state column in the reserved instances page. this state information is about your reservation. active—the listing is available for purchase.canceled—the listing is canceled and isn't available for purchase in the reserved instance marketplace.closed—the reserved instance is not listed. a reserved instance might be  because the sale of the listing was completed.when all the instances in your listing are matched and sold, the my listings tab shows that the total instance count matches the count listed under sold. also, there are no available instances left for your listing, and its status is . when only a portion of your listing is sold, aws retires the reserved instances in the listing and creates the number of reserved instances equal to the reserved instances remaining in the count. so, the listing id and the listing that it represents, which now has fewer reservations for sale, is still active. any future sales of reserved instances in this listing are processed this way. when all the reserved instances in the listing are sold, aws marks the listing as . for example, you create a listing reserved instances listing id 5ec28771-05ff-4b9b-aa31-9e57dexample with a listing count of 5. the my listings tab in the reserved instance console page displays the listing this way: reserved instance listing id 5ec28771-05ff-4b9b-aa31-9e57dexample total reservation count = 5sold = 0available = 5status = active a buyer purchases two of the reservations, which leaves a count of three reservations still available for sale. because of this partial sale, aws creates a new reservation with a count of three to represent the remaining reservations that are still for sale. this is how your listing looks in the my listings tab: reserved instance listing id 5ec28771-05ff-4b9b-aa31-9e57dexample total reservation count = 5sold = 2available = 3status = activeif you cancel your listing and a portion of that listing has already sold, the cancelation is not effective on the portion that has been sold. only the unsold portion of the listing is no longer available in the reserved instance marketplace. when your reserved instance is sold, aws sends you an email notification. each day that there is any kind of activity, you receive one email notification capturing all the activities of the day. for example, you create or sell a listing, or aws sends funds to your account. to track the status of a reserved instance listing in the console, choose reserved instance, my listings. the my listings tab contains the listing state value. it also contains information about the term, listing price, and a breakdown of how many instances in the listing are available, pending, sold, and canceled. you can also use the  command with the appropriate filter to obtain information about your listings. as soon as aws receives funds from the buyer, a message is sent to the registered owner account email for the sold reserved instance. aws sends an automated clearing house (ach) wire transfer to your specified bank account. typically, this transfer occurs between one to three days after your reserved instance has been sold. disbursements take place once a day. you will receive an email with a disbursement report after the funds are released. keep in mind that you can't receive disbursements until aws receives verification from your bank. this can take up to two weeks. the reserved instance that you sold continues to appear when you describe your reserved instances. you receive a cash disbursement for your reserved instances through a wire transfer directly into your bank account. aws charges a service fee of 12 percent of the total upfront price of each reserved instance you sell in the reserved instance marketplace. when you sell in the reserved instance marketplace, aws shares your company’s legal name on the buyer’s statement in accordance with us regulations. in addition, if the buyer calls aws support because the buyer needs to contact you for an invoice or for some other tax-related reason, aws may need to provide the buyer with your email address so that the buyer can contact you directly. for similar reasons, the buyer's zip code and country information are provided to the seller in the disbursement report. as a seller, you might need this information to accompany any necessary transaction taxes that you remit to the government (such as sales tax and value-added tax). aws cannot offer tax advice, but if your tax specialist determines that you need specific additional information, . you can purchase reserved instances from third-party sellers who own reserved instances that they no longer need from the reserved instance marketplace. you can do this using the amazon ec2 console or a command line tool. the process is similar to purchasing reserved instances from aws. for more information, see . there are a few differences between reserved instances purchased in the reserved instance marketplace and reserved instances purchased directly from aws: term—reserved instances that you purchase from third-party sellers have less than a full standard term remaining. full standard terms from aws run for one year or three years.upfront price—third-party reserved instances can be sold at different upfront prices. the usage or recurring fees remain the same as the fees set when the reserved instances were originally purchased from aws.types of reserved instances—only amazon ec2 standard reserved instances can be purchased from the reserved instance marketplace. convertible reserved instances, amazon rds and amazon elasticache reserved instances are not available for purchase on the reserved instance marketplace.basic information about you is shared with the seller, for example, your zip code and country information. this information enables sellers to calculate any necessary transaction taxes that they have to remit to the government (such as sales tax or value-added tax) and is provided as a disbursement report. in rare circumstances, aws might have to provide the seller with your email address, so that they can contact you regarding questions related to the sale (for example, tax questions). for similar reasons, aws shares the legal entity name of the seller on the buyer's purchase invoice. if you need additional information about the seller for tax or related reasons, contact . 
you can monitor the status of your instances by viewing status checks and scheduled events for your instances. a status check gives you the information that results from automated checks performed by amazon ec2. these automated checks detect whether specific issues are affecting your instances. the status check information, together with the data provided by amazon cloudwatch, gives you detailed operational visibility into each of your instances.  you can also see status of specific events that are scheduled for your instances. the status of events provides information about upcoming activities that are planned for your instances, such as rebooting or retirement. they also provide the scheduled start and end time of each event. topics 
secure sockets layer/transport layer security (ssl/tls) creates an encrypted channel between a web server and web client that protects data in transit from being eavesdropped on. this tutorial explains how to add support manually for ssl/tls on an ec2 instance with the amazon linux ami and apache web server. if you plan to offer commercial-grade services, the , which is not discussed here, is a good option. for historical reasons, web encryption is often referred to simply as ssl. while web browsers still support ssl, its successor protocol tls is less vulnerable to attack. the amazon linux ami disables server-side support all versions of ssl by default.  consider tls 1.0 to be unsafe, and both tls 1.0 and tls 1.1 are on track to be formally  by the ietf. this tutorial contains guidance based exclusively on enabling tls 1.2. (a newer tls 1.3 protocol exists in draft form, but is not yet supported on amazon linux 2.) for more information about the updated encryption standards, see  and . this tutorial refers to modern web encryption simply as tls. importantthese procedures are intended for use with the amazon linux ami. if you are trying to set up a lamp web server on an instance with a different distribution, some procedures in this tutorial might not work for you. for information about lamp web servers on ubuntu, see the ubuntu community documentation . for information about red hat enterprise linux, see the customer portal documentation . topics before you begin this tutorial, complete the following steps: launch an ebs-backed instance using the amazon linux ami. for more information, see . configure your security group to allow your instance to accept connections on the following tcp ports:  ssh (port 22)http (port 80)https (port 443)for more information, see . install apache web server. for step-by-step instructions, see . only the http24 package and its dependencies are needed; you can ignore the instructions involving php and mysql.to identify and authenticate web sites, the tls public key infrastructure (pki) relies on the domain name system (dns). to use your ec2 instance to host a public web site, you need to register a domain name for your web server or transfer an existing domain name to your amazon ec2 host. numerous third-party domain registration and dns hosting services are available for this, or you can use . this procedure takes you through the process of setting up tls on amazon linux with a self-signed digital certificate. notea self-signed certificate is acceptable for testing but not production. if you expose your self-signed certificate to the internet, visitors to your site receive security warnings. to enable tls on a server  and confirm that apache is running. if necessary, start apache. to ensure that all of your software packages are up to date, perform a quick software update on your instance. this process may take a few minutes, but it is important to make sure you have the latest security updates and bug fixes. notethe  option installs the updates without asking for confirmation. if you would like to examine the updates before installing, you can omit this option. now that your instance is current, add tls support by installing the apache module : your instance now has the following files that you use to configure your secure server and create a certificate for testing:the configuration file for mod_ssl. it contains "directives" telling apache where to find encryption keys and certificates, the tls protocol versions to allow, and the encryption ciphers to accept.an automatically generated, 2048-bit rsa private key for your amazon ec2 host. during installation, openssl used this key to generate a self-signed host certificate, and you can also use this key to generate a certificate signing request (csr) to submit to a certificate authority (ca).an automatically generated, self-signed x.509 certificate for your server host. this certificate is useful for testing that apache is properly set up to use tls. the  and  files are both in pem format, which consists of base64-encoded ascii characters framed by "begin" and "end" lines, as in this abbreviated example of a certificate: the file names and extensions are a convenience and have no effect on function; you can call a certificate , , or any other file name, so long as the related directive in the  file uses the same name. notewhen you replace the default tls files with your own customized files, be sure that they are in pem format.  restart apache. your apache web server should now support https (secure http) over port 443. test it by typing the ip address or fully qualified domain name of your ec2 instance into a browser url bar with the prefix https://. because you are connecting to a site with a self-signed, untrusted host certificate, your browser may display a series of security warnings.  override the warnings and proceed to the site. if the default apache test page opens, it means that you have successfully configured tls on your server. all data passing between the browser and server is now safely encrypted. to prevent site visitors from encountering warning screens, you need to obtain a certificate that not only encrypts, but also publicly authenticates you as the owner of the site. you can use the following process to obtain a ca-signed certificate: generate a certificate signing request (csr) from a private keysubmit the csr to a certificate authority (ca)obtain a signed host certificateconfigure apache to use the certificatea self-signed tls x.509 host certificate is cryptologically identical to a ca-signed certificate. the difference is social, not mathematical; a ca promises to validate, at a minimum, a domain's ownership before issuing a certificate to an applicant. each web browser contains a list of cas trusted by the browser vendor to do this. an x.509 certificate consists primarily of a public key that corresponds to your private server key, and a signature by the ca that is cryptographically tied to the public key. when a browser connects to a web server over https, the server presents a certificate for the browser to check against its list of trusted cas. if the signer is on the list, or accessible through a chain of trust consisting of other trusted signers, the browser negotiates a fast encrypted data channel with the server and loads the page.  certificates generally cost money because of the labor involved in validating the requests, so it pays to shop around. a list of well-known cas can be found at . a few cas offer basic-level certificates free of charge. the most notable of these is the  project, which also supports automation of the certificate creation and renewal process. for more information about using let's encrypt as your ca, see .  underlying the host certificate is the key. as of 2017,  and  groups recommend using a minimum key (modulus) size of 2048 bits for rsa keys intended to protect documents through 2030. the default modulus size generated by openssl in amazon linux is 2048 bits, which means that the existing auto-generated key is suitable for use in a ca-signed certificate. an alternative procedure is described below for those who desire a customized key, for instance, one with a larger modulus or using a different encryption algorithm.  these instructions for acquiring a ca-signed host certificate do not work unless you own a registered and hosted dns domain. to obtain a ca-signed certificate  and navigate to /etc/pki/tls/private/. this is the directory where the server's private key for tls is stored. if you prefer to use your existing host key to generate the csr, skip to step 3. (optional) generate a new private key. here are some examples of key configurations. any of the resulting keys work with your web server, but they vary in how (and how much) security they implement. example 1: create a default rsa host key. the resulting file, custom.key, is a 2048-bit rsa private key. example 2: create a stronger rsa key with a bigger modulus. the resulting file, custom.key, is a 4096-bit rsa private key. example 3: create a 4096-bit encrypted rsa key with password protection. the resulting file, custom.key, is a 4096-bit rsa private key encrypted with the aes-128 cipher. importantencrypting the key provides greater security, but because an encrypted key requires a password, services depending on it cannot be auto-started. each time you use this key, you must supply the password (in the preceding example, "abcde12345") over an ssh connection. example 4: create a key using a non-rsa cipher. rsa cryptography can be relatively slow because of the size of its public keys, which are based on the product of two large prime numbers. however, it is possible to create keys for tls that use non-rsa ciphers. keys based on the mathematics of elliptic curves are smaller and computationally faster when delivering an equivalent level of security. the result is a 256-bit elliptic curve private key using prime256v1, a "named curve" that openssl supports. its cryptographic strength is slightly greater than a 2048-bit rsa key, . notenot all cas provide the same level of support for elliptic-curve-based keys as for rsa keys. make sure that the new private key has highly restrictive ownership and permissions (owner=root, group=root, read/write for owner only). the commands would be as follows: the commands above should yield the following result:  after you have created and configured a satisfactory key, you can create a csr.  create a csr using your preferred key; the example below uses custom.key: openssl opens a dialog and prompts you for the information shown in the following table. all of the fields except common name are optional for a basic, domain-validated host certificate.       finally, openssl prompts you for an optional challenge password. this password applies only to the csr and to transactions between you and your ca, so follow the ca's recommendations about this and the other optional field, optional company name. the csr challenge password has no effect on server operation.    the resulting file csr.pem contains your public key, your digital signature of your public key, and the metadata that you entered. submit the csr to a ca. this usually consists of opening your csr file in a text editor and copying the contents into a web form. at this time, you may be asked to supply one or more subject alternate names (sans) to be placed on the certificate. if www.example.com is the common name, then example.com would be a good san, and vice versa. a visitor to your site typing in either of these names would see an error-free connection. if your ca web form allows it, include the common name in the list of sans. some cas include it automatically. after your request has been approved, you receive a new host certificate signed by the ca. you might also be instructed to download an intermediate certificate file that contains additional certificates needed to complete the ca's chain of trust. noteyour ca may send you files in multiple formats intended for various purposes. for this tutorial, you should only use a certificate file in pem format, which is usually (but not always) marked with a  or  extension. if you are uncertain which file to use, open the files with a text editor and find the one containing one or more blocks beginning with the following:   the file should also end with the following:   you can also test a file at the command line as follows:   verify that these lines appear in the file. do not use files ending with , , or similar file extensions. place the new ca-signed certificate and any intermediate certificates in the  directory. notethere are several ways to upload your custom key to your ec2 instance, but the most straightforward and informative way is to open a text editor (for example, vi, nano, or notepad) on both your local computer and your instance, and then copy and paste the file contents between them. you need root [sudo] permissions when performing these operations on the ec2 instance. this way, you can see immediately if there are any permission or path problems. be careful, however, not to add any additional lines while copying the contents, or to change them in any way. from inside the  directory, use the following commands to verify that the file ownership, group, and permission settings match the highly restrictive amazon linux defaults (owner=root, group=root, read/write for owner only). the commands above should yield the following result:  the permissions for the intermediate certificate file are less stringent (owner=root, group=root, owner can write, group can read, world can read). the commands would be:  the commands above should yield the following result: if you used a custom key to create your csr and the resulting host certificate, remove or rename the old key from the  directory, and then install the new key there.  notethere are several ways to upload your custom key to your ec2 instance, but the most straightforward and informative way is to open a text editor (vi, nano, notepad, etc.) on both your local computer and your instance, and then copy and paste the file contents between them. you need root [sudo] privileges when performing these operations on the ec2 instance. this way, you can see immediately if there are any permission or path problems. be careful, however, not to add any additional lines while copying the contents, or to change them in any way. from inside the  directory, check that the file ownership, group, and permission settings match the highly restrictive amazon linux defaults (owner=root, group=root, read/write for owner only). the commands would be as follows:  the commands above should yield the following result:  edit  to reflect your new certificate and key files. provide the path and file name of the ca-signed host certificate in apache's  directive: if you received an intermediate certificate file ( in this example), provide its path and file name using apache's  directive: notesome cas combine the host certificate and the intermediate certificates in a single file, making this directive unnecessary. consult the instructions provided by your ca. provide the path and file name of the private key in apache's  directive: save  and restart apache. test your server by entering your domain name into a browser url bar with the prefix . your browser should load the test page over https without generating errors. after your tls is operational and exposed to the public, you should test how secure it really is. this is easy to do using online services such as , which performs a free and thorough analysis of your security setup. based on the results, you may decide to harden the default security configuration by controlling which protocols you accept, which ciphers you prefer, and which you exclude. for more information, see . importantreal-world testing is crucial to the security of your server. small configuration errors may lead to serious security breaches and loss of data. because recommended security practices change constantly in response to research and emerging threats, periodic security audits are essential to good server administration.  on the  site, type the fully qualified domain name of your server, in the form www.example.com. after about two minutes, you receive a grade (from a to f) for your site and a detailed breakdown of the findings. though the overview shows that the configuration is mostly sound, the detailed report flags several potential problems. for example: ✗ the rc4 cipher is supported for use by certain older browsers. a cipher is the mathematical core of an encryption algorithm. rc4, a fast cipher used to encrypt tls data-streams, is known to have several . unless you have very good reasons to support legacy browsers, you should disable this. ✗ old tls versions are supported. the configuration supports tls 1.0 (already deprecated) and tls 1.1 (on a path to deprecation). only tls 1.2 has been recommended since 2018. to correct the tls configuration open the configuration file  in a text editor and comment out the following lines by typing "#" at the beginning of each: add the following directives: these directives explicitly disable ssl versions 2 and 3, as well as tls versions 1.0 and 1.1. the server now refuses to accept encrypted connections with clients using anything except tls 1.2. the verbose wording in the directive communicates more clearly, to a human reader, what the server is configured to do. notedisabling tls versions 1.0 and 1.1 in this manner blocks a small percentage of outdated web browsers from accessing your site. to modify the list of allowed ciphers open the configuration file  and find the section with commented-out examples for configuring sslciphersuite and sslproxyciphersuite. leave these as they are, and below them add the following directives: notethough shown here on several lines for readability, each of these two directives must be on a single line without spaces between the cipher names. these ciphers are a subset of the much longer list of supported ciphers in openssl. they were selected and ordered according to the following criteria: support for forward secrecystrengthspeedspecific ciphers before cipher familiesallowed ciphers before denied ciphersnote that the high-ranking ciphers have ecdhe in their names, for elliptic curve diffie-hellman ephemeral ; the ephemeral indicates forward secrecy. also, rc4 is now among the forbidden ciphers near the end. we recommend that you use an explicit list of ciphers instead relying on defaults or terse directives whose content isn't visible. importantthe cipher list shown here is just one of many possible lists; for instance, you might want to optimize a list for speed rather than forward secrecy.if you anticipate a need to support older clients, you can allow the des-cbc3-sha cipher suite.finally, each update to openssl introduces new ciphers and deprecates old ones. keep your ec2 amazon linux instance up to date, watch for security announcements from , and be alert to reports of new security exploits in the technical press. for more information, see  in the user guide for classic load balancers. uncomment the following line by removing the "#": this command forces the server to prefer high-ranking ciphers, including (in this case) those that support forward secrecy. with this directive turned on, the server tries to establish a strongly secure connection before falling back to allowed ciphers with lesser security. restart apache. if you test the domain again on , you should see that the rc4 vulnerability is gone. my apache webserver won't start unless i supply a password this is expected behavior if you installed an encrypted, password-protected, private server key. you can remove the encryption and password requirement from the key. assuming that you have a private encrypted rsa key called  in the default directory, and that the password on it is abcde12345, run the following commands on your ec2 instance to generate an unencrypted version of the key. apache should now start without prompting you for a password. the  certificate authority is the centerpiece of the electronic frontier foundation (eff) effort to encrypt the entire internet. in line with that goal, let's encrypt host certificates are designed to be created, validated, installed, and maintained with minimal human intervention. the automated aspects of certificate management are carried out by an agent running on the web server. after you install and configure the agent, it communicates securely with let's encrypt and performs administrative tasks on apache and the key management system. this tutorial uses the free  agent because it allows you either to supply a customized encryption key as the basis for your certificates, or to allow the agent itself to create a key based on its defaults. you can also configure certbot to renew your certificates on a regular basis without human interaction, as described in . for more information, consult the certbot  or . certbot is not officially supported on amazon linux ami, but is available for download and functions correctly when installed. we recommend that you make the following backups to protect your data and avoid inconvenience: before you begin, take a snapshot of your amazon ebs root volume. this allows you to restore the original state of your ec2 instance. for information about creating ebs snapshots, see .the procedure below requires you to edit your  file, which controls apache's operation. certbot makes its own automated changes to this and other configuration files. make a backup copy of your entire  directory in case you need to restore it.to install and run certbot enable the extra packages for enterprise linux (epel) repository from the fedora project on your instance. packages from epel are required as dependencies when you run the certbot installation script.  download the latest release of certbot from eff onto your ec2 instance using the following command. make the downloaded file executable. run the file with root permissions and the  flag. at the prompt "is this ok [y/d/n]," type "y" and press enter. at the prompt "enter email address (used for urgent renewal and security notices)," type a contact address and press enter. agree to the let's encrypt terms of service at the prompt. type "a" and press enter to proceed: click through the authorization for eff put you on their mailing list by typing "y" or "n" and press enter. at the prompt shown below, type your common name (the name of your domain as described above) and your subject alternative name (san), separating the two names with a space or a comma. then press enter. in this example, the names have been provided: on an amazon linux system with a default apache configuration, you see output similar to the example below, asking about the first name you provided. type "1" and press enter. next, certbot asks about the second name. type "1" and press enter. at this point, certbot creates your key and a csr: authorize certbot to create and all needed host certificates. when prompted for each name, type "1" and press enter as shown in the example: choose whether to allow insecure connections to your web server. if you choose option 2 (as shown in the example), all connections to your server will either be encrypted or rejected. certbot completes the configuration of apache and reports success and other information: after you complete the installation, test and optimize the security of your server as described in .  certbot is designed to become an invisible, error-resistant part of your server system. by default, it generates host certificates with a short, 90-day expiration time. if you have not previously configured your system to call the command automatically, you must re-run the certbot command manually. this procedure shows how to automate certbot by setting up a cron job. to configure automated certificate renewal open the  file in a text editor, such as vim or nano, using sudo. alternatively, use sudo crontab -e. add a line similar to the following and save the file. here is an explanation of each component:schedules a command to be run at 01:39 and 13:39 every day. the selected values are arbitrary, but the certbot developers suggest running the command at least twice daily. this guarantees that any certificate found to be compromised is promptly revoked and replaced.the command runs with root privileges.the command to be run. the renew subcommand causes certbot to check any previously obtained certificates and to renew those that are approaching expiration. the  flag prevents certbot from upgrading itself without your intervention. restart the cron daemon: 
the following is the recommended process of verifying the validity of the ec2rescue for linux package for linux-based operating systems. when you download an application from the internet, we recommend that you authenticate the identity of the software publisher and check that the application has not been altered or corrupted after it was published. this protects you from installing a version of the application that contains a virus or other malicious code. if, after running the steps in this topic, you determine that the software for ec2rescue for linux is altered or corrupted, do not run the installation file. instead, contact amazon web services. ec2rescue for linux files for linux-based operating systems are signed using gnupg, an open-source implementation of the pretty good privacy (openpgp) standard for secure digital signatures. gnupg (also known as gpg) provides authentication and integrity checking through a digital signature. aws publishes a public key and signatures that you can use to verify the downloaded ec2rescue for linux package. for more information about pgp and gnupg (gpg), see . the first step is to establish trust with the software publisher. download the public key of the software publisher, check that the owner of the public key is who they claim to be, and then add the public key to your keyring. your keyring is a collection of known public keys. after you establish the authenticity of the public key, you can use it to verify the signature of the application. topics if your operating system is linux or unix, the gpg tools may already be installed. to test whether the tools are installed on your system, enter gpg2 at a command prompt. if the gpg tools are installed, you see a gpg command prompt. if the gpg tools are not installed, you see an error stating that the command cannot be found. you can install the gnupg package from a repository. to install gpg tools on debian-based linux from a terminal, run the following command: to install gpg tools on red hat–based linux from a terminal, run the following command: the next step in the process is to authenticate the ec2rescue for linux public key and add it as a trusted key in your gpg keyring. to authenticate and import the ec2rescue for linux public key at a command prompt, use the following command to obtain a copy of our public gpg build key: at a command prompt in the directory where you saved , use the following command to import the ec2rescue for linux public key into your keyring: the command returns results similar to the following: after you've installed the gpg tools, authenticated and imported the ec2rescue for linux public key, and verified that the ec2rescue for linux public key is trusted, you are ready to verify the signature of the ec2rescue for linux installation script. to verify the ec2rescue for linux installation script signature at a command prompt, run the following command to download the signature file for the installation script: verify the signature by running the following command at a command prompt in the directory where you saved  and the ec2rescue for linux installation file. both files must be present. the output should look something like the following: if the output contains the phrase , it means that the signature has successfully been verified, and you can proceed to run the ec2rescue for linux installation script. if the output includes the phrase , check whether you performed the procedure correctly. if you continue to get this response, contact amazon web services and do not run the installation file that you downloaded previously. the following are details about the warnings that you might see: warning: this key is not certified with a trusted signature! there is no indication that the signature belongs to the owner. this refers to your personal level of trust in your belief that you possess an authentic public key for ec2rescue for linux. in an ideal world, you would visit an amazon web services office and receive the key in person. however, more often you download it from a website. in this case, the website is an amazon web services website.gpg2: no ultimately trusted keys found. this means that the specific key is not "ultimately trusted" by you (or by other people whom you trust).for more information, see . 
the following are common tasks you can perform to get started using this tool. topics you can run ec2rescue for linux as shown in the following examples. example example: run all modulesto run all modules, run ec2rescue for linux with no options:   some modules require root access. if you are not a root user, use sudo to run these modules as follows:   example example: run a specific moduleto run only specific modules, use the --only-modules parameter:    for example, this command runs the dig module to query the  domain:   example example: view the resultsyou can view the results in :   for example, view the log file for the dig module:   if aws support has requested the results or to share the results from an s3 bucket, upload them using the ec2rescue for linux cli tool. the output of the ec2rescue for linux commands should provide the commands that you need to use. example example: upload results to aws support   example example: upload results to an s3 bucket   for more information about generating pre-signed urls for amazon s3, see . create a backup for your instance, one or more volumes, or a specific device id using the following commands. example example: back up an instance with an amazon machine image (ami)   example example: back up all volumes associated with the instance   example example: back up a specific volume   ec2rescue for linux includes a help file that gives you information and syntax for each available command. example example: display the general help   example example: list the available modules   example example: display the help for a specific module   for example, use the following command to show the help file for the dig module:   
demand for spot instances can vary significantly from moment to moment, and the availability of spot instances can also vary significantly depending on how many unused ec2 instances are available. it is always possible that your spot instance might be interrupted. therefore, you must ensure that your application is prepared for a spot instance interruption. an on-demand instance specified in an ec2 fleet or spot fleet cannot be interrupted. topics the following are the possible reasons that amazon ec2 might interrupt your spot instances: price – the spot price is greater than your maximum price.capacity – if there are not enough unused ec2 instances to meet the demand for spot instances, amazon ec2 interrupts spot instances. the order in which the instances are interrupted is determined by amazon ec2.constraints – if your request includes a constraint such as a launch group or an availability zone group, these spot instances are terminated as a group when the constraint can no longer be met.you can specify that amazon ec2 should do one of the following when it interrupts a spot instance: stop the spot instancehibernate the spot instanceterminate the spot instancethe default is to terminate spot instances when they are interrupted. to change the interruption behavior, see . you can specify the interruption behavior so that amazon ec2 stops spot instances when they are interrupted if the following requirements are met. requirements for a spot instance request, the type must be . you cannot specify a launch group in the spot instance request.for an ec2 fleet or spot fleet request, the type must be .the root volume must be an ebs volume, not an instance store volume.after a spot instance is stopped by the spot service, only the spot service can restart the spot instance, and the same launch specification must be used. for a spot instance launched by a  spot instance request, the spot service restarts the stopped instance when capacity is available in the same availability zone and for the same instance type as the stopped instance. if instances in an ec2 fleet or spot fleet are stopped and the fleet is of type , the spot service launches replacement instances to maintain the target capacity. the spot service finds the best pools based on the specified allocation strategy (, , or ); it does not prioritize the pool with the earlier stopped instances. later, if the allocation strategy leads to a pool containing the earlier stopped instances, the spot service restarts the stopped instances to meet the target capacity. for example, consider a spot fleet with the  allocation strategy. at initial launch, a  pool meets the  criteria for the launch specification. later, when the  instances are interrupted, the spot service stops the instances and replenishes capacity from another pool that fits the  strategy. this time, the pool happens to be a  pool and the spot service launches  instances to meet the target capacity. similarly, spot fleet could move to a  pool the next time. in each of these transitions, the spot service does not prioritize pools with earlier stopped instances, but rather prioritizes purely on the specified allocation strategy. the  strategy can lead back to pools with earlier stopped instances. for example, if instances are interrupted in the  pool and the  strategy leads it back to the  or  pools, the earlier stopped instances are restarted to fulfill target capacity. while a spot instance is stopped, you can modify some of its instance attributes, but not the instance type. if you detach or delete an ebs volume, it is not attached when the spot instance is started. if you detach the root volume and the spot service attempts to start the spot instance, instance start fails and the spot service terminates the stopped instance. you can terminate a spot instance while it is stopped. if you cancel a spot request, an ec2 fleet, or a spot fleet, the spot service terminates any associated spot instances that are stopped. while a spot instance is stopped, you are charged only for the ebs volumes, which are preserved. with ec2 fleet and spot fleet, if you have many stopped instances, you can exceed the limit on the number of ebs volumes for your account. you can specify the interruption behavior so that amazon ec2 hibernates spot instances when they are interrupted if the following requirements are met. requirements for a spot instance request, the type must be . you cannot specify a launch group in the spot instance request.for an ec2 fleet or spot fleet request, the type must be .the root volume must be an ebs volume, not an instance store volume, and it must be large enough to store the instance memory (ram) during hibernation.the following instances are supported: c3, c4, c5, m4, m5, r3, and r4, with less than 100 gb of memory.the following operating systems are supported: amazon linux 2, amazon linux ami, ubuntu with an aws-tuned ubuntu kernel (linux-aws) greater than 4.4.0-1041, and windows server 2008 r2 and later.install the hibernation agent on a supported operating system, or use one of the following amis, which already include the agent:amazon linux 2amazon linux ami 2017.09.1 or laterubuntu xenial 16.04 20171121 or laterwindows server 2008 r2 ami 2017.11.19 or laterwindows server 2012 or windows server 2012 r2 ami 2017.11.19 or laterwindows server 2016 ami 2017.11.19 or laterwindows server 2019start the agent. we recommend that you use user data to start the agent on instance startup. alternatively, you could start the agent manually.recommendation we strongly recommend that you use an encrypted amazon ebs volume as the root volume, because instance memory is stored on the root volume during hibernation. this ensures that the contents of memory (ram) are encrypted when the data is at rest on the volume and when data is moving between the instance and volume. use one of the following three options to ensure that the root volume is an encrypted amazon ebs volume:ebs “single-step” encryption: in a single run-instances api call, you can launch encrypted ebs-backed ec2 instances from an unencrypted ami. for more information, see .ebs encryption by default: you can enable ebs encryption by default to ensure all new ebs volumes created in your aws account are encrypted. for more information, see .encrypted ami: you can enable ebs encryption by using an encrypted ami to launch your instance. if your ami does not have an encrypted root snapshot, you can copy it to a new ami and request encryption. for more information, see  and . when a spot instance is hibernated by the spot service, the ebs volumes are preserved and instance memory (ram) is preserved on the root volume. the private ip addresses of the instance are also preserved. instance storage volumes and public ip addresses, other than elastic ip addresses, are not preserved. while the instance is hibernating, you are charged only for the ebs volumes. with ec2 fleet and spot fleet, if you have many hibernated instances, you can exceed the limit on the number of ebs volumes for your account. the agent prompts the operating system to hibernate when the instance receives a signal from the spot service. if the agent is not installed, the underlying operating system doesn't support hibernation, or there isn't enough volume space to save the instance memory, hibernation fails and the spot service stops the instance instead. when the spot service hibernates a spot instance, you receive an interruption notice, but you do not have two minutes before the spot instance is interrupted. hibernation begins immediately. while the instance is in the process of hibernating, instance health checks might fail. when the hibernation process completes, the state of the instance is . resuming a hibernated spot instance after a spot instance is hibernated by the spot service, it can only be resumed by the spot service. the spot service resumes the instance when capacity becomes available with a spot price that is less than your specified maximum price. for more information, see . for information about hibernating on-demand instances, see . if you do not specify an interruption behavior, the default is to terminate spot instances when they are interrupted. you can specify the interruption behavior when you create a spot request. the way in which you specify the interruption behavior is different depending on how you request spot instances. if you request spot instances using the , you can specify the interruption behavior as follows: select the persistent request check box and then, from interruption behavior, choose an interruption behavior. if you request spot instances using the , you can specify the interruption behavior as follows: select the maintain target capacity check box and then, from interruption behavior, choose an interruption behavior. if you configure spot instances in a , you can specify the interruption behavior as follows: in the launch template, expand advanced details and select the request spot instances checkbox. choose customize and then, from interruption behavior, choose an interruption behavior. if you configure spot instances in a launch configuration when using the  cli, you can specify the interruption behavior as follows: for , specify an interruption behavior. if you configure spot instances using the  cli, you can specify the interruption behavior as follows: for , specify an interruption behavior. here are some best practices to follow when you use spot instances: use the default maximum price, which is the on-demand price.ensure that your instance is ready to go as soon as the request is fulfilled by using an amazon machine image (ami) that contains the required software configuration. you can also use user data to run commands at start-up.store important data regularly in a place that isn't affected when the spot instance terminates. for example, you can use amazon s3, amazon ebs, or dynamodb.divide the work into small tasks (using a grid, hadoop, or queue-based architecture) or use checkpoints so that you can save your work frequently.use spot instance interruption notices to monitor the status of your spot instances.while we make every effort to provide this warning as soon as possible, it is possible that your spot instance is terminated before the warning can be made available. test your application to ensure that it handles an unexpected instance termination gracefully, even if you are testing for interruption notices. you can do so by running the application using an on-demand instance and then terminating the on-demand instance yourself.you must install a hibernation agent on your instance, unless you used an ami that already includes the agent. you must run the agent on instance startup, whether the agent was included in your ami or you installed it yourself. the following procedures help you prepare a linux instance. for directions to prepare a windows instance, see  in the amazon ec2 user guide for windows instances. to prepare an amazon linux instance verify that your kernel supports hibernation and update the kernel if necessary. if your ami doesn't include the agent, install the agent using the following command. add the following to the user data: to prepare an ubuntu instance if your ami doesn't include the agent, install the agent using the following command. the hibernation agent is only available on ubuntu 16.04 or later. add the following to the user data. the best way for you to gracefully handle spot instance interruptions is to architect your application to be fault-tolerant. to accomplish this, you can take advantage of spot instance interruption notices. a spot instance interruption notice is a warning that is issued two minutes before amazon ec2 stops or terminates your spot instance. if you specify hibernation as the interruption behavior, you receive an interruption notice, but you do not receive a two-minute warning because the hibernation process begins immediately. we recommend that you check for these interruption notices every 5 seconds.  the interruption notice is made available as a cloudwatch event and as an item in the  on the spot instance. when amazon ec2 is going to interrupt your spot instance, it emits an event two minutes prior to the actual interruption (except for hibernation, which gets the interruption notice, but not two minutes in advance, because hibernation begins immediately). this event can be detected by amazon cloudwatch events. for more information about cloudwatch events, see the . for a detailed example that walks you through how to create and use event rules, see . the following is an example of the event for spot instance interruption. the possible values for  are , , and . if your spot instance is marked to be stopped or terminated by the spot service, the  item is present in your . otherwise, it is not present. you can retrieve  as follows. the  item specifies the action and the approximate time, in utc, when the action will occur.  the following example indicates the time at which this instance will be stopped. the following example indicates the time at which this instance will be terminated. if amazon ec2 is not preparing to stop or terminate the instance, or if you terminated the instance yourself,  is not present and you receive an http 404 error when you try to retrieve it. this item is maintained for backward compatibility; you should use  instead. if your spot instance is marked for termination by the spot service, the  item is present in your instance metadata. otherwise, it is not present. you can retrieve  as follows. the  item specifies the approximate time in utc when the instance receives the shutdown signal. for example: if amazon ec2 is not preparing to terminate the instance, or if you terminated the spot instance yourself, the  item is either not present (so you receive an http 404 error) or contains a value that is not a time value. if amazon ec2 fails to terminate the instance, the request status is set to . the  value remains in the instance metadata with the original approximate time, which is now in the past. in the console, the instances pane displays all instances, including spot instances. you can identify a spot instance from the  value in the lifecycle column. the instance state column indicates whether the instance is , , , , , or . for a hibernated spot instance, the instance state is . to find an interrupted spot instance (console) open the amazon ec2 console at . in the navigation pane, choose instances. in the top right corner, choose the show/hide columns icon, and under instance attributes, select lifecycle. for spot instances, lifecycle is . alternatively, in the navigation pane, choose spot requests. you can see both spot instance requests and spot fleet requests. to view the ids of the instances, select a spot instance request or a spot fleet request and choose the instances tab. choose an instance id to display the instance in the instances pane. for each spot instance, you can view its state in the instance state column. to find interrupted spot instances (aws cli)you can list your interrupted spot instances using the  command with the  parameter. to list only the instance ids in the output, add the  parameter. if a spot instance is stopped, hibernated, or terminated, you can use cloudtrail to see whether amazon ec2 interrupted the spot instance. in cloudtrail, the event name  indicates that amazon ec2 interrupted the spot instance. for more information about using cloudtrail, see . when a spot instance (not in a spot block) is interrupted, you’re charged as follows.  when a spot instance in a spot block is interrupted, you’re charged as follows.  
scheduled reserved instances (scheduled instances) enable you to purchase capacity reservations that recur on a daily, weekly, or monthly basis, with a specified start time and duration, for a one-year term. you reserve the capacity in advance, so that you know it is available when you need it. you pay for the time that the instances are scheduled, even if you do not use them. scheduled instances are a good choice for workloads that do not run continuously, but do run on a regular schedule. for example, you can use scheduled instances for an application that runs during business hours or for batch processing that runs at the end of the week. if you require a capacity reservation on a continuous basis, reserved instances might meet your needs and decrease costs. for more information, see . if you are flexible about when your instances run, spot instances might meet your needs and decrease costs. for more information, see . topics amazon ec2 sets aside pools of ec2 instances in each availability zone for use as scheduled instances. each pool supports a specific combination of instance type, operating system, and network. to get started, you must search for an available schedule. you can search across multiple pools or a single pool. after you locate a suitable schedule, purchase it. you must launch your scheduled instances during their scheduled time periods, using a launch configuration that matches the following attributes of the schedule that you purchased: instance type, availability zone, network, and platform. when you do so, amazon ec2 launches ec2 instances on your behalf, based on the specified launch specification. amazon ec2 must ensure that the ec2 instances have terminated by the end of the current scheduled time period so that the capacity is available for any other scheduled instances it is reserved for. therefore, amazon ec2 terminates the ec2 instances three minutes before the end of the current scheduled time period. you can't stop or reboot scheduled instances, but you can terminate them manually as needed. if you terminate a scheduled instance before its current scheduled time period ends, you can launch it again after a few minutes. otherwise, you must wait until the next scheduled time period. the following diagram illustrates the lifecycle of a scheduled instance.  amazon ec2 creates a service-linked role when you purchase a scheduled instance. a service-linked role includes all the permissions that amazon ec2 requires to call other aws services on your behalf. for more information, see  in the iam user guide. amazon ec2 uses the service-linked role named awsserviceroleforec2scheduledinstances to complete the following actions:  - terminate scheduled instances after their schedules complete - add system tags to scheduled instancesif you purchased scheduled instances before october 2017, when amazon ec2 began supporting this service-linked role, amazon ec2 created the awsserviceroleforec2scheduledinstances role in your aws account. for more information, see  in the iam user guide. if you no longer need to use scheduled instances, we recommend that you delete the awsserviceroleforec2scheduledinstances role. after this role is deleted from your account, amazon ec2 will create the role again if you purchase scheduled instances. to purchase a scheduled instance, you can use the scheduled reserved instances reservation wizard. warningafter you purchase a scheduled instance, you can't cancel, modify, or resell your purchase. to purchase a scheduled instance (console) open the amazon ec2 console at . in the navigation pane, under instances, choose scheduled instances. if the currently selected region does not support scheduled instances, the page is unavailable.  choose purchase scheduled instances. on the find available schedules page, do the following: under create a schedule, select the starting date from starting on, the schedule recurrence (daily, weekly, or monthly) from recurring, and the minimum duration from for duration. note that the console ensures that you specify a value for the minimum duration that meets the minimum required utilization for your scheduled instance (1,200 hours per year). under instance details, select the operating system and network from platform. to narrow the results, select one or more instance types from instance type or one or more availability zones from availability zone. choose find schedules. under available schedules, select one or more schedules. for each schedule that you select, set the quantity of instances and choose add to cart. your cart is displayed at the bottom of the page. when you are finished adding and removing schedules from your cart, choose review and purchase. on the review and purchase page, verify your selections and edit them as needed. when you are finished, choose purchase. to purchase a scheduled instance (aws cli)use the  command to list the available schedules that meet your needs, and then use the  command to complete the purchase. after you purchase a scheduled instance, it is available for you to launch during its scheduled time periods. to launch a scheduled instance (console) open the amazon ec2 console at . in the navigation pane, under instances, choose scheduled instances. if the currently selected region does not support scheduled instances, the page is unavailable.  select the scheduled instance and choose launch scheduled instances. on the configure page, complete the launch specification for your scheduled instances and choose review. importantthe launch specification must match the instance type, availability zone, network, and platform of the schedule that you purchased. on the review page, verify the launch configuration and modify it as needed. when you are finished, choose launch. to launch a scheduled instance (aws cli)use the  command to list your scheduled instances, and then use the  command to launch each scheduled instance during its scheduled time periods. scheduled instances are subject to the following limits: the following are the only supported instance types: c3, c4, m4, and r3.the required term is 365 days (one year).the minimum required utilization is 1,200 hours per year.you can purchase a scheduled instance up to three months in advance.they are available in the following regions: us east (n. virginia), us west (oregon), and europe (ireland).
you can assign a security group to an instance when you launch the instance. when you add or remove rules, those changes are automatically applied to all instances to which you've assigned the security group. after you launch an instance, you can change its security groups. for more information, see  in the amazon vpc user guide. you can create, view, update, and delete security groups and security group rules using the amazon ec2 console and the command line tools. topics you can create a custom security group using one of the following methods. you must specify the vpc for which you're creating the security group. to create a security group open the amazon ec2 console at . in the navigation pane, choose security groups. choose create security group. in the basic details section, do the following. enter a descriptive name and brief description for the security group. the name and description can be up to 255 characters long, and they can include . for vpc, choose the vpc in which to create the security group. the security group can only be used in the vpc in which it is created. you can add security group rules now, or you can add them at any time after you have created the security group. for more information about adding security group rules, see . choose create. to create a security group open the amazon ec2 console at . in the navigation pane, choose security groups. choose create security group. specify a name and description for the security group. for vpc, choose the id of the vpc. you can start adding rules, or you can choose create to create the security group now (you can always add rules later). for more information about adding rules, see .  to create a security group use one of the following commands:  (aws cli) (aws tools for windows powershell)you can create a new security group by creating a copy of an existing one. when you copy a security group, the copy is created with the same inbound and outbound rules as the original security group. if the original security group is in a vpc, the copy is created in the same vpc unless you specify a different one. the copy receives a new unique security group id and you must give it a name. you can also add a description. you can't copy a security group from one region to another region. you can create a copy of a security group using one of the following methods. to copy a security group open the amazon ec2 console at . in the navigation pane, choose security groups. select the security group to copy and choose actions, copy to new security group. specify a name and optional description, and change the vpc and security group rules if needed. choose create. to copy a security group open the amazon ec2 console at . in the navigation pane, choose security groups.  select the security group you want to copy, choose actions, copy to new. the create security group dialog opens, and is populated with the rules from the existing security group. specify a name and description for your new security group. for vpc, choose the id of the vpc. when you are done, choose create. you can view information about your security groups using one of the following methods. to view your security groups open the amazon ec2 console at . in the navigation pane, choose security groups. your security groups are listed. to view the details for a specific security group, including its inbound and outbound rules, choose its id in the security group id column. to view your security groups open the amazon ec2 console at . in the navigation pane, choose security groups. (optional) select vpc id from the filter list, then choose the id of the vpc. select a security group. general information is displayed on the description tab, inbound rules on the inbound tab, outbound rules on the outbound tab, and tags on the tags tab. to view your security groups use one of the following commands.  (aws cli) (aws tools for windows powershell)when you add a rule to a security group, the new rule is automatically applied to any instances that are associated with the security group. there might be a short delay before the rule is applied. for more information about choosing security group rules for specific types of access, see . you can add rules to a security group using one of the following methods. to add an inbound rule to a security group open the amazon ec2 console at . in the navigation pane, choose security groups. in the list, select the security group and choose actions, edit inbound rules. choose add rule and do the following. for type, choose the type of protocol to allow. if you choose a custom tcp or udp protocol, you must manually enter the port range to allow.if you choose a custom icmp protocol, you must choose the icmp type name from protocol, and, if applicable, the code name from port range.if you choose any other type, the protocol and port range are configured automatically. for source, do one of the following. choose custom and then enter an ip address in cidr notation, a cidr block, another security group, or a prefix list from which to allow inbound traffic.choose anywhere to allow all inbound traffic of the specified protocol to reach your instance. this option automatically adds the  ipv4 cidr block as an allowed source. this is acceptable for a short time in a test environment, but it's unsafe for production environments. in production, authorize only a specific ip address or range of addresses to access your instance. if your security group is in a vpc that's enabled for ipv6, this option automatically adds a second rule for ipv6 traffic (). choose my ip to allow inbound traffic from only your local computer's public ipv4 address.for description, optionally specify a brief description for the rule. choose preview changes, save rules. to add an outbound rule to a security group open the amazon ec2 console at . in the navigation pane, choose security groups. in the list, select the security group and choose actions, edit outbound rules. choose add rule and do the following. for type, choose the type of protocol to allow. if you choose a custom tcp or udp protocol, you must manually enter the port range to allow.if you choose a custom icmp protocol, you must choose the icmp type name from protocol, and, if applicable, the code name from port range.if you choose any other type, the protocol and port range are configured automatically. for destination, do one of the following. choose custom and then enter an ip address in cidr notation, a cidr block, another security group, or a prefix list for which to allow outbound traffic.choose anywhere to allow outbound traffic to all ip addresses. this option automatically adds the  ipv4 cidr block as an allowed source.  if your security group is in a vpc that's enabled for ipv6, this option automatically adds a second rule for ipv6 traffic (). choose my ip to allow outbound traffic only to your local computer's public ipv4 address.for description, optionally specify a brief description for the rule. choose preview changes, confirm. to add rules to a security group open the amazon ec2 console at . in the navigation pane, choose security groups and select the security group. on the inbound tab, choose edit.  in the dialog, choose add rule and do the following: for type, select the protocol.if you select a custom tcp or udp protocol, specify the port range in port range. if you select a custom icmp protocol, choose the icmp type name from protocol, and, if applicable, the code name from port range.for source, choose one of the following: custom: in the provided field, you must specify an ip address in cidr notation, a cidr block, or another security group.anywhere: automatically adds the  ipv4 cidr block. this option enables all traffic of the specified type to reach your instance. this is acceptable for a short time in a test environment, but it's unsafe for production environments. in production, authorize only a specific ip address or range of addresses to access your instance. if your security group is in a vpc that's enabled for ipv6, the anywhere option creates two rules—one for ipv4 traffic () and one for ipv6 traffic (). my ip: automatically adds the public ipv4 address of your local computer.for description, you can optionally specify a description for the rule.for more information about the types of rules that you can add, see . choose save. you can also specify outbound rules. on the outbound tab, choose edit, add rule, and do the following: for type, select the protocol.if you select a custom tcp or udp protocol, specify the port range in port range. if you select a custom icmp protocol, choose the icmp type name from protocol, and, if applicable, the code name from port range.for destination, choose one of the following: custom: in the provided field, you must specify an ip address in cidr notation, a cidr block, or another security group.anywhere: automatically adds the  ipv4 cidr block. this option enables outbound traffic to all ip addresses. if your security group is in a vpc that's enabled for ipv6, the anywhere option creates two rules—one for ipv4 traffic () and one for ipv6 traffic (). my ip: automatically adds the ip address of your local computer.for description, you can optionally specify a description for the rule.choose save. to add rules to a security group use one of the following commands.  (aws cli) (aws tools for windows powershell)to add one or more egress rules to a security group use one of the following commands.  (aws cli) (aws tools for windows powershell)you can update a security group rule using one of the following methods. when you modify the protocol, port range, or source or destination of an existing security group rule using the console, the console deletes the existing rule and adds a new one for you.  to update a security group rule open the amazon ec2 console at . in the navigation pane, choose security groups. select the security group to update, choose actions, and then choose edit inbound rules to update a rule for inbound traffic or edit outbound rules to update a rule for outbound traffic. update the rule as required and then choose preview changes, confirm. when you modify the protocol, port range, or source or destination of an existing security group rule using the console, the console deletes the existing rule and adds a new one for you.  to update a security group rule open the amazon ec2 console at . in the navigation pane, choose security groups. select the security group to update, and choose the inbound tab to update a rule for inbound traffic or the outbound tab to update a rule for outbound traffic. choose edit. modify the rule entry as required and choose save. you cannot modify the protocol, port range, or source or destination of an existing rule using the amazon ec2 api or a command line tools. instead, you must delete the existing rule and add a new rule. you can, however, update the description of an existing rule. to update the description for an existing inbound rule use one of the following commands.  (aws cli) (aws tools for windows powershell)to update the description for an existing outbound rule use one of the following commands.  (aws cli) (aws tools for windows powershell)when you delete a rule from a security group, the change is automatically applied to any instances associated with the security group. you can delete rules from a security group using one of the following methods. to delete a security group rule open the amazon ec2 console at . in the navigation pane, choose security groups. select the security group to update, choose actions, and then choose edit inbound rules to remove an inbound rule or edit outbound rules to remove an outbound rule. choose the remove button to the right of the rule to delete. choose preview changes, confirm. to delete a security group rule open the amazon ec2 console at . in the navigation pane, choose security groups.  select a security group. on the inbound tab (for inbound rules) or outbound tab (for outbound rules), choose edit. choose delete (a cross icon) next to each rule to delete.  choose save. to remove one or more ingress rules from a security group use one of the following commands.  (aws cli) (aws tools for windows powershell)to remove one or more egress rules from a security group use one of the following commands.  (aws cli) (aws tools for windows powershell)you can't delete a security group that is associated with an instance. you can't delete the default security group. you can't delete a security group that is referenced by a rule in another security group in the same vpc. if your security group is referenced by one of its own rules, you must delete the rule before you can delete the security group. to delete a security group open the amazon ec2 console at . in the navigation pane, choose security groups. select the security group to delete and choose actions, delete security group, delete. to delete a security group open the amazon ec2 console at . in the navigation pane, choose security groups. select a security group and choose actions, delete security group. choose yes, delete. to delete a security group use one of the following commands.  (aws cli) (aws tools for windows powershell)
when you launch an instance, the instance type that you specify determines the hardware of the host computer used for your instance. each instance type offers different compute, memory, and storage capabilities and are grouped in instance families based on these capabilities. select an instance type based on the requirements of the application or software that you plan to run on your instance. amazon ec2 provides each instance with a consistent and predictable amount of cpu capacity, regardless of its underlying hardware. amazon ec2 dedicates some resources of the host computer, such as cpu, memory, and instance storage, to a particular instance. amazon ec2 shares other resources of the host computer, such as the network and the disk subsystem, among instances. if each instance on a host computer tries to use as much of one of these shared resources as possible, each receives an equal share of that resource. however, when a resource is underused, an instance can consume a higher share of that resource while it's available. each instance type provides higher or lower minimum performance from a shared resource. for example, instance types with high i/o performance have a larger allocation of shared resources. allocating a larger share of shared resources also reduces the variance of i/o performance. for most applications, moderate i/o performance is more than enough. however, for applications that require greater or more consistent i/o performance, consider an instance type with higher i/o performance. topics amazon ec2 provides a wide selection of instance types optimized for different use cases. for the best performance, we recommend that you use the following current generation instance types when you launch new instances. for more information about the current generation instance types, see . amazon ec2 provides the instance types in the following table. to determine which instance types meet your requirements, such as supported regions, compute resources, or storage resources, see . amazon web services offers previous generation instances for users who have optimized their applications around these instances and have yet to upgrade. we encourage you to use the latest generation of instances to get the best performance, but we continue to support these previous generation instances. if you are currently using a previous generation instance, you can see which current generation instance would be a suitable upgrade. for more information, see . for more information about the hardware specifications for each amazon ec2 instance type, see . to determine which instance type best meets your needs, we recommend that you launch an instance and use your own benchmark application. because you pay by the instance second, it's convenient and inexpensive to test multiple instance types before making a decision. if your needs change, even after you make a decision, you can resize your instance later. for more information, see . noteamazon ec2 instances typically run on 64-bit virtual intel processors as specified in the instance type product pages. for more information about the hardware specifications for each amazon ec2 instance type, see . however, confusion may result from industry naming conventions for 64-bit cpus. chip manufacturer advanced micro devices (amd) introduced the first commercially successful 64-bit architecture based on the intel x86 instruction set. consequently, the architecture is widely referred to as amd64 regardless of the chip manufacturer. windows and several linux distributions follow this practice. this explains why the internal system information on an ubuntu or windows ec2 instance displays the cpu architecture as amd64 even though the instances are running on intel hardware. the virtualization type of your instance is determined by the ami that you use to launch it. current generation instance types support hardware virtual machine (hvm) only. some previous generation instance types support paravirtual (pv) and some aws regions support pv instances. for more information, see . for best performance, we recommend that you use an hvm ami. in addition, hvm amis are required to take advantage of enhanced networking. hvm virtualization uses hardware-assist technology provided by the aws platform. with hvm virtualization, the guest vm runs as if it were on a native hardware platform, except that it still uses pv network and storage drivers for improved performance. the nitro system is a collection of aws-built hardware and software components that enable high performance, high availability, and high security. in addition, the nitro system provides bare metal capabilities that eliminate virtualization overhead and support workloads that require full access to host hardware. for more information, see . nitro components the following components are part of the nitro system: nitro cardlocal nvme storage volumesnetworking hardware supportmanagementmonitoringsecuritynitro security chip, integrated into the motherboardnitro hypervisor - a lightweight hypervisor that manages memory and cpu allocation and delivers performance that is indistinguishable from bare metal for most workloads.instance types the following instances are built on the nitro system: a1, c5, c5a, c5d, c5n, c6g,  g4, i3en, inf1, m5, m5a, m5ad, m5d, m5dn, m5n, m6g,  , r5, r5a, r5ad, r5d, r5dn, r5n, r6g,  t3, t3a, and z1dbare metal: , , , , ,  , , , , ,  , , ,  , , , , , and learn more for more information, see the following videos: when you select an instance type, this determines the networking and storage features that are available. to describe an instance type, use the  command. networking features ipv6 is supported on all current generation instance types and the c3, r3, and i2 previous generation instance types.to maximize the networking and bandwidth performance of your instance type, you can do the following:launch supported instance types into a cluster placement group to optimize your instances for high performance computing (hpc) applications. instances in a common cluster placement group can benefit from high-bandwidth, low-latency networking. for more information, see .enable enhanced networking for supported current generation instance types to get significantly higher packet per second (pps) performance, lower network jitter, and lower latencies. for more information, see . current generation instance types that are enabled for enhanced networking have the following networking performance attributes:traffic within the same region over private ipv4 or ipv6 can support 5 gbps for single-flow traffic and up to 25 gbps for multi-flow traffic (depending on the instance type).traffic to and from amazon s3 buckets within the same region over the public ip address space or through a vpc endpoint can use all available instance aggregate bandwidth.the maximum transmission unit (mtu) supported varies across instance types. all amazon ec2 instance types support standard ethernet v2 1500 mtu frames. all current generation instances support 9001 mtu, or jumbo frames, and some previous generation instances support them as well. for more information, see .storage features some instance types support ebs volumes and instance store volumes, while other instance types support only ebs volumes. some instance types that support instance store volumes use solid state drives (ssd) to deliver very high random i/o performance. some instance types support nvme instance store volumes. some instance types support nvme ebs volumes. for more information, see  and .to obtain additional, dedicated capacity for amazon ebs i/o, you can launch some instance types as ebs–optimized instances. some instance types are ebs–optimized by default. for more information, see .the following table summarizes the networking and storage features supported by current generation instance types. * the root device volume must be an amazon ebs volume. the following table summarizes the networking and storage features supported by previous generation instance types. there is a limit on the total number of instances that you can launch in a region, and there are additional limits on some instance types. for more information about the default limits, see  for more information about viewing your current limits or requesting an increase in your current limits, see . 
warningdiagnostic interrupts are intended for use by advanced users. incorrect usage could negatively impact your instance. sending a diagnostic interrupt to an instance could trigger an instance to crash and reboot, which could lead to the loss of data. you can send a diagnostic interrupt to an unreachable or unresponsive linux instance to manually trigger a kernel panic. linux operating systems typically crash and reboot when a kernel panic occurs. the specific behavior of the operating system depends on its configuration. a kernel panic can also be used to cause the instance's operating system kernel to perform tasks, such as generating a crash dump file. you can then use the information in the crash dump file to conduct root cause analysis and debug the instance. the crash dump data is generated locally by the operating system on the instance itself. before sending a diagnostic interrupt to your instance, we recommend that you consult the documentation for your operating system and then make the necessary configuration changes. topics diagnostic interrupt is supported on all nitro-based instance types, except a1. for more information, see . before using a diagnostic interrupt, you must configure your instance's operating system. this ensures that it performs the actions that you need when a kernel panic occurs. to configure amazon linux 2 to generate a crash dump when a kernel panic occurs connect to your instance. install kexec and kdump. configure the kernel to reserve an appropriate amount of memory for the secondary kernel. the amount of memory to reserve depends on the total available memory of your instance. open the  file using your preferred text editor, locate the line that starts with , and then add the  parameter in the following format: . for example, to reserve , modify the  file as follows: save the changes and close the  file. rebuild the grub2 configuration file. on instances based on intel and amd processors, the  command sends an unknown non-maskable interrupt (nmi) to the instance. you must configure the kernel to crash when it receives the unknown nmi. open the  file using your preferred text editor and add the following. reboot and reconnect to your instance. verify that the kernel has been booted with the correct  parameter. the following example output indicates successful configuration. verify that the kdump service is running. the following example output shows the result if the kdump service is running. noteby default, the crash dump file is saved to . to change the location, modify the  file using your preferred text editor. to configure amazon linux to generate a crash dump when a kernel panic occurs connect to your instance. install kexec and kdump. configure the kernel to reserve an appropriate amount of memory for the secondary kernel. the amount of memory to reserve depends on the total available memory of your instance. for example, to reserve  for the crash kernel, use the following command.  on instances based on intel and amd processors, the  command sends an unknown non-maskable interrupt (nmi) to the instance. you must configure the kernel to crash when it receives the unknown nmi. open the  file using your preferred text editor and add the following. reboot and reconnect to your instance. verify that the kernel has been booted with the correct  parameter. the following example output indicates successful configuration. verify that the kdump service is running. if the service is running, the command returns the  response. noteby default, the crash dump file is saved to . to change the location, modify the  file using your preferred text editor. to configure suse linux enterprise, ubuntu, or red hat enterprise linuxsee the following websites: noteon instances based on intel and amd processors, the  command sends an unknown non-maskable interrupt (nmi) to the instance. you must configure the kernel to crash when it receives the unknown nmi. add the following to your configuration file.   after you have completed the necessary configuration changes, you can send a diagnostic interrupt to your instance using the aws cli or amazon ec2 api. to send a diagnostic interrupt to your instance (aws cli)use the  command and specify the instance id. 
amazon ec2 and amazon vpc support both the ipv4 and ipv6 addressing protocols. by default, amazon ec2 and amazon vpc use the ipv4 addressing protocol; you can't disable this behavior. when you create a vpc, you must specify an ipv4 cidr block (a range of private ipv4 addresses). you can optionally assign an ipv6 cidr block to your vpc and subnets, and assign ipv6 addresses from that block to instances in your subnet. ipv6 addresses are reachable over the internet. for more information about ipv6, see  in the amazon vpc user guide. topics a private ipv4 address is an ip address that's not reachable over the internet. you can use private ipv4 addresses for communication between instances in the same vpc. for more information about the standards and specifications of private ipv4 addresses, see . we allocate private ipv4 addresses to instances using dhcp. noteyou can create a vpc with a publicly routable cidr block that falls outside of the private ipv4 address ranges specified in rfc 1918. however, for the purposes of this documentation, we refer to private ipv4 addresses (or 'private ip addresses') as the ip addresses that are within the ipv4 cidr range of your vpc. when you launch an instance, we allocate a primary private ipv4 address for the instance. each instance is also given an internal dns hostname that resolves to the primary private ipv4 address; for example, . you can use the internal dns hostname for communication between instances in the same vpc, but we can't resolve the internal dns hostname outside of the vpc. an instance receives a primary private ip address from the ipv4 address range of the subnet. for more information, see  in the amazon vpc user guide. if you don't specify a primary private ip address when you launch the instance, we select an available ip address in the subnet's ipv4 range for you. each instance has a default network interface (eth0) that is assigned the primary private ipv4 address. you can also specify additional private ipv4 addresses, known as secondary private ipv4 addresses. unlike primary private ip addresses, secondary private ip addresses can be reassigned from one instance to another. for more information, see .  a private ipv4 address, regardless of whether it is a primary or secondary address, remains associated with the network interface when the instance is stopped and started, or hibernated and started, and is released when the instance is terminated. a public ip address is an ipv4 address that's reachable from the internet. you can use public addresses for communication between your instances and the internet. each instance that receives a public ip address is also given an external dns hostname; for example, . we resolve an external dns hostname to the public ip address of the instance from outside its vpc, and to the private ipv4 address of the instance from inside its vpc. the public ip address is mapped to the primary private ip address through network address translation (nat). for more information, see . when you launch an instance in a default vpc, we assign it a public ip address by default. when you launch an instance into a nondefault vpc, the subnet has an attribute that determines whether instances launched into that subnet receive a public ip address from the public ipv4 address pool. by default, we don't assign a public ip address to instances launched in a nondefault subnet. you can control whether your instance receives a public ip address as follows: modifying the public ip addressing attribute of your subnet. for more information, see  in the amazon vpc user guide.enabling or disabling the public ip addressing feature during launch, which overrides the subnet's public ip addressing attribute. for more information, see .a public ip address is assigned to your instance from amazon's pool of public ipv4 addresses, and is not associated with your aws account. when a public ip address is disassociated from your instance, it is released back into the public ipv4 address pool, and you cannot reuse it. you cannot manually associate or disassociate a public ip address from your instance. instead, in certain cases, we release the public ip address from your instance, or assign it a new one:  we release your instance's public ip address when it is stopped, hibernated, or terminated. your stopped or hibernated instance receives a new public ip address when it is started.we release your instance's public ip address when you associate an elastic ip address with it. when you disassociate the elastic ip address from your instance, it receives a new public ip address.if the public ip address of your instance in a vpc has been released, it will not receive a new one if there is more than one network interface attached to your instance. if your instance's public ip address is released while it has a secondary private ip address that is associated with an elastic ip address, the instance does not receive a new public ip address.if you require a persistent public ip address that can be associated to and from instances as you require, use an elastic ip address instead. if you use dynamic dns to map an existing dns name to a new instance's public ip address, it might take up to 24 hours for the ip address to propagate through the internet. as a result, new instances might not receive traffic while terminated instances continue to receive requests. to solve this problem, use an elastic ip address. you can allocate your own elastic ip address, and associate it with your instance. for more information, see .  if you assign an elastic ip address to an instance, it receives an ipv4 dns hostname if dns hostnames are enabled. for more information, see  in the amazon vpc user guide. noteinstances that access other instances through their public nat ip address are charged for regional or internet data transfer, depending on whether the instances are in the same region. an elastic ip address is a public ipv4 address that you can allocate to your account. you can associate it to and from instances as you require, and it's allocated to your account until you choose to release it. for more information about elastic ip addresses and how to use them, see . we do not support elastic ip addresses for ipv6. amazon provides a dns server that resolves amazon-provided ipv4 dns hostnames to ipv4 addresses. the amazon dns server is located at the base of your vpc network range plus two. for more information, see  in the amazon vpc user guide. you can optionally associate an ipv6 cidr block with your vpc, and associate ipv6 cidr blocks with your subnets. the ipv6 cidr block for your vpc is automatically assigned from amazon's pool of ipv6 addresses; you cannot choose the range yourself. for more information, see the following topics in the amazon vpc user guide: ipv6 addresses are globally unique, and therefore reachable over the internet. your instance receives an ipv6 address if an ipv6 cidr block is associated with your vpc and subnet, and if one of the following is true: your subnet is configured to automatically assign an ipv6 address to an instance during launch. for more information, see .you assign an ipv6 address to your instance during launch.you assign an ipv6 address to the primary network interface of your instance after launch.you assign an ipv6 address to a network interface in the same subnet, and attach the network interface to your instance after launch. when your instance receives an ipv6 address during launch, the address is associated with the primary network interface (eth0) of the instance. you can disassociate the ipv6 address from the network interface. we do not support ipv6 dns hostnames for your instance. an ipv6 address persists when you stop and start, or hibernate and start, your instance, and is released when you terminate your instance. you cannot reassign an ipv6 address while it's assigned to another network interface—you must first unassign it. you can assign additional ipv6 addresses to your instance by assigning them to a network interface attached to your instance. the number of ipv6 addresses you can assign to a network interface and the number of network interfaces you can attach to an instance varies per instance type. for more information, see . you can view the ip addresses assigned to your instance, assign a public ipv4 address to your instance during launch, or assign an ipv6 address to your instance during launch. topics you can use the amazon ec2 console to determine the private ipv4 addresses, public ipv4 addresses, and elastic ip addresses of your instances. you can also determine the public ipv4 and private ipv4 addresses of your instance from within your instance by using instance metadata. for more information, see . to determine your instance's private ipv4 addresses using the console open the amazon ec2 console at . in the navigation pane, choose instances. select your instance. in the details pane, get the private ipv4 address from the private ips field, and get the internal dns hostname from the private dns field. if you have one or more secondary private ipv4 addresses assigned to network interfaces that are attached to your instance, get those ip addresses from the secondary private ips field.  alternatively, in the navigation pane, choose network interfaces, and then select the network interface that's associated with your instance.  get the primary private ip address from the primary private ipv4 ip field, and the internal dns hostname from the private dns (ipv4) field.  if you've assigned secondary private ip addresses to the network interface, get those ip addresses from the secondary private ipv4 ips field. to determine your instance's public ipv4 addresses using the console open the amazon ec2 console at . in the navigation pane, choose instances.  select your instance. in the details pane, get the public ip address from the ipv4 public ip field, and get the external dns hostname from the public dns (ipv4) field. if one or more elastic ip addresses have been associated with the instance, get the elastic ip addresses from the elastic ips field.  noteif your instance does not have a public ipv4 address, but you've associated an elastic ip address with a network interface for the instance, the ipv4 public ip field displays the elastic ip address. alternatively, in the navigation pane, choose network interfaces, and then select a network interface that's associated with your instance.  get the public ip address from the ipv4 public ip field. an asterisk (*) indicates the public ipv4 address or elastic ip address that's mapped to the primary private ipv4 address.  notethe public ipv4 address is displayed as a property of the network interface in the console, but it's mapped to the primary private ipv4 address through nat. therefore, if you inspect the properties of your network interface on your instance, for example, through  (linux) or  (windows), the public ipv4 address is not displayed. to determine your instance's public ipv4 address from within the instance, you can use instance metadata.  to determine your instance's ipv4 addresses using instance metadata connect to your instance. for more information, see . use the following command to access the private ip address: use the following command to access the public ip address:   note that if an elastic ip address is associated with the instance, the value returned is that of the elastic ip address. you can use the amazon ec2 console to determine the ipv6 addresses of your instances. to determine your instance's ipv6 addresses using the console open the amazon ec2 console at . in the navigation pane, choose instances. select your instance. in the details pane, get the ipv6 addresses from ipv6 ips. to determine your instance's ipv6 addresses using instance metadata connect to your instance. for more information, see . use the following command to view the ipv6 address (you can get the mac address from ). each subnet has an attribute that determines whether instances launched into that subnet are assigned a public ip address. by default, nondefault subnets have this attribute set to false, and default subnets have this attribute set to true. when you launch an instance, a public ipv4 addressing feature is also available for you to control whether your instance is assigned a public ipv4 address; you can override the default behavior of the subnet's ip addressing attribute. the public ipv4 address is assigned from amazon's pool of public ipv4 addresses, and is assigned to the network interface with the device index of eth0. this feature depends on certain conditions at the time you launch your instance.  importantyou can't manually disassociate the public ip address from your instance after launch. instead, it's automatically released in certain cases, after which you cannot reuse it. for more information, see . if you require a persistent public ip address that you can associate or disassociate at will, assign an elastic ip address to the instance after launch instead. for more information, see . to access the public ip addressing feature when launching an instance open the amazon ec2 console at . choose launch instance. select an ami and an instance type, and then choose next: configure instance details. on the configure instance details page, for network, select a vpc. the auto-assign public ip list is displayed. choose enable or disable to override the default setting for the subnet.  importantyou cannot auto-assign a public ip address if you specify more than one network interface. additionally, you cannot override the subnet setting using the auto-assign public ip feature if you specify an existing network interface for eth0.  follow the steps on the next pages of the wizard to complete your instance's setup. for more information about the wizard configuration options, see . on the final review instance launch page, review your settings, and then choose launch to choose a key pair and launch your instance. on the instances page, select your new instance and view its public ip address in ipv4 public ip field in the details pane. the public ip addressing feature is only available during launch. however, whether you assign a public ip address to your instance during launch or not, you can associate an elastic ip address with your instance after it's launched. for more information, see . you can also modify your subnet's public ipv4 addressing behavior. for more information, see . to enable or disable the public ip addressing feature using the command line you can use one of the following commands. for more information about these command line interfaces, see . use the  or the  option with the  command (aws cli)use the  parameter with the  command (aws tools for windows powershell)if your vpc and subnet have ipv6 cidr blocks associated with them, you can assign an ipv6 address to your instance during or after launch. the ipv6 address is assigned from the ipv6 address range of the subnet, and is assigned to the network interface with the device index of eth0.  ipv6 is supported on all current generation instance types and the c3, r3, and i2 previous generation instance types. to assign an ipv6 address to an instance during launch open the amazon ec2 console at . select an ami and an instance type that supports ipv6, and choose next: configure instance details. on the configure instance details page, for network, select a vpc and for subnet, select a subnet. for auto-assign ipv6 ip, choose enable. follow the remaining steps in the wizard to launch your instance. alternatively, you can assign an ipv6 address to your instance after launch. to assign an ipv6 address to your instance after launch open the amazon ec2 console at . in the navigation pane, choose instances. select your instance, choose actions, networking, manage ip addresses. under ipv6 addresses, choose assign new ip. you can specify an ipv6 address from the range of the subnet, or leave the auto-assign value to let amazon choose an ipv6 address for you. choose save. noteif you launched your instance using amazon linux 2016.09.0 or later, or windows server 2008 r2 or later, your instance is configured for ipv6, and no additional steps are needed to ensure that the ipv6 address is recognized on the instance. if you launched your instance from an older ami, you may have to configure your instance manually. for more information, see  in the amazon vpc user guide. to assign an ipv6 address using the command line you can use one of the following commands. for more information about these command line interfaces, see . use the  option with the  command (aws cli)use the  property for  in the  command (aws tools for windows powershell) (aws cli) (aws tools for windows powershell)you can unassign an ipv6 address from an instance using the amazon ec2 console. to unassign an ipv6 address from an instance open the amazon ec2 console at . in the navigation pane, choose instances. select your instance, choose actions, networking, manage ip addresses. under ipv6 addresses, choose unassign for the ipv6 address to unassign. choose yes, update. to unassign an ipv6 address using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell).
the following steps help you to get started with one of the following base amis: amazon linux, amazon linux 2, rhel 7.6, rhel 7.7, rhel 7.8, centos 7, ubuntu 16.04, and ubuntu 18.04. topics an efa requires a security group that allows all inbound and outbound traffic to and from the security group itself. to create an efa-enabled security group open the amazon ec2 console at . in the navigation pane, choose security groups and then choose create security group. in the create security group window, do the following: for security group name, enter a descriptive name for the security group, such as . (optional) for description, enter a brief description of the security group. for vpc, select the vpc into which you intend to launch your efa-enabled instances. choose create. select the security group that you created, and on the description tab, copy the group id. on the inbound and outbound tabs, do the following: choose edit. for type, choose all traffic. for source, choose custom. paste the security group id that you copied into the field. choose save. launch a temporary instance that you can use to install and configure the efa software components. you use this instance to create an efa-enabled ami from which you can launch your efa-enabled instances. to launch a temporary instance open the amazon ec2 console at . choose launch instance. on the choose an ami page, choose one of the following amis: amazon linux, amazon linux 2, rhel 7.6, rhel 7.7, rhel 7.8, centos 7, ubuntu 16.04, and ubuntu 18.04. on the choose an instance type page, select  and then choose next: configure instance details. on the configure instance details page, do the following: for elastic fabric adapter, choose enable. in the network interfaces section, for device eth0, choose new network interface. choose next: add storage. on the add storage page, specify the volumes to attach to the instances, in addition to the volumes specified by the ami (such as the root device volume). then choose next: add tags. on the add tags page, specify a tag that you can use to identify the temporary instance, and then choose next: configure security group. on the configure security group page, for assign a security group, select select an existing security group. then select the security group that you created in step 1. on the review instance launch page, review the settings, and then choose launch to choose a key pair and to launch your instance. install the efa-enabled kernel, efa drivers, libfabric, and open mpi stack that is required to support efa on your temporary instance. to install the efa software connect to the instance you launched. for more information, see . to ensure that all of your software packages are up to date, perform a quick software update on your instance. this process may take a few minutes. amazon linux, amazon linux 2, rhel 7.6/7.7/7.8, centos 7 ubuntu 16.04 and ubuntu 18.04 download the efa software installation files. to download the latest stable version, use the following command. you can also get the latest version by replacing the version number with  in the preceding command. the software installation files are packaged into a compressed  file. extract the files from the compressed  file and navigate into the extracted directory. run the efa software installation script. libfabric is installed in the  directory, while open mpi is installed in the  directory. log out of the instance and then log back in. confirm that the efa software components were successfully installed. the command should return information about the libfabric efa interfaces. the following example shows the command output. to install the nvidia gpu drivers and the nvidia cuda toolkit install the utilities that are needed to install the nvidia gpu drivers and the nvidia cuda toolkit. amazon linux, amazon linux 2, rhel 7.6/7.7/7.8, centos 7 ubuntu 16.04 and ubuntu 18.04 to use the nvidia gpu driver, you must first disable the  open source drivers. install the gcc compiler and the kernel headers package for the version of the kernel that you are currently running. amazon linux, amazon linux 2, rhel 7.6/7.7/7.8, centos 7 ubuntu 16.04 and ubuntu 18.04 add  to the blacklist file. open  using your preferred text editor and add the following. rebuild the grub configuration. amazon linux, amazon linux 2, rhel 7.6/7.7/7.8, centos 7 ubuntu 16.04 and ubuntu 18.04 reboot the instance and reconnect to it. download the nvidia cuda toolkit installer. run the nvidia cuda toolkit installer. when prompted to accept the license agreement, enter  and press enter. at the cuda installer menu, ensure that all of the items are selected, highlight install, and then press enter. add the following statements to the shell startup scripts to ensure that the cuda paths are set each time that the instance starts. for bash shells, add the statements to  and .for tcsh shells, add the statements to .to confirm that the nvidia gpu drivers are functional, run the following command. the command should return information about the nvidia gpus, nvidia gpu drivers, and nvidia cuda toolkit. install nccl. for more information about nccl, see the . nccl requires nvidia cuda 7.0 or later. for more information about installing the latest version, see  on the nvidia website.to install nccl navigate to your home directory. clone the official nccl repository to the instance and navigate into the local cloned repository. build and install nccl and specify the cuda installation directory. the following command assumes that cuda is installed in the default directory. the aws-ofi-nccl plugin maps nccl's connection-oriented transport apis to libfabric's connection-less reliable interface. this enables you to use libfabric as a network provider while running nccl-based applications. for more information about the aws-ofi-nccl plugin, see the . to install the aws-ofi-nccl plugin navigate to your home directory. (ubuntu 16.04 and ubuntu 18.04) install the utilities that are required to install the aws-ofi-nccl plugin. to install the required utilities, run the following command. clone the  branch of the official aws aws-ofi-nccl repository to the instance and navigate into the local cloned repository. to generate the  script, run the  script. to generate the make files, run the  script and specify the mpi, libfabric, nccl, and cuda installation directories.  add the open mpi directory to the  variable. install the aws-ofi-nccl plugin. install the nccl tests. the nccl tests enable you to confirm that nccl is properly installed and that it is operating as expected. for more information about the nccl tests, see the . to install the nccl tests navigate to your home directory. clone the official nccl-tests repository to the instance and navigate into the local cloned repository. add the libfabric directory to the  variable.  amazon linux, amazon linux 2, rhel 7.6/7.7/7.8, centos 7 ubuntu 16.04 and ubuntu 18.04 (amazon linux, amazon linux 2, rhel 7.6/7.7/7.8, centos 7 only) by default, the make file looks for the required libraries in the  directory. however, with the open mpi installed with efa, the libraries are located in . to update the path in the make file, run the following command. install the nccl tests and specify the mpi, nccl, and cuda installation directories. run a test to ensure that your temporary instance is properly configured for efa and nccl.  to test your efa and nccl configuration create a host file that specifies the hosts on which to run the tests. the following command creates a host file named  that includes a reference to the instance itself.run the test and specify the host file () and the number of gpus to use (). the following command runs the  test on 8 gpus on the instance itself, and specifies the following environment variables. —specifies the fabric interface provider. this must be set to .—specifies the minimum number of send credits that the sender requests from the receiver.  is the recommended value for nccl jobs using efa. the value should only be increased for message transfers that are larger than 256 mb.—enables detailed debugging output. you can also specify  to print only the nccl version at the start of the test, or  to receive only error messages.—disables tree algorithms for the test.for more information about the nccl test arguments, see the  in the official nccl-tests repository. amazon linux, amazon linux 2, rhel 7.6/7.7/7.8, centos 7 ubuntu 16.04 and ubuntu 18.04 install the machine learning applications on the temporary instance. the installation procedure varies depending on the specific machine learning application. for more information about installing software on your linux instance, see . noteyou might need to refer to your machine learning application’s documentation for installation instructions. after you have installed the required software components, you create an ami that you can reuse to launch your efa-enabled instances. to create an ami from your temporary instance open the amazon ec2 console at . in the navigation pane, choose instances. select the temporary instance that you created and choose actions, image, create image. in the create image window, do the following: for image name, enter a descriptive name for the ami. (optional) for image description, enter a brief description of the ami. choose create image and then choose close. in the navigation pane, choose amis. locate the ami you created in the list. wait for the status to transition from  to  before continuing to the next step. at this point, you no longer need the temporary instance that you launched. you can terminate the instance to stop incurring charges for it. to terminate the temporary instance open the amazon ec2 console at . in the navigation pane, choose instances. select the temporary instance that you created and then choose actions, instance state, terminate, yes, terminate. launch your efa and nccl-enabled instances into a cluster placement group using the efa-enabled ami and the efa-enabled security group that you created earlier. to launch your efa and nccl-enabled instances into a cluster placement group open the amazon ec2 console at . choose launch instance. on the choose an ami page, choose my amis, find the ami that you created earlier, and then choose select. on the choose an instance type page, select p3dn.24xlarge and then choose next: configure instance details. on the configure instance details page, do the following: for number of instances, enter the number of efa and nccl-enabled instances that you want to launch. for network and subnet, select the vpc and subnet into which to launch the instances. for placement group, select add instance to placement group. for placement group name, select add to a new placement group, and then enter a descriptive name for the placement group. then for placement group strategy, select cluster. for efa, choose enable. in the network interfaces section, for device eth0, choose new network interface. you can optionally specify a primary ipv4 address and one or more secondary ipv4 addresses. if you are launching the instance into a subnet that has an associated ipv6 cidr block, you can optionally specify a primary ipv6 address and one or more secondary ipv6 addresses. choose next: add storage. on the add storage page, specify the volumes to attach to the instances in addition to the volumes specified by the ami (such as the root device volume). then choose next: add tags. on the add tags page, specify tags for the instances, such as a user-friendly name, and then choose next: configure security group. on the configure security group page, for assign a security group, select select an existing security group, and then select the security group that you created earlier. choose review and launch. on the review instance launch page, review the settings, and then choose launch to choose a key pair and to launch your instances. to enable your applications to run across all of the instances in your cluster, you must enable passwordless ssh access from the leader node to the member nodes. the leader node is the instance from which you run your applications. the remaining instances in the cluster are the member nodes. to enable passwordless ssh between the instances in the cluster select one instance in the cluster as the leader node, and connect to it. disable  and enable  on the leader node. open  using your preferred text editor and add the following. generate an rsa key pair. the key pair is created in the  directory. change the permissions of the private key on the leader node. open  using your preferred text editor and copy the key. for each member node in the cluster, do the following: connect to the instance. open  using your preferred text editor and add the public key that you copied earlier. to test that the passwordless ssh is functioning as expected, connect to your leader node and run the following command. you should connect to the member node without being prompted for a key or password. 
you can exchange one or more convertible reserved instances for another convertible reserved instance with a different configuration, including instance family, operating system, and tenancy. there are no limits to how many times you perform an exchange, as long as the target convertible reserved instance is of an equal or higher value than the convertible reserved instances that you are exchanging. when you exchange your convertible reserved instance, the number of instances for your current reservation is exchanged for a number of instances that cover the equal or higher value of the configuration of the target convertible reserved instance. amazon ec2 calculates the number of reserved instances that you can receive as a result of the exchange. topics if the following conditions are met, amazon ec2 processes your exchange request. your convertible reserved instance must be: activenot pending a previous exchange requestthe following rules apply: convertible reserved instances can only be exchanged for other convertible reserved instances currently offered by aws.convertible reserved instances are associated with a specific region, which is fixed for the duration of the reservation's term. you cannot exchange a convertible reserved instance for a convertible reserved instance in a different region.you can exchange one or more convertible reserved instances at a time for one convertible reserved instance only.to exchange a portion of a convertible reserved instance, you can modify it into two or more reservations, and then exchange one or more of the reservations for a new convertible reserved instance. for more information, see . for more information about modifying your reserved instances, see .all upfront convertible reserved instances can be exchanged for partial upfront convertible reserved instances, and vice versa. noteif the total upfront payment required for the exchange (true-up cost) is less than $0.00, aws automatically gives you a quantity of instances in the convertible reserved instance that ensures that true-up cost is $0.00 or more. noteif the total value (upfront price + hourly price * number of remaining hours) of the new convertible reserved instance is less than the total value of the exchanged convertible reserved instance, aws automatically gives you a quantity of instances in the convertible reserved instance that ensures that the total value is the same or higher than that of the exchanged convertible reserved instance.to benefit from better pricing, you can exchange a no upfront convertible reserved instance for an all upfront or partial upfront convertible reserved instance.you cannot exchange all upfront and partial upfront convertible reserved instances for no upfront convertible reserved instances.you can exchange a no upfront convertible reserved instance for another no upfront convertible reserved instance only if the new convertible reserved instance's hourly price is the same or higher than the exchanged convertible reserved instance's hourly price.  noteif the total value (hourly price * number of remaining hours) of the new convertible reserved instance is less than the total value of the exchanged convertible reserved instance, aws automatically gives you a quantity of instances in the convertible reserved instance that ensures that the total value is the same or higher than that of the exchanged convertible reserved instance.if you exchange multiple convertible reserved instances that have different expiration dates, the expiration date for the new convertible reserved instance is the date that's furthest in the future.if you exchange a single convertible reserved instance, it must have the same term (1-year or 3-years) as the new convertible reserved instance. if you merge multiple convertible reserved instances with different term lengths, the new convertible reserved instance has a 3-year term. for more information, see .after you exchange a convertible reserved instance, the original reservation is retired. its end date is the start date of the new reservation, and the end date of the new reservation is the same as the end date of the original convertible reserved instance. for example, if you modify a three-year reservation that had 16 months left in its term, the resulting modified reservation is a 16-month reservation with the same end date as the original one.exchanging convertible reserved instances is free. however, you may be required to pay a true-up cost, which is a prorated upfront cost of the difference between the convertible reserved instances that you had and the convertible reserved instances that you receive from the exchange. each convertible reserved instance has a list value. this list value is compared to the list value of the convertible reserved instances that you want in order to determine how many instance reservations you can receive from the exchange. for example: you have 1 x $35-list value convertible reserved instance that you want to exchange for a new instance type with a list value of $10. you can exchange your convertible reserved instance for three $10 convertible reserved instances. it's not possible to purchase half reservations; therefore you must purchase an additional convertible reserved instance to cover the remainder: the fourth convertible reserved instance has the same end date as the other three. if you are exchanging partial or all upfront convertible reserved instances, you pay the true-up cost for the fourth reservation. if the remaining upfront cost of your convertible reserved instances is $500, and the target reservation would normally cost $600 on a prorated basis, you are charged $100. if you merge two or more convertible reserved instances, the term of the new convertible reserved instance must be the same as the original convertible reserved instances, or the highest of the original convertible reserved instances. the expiration date for the new convertible reserved instance is the expiration date that's furthest in the future. for example, you have the following convertible reserved instances in your account: you can merge  and  and exchange them for a 1-year convertible reserved instance. you cannot exchange them for a 3-year convertible reserved instance. the expiration date of the new convertible reserved instance is 2018-12-31.you can merge  and  and exchange them for a 3-year convertible reserved instance. you cannot exchange them for a 1-year convertible reserved instance. the expiration date of the new convertible reserved instance is 2018-07-31.you can merge  and  and exchange them for a 3-year convertible reserved instance. you cannot exchange them for a 1-year convertible reserved instance. the expiration date of the new convertible reserved instance is 2019-12-31.you can use the modification process to split your convertible reserved instance into smaller reservations, and then exchange one or more of the new reservations for a new convertible reserved instance. the following examples demonstrate how you can do this. example example: convertible reserved instance with multiple instancesin this example, you have a  convertible reserved instance with four instances in the reservation. to exchange two  instances for an  instance:   modify the  convertible reserved instance by splitting it into two  convertible reserved instances with two instances each. exchange one of the new  convertible reserved instances for an  convertible reserved instance.  example example: convertible reserved instance with a single instancein this example, you have a  convertible reserved instance. to change it to a smaller  instance and a  instance:   modify the  convertible reserved instance by splitting it into two  convertible reserved instances. a single  instance has the same instance size footprint as two  instances. exchange one of the new  convertible reserved instances for an  convertible reserved instance.  for more information, see  and . you can exchange your convertible reserved instances using the amazon ec2 console or a command line tool. you can search for convertible reserved instances offerings and select your new configuration from the choices provided. to exchange convertible reserved instances using the amazon ec2 console open the amazon ec2 console at . choose reserved instances, select the convertible reserved instances to exchange, and choose actions, exchange reserved instance. select the attributes of the desired configuration using the drop-down menus, and choose find offering.  select a new convertible reserved instance the instance count column displays the number of reserved instances that you receive for the exchange. when you have selected a convertible reserved instance that meets your needs, choose exchange. the reserved instances that were exchanged are retired, and the new reserved instances are displayed in the amazon ec2 console. this process can take a few minutes to propagate. to exchange a convertible reserved instance, first find a target convertible reserved instance that meets your needs:  (aws cli) (tools for windows powershell)get a quote for the exchange, which includes the number of reserved instances you get from the exchange, and the true-up cost for the exchange:  (aws cli) (tools for windows powershell)finally, perform the exchange:  (aws cli) (tools for windows powershell)
with ec2-classic, your instances run in a single, flat network that you share with other customers. with amazon vpc, your instances run in a virtual private cloud (vpc) that's logically isolated to your aws account. the ec2-classic platform was introduced in the original release of amazon ec2. if you created your aws account after 2013-12-04, it does not support ec2-classic, so you must launch your amazon ec2 instances in a vpc. if your account does not support ec2-classic, we create a default vpc for you. by default, when you launch an instance, we launch it into your default vpc. alternatively, you can create a nondefault vpc and specify it when you launch an instance. the amazon ec2 console indicates which platforms you can launch instances into for the selected region, and whether you have a default vpc in that region. verify that the region you'll use is selected in the navigation bar. on the amazon ec2 console dashboard, look for supported platforms under account attributes. the dashboard displays the following under account attributes to indicate that the account supports both the ec2-classic platform and vpcs in this region, but the region does not have a default vpc.  the output of the  command includes both the  and  values for the  attribute. the dashboard displays the following under account attributes to indicate that the account requires a vpc to launch instances in this region, does not support the ec2-classic platform in this region, and the region has a default vpc with the identifier .  the output of the  command includes only the  value for the  attribute. most of the newer instance types require a vpc. the following are the only instance types supported in ec2-classic: general purpose: m1, m3, and t1compute optimized: c1, c3, and cc2memory optimized: cr1, m2, and r3storage optimized: d2, hs1, and i2accelerated computing: g2if your account supports ec2-classic but you have not created a nondefault vpc, you can do one of the following to launch instances that require a vpc: create a nondefault vpc and launch your vpc-only instance into it by specifying a subnet id or a network interface id in the request. note that you must create a nondefault vpc if you do not have a default vpc and you are using the aws cli, amazon ec2 api, or aws sdk to launch a vpc-only instance.launch your vpc-only instance using the amazon ec2 console. the amazon ec2 console creates a nondefault vpc in your account and launches the instance into the subnet in the first availability zone. the console creates the vpc with the following attributes:one subnet in each availability zone, with the public ipv4 addressing attribute set to  so that instances receive a public ipv4 address. for more information, see  in the amazon vpc user guide.an internet gateway, and a main route table that routes traffic in the vpc to the internet gateway. this enables the instances you launch in the vpc to communicate over the internet. for more information, see  in the amazon vpc user guide.a default security group for the vpc and a default network acl that is associated with each subnet. for more information, see  in the amazon vpc user guide.if you have other resources in ec2-classic, you can take steps to migrate them to a vpc. for more information, see . the following table summarizes the differences between instances launched in ec2-classic, instances launched in a default vpc, and instances launched in a nondefault vpc. if you're using ec2-classic, you must use security groups created specifically for ec2-classic. when you launch an instance in ec2-classic, you must specify a security group in the same region as the instance. you can't specify a security group that you created for a vpc when you launch an instance in ec2-classic. after you launch an instance in ec2-classic, you can't change its security groups. however, you can add rules to or remove rules from a security group, and those changes are automatically applied to all instances that are associated with the security group after a short period. your aws account automatically has a default security group per region for ec2-classic. if you try to delete the default security group, you'll get the following error: client.invalidgroup.reserved: the security group 'default' is reserved. you can create custom security groups. the security group name must be unique within your account for the region. to create a security group for use in ec2-classic, choose no vpc for the vpc. you can add inbound rules to your default and custom security groups. you can't change the outbound rules for an ec2-classic security group. when you create a security group rule, you can use a different security group for ec2-classic in the same region as the source or destination. to specify a security group for another aws account, add the aws account id as a prefix; for example, . in ec2-classic, you can have up to 500 security groups in each region for each account. you can add up to 100 rules to a security group. you can have up to 800 security group rules per instance. this is calculated as the multiple of rules per security group and security groups per instance. if you reference other security groups in your security group rules, we recommend that you use security group names that are 22 characters or less in length. amazon provides a dns server that resolves amazon-provided ipv4 dns hostnames to ipv4 addresses. in ec2-classic, the amazon dns server is located at . if you create a custom firewall configuration in ec2-classic, you must create a rule in your firewall that allows inbound traffic from port 53 (dns)—with a destination port from the ephemeral range—from the address of the amazon dns server; otherwise, internal dns resolution from your instances fails. if your firewall doesn't automatically allow dns query responses, then you need to allow traffic from the ip address of the amazon dns server. to get the ip address of the amazon dns server, use the following command from within your instance: if your account supports ec2-classic, there's one pool of elastic ip addresses for use with the ec2-classic platform and another for use with your vpcs. you can't associate an elastic ip address that you allocated for use with a vpc with an instance in ec2-classic, and vice- versa. however, you can migrate an elastic ip address you've allocated for use in the ec2-classic platform for use with a vpc. you cannot migrate an elastic ip address to another region. to allocate an elastic ip address for use in ec2-classic using the console open the amazon ec2 console at . in the navigation pane, choose elastic ips. choose allocate new address.  select classic, and then choose allocate. close the confirmation screen. if your account supports ec2-classic, you can migrate elastic ip addresses that you've allocated for use with ec2-classic platform to be used with a vpc, within the same region. this can assist you to migrate your resources from ec2-classic to a vpc; for example, you can launch new web servers in your vpc, and then use the same elastic ip addresses that you used for your web servers in ec2-classic for your new vpc web servers. after you've migrated an elastic ip address to a vpc, you cannot use it with ec2-classic. however, if required, you can restore it to ec2-classic. you cannot migrate an elastic ip address that was originally allocated for use with a vpc to ec2-classic. to migrate an elastic ip address, it must not be associated with an instance. for more information about disassociating an elastic ip address from an instance, see . you can migrate as many ec2-classic elastic ip addresses as you can have in your account. however, when you migrate an elastic ip address, it counts against your elastic ip address limit for vpcs. you cannot migrate an elastic ip address if it will result in your exceeding your limit. similarly, when you restore an elastic ip address to ec2-classic, it counts against your elastic ip address limit for ec2-classic. for more information, see .  you cannot migrate an elastic ip address that has been allocated to your account for less than 24 hours. you can migrate an elastic ip address from ec2-classic using the amazon ec2 console or the amazon vpc console. this option is only available if your account supports ec2-classic. to move an elastic ip address using the amazon ec2 console open the amazon ec2 console at . in the navigation pane, choose elastic ips. select the elastic ip address, and choose actions, move to vpc scope. in the confirmation dialog box, choose move elastic ip. you can restore an elastic ip address to ec2-classic using the amazon ec2 console or the amazon vpc console. to restore an elastic ip address to ec2-classic using the amazon ec2 console open the amazon ec2 console at . in the navigation pane, choose elastic ips. select the elastic ip address, choose actions, restore to ec2 scope. in the confirmation dialog box, choose restore. after you've performed the command to move or restore your elastic ip address, the process of migrating the elastic ip address can take a few minutes. use the  command to check whether your elastic ip address is still moving, or has completed moving. after you've moved your elastic ip address, you can view its allocation id on the elastic ips page in the allocation id field. if the elastic ip address is in a moving state for longer than 5 minutes, contact . to move an elastic ip address using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)to restore an elastic ip address to ec2-classic using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)to describe the status of your moving addresses using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)some resources and features in your aws account can be shared or accessed between ec2-classic and a vpc, for example, through classiclink. for more information, see . if your account supports ec2-classic, you might have set up resources for use in ec2-classic. if you want to migrate from ec2-classic to a vpc, you must recreate those resources in your vpc. for more information about migrating from ec2-classic to a vpc, see . the following resources can be shared or accessed between ec2-classic and a vpc. the following resources can't be shared or moved between ec2-classic and a vpc: spot instances
with amazon ebs, you can use any of the standard raid configurations that you can use with a traditional bare metal server, as long as that particular raid configuration is supported by the operating system for your instance. this is because all raid is accomplished at the software level. for greater i/o performance than you can achieve with a single volume, raid 0 can stripe multiple volumes together; for on-instance redundancy, raid 1 can mirror two volumes together. amazon ebs volume data is replicated across multiple servers in an availability zone to prevent the loss of data from the failure of any single component. this replication makes amazon ebs volumes ten times more reliable than typical commodity disk drives. for more information, see  in the amazon ebs product detail pages. noteyou should avoid booting from a raid volume. grub is typically installed on only one device in a raid array, and if one of the mirrored devices fails, you may be unable to boot the operating system. if you need to create a raid array on a windows instance, see  in the amazon ec2 user guide for windows instances. topics the following table compares the common raid 0 and raid 1 options. importantraid 5 and raid 6 are not recommended for amazon ebs because the parity write operations of these raid modes consume some of the iops available to your volumes. depending on the configuration of your raid array, these raid modes provide 20-30% fewer usable iops than a raid 0 configuration. increased cost is a factor with these raid modes as well; when using identical volume sizes and speeds, a 2-volume raid 0 array can outperform a 4-volume raid 6 array that costs twice as much.  creating a raid 0 array allows you to achieve a higher level of performance for a file system than you can provision on a single amazon ebs volume. a raid 1 array offers a "mirror" of your data for extra redundancy. before you perform this procedure, you need to decide how large your raid array should be and how many iops you want to provision. the resulting size of a raid 0 array is the sum of the sizes of the volumes within it, and the bandwidth is the sum of the available bandwidth of the volumes within it. the resulting size and bandwidth of a raid 1 array is equal to the size and bandwidth of the volumes in the array. for example, two 500 gib amazon ebs  volumes with 4,000 provisioned iops each will create a 1000 gib raid 0 array with an available bandwidth of 8,000 iops and 1,000 mib/s of throughput or a 500 gib raid 1 array with an available bandwidth of 4,000 iops and 500 mib/s of throughput. this documentation provides basic raid setup examples. for more information about raid configuration, performance, and recovery, see the linux raid wiki at . use the following procedure to create the raid array. note that you can get directions for windows instances from  in the amazon ec2 user guide for windows instances. to create a raid array on linux create the amazon ebs volumes for your array. for more information, see . importantcreate volumes with identical size and iops performance values for your array. make sure you do not create an array that exceeds the available bandwidth of your ec2 instance. for more information, see . attach the amazon ebs volumes to the instance that you want to host the array. for more information, see . use the mdadm command to create a logical raid device from the newly attached amazon ebs volumes. substitute the number of volumes in your array for number_of_volumes and the device names for each volume in the array (such as ) for device_name. you can also substitute my_raid with your own unique name for the array. noteyou can list the devices on your instance with the lsblk command to find the device names. (raid 0 only) to create a raid 0 array, execute the following command (note the  option to stripe the array): (raid 1 only) to create a raid 1 array, execute the following command (note the  option to mirror the array): allow time for the raid array to initialize and synchronize. you can track the progress of these operations with the following command: the following is example output: in general, you can display detailed information about your raid array with the following command: the following is example output: create a file system on your raid array, and give that file system a label to use when you mount it later. for example, to create an ext4 file system with the label my_raid, execute the following command: depending on the requirements of your application or the limitations of your operating system, you can use a different file system type, such as ext3 or xfs (consult your file system documentation for the corresponding file system creation command). to ensure that the raid array is reassembled automatically on boot, create a configuration file to contain the raid information: noteif you are using a linux distribution other than amazon linux, this file may need to be placed in different location. for more information, consult man mdadm.conf on your linux system.. create a new ramdisk image to properly preload the block device modules for your new raid configuration: create a mount point for your raid array. finally, mount the raid device on the mount point that you created: your raid device is now ready for use. (optional) to mount this amazon ebs volume on every system reboot, add an entry for the device to the  file. create a backup of your  file that you can use if you accidentally destroy or delete this file while you are editing it. open the  file using your favorite text editor, such as nano or vim. comment out any lines starting with "" and, at the end of the file, add a new line for your raid volume using the following format: the last three fields on this line are the file system mount options, the dump frequency of the file system, and the order of file system checks done at boot time. if you don't know what these values should be, then use the values in the example below for them (. for more information about  entries, see the fstab manual page (by entering man fstab on the command line). for example, to mount the ext4 file system on the device with the label my_raid at the mount point , add the following entry to . noteif you ever intend to boot your instance without this volume attached (for example, so this volume could move back and forth between different instances), you should add the  mount option that allows the instance to boot even if there are errors in mounting the volume. debian derivatives, such as ubuntu, must also add the  mount option. after you've added the new entry to , you need to check that your entry works. run the sudo mount -a command to mount all file systems in . if the previous command does not produce an error, then your  file is ok and your file system will mount automatically at the next boot. if the command does produce any errors, examine the errors and try to correct your . warningerrors in the  file can render a system unbootable. do not shut down a system that has errors in the  file. (optional) if you are unsure how to correct  errors, you can always restore your backup  file with the following command. if you want to back up the data on the ebs volumes in a raid array using snapshots, you must ensure that the snapshots are consistent. this is because the snapshots of these volumes are created independently. to restore ebs volumes in a raid array from snapshots that are out of sync would degrade the integrity of the array. to create a consistent set of snapshots for your raid array, use . multi-volume snapshots allow you to take point-in-time, data coordinated, and crash-consistent snapshots across multiple ebs volumes attached to an ec2 instance. you do not have to stop your instance to coordinate between volumes to ensure consistency because snapshots are automatically taken across multiple ebs volumes. for more information, see the steps for creating multi-volume snapshots under .  
instance metadata is data about your instance that you can use to configure or manage the running instance. instance metadata is divided into categories, for example, host name, events, and security groups. you can also use instance metadata to access user data that you specified when launching your instance. for example, you can specify parameters for configuring your instance, or include a simple script. you can build generic amis and use user data to modify the configuration files supplied at launch time. for example, if you run web servers for various small businesses, they can all use the same generic ami and retrieve their content from the amazon s3 bucket that you specify in the user data at launch. to add a new customer at any time, create a bucket for the customer, add their content, and launch your ami with the unique bucket name provided to your code in the user data. if you launch more than one instance at the same time, the user data is available to all instances in that reservation. each instance that is part of the same reservation has a unique  number, allowing you to write code that controls what to do. for example, the first host might elect itself as an initial master node in a cluster. for a detailed ami launch example, see . ec2 instances can also include dynamic data, such as an instance identity document that is generated when the instance is launched. for more information, see . importantalthough you can only access instance metadata and user data from within the instance itself, the data is not protected by authentication or cryptographic methods. anyone who has direct access to the instance, and potentially any software running on the instance, can view its metadata. therefore, you should not store sensitive data, such as passwords or long-lived encryption keys, as user data. topics 
you can use the following methods to troubleshoot an unreachable instance. topics the ability to reboot instances that are otherwise unreachable is valuable for both troubleshooting and general instance management.  just as you can reset a computer by pressing the reset button, you can reset ec2 instances using the amazon ec2 console, cli, or api. for more information, see .  warningfor windows instances, this operation performs a hard reboot that might result in data corruption.  console output is a valuable tool for problem diagnosis. it is especially useful for troubleshooting kernel problems and service configuration issues that could cause an instance to terminate or become unreachable before its ssh daemon can be started.  for linux/unix, the instance console output displays the exact console output that would normally be displayed on a physical monitor attached to a computer. the console output returns buffered information that was posted shortly after an instance transition state (start, stop, reboot, and terminate). the posted output is not continuously updated; only when it is likely to be of the most value. for windows instances, the instance console output includes the last three system event log errors. you can optionally retrieve the latest serial console output at any time during the instance lifecycle. this option is only supported on . it is not supported through the amazon ec2 console. noteonly the most recent 64 kb of posted output is stored, which is available for at least 1 hour after the last posting. only the instance owner can access the console output. you can retrieve the console output for your instances using the console or the command line. to get console output using the console open the amazon ec2 console at . in the left navigation pane, choose instances, and select the instance. choose actions, instance settings, get system log. to get console output using the command line you can use one of the following commands. for more information about these command line interfaces, see .  (aws cli) (aws tools for windows powershell)for more information about common system log errors, see . if you are unable to reach your instance via ssh or rdp, you can capture a screenshot of your instance and view it as an image. the image can provide visibility as to the status of the instance, and allows for quicker troubleshooting. you can generate screenshots while the instance is running or after it has crashed. there is no data transfer cost for this screenshot. the image is generated in jpg format and is no larger than 100 kb. this feature is not supported when the instance is using an nvidia grid driver or on bare metal instances (instances of type ). this feature is available in the following regions:  us east (n. virginia) regionus east (ohio) regionus west (oregon) regionus west (n. california) regioneurope (ireland) regioneurope (frankfurt) regionasia pacific (tokyo) regionasia pacific (seoul) regionasia pacific (singapore) regionasia pacific (sydney) region)south america (são paulo) region)asia pacific (mumbai) regioncanada (central) region)europe (london) regioneurope (paris) regionto access the instance console open the amazon ec2 console at . in the left navigation pane, choose instances. select the instance to capture. choose actions, instance settings. choose get instance screenshot. right-click on the image to download and save it. to capture a screenshot using the command line you can use one of the following commands. the returned content is base64-encoded. for more information about these command line interfaces, see .  (aws cli) (amazon ec2 query api)if there is an unrecoverable issue with the hardware of an underlying host computer, aws may schedule an instance stop event. you are notified of such an event ahead of time by email. to recover an amazon ebs-backed instance running on a host computer that failed back up any important data on your instance store volumes to amazon ebs or amazon s3. stop the instance. start the instance. restore any important data. for more information, see . to recover an instance store-backed instance running on a host computer that failed create an ami from the instance. upload the image to amazon s3. back up important data to amazon ebs or amazon s3. terminate the instance. launch a new instance from the ami. restore any important data to the new instance. for more information, see . 
you can see the credit balance for each instance in the amazon ec2 per-instance metrics of the cloudwatch console. topics t3, t3a, and t2 instances have these additional cloudwatch metrics, which are updated every five minutes:  – the number of cpu credits spent during the measurement period. – the number of cpu credits that an instance has accrued. this balance is depleted when the cpu bursts and cpu credits are spent more quickly than they are earned. – the number of surplus cpu credits spent to sustain cpu utilization when the  value is zero. – the number of surplus cpu credits exceeding the  that can be earned in a 24-hour period, and thus attracting an additional charge.the last two metrics apply only to instances configured as . the following table describes the cloudwatch metrics for burstable performance instances. for more information, see . the cpu credit usage of instances is calculated using the instance cloudwatch metrics described in the preceding table. amazon ec2 sends the metrics to cloudwatch every five minutes. a reference to the prior value of a metric at any point in time implies the previous value of the metric, sent five minutes ago. the cpu credit balance increases if cpu utilization is below the baseline, when the credits spent are less than the credits earned in the prior five-minute interval. the cpu credit balance decreases if cpu utilization is above the baseline, when the credits spent are more than the credits earned in the prior five-minute interval. mathematically, this is captured by the following equation: example   the size of the instance determines the number of credits that the instance can earn per hour and the number of earned credits that it can accrue in the credit balance. for information about the number of credits earned per hour, and the credit balance limit for each instance size, see the . examplethis example uses a  instance. to calculate the  value of the instance, use the preceding equation as follows:  – the current credit balance to calculate. – the credit balance five minutes ago. in this example, the instance had accrued two credits. – a  instance earns six credits per hour. – represents the five-minute interval between cloudwatch metric publication. multiply the credits earned per hour by 5/60 (five minutes) to get the number of credits that the instance earned in the past five minutes. a  instance earns 0.5 credits every five minutes. – how many credits the instance spent in the past five minutes. in this example, the instance spent one credit in the past five minutes.using these values, you can calculate the  value: example   when a t3, t3a, or t2 instance needs to burst above the baseline, it always spends accrued credits before spending surplus credits. when it depletes its accrued cpu credit balance, it can spend surplus credits to burst cpu for as long as it needs. when cpu utilization falls below the baseline, surplus credits are always paid down before the instance accrues earned credits. we use the term  in the following equations to reflect the activity that occurs in this five-minute interval. we use this value to arrive at the values for the  and  cloudwatch metrics.  example   a value of  for  indicates that the instance spent all its earned credits for bursting, and no surplus credits were spent. as a result, both  and  are set to . a positive  value indicates that the instance accrued earned credits, and previous surplus credits, if any, were paid down. as a result, the  value is assigned to , and the  is set to . the instance size determines the  that it can accrue. example   a negative  value indicates that the instance spent all its earned credits that it accrued and, in addition, also spent surplus credits for bursting. as a result, the  value is assigned to  and  is set to . again, the instance size determines the  that it can accrue. example   if the surplus credits spent exceed the maximum credits that the instance can accrue, the surplus credit balance is set to the maximum, as shown in the preceding equation. the remaining surplus credits are charged as represented by the  metric. example   finally, when the instance terminates, any surplus credits tracked by the  are charged. if the instance is switched from  to , any remaining  is also charged. 
spot instance requests are subject to the following limits: topics limits on spot instance requests are dynamic. when your account is new, there is an initial default limit on spot instance requests per region. this limit can increase over time. if you submit a spot instance request and you receive the error , you can request a limit increase for spot instance requests. for more information about viewing your current limits and requesting limit increases, see . if you terminate spot instances but do not cancel the spot instance requests, the requests count against your dynamic spot instance request limit until amazon ec2 detects the spot instance terminations and closes the requests. the usual amazon ec2 limits apply to instances launched by a spot fleet or an ec2 fleet, such as spot request price limits, instance limits, and volume limits. in addition, the following limits apply: the number of active spot fleets and ec2 fleets per region: 1,000*the number of launch specifications per fleet: 300*the launch template plus number of overrides per fleet: 300*the size of the user data in a launch specification: 16 kb*the target capacity per spot fleet or ec2 fleet: 10,000the target capacity across all spot fleets and ec2 fleets in a region: 100,000a spot fleet request or an ec2 fleet request can't span regions.a spot fleet request or an ec2 fleet request can't span different subnets from the same availability zone.if you need more than the default limits for target capacity, complete the aws support center  form to request a limit increase. for limit type, choose ec2 fleet, choose a region, and then choose target fleet capacity per fleet (in units) or target fleet capacity per region (in units), or both. * these are hard limits. you cannot request a limit increase for these limits. if you plan to use your t3 spot instances immediately and for a short duration, with no idle time for accruing cpu credits, we recommend that you launch your t3 spot instances in  mode to avoid paying higher costs. if you launch your t3 spot instances in  mode and burst cpu immediately, you'll spend surplus credits for bursting. if you use the instance for a short duration, your instance doesn't have time to accrue cpu credits to pay down the surplus credits, and you are charged for the surplus credits when you terminate your instance.  mode for t3 spot instances is suitable only if the instance runs for long enough to accrue cpu credits for bursting. otherwise, paying for surplus credits makes t3 spot instances more expensive than m5 or c5 instances. for more information, see . launch credits are meant to provide a productive initial launch experience for t2 instances by providing sufficient compute resources to configure the instance. repeated launches of t2 instances to access new launch credits is not permitted. if you require sustained cpu, you can earn credits (by idling over some period), use , or use an instance type with dedicated cpu (for example, ). 
you can create, use, and manage an efa much like any other elastic network interface in amazon ec2. however, unlike elastic network interfaces, efas cannot be attached to or detached from an instance in a running state. to use an efa, you must do the following: use one of the following supported instance types: , , , , , , , , , , and .use one of the following supported amis: amazon linux, amazon linux 2, rhel 7.6, rhel 7.7, rhel 7.8, centos 7, ubuntu 16.04, and ubuntu 18.04.install the efa software components. for more information, see  and .use a security group that allows all inbound and outbound traffic to and from the security group itself. for more information, see .topics you can create an efa in a subnet in a vpc. you can't move the efa to another subnet after it's created, and you can only attach it to stopped instances in the same availability zone. to create a new efa using the console open the amazon ec2 console at . in the navigation pane, choose network interfaces. choose create network interface. for description, enter a descriptive name for the efa. for subnet, select the subnet in which to create the efa. for private ip, enter the primary private ipv4 address. if you don't specify an ipv4 address, we select an available private ipv4 address from the selected subnet. (ipv6 only) if you selected a subnet that has an associated ipv6 cidr block, you can optionally specify an ipv6 address in the ipv6 ip field. for security groups, select one or more security groups. for efa, choose enabled. choose yes, create. to create a new efa using the aws cliuse the  command and for , specify , as shown in the following example. you can attach an efa to any supported instance that is in the  state. you cannot attach an efa to an instance that is in the  state. for more information about the supported instance types, see . you attach an efa to an instance in the same way that you attach an elastic network interface to an instance. for more information, see . to attach an existing efa when launching an instance (aws cli)use the  command and for networkinterfaceid, specify the id of the efa, as shown in the following example. to attach a new efa when launching an instance (aws cli)use the  command and for interfacetype, specify , as shown in the following example. you can create a launch template that contains the configuration information needed to launch efa-enabled instances. to create an efa-enabled launch template, create a new launch template and specify a supported instance type, your efa-enabled ami, and an efa-enabled security group. for more information, see . you can leverage launch templates to launch efa-enabled instances with other aws services, such as aws batch. for more information about creating launch templates, see . if you have an elastic ip (ipv4) address, you can associate it with an efa. if your efa is provisioned in a subnet that has an associated ipv6 cidr block, you can assign one or more ipv6 addresses to the efa. you assign an elastic ip (ipv4) and ipv6 address to an efa in the same way that you assign an ip address to an elastic network interface. for more information, see: you unassign an elastic ip (ipv4) and ipv6 address from an efa in the same way that you unassign an ip address from an elastic network interface. for more information, see:  you can change the security group that is associated with an efa. to enable os-bypass functionality, the efa must be a member of a security group that allows all inbound and outbound traffic to and from the security group itself. you change the security group that is associated with an efa in the same way that you change the security group that is associated with an elastic network interface. for more information, see . to detach an efa from an instance, you must first stop the instance. you cannot detach an efa from an instance that is in the running state. you detach an efa from an instance in the same way that you detach an elastic network interface from an instance. for more information, see . you can view all of the efas in your account. you view efas in the same way that you view elastic network interfaces. for more information, see . to delete an efa, you must first detach it from the instance. you cannot delete an efa while it is attached to an instance. you delete efas in the same way that you delete elastic network interfaces. for more information, see . 
when your needs change, you can modify your standard or convertible reserved instances and continue to benefit from the billing benefit. you can modify attributes such as the availability zone, instance size (within the same instance family), and scope of your reserved instance. noteyou can also exchange a convertible reserved instance for another convertible reserved instance with a different configuration. for more information, see . you can modify all or a subset of your reserved instances. you can separate your original reserved instances into two or more new reserved instances. for example, if you have a reservation for 10 instances in  and decide to move 5 instances to , the modification request results in two new reservations: one for 5 instances in  and the other for 5 instances in . you can also merge two or more reserved instances into a single reserved instance. for example, if you have four  reserved instances of one instance each, you can merge them to create one  reserved instance. for more information, see . after modification, the benefit of the reserved instances is applied only to instances that match the new parameters. for example, if you change the availability zone of a reservation, the capacity reservation and pricing benefits are automatically applied to instance usage in the new availability zone. instances that no longer match the new parameters are charged at the on-demand rate, unless your account has other applicable reservations. if your modification request succeeds: the modified reservation becomes effective immediately and the pricing benefit is applied to the new instances beginning at the hour of the modification request. for example, if you successfully modify your reservations at 9:15pm, the pricing benefit transfers to your new instance at 9:00pm. you can get the effective date of the modified reserved instances by using the  command.the original reservation is retired. its end date is the start date of the new reservation, and the end date of the new reservation is the same as the end date of the original reserved instance. if you modify a three-year reservation that had 16 months left in its term, the resulting modified reservation is a 16-month reservation with the same end date as the original one.the modified reservation lists a $0 fixed price and not the fixed price of the original reservation.the fixed price of the modified reservation does not affect the discount pricing tier calculations applied to your account, which are based on the fixed price of the original reservation.if your modification request fails, your reserved instances maintain their original configuration, and are immediately available for another modification request. there is no fee for modification, and you do not receive any new bills or invoices. you can modify your reservations as frequently as you like, but you cannot change or cancel a pending modification request after you submit it. after the modification has completed successfully, you can submit another modification request to roll back any changes you made, if needed. topics you can modify these attributes as follows. requirements amazon ec2 processes your modification request if there is sufficient capacity for your target configuration (if applicable), and if the following conditions are met: the reserved instance cannot be modified before or at the same time that you purchase itthe reserved instance must be activethere cannot be a pending modification requestthe reserved instance is not listed in the reserved instance marketplacethere must be a match between the instance size footprint of the active reservation and the target configuration. for more information, see .the input reserved instances are all standard reserved instances or all convertible reserved instances, not some of each typethe input reserved instances must expire within the same hour, if they are standard reserved instancesthe reserved instance is not a g4 instance.you can modify the instance size of a reserved instance if the following requirements are met. requirements the platform is linux/unix.you must select another instance size in the same instance family. for example, you cannot modify an reserved instance from  to , whether you use the same size or a different size. you cannot modify the instance size of reserved instances for the following instances, because each of these instance families has only one size: the original and modified reserved instance must have the same instance size footprint.topics each reserved instance has an instance size footprint, which is determined by the normalization factor of the instance size and the number of instances in the reservation. when you modify the instance sizes in an reserved instance, the footprint of the target configuration must match that of the original configuration, otherwise the modification request is not processed. to calculate the instance size footprint of a reserved instance, multiply the number of instances by the normalization factor. in the amazon ec2 console, the normalization factor is measured in units. the following table describes the normalization factor for the instance sizes in an instance family. for example,  has a normalization factor of 2, so a reservation for four  instances has a footprint of 8 units. you can allocate your reservations into different instance sizes across the same instance family as long as the instance size footprint of your reservation remains the same. for example, you can divide a reservation for one  (1 @ 4 units) instance into four  (4 @ 1 unit) instances. similarly, you can combine a reservation for four  instances into one  instance. however, you cannot change your reservation for two  instances into one  instance because the footprint of the modified reservation (4 units) is larger than the footprint of the existing reservation (2 units). in the following example, you have a reservation with two  instances (1 unit) and a reservation with one  instance (1 unit). if you merge both of these reservations to a single reservation with one  instance (2 units), the footprint of the modified reservation equals the footprint of the combined reservations.  you can also modify a reservation to divide it into two or more reservations. in the following example, you have a reservation with a  instance (2 units). you can divide the reservation into two reservations, one with two  instances (.5 units) and the other with three  instances (1.5 units).  you can modify a reservation with  instances using other sizes within the same instance family. similarly, you can modify a reservation with instances other than bare metal instances using the  size within the same instance family. generally, a bare metal instance is the same size as the largest available instance size within the same instance family. for example, an  instance is the same size as an  instance, so they have the same normalization factor. the following table describes the normalization factor for the bare metal instance sizes in the instance families that have bare metal instances. the normalization factor for  instances depends on the instance family, unlike the other instance sizes. for example, an  instance has a normalization factor of 128. if you purchase an  default tenancy amazon linux/unix reserved instance, you can divide the reservation as follows: an  is the same size as an  instance, so its normalization factor is 128 (128/1). the reservation for one  instance can be modified into one  instance.an  is half the size of an  instance, so its normalization factor is 64 (128/2). the reservation for one  instance can be divided into two  instances.an  is a quarter the size of an  instance, so its normalization factor is 32 (128/4). the reservation for one  instance can be divided into four  instances.before you modify your reserved instances, ensure that you have read the applicable . before you modify the instance size, calculate the total  of the reservations that you want to modify and ensure that it matches the total instance size footprint of your target configurations. to modify your reserved instances using the aws management console open the amazon ec2 console at . on the reserved instances page, select one or more reserved instances to modify, and choose actions, modify reserved instances. noteif your reserved instances are not in the active state or cannot be modified, modify reserved instances is disabled. the first entry in the modification table displays attributes of selected reserved instances, and at least one target configuration beneath it. the units column displays the total instance size footprint. choose add for each new configuration to add. modify the attributes as needed for each configuration, and then choose continue: scope: choose whether the configuration applies to an availability zone or to the whole region.availability zone: choose the required availability zone. not applicable for regional reserved instances.instance type: select the required instance type. the combined configurations must equal the instance size footprint of your original configurations.count: specify the number of instances. to split the reserved instances into multiple configurations, reduce the count, choose add, and specify a count for the additional configuration. for example, if you have a single configuration with a count of 10, you can change its count to 6 and add a configuration with a count of 4. this process retires the original reserved instance after the new reserved instances are activated.to confirm your modification choices when you finish specifying your target configurations, choose submit modifications. you can determine the status of your modification request by looking at the state column in the reserved instances screen. the following are the possible states. active (pending modification) — transition state for original reserved instancesretired (pending modification) — transition state for original reserved instances while new reserved instances are being createdretired — reserved instances successfully modified and replacedactive — one of the following:new reserved instances created from a successful modification requestoriginal reserved instances after a failed modification requestto modify your reserved instances using the command line to modify your reserved instances, you can use one of the following commands:  (aws cli) (aws tools for windows powershell)to get the status of your modification request (, , or ), use one of the following commands:  (aws cli) (aws tools for windows powershell)if the target configuration settings that you requested were unique, you receive a message that your request is being processed. at this point, amazon ec2 has only determined that the parameters of your modification request are valid. your modification request can still fail during processing due to unavailable capacity. in some situations, you might get a message indicating incomplete or failed modification requests instead of a confirmation. use the information in such messages as a starting point for resubmitting another modification request. ensure that you have read the applicable  before submitting the request. not all selected reserved instances can be processed for modificationamazon ec2 identifies and lists the reserved instances that cannot be modified. if you receive a message like this, go to the reserved instances page in the amazon ec2 console and check the information for the reserved instances. error in processing your modification requestyou submitted one or more reserved instances for modification and none of your requests can be processed. depending on the number of reservations you are modifying, you can get different versions of the message.  amazon ec2 displays the reasons why your request cannot be processed. for example, you might have specified the same target configuration—a combination of availability zone and platform—for one or more subsets of the reserved instances you are modifying. try submitting the modification requests again, but ensure that the instance details of the reservations match, and that the target configurations for all subsets being modified are unique. 
amazon ec2 provides enhanced networking capabilities through the elastic network adapter (ena). to use enhanced networking, you must install the required ena module and enable ena support. topics to prepare for enhanced networking using the ena, set up your instance as follows: select from the following supported instance types: a1, c5, c5a, c5d, c5n, c6g,  f1, g3, g4, h1, i3, i3en, inf1, , m5, m5a, m5ad, m5d, m5dn, m5n, m6g,  p2, p3, r4, r5, r5a, r5ad, r5d, r5dn, r5n, r6g,  t3, t3a, , , , , , x1, x1e, and z1d.launch the instance using a supported version of the linux kernel and a supported distribution, so that ena enhanced networking is enabled for your instance automatically. for more information, see .ensure that the instance has internet connectivity.install and configure the  or the  on any computer you choose, preferably your local desktop or laptop. for more information, see . enhanced networking cannot be managed from the amazon ec2 console.if you have important data on the instance that you want to preserve, you should back that data up now by creating an ami from your instance. updating kernels and kernel modules, as well as enabling the  attribute, might render incompatible instances or operating systems unreachable. if you have a recent backup, your data will still be retained if this happens.the following documentation provides a summary of the network performance for the instance types that support ena enhanced networking: the following amis include the required ena module and have ena support enabled: amazon linux 2amazon linux ami 2018.03ubuntu 14.04 (with  kernel) or laterred hat enterprise linux 7.4 or latersuse linux enterprise server 12 sp2 or latercentos 7.4.1708 or laterfreebsd 11.1 or laterdebian gnu/linux 9 or laterto test whether enhanced networking is already enabled, verify that the  module is installed on your instance and that the  attribute is set. if your instance satisfies these two conditions, then the ethtool -i ethn command should show that the module is in use on the network interface. kernel module (ena)to verify that the  module is installed, use the modinfo command as shown in the following example. in the above amazon linux case, the  module is installed. in the above ubuntu instance, the module is not installed, so you must first install it. for more information, see . instance attribute (enasupport) to check whether an instance has the enhanced networking  attribute set, use one of the following commands. if the attribute is set, the response is true.  (aws cli)  (tools for windows powershell) image attribute (enasupport)to check whether an ami has the enhanced networking  attribute set, use one of the following commands. if the attribute is set, the response is true.  (aws cli)  (tools for windows powershell) network interface driveruse the following command to verify that the  module is being used on a particular interface, substituting the interface name that you want to check. if you are using a single interface (default), it this is . if the operating system supports , this could be a name like . in the following example, the  module is not loaded, because the listed driver is . in this example, the  module is loaded and at the minimum recommended version. this instance has enhanced networking properly configured. amazon linux 2 and the latest versions of the amazon linux ami include the module required for enhanced networking with ena installed and have ena support enabled. therefore, if you launch an instance with an hvm version of amazon linux on a supported instance type, enhanced networking is already enabled for your instance. for more information, see . if you launched your instance using an older amazon linux ami and it does not have enhanced networking enabled already, use the following procedure to enable enhanced networking. to enable enhanced networking on amazon linux ami connect to your instance. from the instance, run the following command to update your instance with the newest kernel and kernel modules, including : from your local computer, reboot your instance using the amazon ec2 console or one of the following commands:  (aws cli),  (aws tools for windows powershell). connect to your instance again and verify that the  module is installed and at the minimum recommended version using the modinfo ena command from . [ebs-backed instance] from your local computer, stop the instance using the amazon ec2 console or one of the following commands:  (aws cli),  (aws tools for windows powershell). if your instance is managed by aws opsworks, you should stop the instance in the aws opsworks console so that the instance state remains in sync. [instance store-backed instance] you can't stop the instance to modify the attribute. instead, proceed to this procedure: . from your local computer, enable the enhanced networking attribute using one of the following commands:  (aws cli)  (tools for windows powershell) (optional) create an ami from the instance, as described in . the ami inherits the enhanced networking  attribute from the instance. therefore, you can use this ami to launch another instance with enhanced networking enabled by default. from your local computer, start the instance using the amazon ec2 console or one of the following commands:  (aws cli),  (aws tools for windows powershell). if your instance is managed by aws opsworks, you should start the instance in the aws opsworks console so that the instance state remains in sync. connect to your instance and verify that the  module is installed and loaded on your network interface using the ethtool -i ethn command from . if you are unable to connect to your instance after enabling enhanced networking, see . to enable enhanced networking on amazon linux ami (instance store-backed instances) follow the previous procedure until the step where you stop the instance. create a new ami as described in , making sure to enable the enhanced networking attribute when you register the ami.  (aws cli)  (aws tools for windows powershell) the latest ubuntu hvm amis include the module required for enhanced networking with ena installed and have ena support enabled. therefore, if you launch an instance with the latest ubuntu hvm ami on a supported instance type, enhanced networking is already enabled for your instance. for more information, see .  if you launched your instance using an older ami and it does not have enhanced networking enabled already, you can install the  kernel package to get the latest enhanced networking drivers and update the required attribute. to install the linux-aws kernel package (ubuntu 16.04 or later)ubuntu 16.04 and 18.04 ship with the ubuntu custom kernel (linux-aws kernel package). to use a different kernel, contact . to install the linux-aws kernel package (ubuntu trusty 14.04) connect to your instance. update the package cache and packages. importantif during the update process you are prompted to install , use  to install  onto, and then choose to keep the current version of . [ebs-backed instance] from your local computer, stop the instance using the amazon ec2 console or one of the following commands:  (aws cli),  (aws tools for windows powershell). if your instance is managed by aws opsworks, you should stop the instance in the aws opsworks console so that the instance state remains in sync. [instance store-backed instance] you can't stop the instance to modify the attribute. instead, proceed to this procedure: . from your local computer, enable the enhanced networking attribute using one of the following commands:  (aws cli)  (tools for windows powershell) (optional) create an ami from the instance, as described in . the ami inherits the enhanced networking  attribute from the instance. therefore, you can use this ami to launch another instance with enhanced networking enabled by default. from your local computer, start the instance using the amazon ec2 console or one of the following commands:  (aws cli),  (aws tools for windows powershell). if your instance is managed by aws opsworks, you should start the instance in the aws opsworks console so that the instance state remains in sync. to enable enhanced networking on ubuntu (instance store-backed instances) follow the previous procedure until the step where you stop the instance. create a new ami as described in , making sure to enable the enhanced networking attribute when you register the ami.  (aws cli)  (aws tools for windows powershell) the latest amis for red hat enterprise linux, suse linux enterprise server, and centos include the module required for enhanced networking with ena and have ena support enabled. therefore, if you launch an instance with the latest ami on a supported instance type, enhanced networking is already enabled for your instance. for more information, see . the following procedure provides the general steps for enabling enhanced networking on a linux distribution other than amazon linux ami or ubuntu. for more information, such as detailed syntax for commands, file locations, or package and tool support, see the documentation for your linux distribution. to enable enhanced networking on linux connect to your instance. clone the source code for the  module on your instance from github at . (suse linux enterprise server 12 sp2 and later include ena 2.02 by default, so you are not required to download and compile the ena driver. for suse linux enterprise server 12 sp2 and later, you should file a request to add the driver version you want to the stock kernel).  compile and install the  module on your instance. these steps depend on the linux distribution. for more information about compiling the module on red hat enterprise linux, see the . run the sudo depmod command to update module dependencies. update  on your instance to ensure that the new module loads at boot time. for example, if your distribution supports dracut, you can use the following command. determine if your system uses predictable network interface names by default. systems that use systemd or udev versions 197 or greater can rename ethernet devices and they do not guarantee that a single network interface will be named . this behavior can cause problems connecting to your instance. for more information and to see other configuration options, see  on the freedesktop.org website. you can check the systemd or udev versions on rpm-based systems with the following command. in the above red hat enterprise linux 7 example, the systemd version is 208, so predictable network interface names must be disabled. disable predictable network interface names by adding the  option to the  line in . rebuild the grub configuration file. [ebs-backed instance] from your local computer, stop the instance using the amazon ec2 console or one of the following commands:  (aws cli),  (aws tools for windows powershell). if your instance is managed by aws opsworks, you should stop the instance in the aws opsworks console so that the instance state remains in sync. [instance store-backed instance] you can't stop the instance to modify the attribute. instead, proceed to this procedure: . from your local computer, enable the enhanced networking  attribute using one of the following commands:  (aws cli)  (tools for windows powershell) (optional) create an ami from the instance, as described in  . the ami inherits the enhanced networking  attribute from the instance. therefore, you can use this ami to launch another instance with enhanced networking enabled by default. importantif your instance operating system contains an  file, you must delete it before creating the ami. this file contains the mac address for the ethernet adapter of the original instance. if another instance boots with this file, the operating system will be unable to find the device and  might fail, causing boot issues. this file is regenerated at the next boot cycle, and any instances launched from the ami create their own version of the file. from your local computer, start the instance using the amazon ec2 console or one of the following commands:  (aws cli),  (aws tools for windows powershell). if your instance is managed by aws opsworks, you should start the instance in the aws opsworks console so that the instance state remains in sync. (optional) connect to your instance and verify that the module is installed. if you are unable to connect to your instance after enabling enhanced networking, see . to enable enhanced networking on linux (instance store–backed instances) follow the previous procedure until the step where you stop the instance. create a new ami as described in , making sure to enable the enhanced networking attribute when you register the ami.  (aws cli)  (aws tools for windows powershell) this method is for testing and feedback purposes only. it is not intended for use with production deployments. for production deployments, see . importantusing dkms voids the support agreement for your subscription. using kmod configurations is an acceptable alternative for running the latest available kernel modules. to enable enhanced networking with ena on ubuntu (ebs-backed instances) follow steps 1 and 2 in . install the  packages to compile the kernel module and the  package so that your  module is rebuilt every time your kernel is updated. clone the source for the  module on your instance from github at . move the  package to the  directory so dkms can find it and build it for each kernel update. append the version number (you can find the current version number in the release notes) of the source code to the directory name. for example, version  is shown in the following example. create the dkms configuration file with the following values, substituting your version of . create the file. edit the file and add the following values. add, build, and install the  module on your instance using dkms. add the module to dkms. build the module using the dkms command. install the module using dkms. rebuild  so the correct module is loaded at boot time. verify that the  module is installed using the modinfo ena command from . continue with step 3 in .  for additional information about troubleshooting your ena adapter, see . 
to create an instance store-backed linux ami, start from an instance that you've launched from an existing instance store-backed linux ami. after you've customized the instance to suit your needs, bundle the volume and register a new ami, which you can use to launch new instances with these customizations. the ami creation process is different for amazon ebs-backed amis. for more information about the differences between amazon ebs-backed and instance store-backed instances, and how to determine the root device type for your instance, see . if you need to create an amazon ebs-backed linux ami, see . the following diagram summarizes the process of creating an ami from an instance store-backed instance.  first, launch an instance from an ami that's similar to the ami that you'd like to create. you can connect to your instance and customize it. when the instance is set up the way you want it, you can bundle it. it takes several minutes for the bundling process to complete. after the process completes, you have a bundle, which consists of an image manifest () and files (xx) that contain a template for the root volume. next you upload the bundle to your amazon s3 bucket and then register your ami. when you launch an instance using the new ami, we create the root volume for the instance using the bundle that you uploaded to amazon s3. the storage space used by the bundle in amazon s3 incurs charges to your account until you delete it. for more information, see . if you add instance store volumes to your instance in addition to the root device volume, the block device mapping for the new ami contains information for these volumes, and the block device mappings for instances that you launch from the new ami automatically contain information for these volumes. for more information, see . before you can create an ami, you must complete the following tasks: install the ami tools. for more information, see .install the aws cli. for more information, see .ensure that you have an amazon s3 bucket for the bundle. to create an amazon s3 bucket, open the amazon s3 console and click create bucket. alternatively, you can use the aws cli  command.ensure that you have your aws account id. for more information, see  in the aws general reference.ensure that you have your access key id and secret access key. for more information, see  in the aws general reference.ensure that you have an x.509 certificate and corresponding private key.if you need to create an x.509 certificate, see . the x.509 certificate and private key are used to encrypt and decrypt your ami.[china (beijing)] use the  certificate.[aws govcloud (us-west)] use the  certificate.connect to your instance and customize it. for example, you can install software and applications, copy data, delete temporary files, and modify the linux configuration.tasks 
as your needs change, you might find that your instance is over-utilized (the instance type is too small) or under-utilized (the instance type is too large). if this is the case, you can change the size of your instance. for example, if your  instance is too small for its workload, you can change it to another instance type that is appropriate for the workload. you might also want to migrate from a previous generation instance type to a current generation instance type to take advantage of some features; for example, support for ipv6. if the root device for your instance is an ebs volume, you can change the size of the instance simply by changing its instance type, which is known as resizing it. if the root device for your instance is an instance store volume, you must migrate your application to a new instance with the instance type that you need. for more information about root device volumes, see . when you resize an instance, you must select an instance type that is compatible with the configuration of the instance. if the instance type that you want is not compatible with the instance configuration you have, then you must migrate your application to a new instance with the instance type that you need. importantwhen you resize an instance, the resized instance usually has the same number of instance store volumes that you specified when you launched the original instance. with instance types that support nvme instance store volumes (which are available by default), the resized instance might have additional instance store volumes, depending on the ami. otherwise, you can migrate your application to an instance with a new instance type manually, specifying the number of instance store volumes that you need when you launch the new instance. topics you can resize an instance only if its current instance type and the new instance type that you want are compatible in the following ways: virtualization type: linux amis use one of two types of virtualization: paravirtual (pv) or hardware virtual machine (hvm). you can't resize an instance that was launched from a pv ami to an instance type that is hvm only. for more information, see . to check the virtualization type of your instance, see the virtualization field on the details pane of the instances screen in the amazon ec2 console.architecture: amis are specific to the architecture of the processor, so you must select an instance type with the same processor architecture as the current instance type. for example:if you are resizing an instance type with a processor based on the arm architecture, you are limited to the instance types that support a processor based on the arm architecture, such as a1 and m6g.the following instance types are the only instance types that support 32-bit amis: , , , , , , , , and . if you are resizing a 32-bit instance, you are limited to these instance types.network: newer instance types must be launched in a vpc. therefore, you can't resize an instance in the ec2-classic platform to a instance type that is available only in a vpc unless you have a nondefault vpc. to check whether your instance is in a vpc, check the vpc id value on the details pane of the instances screen in the amazon ec2 console. for more information, see .enhanced networking: instance types that support  require the necessary drivers installed. for example, the a1, c5, c5a, c5d, c5n, c6g,  f1, g3, g4, h1, i3, i3en, inf1, , m5, m5a, m5ad, m5d, m5dn, m5n, m6g,  p2, p3, r4, r5, r5a, r5ad, r5d, r5dn, r5n, r6g,  t3, t3a, , , , , , x1, x1e, and z1d instance types require ebs-backed amis with the elastic network adapter (ena) drivers installed. to resize an existing instance to an instance type that supports enhanced networking, you must first install the  or  on your instance, as appropriate.nvme: ebs volumes are exposed as nvme block devices on instances built on the . if you resize an instance from an instance type that does not support nvme to an instance type that supports nvme, you must first install the  on your instance. also, the device names for devices that you specify in the block device mapping are renamed using nvme device names (). therefore, to mount file systems at boot time using , you must use uuid/label instead of device names.ami: for information about the amis required by instance types that support enhanced networking and nvme, see the release notes in the following documentation:you must stop your amazon ebs–backed instance before you can change its instance type. when you stop and start an instance, be aware of the following: we move the instance to new hardware; however, the instance id does not change.if your instance has a public ipv4 address, we release the address and give it a new public ipv4 address. the instance retains its private ipv4 addresses, any elastic ip addresses, and any ipv6 addresses.if your instance is in an auto scaling group, the amazon ec2 auto scaling service marks the stopped instance as unhealthy, and may terminate it and launch a replacement instance. to prevent this, you can suspend the scaling processes for the group while you're resizing your instance. for more information, see  in the amazon ec2 auto scaling user guide.if your instance is in a  and, after changing the instance type, the instance start fails, try the following: stop all the instances in the cluster placement group, change the instance type for the affected instance, and then restart all the instances in the cluster placement group. ensure that you plan for downtime while your instance is stopped. stopping and resizing an instance may take a few minutes, and restarting your instance may take a variable amount of time depending on your application's startup scripts.for more information, see . use the following procedure to resize an amazon ebs–backed instance using the aws management console. to resize an amazon ebs–backed instance (optional) if the new instance type requires drivers that are not installed on the existing instance, you must connect to your instance and install the drivers first. for more information, see . open the amazon ec2 console. in the navigation pane, choose instances. select the instance and choose actions, instance state, stop. in the confirmation dialog box, choose yes, stop. it can take a few minutes for the instance to stop. with the instance still selected, choose actions, instance settings, change instance type. this action is disabled if the instance state is not . in the change instance type dialog box, do the following: from instance type, select the instance type that you want. if the instance type that you want does not appear in the list, then it is not compatible with the configuration of your instance (for example, because of virtualization type). for more information, see . (optional) if the instance type that you selected supports ebs–optimization, select ebs-optimized to enable ebs–optimization or deselect ebs-optimized to disable ebs–optimization. if the instance type that you selected is ebs–optimized by default, ebs-optimized is selected and you can't deselect it. choose apply to accept the new settings. to restart the stopped instance, select the instance and choose actions, instance state, start. in the confirmation dialog box, choose yes, start. it can take a few minutes for the instance to enter the  state. (troubleshooting) if your instance won't boot, it is possible that one of the requirements for the new instance type was not met. for more information, see  when you want to move your application from one instance store-backed instance to an instance store-backed instance with a different instance type, you must migrate it by creating an image from your instance, and then launching a new instance from this image with the instance type that you need. to ensure that your users can continue to use the applications that you're hosting on your instance uninterrupted, you must take any elastic ip address that you've associated with your original instance and associate it with the new instance. then you can terminate the original instance. to migrate an instance store-backed instance back up any data on your instance store volumes that you need to keep to persistent storage. to migrate data on your ebs volumes that you need to keep, take a snapshot of the volumes (see ) or detach the volume from the instance so that you can attach it to the new instance later (see ). create an ami from your instance store-backed instance by satisfying the prerequisites and following the procedures in . when you are finished creating an ami from your instance, return to this procedure. open the amazon ec2 console and in the navigation pane, choose amis. from the filter lists, choose owned by me, and choose the image that you created in the previous step. notice that ami name is the name that you specified when you registered the image and source is your amazon s3 bucket. noteif you do not see the ami that you created in the previous step, make sure that you have selected the region in which you created your ami. choose launch. when you specify options for the instance, be sure to select the new instance type that you want. if the instance type that you want can't be selected, then it is not compatible with configuration of the ami that you created (for example, because of virtualization type). you can also specify any ebs volumes that you detached from the original instance. it can take a few minutes for the instance to enter the  state. (optional) you can terminate the instance that you started with, if it's no longer needed. select the instance and verify that you are about to terminate the original instance, not the new instance (for example, check the name or launch time). choose actions, instance state, terminate. if the current configuration of your instance is incompatible with the new instance type that you want, then you can't resize the instance to that instance type. instead, you can migrate your application to a new instance with a configuration that is compatible with the new instance type that you want. if you want to move from an instance launched from a pv ami to an instance type that is hvm only, the general process is as follows: to migrate your application to a compatible instance back up any data on your instance store volumes that you need to keep to persistent storage. to migrate data on your ebs volumes that you need to keep, create a snapshot of the volumes (see ) or detach the volume from the instance so that you can attach it to the new instance later (see ). launch a new instance, selecting the following: an hvm ami.the hvm only instance type.if you are using an elastic ip address, select the vpc that the original instance is currently running in.any ebs volumes that you detached from the original instance and want to attach to the new instance, or new ebs volumes based on the snapshots that you created.if you want to allow the same traffic to reach the new instance, select the security group that is associated with the original instance.install your application and any required software on the instance. restore any data that you backed up from the instance store volumes of the original instance. if you are using an elastic ip address, assign it to the newly launched instance as follows: in the navigation pane, choose elastic ips. select the elastic ip address that is associated with the original instance and choose actions, disassociate address. when prompted for confirmation, choose disassociate address. with the elastic ip address still selected, choose actions, associate address. from instance, select the new instance, and then choose associate. (optional) you can terminate the original instance if it's no longer needed. select the instance and verify that you are about to terminate the original instance, not the new instance (for example, check the name or launch time). choose actions, instance state, terminate. 
you can use iam policies to grant users permissions to view and work with specific resources in the amazon ec2 console. you can use the example policies in the previous section; however, they are designed for requests that are made with the aws cli or an aws sdk. the console uses additional api actions for its features, so these policies may not work as expected. for example, a user that has permission to use only the  api action will encounter errors when trying to view volumes in the console. this section demonstrates policies that enable users to work with specific parts of the console. tipto help you work out which api actions are required to perform tasks in the console, you can use a service such as aws cloudtrail. for more information, see the . if your policy does not grant permission to create or modify a specific resource, the console displays an encoded message with diagnostic information. you can decode the message using the  api action for aws sts, or the  command in the aws cli. topics for additional information about creating policies for the amazon ec2 console, see the following aws security blog post: . to allow users to view all resources in the amazon ec2 console, you can use the same policy as the following example: . users cannot perform any actions on those resources or create new resources, unless another statement grants them permission to do so. view instances, amis, and snapshots alternatively, you can provide read-only access to a subset of resources. to do this, replace the * wildcard in the  api action with specific  actions for each resource. the following policy allows users to view all instances, amis, and snapshots in the amazon ec2 console. the  action allows users to view public amis. the console requires the tagging information to display public amis; however, you can remove this action to allow users to view only private amis. notethe amazon ec2  api actions do not support resource-level permissions, so you cannot control which individual resources users can view in the console. therefore, the * wildcard is necessary in the  element of the above statement. for more information about which arns you can use with which amazon ec2 api actions, see  in the iam user guide. view instances and cloudwatch metrics the following policy allows users to view instances in the amazon ec2 console, as well as cloudwatch alarms and metrics in the monitoring tab of the instances page. the amazon ec2 console uses the cloudwatch api to display the alarms and metrics, so you must grant users permission to use the  and  actions. the amazon ec2 launch wizard is a series of screens with options to configure and launch an instance. your policy must include permission to use the api actions that allow users to work with the wizard's options. if your policy does not include permission to use those actions, some items in the wizard cannot load properly, and users cannot complete a launch. basic launch wizard access to complete a launch successfully, users must be given permission to use the  api action, and at least the following api actions: : to view and select an ami. : to view the available network options.: to view all available subnets for the chosen vpc.  or : to view and select an existing security group, or to create a new one.  or : to select an existing key pair, or to create a new one.: to add inbound rules.you can add api actions to your policy to provide more options for users, for example: : to view and select a specific availability zone.: to view and select existing network interfaces for the selected subnet.to add outbound rules to vpc security groups, users must be granted permission to use the  api action. to modify or delete existing rules, users must be granted permission to use the relevant  api action.: to tag the resources that are created by . for more information, see . if users do not have permission to use this action and they attempt to apply tags on the tagging page of the launch wizard, the launch fails. importantbe careful about granting users permission to use the  action, because doing so limits your ability to use the  condition key to restrict their use of other resources. if you grant users permission to use the  action, they can change a resource's tag in order to bypass those restrictions. for more information, see .to use systems manager parameters when selecting an ami, you must add  and  to your policy.  grants your iam users the permission to view and select systems manager parameters.  grants your iam users the permission to get the values of the systems manager parameters. you can also restrict access to specific systems manager parameters. for more information, see restrict access to specific systems manager parameters later in this section.currently, the amazon ec2  api actions do not support resource-level permissions, so you cannot restrict which individual resources users can view in the launch wizard. however, you can apply resource-level permissions on the  api action to restrict which resources users can use to launch an instance. the launch fails if users select options that they are not authorized to use.  restrict access to a specific instance type, subnet, and region the following policy allows users to launch  instances using amis owned by amazon, and only into a specific subnet (). users can only launch in the sa-east-1 region. if users select a different region, or select a different instance type, ami, or subnet in the launch wizard, the launch fails.  the first statement grants users permission to view the options in the launch wizard or to create new ones, as explained in the example above. the second statement grants users permission to use the network interface, volume, key pair, security group, and subnet resources for the  action, which are required to launch an instance into a vpc. for more information about using the  action, see . the third and fourth statements grant users permission to use the instance and ami resources respectively, but only if the instance is a  instance, and only if the ami is owned by amazon. restrict access to specific systems manager parameters the following policy grants access to use systems manager parameters with a specific name. the first statement grants users the permission to view systems manager parameters when selecting an ami in the launch wizard. the second statement grants users the permission to only use parameters that are named . the following policy grants users permission to view and create volumes, and attach and detach volumes to specific instances.  users can attach any volume to instances that have the tag "", and also detach volumes from those instances. to attach a volume using the amazon ec2 console, it is helpful for users to have permission to use the  action, as this allows them to select an instance from a pre-populated list in the attach volume dialog box. however, this also allows users to view all instances on the instances page in the console, so you can omit this action. in the first statement, the  action is necessary to ensure that a user can select an availability zone when creating a volume. users cannot tag the volumes that they create (either during or after volume creation). view security groups and add and remove rules the following policy grants users permission to view security groups in the amazon ec2 console, to add and remove inbound and outbound rules, and to modify rule descriptions for existing security groups that have the tag . in the first statement, the  action allows users to view tags in the console, which makes it easier for users to identify the security groups that they are allowed to modify. working with the create security group dialog box you can create a policy that allows users to work with the create security group dialog box in the amazon ec2 console. to use this dialog box, users must be granted permission to use at the least the following api actions: : to create a new security group. : to view a list of existing vpcs in the vpc list.with these permissions, users can create a new security group successfully, but they cannot add any rules to it. to work with rules in the create security group dialog box, you can add the following api actions to your policy: : to add inbound rules.: to add outbound rules to vpc security groups.: to modify or delete existing inbound rules. this is useful to allow users to use the copy to new feature in the console. this feature opens the create security group dialog box and populates it with the same rules as the security group that was selected. : to modify or delete outbound rules for vpc security groups. this is useful to allow users to modify or delete the default outbound rule that allows all outbound traffic.: to cater for when invalid rules cannot be saved. the console first creates the security group, and then adds the specified rules. if the rules are invalid, the action fails, and the console attempts to delete the security group. the user remains in the create security group dialog box so that they can correct the invalid rule and try to create the security group again. this api action is not required, but if a user is not granted permission to use it and attempts to create a security group with invalid rules, the security group is created without any rules, and the user must add them afterward.: to add or update descriptions of ingress (inbound) security group rules.: to add or update descriptions of egress (outbound) security group rules.currently, the  api action does not support resource-level permissions; however, you can apply resource-level permissions to the  and  actions to control how users can create rules. the following policy grants users permission to use the create security group dialog box, and to create inbound and outbound rules for security groups that are associated with a specific vpc (). users can create security groups for ec2-classic or another vpc, but they cannot add any rules to them. similarly, users cannot add any rules to any existing security group that's not associated with vpc . users are also granted permission to view all security groups in the console. this makes it easier for users to identify the security groups to which they can add inbound rules. this policy also grants users permission to delete security groups that are associated with vpc .  to allow users to view elastic ip addresses in the amazon ec2 console, you must grant users permission to use the  action. to allow users to work with elastic ip addresses, you can add the following actions to your policy. : to allocate an elastic ip address.: to release an elastic ip address.: to associate an elastic ip address with an instance or a network interface. and : to work with the associate address screen. the screen displays the available instances or network interfaces to which you can associate an elastic ip address.: to disassociate an elastic ip address from an instance or a network interface.the following policy allows users to view, allocate, and associate elastic ip addresses with instances. users cannot associate elastic ip addresses with network interfaces, disassociate elastic ip addresses, or release them. the following policy can be attached to an iam user. it gives the user access to view and modify reserved instances in your account, as well as purchase new reserved instances in the aws management console. this policy allows users to view all the reserved instances, as well as on-demand instances, in the account. it's not possible to set resource-level permissions for individual reserved instances. the  action is necessary to ensure that the amazon ec2 console can display information about the availability zones in which you can purchase reserved instances. the  action is not required, but ensures that the user can view the instances in the account and purchase reservations to match the correct specifications. you can adjust the api actions to limit user access, for example removing  and  means the user has read-only access. 
you can get statistics for the cloudwatch metrics for your instances. topics statistics are metric data aggregations over specified periods of time. cloudwatch provides statistics based on the metric data points provided by your custom data or provided by other services in aws to cloudwatch. aggregations are made using the namespace, metric name, dimensions, and the data point unit of measure, within the time period you specify. the following table describes the available statistics. 
this section explains how to use the amazon simple storage service (amazon s3) console to manage access permissions for an amazon s3 object by using access control lists (acls). acls are resource-based access policies that grant access permissions to buckets and objects. for more information about managing access permissions with resource-based policies, see  in the amazon simple storage service developer guide. bucket and object permissions are independent of each other. an object does not inherit the permissions from its bucket. for example, if you create a bucket and grant write access to a user, you can't access that user’s objects unless the user explicitly grants you access. you can grant permissions to other aws accounts or predefined groups. the user or group that you grant permissions to is called the grantee. by default, the owner, which is the aws account that created the bucket, has full permissions. each permission you grant for a user or a group adds an entry in the acl that is associated with the object. the acl lists grants, which identify the grantee and the permission granted. for more information about acls, see  in the amazon simple storage service developer guide. to set permissions for an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object for which you want to set permissions. choose permissions. you can manage object access permissions for the following:  1.  access for object owner  access for other aws accounts to grant permissions to an aws user from a different aws account, under access for other aws accounts, choose add account. in the enter an id field, enter the canonical id of the aws user that you want to grant object permissions to. for information about finding a canonical id, see  in the amazon web services general reference. you can add as many as 99 users. select the check boxes for the permissions that you want to grant to the user, and then choose save. to display information about the permissions, choose the help icons. public access  warninguse caution when granting the everyone group anonymous access to your amazon s3 objects. when you grant access to this group, anyone in the world can access your object. if you need to grant access to everyone, we highly recommend that you only grant permissions to read objects.we highly recommend that you do not grant the everyone group write object permissions. doing so allows anyone to overwrite the acl permissions for the object. you can also set object permissions when you upload objects. for more information about setting permissions when uploading objects, see .  
this section describes how to use the amazon s3 console to create a folder. to create a folder sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to create a folder in. choose create folder. enter a name for the folder (for example, favorite-pics). choose the encryption setting for the folder object, and then choose save. if you don't want to use encryption with your folder, choose none.  if you want to encrypt your object using keys that are managed by amazon s3, follow these steps: choose aes-256.  for more information about using amazon s3 server-side encryption to encrypt your data, see  in the amazon simple storage service developer guide. if you want to encrypt your object using aws kms, follow these steps: choose aws-kms. choose an aws kms customer master key (cmk). the list shows  that you have created and your aws managed cmk for amazon s3. for more information about creating a customer managed aws kms cmk, see  in the aws key management service developer guide.  importantto encrypt objects in the bucket, you can use only the cmks that are enabled in the same aws region as the bucket. amazon s3 only supports symmetric cmks and does not support asymmetric cmks. for more information, see .the amazon s3 console only lists 100 kms cmks per region. if you have more than 100 cmks in the same region, you can see only the first 100 cmks in the s3 console. to use anaws kms kms cmk that is not listed in the console, choose custom kms arn, and enter the kms cmk arn. choose save. 
to upload your data (photos, videos, documents etc.) to amazon s3, you must first create an s3 bucket in one of the aws regions. you can then upload an unlimited number of data objects to the bucket. the data that you store in amazon s3 consists of objects. every object resides within a bucket that you create in a specific aws region. every object that you store in amazon s3 resides in a bucket.  objects stored in a region never leave the region unless you explicitly transfer them to another region. for example, objects stored in the europe (ireland) region never leave it. the objects stored in an aws region physically remain in that region. amazon s3 does not keep copies of objects or move them to any other region. however, you can access the objects from anywhere, as long as you have necessary permissions to do so. before you can upload an object into amazon s3, you must have write permissions to a bucket. objects can be any file type: images, backups, data, movies, etc. you can have an unlimited number of objects in a bucket. the maximum size of file you can upload by using the amazon s3 console is 160 gb. to upload a file larger than 160 gb, use the aws cli, aws sdk, or amazon s3 rest api. for more information, see  in the amazon simple storage service developer guide. the following topics explain how to use the amazon s3 console to upload, delete, and manage objects. noteif you rename an object, or change any of the properties; storage class,  enryption, metadata, a new object is created to replace the old one. if s3 versioning is enabled, a new version of the object is created, and the existing object becomes an older version. the role that changes the property also becomes the owner of the new object or (object version). topics 
this section explains how to use the console to view the properties of an object. to view the properties of an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object you want to view the properties for. choose properties. on the properties page, you can configure the following properties for the object. noteif you change the properties; storage class,  enryption, metadata, a new object is created to replace the old one. if s3 versioning is enabled, a new version of the object is created, and the existing object becomes an older version. the role that changes the property also becomes the owner of the new object or (object version). storage class – each object in amazon s3 has a storage class associated with it. the storage class that you choose to use depends on how frequently you access the object. the default storage class for s3 objects is standard. you choose which storage class to use when you upload an object. for more information about storage classes, see  in the amazon simple storage service developer guide. to change the storage class after you upload an object, choose storage class. choose the storage class that you want, and then choose save. encryption – you can encrypt your s3 objects. for more information, see .  metadata – each object in amazon s3 has a set of name-value pairs that represents its metadata. for information on adding metadata to an s3 object, see . tags – you can add tags to an s3 object. for more information, see . 
amazon s3 block public access prevents the application of any settings that allow public access to data within s3 buckets. this section describes how to edit block public access settings for all the s3 buckets in your aws account. for information about blocking public using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. to edit block public access settings for all the s3 buckets in an aws account sign in to the aws management console and open the amazon s3 console at . choose block public access (account settings). choose edit to change the block public access settings for all the buckets in your aws account. choose the settings that you want to change, and then choose save. when you're asked for confirmation, enter confirm. then choose confirm to save your changes. 
 you can use amazon s3 access points to manage access to your s3 objects. amazon s3 access points are named network endpoints that are attached to buckets that you can use to perform s3 object operations, such as uploading and retrieving objects. a bucket can have up to 1,000 access points attached, and each access point enforces distinct permissions and network controls to give you fine-grained control over access to your s3 objects.  for more information about amazon s3 access points, see  in the amazon simple storage service developer guide. the following topics explain how to use the s3 management console to create, manage, and use amazon s3 access points. topics 
you can host a static website on amazon s3. on a static website, individual webpages include static content. a static website might also contain client-side scripts. by contrast, a dynamic website relies on server-side processing, including server-side scripts such as php, jsp, or asp.net. amazon s3 does not support server-side scripting. you can use the following quick procedures to configure an s3 bucket for static website hosting in the amazon s3 console. for more information and detailed walkthroughs, see  in the amazon simple storage service developer guide. topics sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to enable static website hosting for. choose properties. choose static website hosting. choose use this bucket to host a website. in index document, enter the name of the index document, typically .  when you configure a bucket for website hosting, you must specify an index document. amazon s3 returns this index document when requests are made to the root domain or any of the subfolders. for more information, see  in the amazon simple storage service developer guide. (optional) if you want to provide your own custom error document for 4xx class errors, in error document, enter the custom error document filename.  if you do not specify a custom error document and an error occurs, amazon s3 returns a default html error document. for more information, see  in the amazon simple storage service developer guide. (optional) if you want to specify advanced redirection rules, in the edit redirection rules box, enter xml to describe the rules. for example, you can conditionally route requests according to specific object key names or prefixes in the request. for more information, see  in the amazon simple storage service developer guide. choose save. upload the index document to your bucket. for step-by-step instructions on uploading an object to an s3 bucket, see .  upload other files for your website, including optional custom error documents. in the next section, you set the permissions required to access your bucket as a static website.. by default, amazon s3 blocks public access to your account and buckets. if you want to use a bucket to host a static website, you can use these steps to edit your block public access settings.  warningbefore you complete this step, review  to ensure that you understand and accept the risks involved with allowing public access. when you turn off block public access settings to make your bucket public, anyone on the internet can access your bucket. we recommend that you block all public access to your buckets. open the amazon s3 console at . choose the name of the bucket that you have configured as a static website. choose permissions. choose edit. clear block all public access, and choose save. warningbefore you complete this step, review  to ensure you understand and accept the risks involved with allowing public access. when you turn off block public access settings to make your bucket public, anyone on the internet can access your bucket. we recommend that you block all public access to your buckets. in the confirmation box, enter confirm, and then choose confirm. under s3 buckets, the access for your bucket updates to objects can be public. you can now add a bucket policy to make the objects in the bucket publicly readable. if the access still displays as bucket and objects not public, you might have to  for your account before adding a bucket policy. after you edit s3 block public access settings, you can add a bucket policy to grant public read access to your bucket. when you grant public read access, anyone on the internet can access your bucket. importantthe following policy is an example only and allows full access to the contents of your bucket. before you proceed with this step, review  to ensure that you understand the best practices for securing the files in your s3 bucket and risks involved in granting public access. under buckets, choose the name of your bucket. choose permissions. choose bucket policy. to grant public read access for your website, copy the following bucket policy, and paste it in the bucket policy editor. update the  to include your bucket name. in the preceding example bucket policy, example.com is the bucket name. to use this bucket policy with your own bucket, you must update this name to match your bucket name. choose save. a warning appears indicating that the bucket has public access. in bucket policy, a public label appears. if you see an error that says , confirm that the bucket name in the bucket policy matches your bucket name. for information about adding a bucket policy, see  if you get an error - access denied warning and the bucket policy editor does not allow you to save the bucket policy, check your account-level and bucket-level block public access settings to confirm that you allow public access to the bucket. once you configure your bucket as a static website and set permissions, you can access your website through an amazon s3 website endpoint. for more information, see  in the amazon simple storage service developer guide. for a complete list of amazon s3 website endpoints, see  in the amazon web services general reference. 
this section describes how to configure advanced s3 bucket property settings for object replication, event notification, and transfer acceleration. topics 
there are three types of amazon cloudwatch metrics for amazon s3: storage metrics are reported once per day and are provided to all customers at no additional cost.replication metrics are available 15 minutes after enabling a replication rule with s3 replication time control (s3 rtc). for more information, see request metrics are available at 1-minute intervals after some latency to process, and the metrics are billed at the standard cloudwatch rate. for more information about cloudwatch metrics for amazon s3, see  in the amazon simple storage service developer guide. to get request metrics, you must opt into them by configuring them on the aws management console or using the amazon s3 api. to configure request metrics on a bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the objects you want request metrics for. choose the management tab, and then choose metrics. choose requests. in the left pane, choose the edit icon next to the name of the bucket. select the request metrics check box. this also enables data transfer metrics. choose save. you have now created a metrics configuration for all the objects in an amazon s3 bucket. about 15 minutes after cloudwatch begins tracking these request metrics, you can see graphs for the metrics on the amazon s3 or cloudwatch console.  you can also define a filter so that the metrics are only collected and reported on a subset of objects in the bucket. for more information, see  
to upload your data (photos, videos, documents etc.) to amazon s3, you must first create an s3 bucket in one of the aws regions. you can then upload your data objects to the bucket. every object you store in amazon s3 resides in a bucket. you can use buckets to group related objects in the same way that you use a directory to group files in a file system.  amazon s3 creates buckets in the aws region that you specify. you can choose any aws region that is geographically close to you to optimize latency, minimize costs, or address regulatory requirements. for example, if you reside in europe, you might find it advantageous to create buckets in the europe (ireland) or europe (frankfurt) regions. for a list of amazon s3 aws regions, see  in the amazon web services general reference. you are not charged for creating a bucket. you are only charged for storing objects in the bucket and for transferring objects out of the bucket. for more information about pricing, see . amazon s3 bucket names are globally unique, regardless of the aws region in which you create the bucket. you specify the name at the time you create the bucket. for bucket naming guidelines, see  in the amazon simple storage service developer guide. the following topics explain how to use the amazon s3 console to create, delete, and manage buckets. topics 
amazon s3 provides a set of tools to help you manage your s3 batch operations jobs after you create them. for more information about managing s3 batch operations, see  in the amazon simple storage service developer guide.    in the amazon simple storage service developer guide in the amazon simple storage service developer guide in the amazon simple storage service developer guide
you can change the display language of the aws management console. several languages are supported. to change the console language sign in to the aws management console and open the amazon s3 console at . scroll down until you see the bar at the bottom of the window, and then choose the language on the left side of the bar. choose the language that you want from the menu. this will change the langauge for the entire aws management console. 
amazon simple storage service (amazon s3) transfer acceleration enables fast, easy, and secure file transfers between your client and an s3 bucket over long distances. this topic describes how to enable amazon s3 transfer acceleration for a bucket. for more information, see  in the amazon simple storage service developer guide. to enable transfer acceleration for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the buckets list, choose the name of the bucket that you want to enable transfer acceleration for. choose properties. under advanced settings, choose transfer acceleration. (optional) if you want to compare accelerated and non-accelerated upload speeds, to open the , choose the want to compare your data transfer speed by region?. the speed comparison tool uses multipart upload to transfer a file from your browser to various aws regions with and without amazon s3 transfer acceleration. you can compare the upload speed for direct uploads and transfer accelerated uploads by region.  choose enabled, and choose save. endpoint displays the transfer acceleration endpoint for your bucket. you use this endpoint to access accelerated data transfers to and from your bucket. if you suspend transfer acceleration, the accelerate endpoint no longer works. 
this section describes how to enable an aws cloudtrail trail to log data events for objects in an s3 bucket by using the amazon s3 console. cloudtrail supports logging amazon s3 object-level api operations such as , , and . these events are called data events. by default, cloudtrail trails don't log data events, but you can configure trails to log data events for s3 buckets that you specify, or to log data events for all the amazon s3 buckets in your aws account. for more information, see . cloudtrail does not populate data events in the cloudtrail event history. additionally, not all bucket-level actions are populated in the cloudtrail event history. for more information, see  to configure a trail to log data events for an s3 bucket, you can use either the aws cloudtrail console or the amazon s3 console. if you are configuring a trail to log data events for all the amazon s3 buckets in your aws account, it's easier to use the cloudtrail console. for information about using the cloudtrail console to configure a trail to log s3 data events, see  in the aws cloudtrail user guide.  importantadditional charges apply for data events. for more information, see .  the following procedure shows how to use the amazon s3 console to enable a cloudtrail trail to log data events for an s3 bucket. to enable cloudtrail data events logging for objects in an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket. choose properties. choose object-level logging. choose an existing cloudtrail trail in the drop-down menu.  the trail you select must be in the same aws region as your bucket, so the drop-down list contains only trails that are in the same region as the bucket or trails that were created for all regions.  if you need to create a trail, choose the cloudtrail console link to go to the cloudtrail console. for information about how to create trails in the cloudtrail console, see  in the aws cloudtrail user guide. under events, choose one of the following: read to specify that you want cloudtrail to log amazon s3 read apis such as . write to log amazon s3 write apis such as . read and write to log both read and write object apis.for a list of supported data events that cloudtrail logs for amazon s3 objects, see  in the amazon simple storage service developer guide. choose create to enable object-level logging for the bucket. to disable object-level logging for the bucket, you must go to the cloudtrail console and remove the bucket name from the trail's data events. noteif you use the cloudtrail console or the amazon s3 console to configure a trail to log data events for an s3 bucket, the amazon s3 console shows that object-level logging is enabled for the bucket.  for information about enabling object-level logging when you create an s3 bucket, see .  in the amazon simple storage service developer guide  in the aws cloudtrail user guide
this topic explains how to view the properties for an s3 bucket. to view the properties for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to view the properties for. choose properties. on the properties page, you can configure the following properties for the bucket. versioning – versioning enables you to keep multiple versions of an object in one bucket. by default, versioning is disabled for a new bucket. for information about enabling versioning, see . server access logging – server access logging provides detailed records for the requests that are made to your bucket. by default, amazon s3 does not collect server access logs. for information about enabling server access logging, see . static website hosting – you can host a static website on amazon s3. to enable static website hosting, choose static website hosting and then specify the settings you want to use. for more information, see . object-level logging – object-level logging records object-level api activity by using cloudtrail data events. for information about enabling object-level logging, see . tags – with aws cost allocation, you can use bucket tags to annotate billing for your use of a bucket. a tag is a key-value pair that represents a label that you assign to a bucket. to add tags, choose tags, and then choose add tag. for more information, see  in the amazon simple storage service developer guide.  transfer acceleration – amazon s3 transfer acceleration enables fast, easy, and secure transfers of files over long distances between your client and an s3 bucket. for information about enabling transfer acceleration, see . events – you can enable certain amazon s3 bucket events to send a notification message to a destination whenever the events occur. to enable events, choose events and then specify the settings you want to use. for more information, see .  requester pays – you can enable requester pays so that the requester (instead of the bucket owner) pays for requests and data transfers. for more information, see  in the amazon simple storage service developer guide.  
in the amazon s3 console, you can copy an object to a bucket or a folder. for more information, see  in the amazon simple storage service developer guide. to copy an object sign in to the aws management console and open the amazon s3 console at . in the buckets list, choose your bucket name. on the overview tab, select the check box beside the object that you want to copy. choose actions, and choose copy. choose the copy destination: if you want to copy your object to a bucket, select the bucket.if you want to copy your object to a folder in a bucket, choose the bucket name, and then choose the folder.under review, confirm the copy details, and choose copy. amazon s3 copies your object to the destination. 
amazon s3 block public access prevents the application of any settings that allow public access to data within s3 buckets. this section describes how to edit block public access settings for one or more s3 buckets. for information about blocking public access using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. topics follow these steps if you need to change the public access settings for a single s3 bucket. to edit the amazon s3 block public access settings for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose permissions. choose edit to change the public access settings for the bucket. for more information about the four amazon s3 block public access settings, see  in the amazon simple storage service developer guide. choose the setting that you want to change, and then choose save. when you're asked for confirmation, enter confirm. then choose confirm to save your changes. follow these steps if you need to change the public access settings for more than one s3 bucket. to edit the amazon s3 block public access settings for multiple s3 buckets sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the buckets that you want, and then choose edit public access settings. choose the setting that you want to change, and then choose save. when you're asked for confirmation, enter confirm. then choose confirm to save your changes. you can change amazon s3 block public access settings when you create a bucket. for more information, see .  
s3 batch operations performs large-scale batch operations on amazon s3 objects. you can use s3 batch operations to copy objects, set object tags or access control lists (acls), initiate object restores from amazon s3 glacier, or invoke an aws lambda function to perform custom actions using your objects. you can perform these operations on a custom list of objects, or you can use an amazon s3 inventory report to make generating even the largest lists of objects easy. s3 batch operations use the same amazon s3 apis that you already use, so you'll find the interface familiar. for information about performing s3 batch operations using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide.  the following topics explain how to use the amazon s3 console to configure and run batch operations. topics 
this section explains how to configure amazon s3 storage management tools. topics 
before you can enable event notifications for your bucket you must set up one of the following destination types: an amazon sns topicamazon simple notification service (amazon sns) is a web service that coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. you can use the amazon sns console to create an amazon sns topic that your notifications can be sent to. the amazon sns topic must be in the same region as your amazon s3 bucket. for information about creating an amazon sns topic, see  in the amazon simple notification service developer guide.before you can use the amazon sns topic that you create as an event notification destination, you need the following:   the amazon resource name (arn) for the amazon sns topica valid amazon sns topic subscription (the topic subscribers are notified when a message is published to your amazon sns topic)a permissions policy that you set up in the amazon sns console (as shown in the following example) an amazon sqs queueyou can use the amazon sqs console to create an amazon sqs queue that your notifications can be sent to. the amazon sqs queue must be in the same region as your amazon s3 bucket. for information about creating an amazon sqs queue, see  in the amazon simple queue service developer guide.before you can use the amazon sqs queue as an event notification destination, you need the following:   the amazon resource name (arn) for the amazon sqs topica permissions policy that you set up in the amazon sqs console (as shown in the following example) a lambda functionyou can use the aws lambda console to create a lambda function. the lambda function must be in the same region as your s3 bucket. for information about creating a lambda function, see the .before you can use the lambda function as an event notification destination, you must have the name or the arn of a lambda function to set up the lambda function as a event notification destination.  warningif your notification ends up writing to the bucket that triggers the notification, this could cause an execution loop. for example, if the bucket triggers a lambda function each time an object is uploaded, and the function uploads an object to the bucket, then the function indirectly triggers itself. to avoid this, use two buckets, or configure the trigger to only apply to a prefix used for incoming objects.for more information and an example of using amazon s3 notifications with aws lambda, see  in the aws lambda developer guide.  
replication is the automatic, asynchronous copying of objects across buckets in the same or different aws regions. it replicates newly created objects and object updates from a source bucket to a specified destination bucket.  you use the amazon s3 console to add replication rules to the source bucket. replication rules define the source bucket objects to replicate and the destination bucket where the replicated objects are stored. for more information about replication, see  in the amazon simple storage service developer guide. you can manage replication rules on the replication page. you can add, view, enable, disable, delete, and change the priority of the replication rules. for information about adding replication rules to a bucket, see . to manage the replication rules for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose management, and then choose replication. you change the replication rules in the following ways. to change settings that affect all the replication rules in the bucket, choose edit global settings. you can change the destination bucket, and the iam role. if needed, you can copy the required bucket policy for cross-account destination buckets. to change a replication rule, select the rule and choose edit, which starts the replication wizard to help you make the change. for information about using the wizard, see .to enable or disable a replication rule, select the rule, choose more, and in the drop-down list, choose enable rule or disable rule. you can also disable, enable, or delete all the rules in the bucket from the more drop-down list.to change the rule priorities, choose edit priorities. you can then change the priority for each rule under the priority column heading. choose save to save your changes. you set rule priorities to avoid conflicts caused by objects that are included in the scope of more than one rule. in the case of overlapping rules, amazon s3 uses the rule priority to determine which rule to apply. the higher the number, the higher the priority. for more information about rule priority, see  in the amazon simple storage service developer guide.  in the amazon simple storage service developer guide
for the latest aws terminology, see the  in the aws general reference. 
this section explains how to use the amazon simple storage service (amazon s3) console to add a new bucket policy or edit an existing bucket policy. a bucket policy is a resource-based aws identity and access management (iam) policy. you add a bucket policy to a bucket to grant other aws accounts or iam users access permissions for the bucket and the objects in it. object permissions apply only to the objects that the bucket owner creates. for more information about bucket policies, see  in the amazon simple storage service developer guide.  for examples of amazon s3 bucket policies, see  in the amazon simple storage service developer guide.  to create or edit a bucket policy sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to create a bucket policy for or whose bucket policy you want to edit. choose permissions, and then choose bucket policy. in the bucket policy editor text box, type or copy and paste a new bucket policy, or edit an existing policy. the bucket policy is a json file. the text you type in the editor must be valid json. choose save. noteamazon s3 displays the amazon resource name (arn) for the bucket next to the bucket policy editor title. for more information about arns, see  in the amazon web services general reference.directly below the bucket policy editor text box is a link to the policy generator, which you can use to create a bucket policy. 
before you can upload data to amazon s3, you must create a bucket in one of the aws regions to store your data. after you create a bucket, you can upload an unlimited number of data objects to the bucket.  the aws account that creates the bucket owns it. by default, you can create up to 100 buckets in each of your aws accounts. if you need additional buckets, you can increase your account bucket quota to a maximum of 1,000 buckets by submitting a service quota increase. for information about how to increase your bucket quota, see  in the aws general reference.  buckets have configuration properties, including geographical region, access settings for the objects in the bucket, and other metadata.  to create a bucket sign in to the aws management console and open the amazon s3 console at . choose create bucket. the create bucket wizard opens. in bucket name, enter a dns-compliant name for your bucket. the bucket name must: be unique across all of amazon s3.be between 3 and 63 characters long.not contain uppercase characters.start with a lowercase letter or number.after you create the bucket, you can't change its name. for information about naming buckets, see  in the amazon simple storage service developer guide. importantavoid including sensitive information, such as account numbers, in the bucket name. the bucket name is visible in the urls that point to the objects in the bucket. in region, choose the aws region where you want the bucket to reside.  choose a region close to you to minimize latency and costs and address regulatory requirements. objects stored in a region never leave that region unless you explicitly transfer them to another region. for a list of amazon s3 aws regions, see  in the amazon web services general reference. in bucket settings for block public access, choose the block public access settings that you want to apply to the bucket.  we recommend that you leave all settings enabled unless you know you need to turn one or more of them off for your use case, such as to host a public website. block public access settings that you enable for the bucket will also be enabled for all access points that you create on the bucket. for more information about blocking public access, see  in the amazon simple storage service developer guide. (optional) if you want to enable s3 object lock: choose advanced settings, and read the message that appears. importantyou can only enable s3 object lock for a bucket when you create it. if you enable object lock for the bucket, you can't disable it later. enabling object lock also enables versioning for the bucket. after you enable object lock for the bucket, you must configure the object lock settings before any objects in the bucket will be protected. for more information about configuring protection for objects, see . if you want to enable object lock, enter enable in the text box and choose confirm. for more information about the s3 object lock feature, see  in the amazon simple storage service developer guide. choose create bucket. 
this section explains how to use the amazon s3 console to delete folders from an s3 bucket.  for information about amazon s3 features and pricing, see . to delete folders from an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to delete folders from. in the name list, select the check box next to the folders and objects that you want to delete, choose more, and then choose delete. in the delete objects dialog box, verify that the names of the folders you selected for deletion are listed and then choose delete. 
latest documentation update: march 27, 2019 the following table describes the important changes in each release of the amazon simple storage service console user guide from june 19, 2018, onward. for notification about updates to this documentation, you can subscribe to an rss feed. the following table describes the important changes in each release of the amazon simple storage service console user guide before june 19, 2018.  
amazon s3 block public access prevents the application of any settings that allow public access to data within s3 buckets. you can configure block public access settings for an individual s3 bucket or for all the buckets in your account. for information about blocking public access using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. the following topics explain how to use the amazon s3 console to configure block public access settings:  the following sections explain viewing bucket access status and searching by access types. the list buckets view shows whether your bucket is publicly accessible. amazon s3 labels the permissions for a bucket as follows: public – everyone has access to one or more of the following: list objects, write objects, read and write permissions. objects can be public – the bucket is not public, but anyone with the appropriate permissions can grant public access to objects. buckets and objects not public – the bucket and objects do not have any public access.only authorized users of this account – access is isolated to iam users and roles in this account and aws service principals because there is a policy that grants public access.the access column shows the access status of the listed buckets.  you can also filter bucket searches by access type. choose an access type from the drop-down list that is next to the search for buckets bar.   
this section explains how to create an amazon s3 access point using the aws management console. for information about creating access points using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. an access point is associated with exactly one amazon s3 bucket. before you begin, make sure that you have created a bucket that you want to use with this access point. for more information about creating buckets, see . to create an access point sign in to the aws management console and open the amazon s3 console at . in the s3 buckets section, select the bucket that you want to attach this access point to. on the bucket detail page, choose the access points tab. choose create access point. enter your desired name for the access point in the access point name field. choose a network access type. if you choose virtual private cloud (vpc), enter the vpc id that you want to use with the access point. for more information about network access type for access points, see  in the amazon simple storage service developer guide. select the block public access settings that you want to apply to the access point. all block public access settings are enabled by default for new access points, and we recommend that you leave all settings enabled unless you know you have a specific need to disable any of them. amazon s3 currently doesn't support changing an access point's block public access settings after the access point has been created. for more information about using amazon s3 block public access with access points, see  in the amazon simple storage service developer guide. (optional) specify the access point policy. the console automatically displays the amazon resource name (arn) for the access point, which you can use in the policy. choose create access point. 
this section explains how to use the amazon s3 console to delete objects. because all objects in your s3 bucket incur storage costs, you should delete objects that you no longer need. if you are collecting log files, for example, it's a good idea to delete them when they're no longer needed. you can set up a lifecycle rule to automatically delete objects such as log files. for information about amazon s3 features and pricing, see . to delete objects from an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to delete an object from. you can delete objects from an s3 bucket in any of the following ways: in the name list, select the check box next to the objects and folders that you want to delete, choose actions, and then choose delete from the drop-down menu. in the delete objects dialog box, verify that the name(s) of the object(s) and/or folder(s) you selected for deletion are listed and then choose delete. or, choose the name of the object that you want to delete, choose latest version, and then choose the trash can icon.
by using the amazon s3 analytics storage class analysis tool, you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. storage class analysis observes data access patterns to help you determine when to transition less frequently accessed standard storage to the standard_ia (ia, for infrequent access) storage class. for more information about standard_ia, see the  and  in the amazon simple storage service developer guide. importantstorage class analysis does not give recommendations for transitions to the onezone_ia or s3 glacier storage classes. for more information about analytics, see  in the amazon simple storage service developer guide. to configure storage class analysis sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket for which you want to configure storage class analysis. choose the management tab, and then choose analytics. choose add. type a name for the filter. if you want to analyze the whole bucket, leave the prefix / tags field empty. in the prefix / tags  field, type text for the prefix or tag for the objects that you want to analyze, or choose from the dropdown list that appears when you start typing. if you chose tag, enter a value for the tag. you can enter one prefix and multiple tags. optionally, you can choose export data to export analysis reports to a comma-separated values (.csv) flat file. choose a destination bucket where the file can be stored. you can type a prefix for the destination bucket. the destination bucket must be in the same aws region as the bucket for which you are setting up the analysis. the destination bucket can be in a different aws account. choose save. amazon s3 creates a bucket policy on the destination bucket that grants amazon s3 write permission. this allow it to write the export data to the bucket.  if an error occurs when you try to create the bucket policy, you'll be given instructions on how to fix it. for example, if you chose a destination bucket in another aws account and do not have permissions to read and write to the bucket policy, you'll see the following message. you must have the destination bucket owner add the displayed bucket policy to the destination bucket. if the policy is not added to the destination bucket you won’t get the export data because amazon s3 doesn’t have permission to write to the destination bucket. if the source bucket is owned by a different account than that of the current user, then the correct account id of the source bucket must be substituted in the policy.  for information about the exported data and how the filter works, see  in the amazon simple storage service developer guide. more info  
there are three types of amazon cloudwatch metrics for amazon s3: storage metrics, request metrics, and replication metrics. replication metrics are available 15 minutes after a replication rule with s3 replication time control (s3 rtc) has been enabled. replication metrics are billed at the standard amazon cloudwatch rate. they are turned on automatically when you enable replication with s3 rtc using the aws management console or the amazon s3 api. replication metrics track the rule ids of the replication configuration. a replication rule id can be specific to a prefix, a tag, or a combination of both. for more information about s3 replication time control (s3 rtc), see  in the amazon simple storage service developer guide. for more information about cloudwatch metrics for amazon s3, see  in the amazon simple storage service developer guide. to view replication metrics sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the objects you want replication metrics for. choose the management tab, and then choose metrics. choose replication.   in the rule ids list in the left pane, select the rule ids that you want. if you have several rule ids to choose from, you can search for the ids that you want. after choosing the rule ids that you want, choose display graphs below the rule ids selection box. you can then view the replication metrics replication latency (in seconds), operations pending replication, and bytes pending replication for the rules that you selected. amazon cloudwatch begins reporting replication metrics 15 minutes after you enable s3 rtc on the respective replication rule. you can view replication metrics on the amazon s3 or cloudwatch console. for information, see  in the amazon simple storage service developer guide. 
amazon s3 has a flat structure instead of a hierarchy like you would typically see in a file system. however, for the sake of organizational simplicity, the amazon s3 console supports a folder concept as a way to group objects. in amazon s3, the folder is a naming prefix for an object or group of objects. for more information, see  we recommend blocking all public access to your amazon s3 folders and buckets unless you specifically require a public folder or bucket. when you make a folder public, anyone on the internet can view all the objects that are grouped in that folder. in the amazon s3 console, you can make a folder public. you can also make a folder public by creating a bucket policy that limits access by prefix. for more information, see .  warningafter you make a folder public in the amazon s3 console, you can't make it private again. instead, you must set permissions on each individual object in the public folder so that the objects have no public access. for more information, see  
access analyzer for s3 alerts you to s3 buckets that are configured to allow access to anyone on the internet or other aws accounts, including aws accounts outside of your organization. for each public or shared bucket, you receive findings into the source and level of public or shared access. for example, access analyzer for s3 might show that a bucket has read or write access provided through a bucket access control list (acl), a bucket policy, or an access point policy. armed with this knowledge, you can take immediate and precise corrective action to restore your bucket access to what you intended.  when reviewing an at-risk bucket in access analyzer for s3, you can block all public access to the bucket with a single click. we recommend that you block all access to your buckets unless you require public access to support a specific use case. before you block all public access, ensure that your applications will continue to work correctly without public access. for more information, see  in the amazon simple storage service developer guide. you can also drill down into bucket-level permission settings to configure granular levels of access. for specific and verified use cases that require public access, such as static website hosting, public downloads, or cross-account sharing, you can acknowledge and record your intent for the bucket to remain public or shared by archiving the findings for the bucket. you can revisit and modify these bucket configurations at any time. you can also download your findings as a csv report for auditing purposes. access analyzer for s3 is available at no extra cost on the amazon s3 console. access analyzer for s3 is powered by aws identity and access management (iam) access analyzer. to use access analyzer for s3 in the amazon s3 console, you must visit the iam console and enable iam access analyzer on a per-region basis.  for more information about iam access analyzer, see  in the iam user guide. for more information about access analyzer for s3, review the following sections. importantaccess analyzer for s3 requires an account-level analyzer. to use access analyzer for s3, you must visit iam access analyzer and create an analyzer that has an account as the zone of trust. for more information, see  in iam user guide. when a bucket policy or bucket acl is added or modified, access analyzer generates and updates findings based on the change within 30 minutes. findings related to account level block public access settings may not be generated or updated for up to 6 hours after you change the settings. topics access analyzer for s3 provides findings for buckets that can be accessed outside your aws account. buckets that are listed under buckets with public access can be accessed by anyone on the internet. if access analyzer for s3 identifies public buckets, you also see a warning at the top of the page that shows you the number of public buckets in your region. buckets listed under buckets with access from other aws accounts — including third-party aws accounts are shared conditionally with other aws accounts, including accounts outside of your organization.  for each bucket, access analyzer for s3 provides the following information: bucket namediscovered by access analyzer ‐ when access analyzer for s3 discovered the public or shared bucket access.shared through ‐ how the bucket is shared—through a bucket policy, a bucket acl, or an access point policy. a bucket can be shared through both policies and acls. if you want to find and review the source for your bucket access, you can use the information in this column as a starting point for taking immediate and precise corrective action. status ‐ the status of the bucket finding. access analyzer for s3 displays findings for all public and shared buckets. active ‐ finding has not been reviewed. archived ‐ finding has been reviewed and confirmed as intended. all ‐ all findings for buckets that are public or shared with other aws accounts, including aws accounts outside of your organization.access level ‐ access permissions granted for the bucket:list ‐ list resources.read ‐ read but not edit resource contents and attributes.write ‐ create, delete, or modify resources.permissions ‐ grant or modify resource permissions.tagging ‐ update tags associated with the resource.to use access analyzer for s3, you must complete the following prerequisite steps. grant the required permissions. for more information, see  in the iam user guide. visit iam to create an account-level analyzer for each region where you want to use access analyzer. access analyzer for s3 requires an account-level analyzer. to use access analyzer for s3, you must create an analyzer that has an account as the zone of trust. for more information, see  in iam user guide. if you want to block all access to a bucket in a single click, you can use the block all public access button in access analyzer for s3. when you block all public access to a bucket, no public access is granted. we recommend that you block all public access to your buckets unless you require public access to support a specific and verified use case. before you block all public access, ensure that your applications will continue to work correctly without public access. if you don't want to block all public access to your bucket, you can edit your block public access settings on the amazon s3 console to configure granular levels of access to your buckets. for more information, see  in the amazon simple storage service developer guide. in rare events, access analyzer for s3 might report no findings for a bucket that an amazon s3 block public access evaluation reports as public. this happens because amazon s3 block public access reviews policies for current actions and any potential actions that might be added in the future, leading to a bucket becoming public. on the other hand, access analyzer for s3 only analyzes the current actions specified for the amazon s3 service in the evaluation of access status. to block all public access to a bucket using access analyzer for s3 sign in to the aws management console and open the amazon s3 console at . in the navigation pane on the left, under dashboards, choose access analyzer for s3. in access analyzer for s3, choose a bucket. choose block all public access. to confirm your intent to block all public access to the bucket, in block all public access (bucket settings), enter confirm. amazon s3 blocks all public access to your bucket. the status of the bucket finding updates to resolved, and the bucket disappears from the access analyzer for s3 listing. if you want to review resolved buckets, open iam access analyzer on the iam console. if you did not intend to grant access to the public or other aws accounts, including accounts outside of your organization, you can modify the bucket acl, bucket policy, or access point policy to remove the access to the bucket. the shared through column shows all sources of bucket access: bucket policy, bucket acl, and/or access point policy. to review and change a bucket policy, a bucket acl, or an access point policy open the amazon s3 console at . in the navigation pane, choose access analyzer for s3. to see whether public access or shared access is granted through a bucket policy, a bucket acl, or an access point policy, look in the shared through column. under bucket name, choose the name of the bucket with the bucket policy, bucket acl, or access point policy that you want to change or review. if you want to change or view a bucket acl: choose permissions. choose access control list. review your bucket acl, and make changes as required. for more information, see  if you want to change or review a bucket policy: choose permissions. choose bucket policy. review or change your bucket policy as required. for more information, see  if you want to review or change an access point policy: choose access points. choose the access point name. review or change access as required.  for more information, see . if you edit or remove a bucket acl, a bucket policy, or an access point policy to remove public or shared access, the status for the bucket findings updates to resolved. the resolved bucket findings disappear from the access analyzer for s3 listing, but you can view them in iam access analyzer. if a bucket grants access to the public or other aws accounts, including accounts outside of your organization, to support a specific use case (for example, a static website, public downloads, or cross-account sharing), you can archive the finding for the bucket. when you archive bucket findings, you acknowledge and record your intent for the bucket to remain public or shared. archived bucket findings remain in your access analyzer for s3 listing so that you always know which buckets are public or shared. to archive bucket findings in access analyzer for s3 open the amazon s3 console at . in the navigation pane, choose access analyzer for s3. in access analyzer for s3, choose an active bucket. to acknowledge your intent for this bucket to be accessed by the public or other aws accounts, including accounts outside of your organization, choose archive. enter confirm, and choose archive. after you archive findings, you can always revisit them and change their status back to active, indicating that the bucket requires another review.  to activate an archived bucket finding in access analyzer for s3 open the amazon s3 console at . in the navigation pane, choose access analyzer for s3. choose the archived bucket findings. choose mark as active. if you need to see more information about a bucket, you can open the bucket finding details in iam access analyzer on the iam console. to view finding details in access analyzer for s3 open the amazon s3 console at . in the navigation pane, choose access analyzer for s3. in access analyzer for s3, choose a bucket. choose view details. the finding details open in iam access analyzer on the iam console.  you can download your bucket findings as a csv report that you can use for auditing purposes. the report includes the same information that you see in access analyzer for s3 on the amazon s3 console. to download a report open the amazon s3 console at . in the navigation pane on the left, choose access analyzer for s3. in the region filter, choose the region. access analyzer for s3 updates to shows buckets for the chosen region. choose download report. a csv report is generated and saved to your computer. 
there are three types of amazon cloudwatch metrics for amazon s3: storage metrics, request metrics and replication. storage metrics are reported once per day and are provided to all customers at no additional cost. request metrics are available at 1 minute intervals after some latency to process, and metrics are billed at the standard cloudwatch rate. to get request metrics, you must opt into them by configuring them in the console or with the amazon s3 api. for more conceptual information about cloudwatch metrics for amazon s3, see  in the amazon simple storage service developer guide. to filter request metrics on a subset of objects in a bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that has the objects you want to get request metrics for. choose the management tab. and then choose metrics. choose requests. from filters in the left-side pane, choose add. provide a name for this metrics configuration. provide one or more prefixes or tags, separated by commas, in prefix /tags that you want to monitor. from the drop down, select whether the value you provided is a tag or a prefix. choose save. you have now created a metrics configuration for request metrics on a subset of the objects in an amazon s3 bucket. about 15 minutes after cloudwatch begins tracking these request metrics, you can see graphs for the metrics in both the amazon s3 or cloudwatch consoles. you can also request metrics at the bucket level. for information, see  
in amazon s3, buckets and objects are the primary resources, and objects are stored in buckets. amazon s3 has a flat structure instead of a hierarchy like you would see in a file system. however, for the sake of organizational simplicity, the amazon s3 console supports the folder concept as a means of grouping objects. amazon s3 does this by using a shared name prefix for objects (that is, objects have names that begin with a common string). object names are also referred to as key names. for example, you can create a folder on the console named  and store an object named  in it. the object is then stored with the key name , where  is the prefix. here are two more examples:  if you have three objects in your bucket—, , and —the console will show a folder named . if you open the folder in the console, you will see three objects: , , and .if you have an object named , the console will show you a folder named  containing the folder  and the object .topics you can have folders within folders, but not buckets within buckets. you can upload and copy objects directly into a folder. folders can be created, deleted, and made public, but they cannot be renamed. objects can be copied from one folder to another.  importantthe amazon s3 console treats all objects that have a forward slash ("/") character as the last (trailing) character in the key name as a folder, for example . you can't upload an object that has a key name with a trailing "/" character using the amazon s3 console. however, you can upload objects that are named with a trailing "/" with the amazon s3 api by using the aws cli, aws sdks, or rest api.an object that is named with a trailing "/" appears as a folder in the amazon s3 console. the amazon s3 console does not display the content and metadata for such an object. when you use the console to copy an object named with a trailing "/", a new folder is created in the destination location, but the object's data and metadata are not copied.  
you can enable certain amazon s3 bucket events to send a notification message to a destination whenever the events occur. this section explains how to use the amazon s3 console to enable event notifications. for information about using event notifications with the aws sdks and the amazon s3 rest apis, see  in the amazon simple storage service developer guide.  topics when you configure event notifications for a bucket, you must specify the type of events for which you want to receive notifications. for a complete list of event types, see  section in the amazon simple storage service developer guide.  in the amazon s3 console, you have the following options for configuring event notifications. you can choose a single option or multiple options. object creationobjectcreated (all) – receive a notification when an object is created in your bucket for any of the following object creation actions: put, post, copy, and multipart upload completed.put, post, copy, and multipart upload completed – receive a notification for one of these specific object creation actions.object deletion objectdelete (all) – receive a notification any time an object in your bucket is deleted.delete marker created – receive a notification when a delete marker is created for a versioned object.  for information about deleting versioned objects, see . for information about object versioning, see  and . object restoration from the s3 glacier storage class restore initiated – receive a notification for initiation of object restoration.restore completed – receive a notification for completion of object restoration.reduced redundancy storage (rrs) object lost eventsobject in rss lost – receive a notification that an object of the rrs storage class has been lostobjects eligible for replication using amazon s3 replication time controlreplication time missed threshold – receive a notification that an object failed to replicate.replication time completed after threshold – receive a notification that an object exceeded the 15-minute threshold for replication.replication time not tracked – receive a notification that an object replicated after the 15-minute threshold.replication time failed – receive a notification that an object that was eligible for replication is no longer being tracked by replication metrics.notewhen you delete the last object from a folder, amazon s3 can generate an object creation event. when there are multiple objects with the same prefix with a trailing slash (/) as part of their names, those objects are shown as being part of a folder in the amazon s3 console. the name of the folder is formed from the characters preceding the trailing slash (/).when you delete all the objects listed under that folder, no actual object is available to represent the empty folder. under such circumstances, the amazon s3 console creates a zero-byte object to represent that folder. if you enabled event notification for the creation of objects, the zero-byte object creation action that is taken by the console triggers an object creation event.the amazon s3 console displays a folder under the following circumstances:when a zero-byte object has a trailing slash (/) in its name. in this case, there is an actual amazon s3 object of 0 bytes that represents a folder. if the object has a slash (/) within its name. in this case, there isn't an actual object representing the folder. when you configure event notifications for a bucket, you also choose the notification destination. event notification messages can be sent to the following destinations: amazon simple notification service (amazon sns) topic – coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. for information about the amazon sns topic format, see .amazon simple queue service (amazon sqs) queue – offers reliable and scalable hosted queues for storing messages as they travel between computers. for information about amazon sqs, see  in the amazon simple queue service developer guide.aws lambda  – invoke a lambda function and provide the event message as an argument. when you create a lambda function, you package up and upload your custom code to aws lambda. aws lambda uses the aws infrastructure to run the code on your behalf. for information about using lambda with amazon s3, see  in the aws lambda developer guide.for more information about granting the amazon s3 service principal the permissions required to publish event notifications to a destination, see  in the amazon s3 developer guide. warningif your notification ends up writing to the bucket that triggers the notification, this could cause an execution loop. for example, if the bucket triggers a lambda function each time an object is uploaded, and the function uploads an object to the bucket, then the function indirectly triggers itself. to avoid this, use two buckets, or configure the trigger to only apply to a prefix used for incoming objects.for more information and an example of using amazon s3 notifications with aws lambda, see  in the aws lambda developer guide.  before you can enable event notifications for your bucket, you must set up one of the destination types. for more information, see  to enable and configure event notifications for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to enable events for. choose properties. under advanced settings, choose events. choose add notification. in name, enter a descriptive name for your event notification.  if you don't enter a name, a guid is generated and used for the name. under events, select one or more events.  for a listing of the event types, see . to filter event notifications by a prefix or suffix, enter a prefix or a suffix.  for example, you can set up a prefix filter so that you receive notifications only when files are added to a specific folder (for example, ). for more information, see . choose the event notification destination: sns topic, sqs queue, or lambda function.  for a description of the destinations, see . when you choose your send to destination, a box appears for you to enter your specific sns, sqs, or lambda function destination. in the example image below, the send to location is sns topic, and you can see a sns box for the sns topic name. in the box that appears, choose or enter the destination sns, sqs, or lambda function.  you can choose or enter the sns, sqs, or lambda function name, or you can choose to the destination amazon resource name (arn). the example screenshot below shows the add sns topic arn option. if you chose add arn, enter the sns topic, sqs queue, or lambda function arn. choose save.  amazon s3 sends a test message to the event notification destination. 
you can empty a bucket, which deletes all of the objects in the bucket without deleting the bucket. when you empty a bucket with versioning enabled, all versions of all the objects in the bucket are deleted. for more information, see  and  in the amazon simple storage service developer guide. to empty an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, select the option next to the name of the bucket that you want to empty and then choose empty. on the empty bucket page, confirm that you want to empty the bucket by entering the bucket name into the text field, and then choose empty. (optional) monitor the progress of the bucket emptying process on the empty bucket: status page. 
this topic explains how to use the aws management console to upload one or more files or entire folders to an amazon s3 bucket. before you can upload files and folders to an amazon s3 bucket, you need write permissions for the bucket. for more information about access permissions, see . for information about uploading files programmatically, see  in the amazon simple storage service developer guide.  when you upload a file to amazon s3, it is stored as an s3 object. objects consist of the file data and metadata that describes the object. you can have an unlimited number of objects in a bucket. you can upload any file type—images, backups, data, movies, etc.—into an s3 bucket. the maximum size of a file that you can upload by using the amazon s3 console is 160 gb. to upload a file larger than 160 gb, use the aws cli, aws sdk, or amazon s3 rest api. for more information, see  in the amazon simple storage service developer guide.  you can upload files by dragging and dropping or by pointing and clicking. to upload folders, you must drag and drop them. drag and drop functionality is supported only for the chrome and firefox browsers. for information about which chrome and firefox browser versions are supported, see .  when you upload a folder, amazon s3 uploads all of the files and subfolders from the specified folder to your bucket. it then assigns an object key name that is a combination of the uploaded file name and the folder name. for example, if you upload a folder called  that contains two files,  and , amazon s3 uploads the files and then assigns the corresponding key names,  and . the key names include the folder name as a prefix. the amazon s3 console displays only the part of the key name that follows the last “/”. for example, within an images folder the  and  objects are displayed as  and a . if you upload individual files and you have a folder open in the amazon s3 console, when amazon s3 uploads the files, it includes the name of the open folder as the prefix of the key names. for example, if you have a folder named  open in the amazon s3 console and you upload a file named , the key name is . however, the object is displayed in the console as  in the  folder. if you upload individual files and you do not have a folder open in the amazon s3 console, when amazon s3 uploads the files, it assigns only the file name as the key name. for example, if you upload a file named , the key name is . for more information on key names, see  in the amazon simple storage service developer guide. if you upload an object with a key name that already exists in a versioning-enabled bucket, amazon s3 creates another version of the object instead of replacing the existing object. for more information about versioning, see . topics if you are using the chrome or firefox browsers, you can choose the folders and files to upload, and then drag and drop them into the destination bucket. dragging and dropping is the only way that you can upload folders. to upload folders and files to an s3 bucket by using drag and drop sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to upload your folders or files to. in a window other than the console window, select the files and folders that you want to upload. then drag and drop your selections into the console window that lists the objects in the destination bucket. the files you chose are listed in the upload dialog box. in the upload dialog box, do one of the following:  drag and drop more files and folders to the console window that displays the upload dialog box. to add more files, you can also choose add more files. this option works only for files, not folders. to immediately upload the listed files and folders without granting or removing permissions for specific users or setting public permissions for all of the files that you're uploading, choose upload. for information about object access permissions, see .  to set permissions or properties for the files that you are uploading, choose next. on the set permissions page, under manage users you can change the permissions for the aws account owner. the owner refers to the aws account root user, and not an aws identity and access management (iam) user. for more information about the root user, see .  choose add account to grant access to another aws account. for more information about granting permissions to another aws account, see . under manage public permissions you can grant read access to your objects to the general public (everyone in the world), for all of the files that you're uploading. granting public read access is applicable to a small subset of use cases such as when buckets are used for websites. we recommend that you do not change the default setting of do not grant public read access to this object(s). you can always make changes to object permissions after you upload the object. for information about object access permissions, see .  when you're done configuring permissions, choose next. on the set properties page, choose the storage class and encryption method to use for the files that you are uploading. you can also add or modify metadata.  choose a storage class for the files you're uploading. for more information about storage classes, see  in the amazon simple storage service developer guide. choose the type of encryption for the files that you're uploading. if you don't want to encrypt them, choose none.  to encrypt the uploaded files using keys that are managed by amazon s3, choose amazon s3 master-key. for more information, see  in the amazon simple storage service developer guide. to encrypt the uploaded files using the aws key management service (aws kms), choose aws kms master-key. then choose a customer master key (cmk) from the list of aws kms cmks. noteto encrypt objects in a bucket, you can use only cmks that are available in the same aws region as the bucket.  you can give an external account the ability to use an object that is protected by an aws kms cmk. to do this, select custom kms arn from the list and enter the amazon resource name (arn) for the external account. administrators of an external account that have usage permissions to an object protected by your aws kms cmk can further restrict access by creating a resource-level iam policy.  for more information about creating an aws kms cmk, see  in the aws key management service developer guide. for more information about protecting data with aws kms, see  in the amazon simple storage service developer guide. metadata for amazon s3 objects is represented by a name-value (key-value) pair. there are two kinds of metadata: system-defined metadata and user-defined metadata. if you want to add amazon s3 system-defined metadata to all of the objects you are uploading, for header, select a header. you can select common http headers, such as content-type and content-disposition. type a value for the header, and then choose save. for a list of system-defined metadata and information about whether you can add the value, see  in the amazon simple storage service developer guide. any metadata starting with prefix  is treated as user-defined metadata. user-defined metadata is stored with the object, and is returned when you download the object.  to add user-defined metadata to all of the objects that you are uploading, type  plus a custom metadata name in the header field. type a value for the header, and then choose save. both the keys and their values must conform to us-ascii standards. user-defined metadata can be as large as 2 kb. for more information about user-defined metadata, see  in the amazon simple storage service developer guide. object tagging gives you a way to categorize storage. each tag is a key-value pair. key and tag values are case sensitive. you can have up to 10 tags per object.  to add tags to all of the objects that you are uploading, type a tag name in the key field. type a value for the tag, and then choose save. a tag key can be up to 128 unicode characters in length and tag values can be up to 255 unicode characters in length. for more information about object tags, see  in the amazon simple storage service developer guide. choose next. on the upload review page, verify that your settings are correct, and then choose upload. to make changes, choose previous. to see the progress of the upload, choose in progress at the bottom of the browser window. to see a history of your uploads and other operations, choose success. this procedure explains how to upload files into an s3 bucket by choosing upload. to upload files to an s3 bucket by pointing and clicking sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to upload your files to. choose upload. in the upload dialog box, choose add files. choose one or more files to upload, and then choose open. after you see the files that you chose listed in the upload dialog box, do one of the following:  to add more files, choose add more files. to immediately upload the listed files, choose upload.  to set permissions or properties for the files that you are uploading, choose next. to set permissions and properties, start with step 5 of . .
you can delete an empty bucket, and when you're using the aws management console, you can delete a bucket that contains objects. if you delete a bucket that contains objects, all the objects in the bucket are permanently deleted.  when you delete a bucket with versioning enabled, all versions of all the objects in the bucket are permanently deleted. for more information about versioning, see  in the amazon simple storage service developer guide. before deleting a bucket, consider the following: bucket names are unique. if you delete a bucket, another aws user can use the name. when you delete a bucket that contains objects, all the objects in the bucket are permanently deleted, including objects that transitioned to the  storage class.if the bucket hosts a static website, and you created and configured an amazon route 53 hosted zone as described in : you must clean up the route 53 hosted zone settings that are related to the bucket as described in .if the bucket receives log data from elastic load balancing (elb): we recommend that you stop the delivery of elb logs to the bucket before deleting it. after you delete the bucket, if another user creates a bucket using the same name, your log data could potentially be delivered to that bucket. for information about elb access logs, see  in the user guide for classic load balancers and  in the user guide for application load balancers.importantif you want to continue to use the same bucket name, don't delete the bucket. we recommend that you empty the bucket and keep it. after a bucket is deleted, the name becomes available to reuse, but the name might not be available for you to reuse for various reasons. for example, it might take some time before the name can be reused, and some other account could create a bucket with that name before you do. to delete an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the buckets list, select the option next to the name of the bucket that you want to delete and then choose delete at the top of the page. on the delete bucket page, confirm that you want to delete the bucket by entering the bucket name into the text field, and then choose delete bucket. noteif the bucket contains any objects, empty the bucket before deleting it by selecting the empty bucket configuration link in the this bucket is not empty error alert and following the instructions on the empty bucket page. then return to the delete bucket page and delete the bucket. 
replication is the automatic, asynchronous copying of objects across buckets in the same or different aws regions. replication copies newly created objects and object updates from a source bucket to a destination bucket. for more information about replication concepts and how to use replication with the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide.  replication requires versioning to be enabled on both the source and destination buckets. to review the full list of requirements, see  in the amazon simple storage service developer guide. for more information about versioning, see  the object replicas in the destination bucket are exact replicas of the objects in the source bucket. they have the same key names and the same metadata—for example, creation time, owner, user-defined metadata, version id, access control list (acl), and storage class. optionally, you can explicitly specify a different storage class for object replicas. and regardless of who owns the source bucket or the source object, you can choose to change replica ownership to the aws account that owns the destination bucket. for more information, see  in the amazon simple storage service developer guide.  you can use s3 replication time control (s3 rtc) to replicate your data in the same aws region or across different aws regions in a predictable timeframe. s3 rtc replicates 99.99 percent of new objects stored in amazon s3 within 15 minutes and most objects within seconds. for more information, see  in the amazon simple storage service developer guide. note about replication and lifecycle rulesmetadata for an object remains identical between original objects and replica objects. lifecycle rules abide by the creation time of the original object, and not by when the replicated object becomes available in the destination bucket. however, lifecycle does not act on objects that are pending replication until replication is complete. you use the amazon s3 console to add replication rules to the source bucket. replication rules define which source bucket objects to replicate and the destination bucket where the replicated objects are stored. you can create a rule to replicate all the objects in a bucket or a subset of objects with a specific key name prefix, one or more object tags, or both. a destination bucket can be in the same aws account as the source bucket, or it can be in a different account. if you specify an object version id to delete, amazon s3 deletes that object version in the source bucket. but it doesn't replicate the deletion in the destination bucket. in other words, it doesn't delete the same object version from the destination bucket. this protects data from malicious deletions. if the destination bucket is in a different account from the source bucket, you must add a bucket policy to the destination bucket to grant the owner of the source bucket account permission to replicate objects in the destination bucket. the amazon s3 console builds this required bucket policy for you to copy and add to the destination bucket in the other account.  when you add a replication rule to a bucket, the rule is enabled by default, so it starts working as soon as you save it.  topics follow these steps to configure a replication rule when the destination bucket is in the same aws account as the source bucket. sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose management, choose replication, and then choose add rule. in the replication rule wizard, under set source, you have the following options for setting the replication source: to replicate the whole bucket, choose entire bucket bucket-name. to replicate all objects that have the same prefix (for example, all objects that have names that begin with the string ), choose prefix or tags. enter a prefix in the box, choose the prefix from the drop-down list, and then press enter. if you enter a prefix that is the name of a folder, you must use / (forward slash) as the last character (for example, ). for more information about prefixes, see  in the amazon simple storage service developer guide.to replicate all objects with one or more object tags, enter a tag in the box, choose the tag from the drop-down list, and then press enter. enter a tag value and then press enter. repeat the procedure to add another tag. you can combine a prefix and tags. for more information about object tags, see  in the amazon simple storage service developer guide.the new schema supports prefix and tag filtering and the prioritization of rules. for more information about the new schema, see  in the amazon simple storage service developer guide. the developer guide describes the xml used with the amazon s3 api that works behind the user interface. in the developer guide, the new schema is described as replication configuration xml v2. to replicate objects in the source bucket that are encrypted with aws key management service (aws kms), under replication criteria, select replicate objects encrypted with aws kms. under choose one or more keys for decrypting source objects are the source aws kms customer master keys (cmks) that you allow replication to use. all source cmks are included by default. you can choose to narrow the cmk selection.  objects encrypted by aws kms cmks that you do not select are not replicated. a cmk or a group of cmks is chosen for you, but you can choose the cmks if you want. for information about using aws kms with replication, see  in the amazon simple storage service developer guide. importantwhen you replicate objects that are encrypted with aws kms, the aws kms request rate doubles in the source region and increases in the destination region by the same amount. these increased call rates to aws kms are due to the way that data is re-encrypted using the customer master key (cmk) that you define for the replication destination region. aws kms has a request rate limit that is per calling account per region. for information about the limit defaults, see  in the aws key management service developer guide.if your current amazon s3 put object request rate during replication is more than half the default aws kms rate limit for your account, we recommend that you request an increase to your aws kms request rate limit. to request an increase, create a case in the aws support center at . for example, suppose that your current put object request rate is 1,000 requests per second and you use aws kms to encrypt your objects. in this case, we recommend that you ask aws support to increase your aws kms rate limit to 2,500 requests per second, in both your source and destination regions (if different), to ensure that there is no throttling by aws kms.to see your put object request rate in the source bucket, view  in the amazon cloudwatch request metrics for amazon s3. for information about viewing cloudwatch metrics, see  choose next. to choose a destination bucket from the account that you're currently using, on the set destination page, under destination bucket, choose buckets in this account. enter the name of the destination bucket for the replication, or choose a name in the drop-down list. if you want to choose a destination bucket from a different aws account, see . if versioning is not enabled on the destination bucket, you get a warning message that contains an enable versioning button. choose this button to enable versioning on the bucket. if you chose to replicate objects encrypted with aws kms, under destination encryption settings, enter the amazon resource name (arn) of the aws kms cmk to use to encrypt the replicas in the destination bucket. you can find the arn for your aws kms cmk in the iam console, under encryption keys.  or, you can choose a cmk name from the drop-down list. for more information about creating an aws kms cmk, see  in the aws key management service developer guide. importantthe amazon s3 console lists only 100 aws kms cmks per aws region. if you have more than 100 cmks in the same region, you can see only the first 100 cmks in the s3 console. to use a kms cmk that is not listed in the console, choose custom kms arn, and enter the kms cmk arn. if you want to replicate your data into a specific storage class in the destination bucket, on the set destination page, under destination options, select change the storage class for the replicated object(s). then choose the storage class that you want to use for the replicated objects in the destination bucket. if you don't select this option, the storage class for replicated objects is the same class as the original objects.  similarly, if you want to change object ownership in the destination bucket, choose change object ownership to the destination bucket owner. for more information about this option, see .  if you want to enable s3 replication time control (s3 rtc) in your replication configuration, select replication time control. notewhen you use s3 rtc, additional per-gb data transfer fees and cloudwatch metrics fees apply. choose next. set up an aws identity and access management (iam) role that amazon s3 can assume to replicate objects on your behalf. to set up an iam role, on the configure options page, under select role, do one of the following: we highly recommend that you choose create new role to have amazon s3 create a new iam role for you. when you save the rule, a new policy is generated for the iam role that matches the source and destination buckets that you choose. the name of the generated role is based on the bucket names and uses the following naming convention: replication_role_for_source-bucket_to_destination-bucket.you can choose to use an existing iam role. if you do, you must choose a role that grants amazon s3 the necessary permissions for replication. replication fails if this role does not grant amazon s3 sufficient permissions to follow your replication rule.  importantwhen you add a replication rule to a bucket, you must have the  permission to be able to pass the iam role that grants amazon s3 replication permissions. for more information, see  in the iam user guide.under rule name, enter a name for your rule to help identify the rule later. the name is required and must be unique within the bucket. if the bucket has existing replication rules, you are instructed to set a priority for the rule. you must set a priority for the rule to avoid conflicts caused by objects that are included in the scope of more than one rule. in the case of overlapping rules, amazon s3 uses the rule priority to determine which rule to apply. the higher the number, the higher the priority. for more information about rule priority, see  in the amazon simple storage service developer guide. under status, enabled is selected by default. an enabled rule starts to work as soon as you save it. if you want to enable the rule later, select disabled. choose next. on the review page, review your replication rule. if it looks correct, choose save. otherwise, choose previous to edit the rule before saving it.  after you save your rule, you can edit, enable, disable, or delete your rule on the replication page. follow these steps to configure a replication rule when the destination bucket is in a different aws account than the source bucket. sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose management, choose replication, and then choose add rule. if you have never created a replication rule before, start with . on the replication rule wizard set destination page, under destination bucket, choose buckets in another account. then enter the name of the destination bucket and the account id from a different aws account. choose save. after you save the destination bucket name and account id, you might get a warning message telling you to add a bucket policy to the destination bucket so that amazon s3 can verify whether versioning is enabled on the bucket. you are presented with a bucket policy in a few steps that you can copy and add to the destination bucket in the other account. for information about adding a bucket policy to an s3 bucket and versioning, see  and  if you chose to replicate objects encrypted with aws kms, under destination encryption settings, enter the amazon resource name (arn) aws kms cmk to use to encrypt the replicas in the destination bucket.  for more information about creating an aws kms cmk, see  in the aws key management service developer guide. on the set destination page, under destination options: to replicate your data into a specific storage class in the destination bucket, select change the storage class for the replicated object(s). then choose the storage class that you want to use for the replicated objects in the destination bucket. if you don't select this option, the storage class for replicated objects is the same class as the original objects.to change the object ownership of the replica objects to the destination bucket owner, select change object ownership to destination owner. this option enables you to separate object ownership of the replicated data from the source. if asked, type the account id of the destination bucket. when you select this option, regardless of who owns the source bucket or the source object, the aws account that owns the destination bucket is granted full permission to replica objects. for more information, see  in the amazon simple storage service developer guide. if you want to add s3 replication time control (s3 rtc) to your replication configuration, select replication time control. notewhen you use s3 rtc, additional per-gb data transfer fees and cloudwatch metrics fees apply.choose next. set up an aws identity and access management (iam) role that amazon s3 can assume to perform replication of objects on your behalf. to set up an iam role, on the configure options page, under select role, do one of the following: we highly recommend that you choose create new role to have amazon s3 create a new iam role for you. when you save the rule, a new policy is generated for the iam role that matches the source and destination buckets that you choose. the name of the generated role is based on the bucket names and uses the following naming convention: replication_role_for_source-bucket_to_destination-bucket.you can choose to use an existing iam role. if you do, you must choose a role that allows amazon s3 to replicate objects from the source bucket to the destination bucket on your behalf.a bucket policy is provided on the configure options page that you can copy and add to the destination bucket in the other account. for information about adding a bucket policy to an s3 bucket, see  if you chose to replicate objects encrypted with aws kms, an aws kms key policy is provided on the configure options page. you can copy this policy to add to the key policy for the aws kms cmk that you are using. the key policy grants the source bucket owner permission to use the cmk. for information about updating the key policy, see . on the review page, review your replication rule. if it looks correct, choose save. otherwise, choose previous to edit the rule before saving it.  after you save your rule, you can edit, enable, disable, or delete your rule on the replication page. follow the instructions given on the replication page under the warning message, the replication rule is saved, but additional settings are required in the destination account. sign out of the aws account that you are currently in, and then sign in to the destination account.  importantreplication fails until you sign in to the destination account and complete the following steps. after you sign in to the destination account, choose the management tab, choose replication, and then choose receive objects on the actions menu.  on the receive objects page, you can do the following: enable versioning on the destination bucket.apply the bucket policy provided by amazon s3 to the destination bucket.copy the aws kms key policy that you need to update the aws kms cmk that is being used to encrypt the replica objects in the destination bucket. for information about updating the key policy, see .you must grant permissions to the account of the source bucket owner to encrypt using your aws kms cmk with a key policy. the following procedure describes how to use the aws identity and access management (iam) console to modify the key policy for the aws kms cmk that is being used to encrypt the replica objects in the destination bucket. to grant permissions to encrypt using your aws kms cmk sign in to the aws management console using the aws account that owns the aws kms cmk. open the aws kms console at . choose the alias of the cmk that you want to encrypt with. in the key policy section of the page, choose switch to policy view. choose edit to edit key policy. using the key policy editor, insert the key policy provided by amazon s3 into the existing key policy, and then choose save changes. you might want to add the policy to the end of the existing policy.  for more information about creating and editing aws kms cmks, see  in the aws key management service developer guide.   in the amazon simple storage service developer guide
this section describes how to create a s3 batch operations job. for information about performing batch operations using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. to create a batch job sign in to the aws management console and open the amazon s3 console at . choose batch operations on the navigation pane of the amazon s3 console. choose create job. choose the region where you want to create your job. under manifest format, choose the type of manifest object to use. if you choose s3 inventory report, enter the path to the manifest.json object that amazon s3 generated as part of the csv-formatted inventory report, and optionally the version id for the manifest object if you want to use a version other than the most recent.if you choose csv, enter the path to a csv-formatted manifest object. the manifest object must follow the format described in the console. you can optionally include the version id for the manifest object if you want to use a version other than the most recent.choose next under operation, choose the operation that you want to perform on all objects listed in the manifest. fill out the information for the operation you chose and then choose next. fill out the information for configure additional options and then choose next. for review, verify the settings. if you need to make changes, choose previous. otherwise, choose create job.  in the amazon simple storage service developer guide in the amazon simple storage service developer guide in the amazon simple storage service developer guide
*copyright &copy; 2020 amazon web services, inc. and/or its affiliates. all rights reserved.* amazon's trademarks and trade dress may not be used in       connection with any product or service that is not amazon's,       in any manner that is likely to cause confusion among customers,       or in any manner that disparages or discredits amazon. all other       trademarks not owned by amazon are the property of their respective      owners, who may or may not be affiliated with, connected to, or       sponsored by amazon. 
versioning enables you to keep multiple versions of an object in one bucket. this section describes how to enable object versioning on a bucket. for more information about versioning support in amazon s3, see  and  in the amazon simple storage service developer guide. to enable or disable versioning on an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to enable versioning for. choose properties. choose versioning. choose enable versioning or suspend versioning, and then choose save. noteyou can use aws multi-factor authentication (mfa) with versioning. when you use mfa with versioning, you must provide your aws account’s access keys and a valid code from the account’s mfa device in order to permanently delete an object version or suspend or reactivate versioning. to use mfa with versioning, you enable . however, you cannot enable  using the aws management console. you must use the aws cli or api. for more information, see . 
this section explains how to use the amazon s3 console to recover (undelete) deleted objects. to be able to undelete a deleted object, you must have had versioning enabled on the bucket that contains the object before the object was deleted. for information about enabling versioning, see . when you delete an object in a versioning-enabled bucket, all versions remain in the bucket and amazon s3 creates a delete marker for the object. to undelete the object, you must delete this delete marker. for more information about versioning and delete markers, see  in the amazon simple storage service developer guide. to recover deleted objects from an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. to see a list of the versions of the objects in the bucket, select show. you'll be able to see the delete markers for deleted objects.  to undelete an object, you must delete the delete marker. select the check box next to the delete marker of the object to recover, and then choose delete from the actions menu. then, choose hide and you'll see the undeleted object listed.  in the amazon simple storage service developer guide
this section explains how to use the amazon s3 console to restore an object that has been archived to the s3 glacier or s3 glacier deep archive storage classes. objects stored in the s3 glacier or s3 glacier deep archive are not immediately accessible. to access an object in this class, you must restore a temporary copy of it to its s3 bucket for the duration (number of days) that you specify. for information about the s3 glacier or s3 glacier deep archive storage classes, see  in the amazon simple storage service developer guide.  when you restore an archive, you pay for both the archive and the restored copy. because there is a storage cost for the copy, restore objects only for the duration you need them. if you want a permanent copy of the object, create a copy of it in your s3 bucket. for information about amazon s3 features and pricing, see . after restoring an object, you can download it from the overview page. for more information, see . topics the following are the available retrieval options when restoring an archived object:   - expedited retrievals allow you to quickly access your data stored in the s3 glacier storage class when occasional urgent requests for a subset of archives are required. for all but the largest archived objects (250 mb+), data accessed using expedited retrievals is typically made available within 1–5 minutes. provisioned capacity ensures that retrieval capacity for expedited retrievals is available when you need it. for more information, see . expedited retrievals and provisioned capacity are not available for objects stored in the s3 glacier deep archive storage class. - standard retrievals allow you to access any of your archived objects within several hours. this is the default option for the s3 glacier and s3 glacier deep archive retrieval requests that do not specify the retrieval option. standard retrievals typically finish within 3–5 hours for objects stored in the s3 glacier storage class. they typically finish within 12 hours for objects stored in the s3 glacier deep archive storage class.  - bulk retrievals are the lowest-cost retrieval option in amazon s3 glacier, enabling you to retrieve large amounts, even petabytes, of data inexpensively. bulk retrievals typically finish within 5–12 hours for objects stored in the s3 glacier storage class. they typically finish within 48 hours for objects stored in the s3 glacier deep archive storage class.for more information about retrieval options, see  in the amazon simple storage service developer guide. this topic explains how to use the amazon s3 console to restore an object that has been archived to the s3 glacier or s3 glacier deep archive storage classes. (the console uses the names glacier and glacier deep archive for these storage classes.) to restore archived s3 objects sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the objects that you want to restore. in the name list, select the object or objects that you want to restore, choose actions, and then choose restore. in the initiate restore dialog box, type the number of days that you want your archived data to be accessible.  choose one of the following retrieval options from the retrieval options menu. choose bulk retrieval or standard retrieval, and then choose restore. choose expedited retrieval (available only for the glacier storage class).provisioned capacity is only available for the glacier storage class. if you have provisioned capacity, choose restore to start a provisioned retrieval. if you have provisioned capacity, all of your expedited retrievals are served by your provisioned capacity. for more information about provisioned capacity, see .  if you don't have provisioned capacity and you don't want to buy it, choose restore. if you don't have provisioned capacity, but you want to buy it, choose add capacity unit, and then choose buy. when you get the purchase succeeded message, choose restore to start provisioned retrieval.you can upgrade the speed of your restoration while it is in progress. to upgrade an in-progress restore to a faster tier in the name list, select one or more of the objects that you are restoring, choose actions, and then choose restore from glacier. for information about checking the restoration status of an object, see . choose the tier that you want to upgrade to and then choose restore. for more information about upgrading to a faster restore tier, see see  in the amazon simple storage service developer guide.  to check the progress of the restoration, see the object overview panel. for information about the overview panel, see .  the overview section shows that restoration is in progress.  when the temporary copy of the object is available, the object's overview section shows the restoration expiry date. this is when amazon s3 will remove the restored copy of your archive.   restored objects are stored only for the number of days that you specify. if you want a permanent copy of the object, create a copy of it in your amazon s3 bucket.  amazon s3 calculates the expiry date by adding the number of days that you specify to the time you request to restore the object, and then rounding to the next day at midnight utc. this calculation applies to the initial restoration of the object and to any extensions to availability that you request. for example, if an object was restored on 10/15/2012 10:30 am utc and the number of days that you specified is 3, then the object is available until 10/19/2012 00:00 utc. if, on 10/16/2012 11:00 am utc you change the number of days that you want it to be accessible to 1, then amazon s3 makes the restored object available until 10/18/2012 00:00 utc. after restoring an object, you can download it from the overview page. for more information, see .   in the amazon s3 developer guide. in the aws cli command reference.
object tagging gives you a way to categorize storage. this topic explains how to use the console to add tags to an s3 object after the object has been uploaded. for information about adding tags to an object when the object is being uploaded, see .  each tag is a key-value pair that adheres to the following rules: you can associate up to 10 tags with an object. tags associated with an object must have unique tag keys.a tag key can be up to 128 unicode characters in length and tag values can be up to 255 unicode characters in length.key and tag values are case sensitive.  for more information about object tags, see  in the amazon simple storage service developer guide. to add tags to an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object you want to add tags to. choose properties. choose tags and then choose add tag. each tag is a key-value pair. type a key and a value. then choose add tag to add another tag or choose save.  you can enter up to 10 tags for an object. 
this section explains how to use the amazon simple storage service (amazon s3) console to grant access permissions to your buckets and objects. it also explains how to use amazon s3 block public access to prevent the application of any settings that allow public access to data within s3 buckets.   buckets and objects are amazon s3 resources.  you grant access permissions to your buckets and objects by using resource-based access policies. you can associate an access policy with a resource. an access policy describes who has access to resources. the resource owner is the aws account that creates the resource. for more information about resource ownership and access policies, see  in the amazon simple storage service developer guide.  bucket access permissions specify which users are allowed access to the objects in a bucket and which types of access they have. object access permissions specify which users are allowed access to the object and which types of access they have. for example, one user might have only read permission, while another might have read and write permissions. bucket and object permissions are independent of each other. an object does not inherit the permissions from its bucket. for example, if you create a bucket and grant write access to a user, you can't access that user’s objects unless the user explicitly grants you access.  to grant access to your buckets and objects to other aws accounts and to the general public, you use resource-based access policies known as access control lists (acls).  a bucket policy is a resource-based aws identity and access management (iam) policy that grants other aws accounts or iam users access to an s3 bucket. bucket policies supplement, and in many cases, replace acl-based access policies.  for more information about using iam with amazon s3, see  in the amazon simple storage service developer guide.  for more in-depth information about managing access permissions, see  in the amazon simple storage service developer guide. this section also explains how to use the amazon s3 console to add a cross-origin resource sharing (cors) configuration to an s3 bucket. cors allows client web applications that are loaded in one domain to interact with resources in another domain. topics 
this section explains how to use the amazon simple storage service (amazon s3) console to manage access permissions for s3 buckets by using access control lists (acls). acls are resource-based access policies that grant access permissions to buckets and objects. for more information about managing access permissions with resource-based policies, see  in the amazon simple storage service developer guide. you can grant permissions to other aws account users or to predefined groups. the user or group that you are granting permissions to is called the grantee. by default, the owner, which is the aws account that created the bucket, has full permissions. each permission you grant for a user or group adds an entry in the acl that is associated with the bucket. the acl lists grants, which identify the grantee and the permission granted. for more information about acls, see  in the amazon simple storage service developer guide. to set acl access permissions for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to set permissions for. choose permissions, and then choose access control list. you can manage bucket access permissions for the following:  1.  access for your aws account root user  access for other aws accounts to grant permissions to an aws user from a different aws account, under access for other aws accounts, choose add account. in the enter an id field, enter the canonical id of the aws user that you want to grant bucket permissions to. for information about finding a canonical id, see  in the amazon web services general reference. you can add as many as 99 users. select the check boxes next to the permissions that you want to grant to the user, and then choose save. to display information about the permissions, choose the help icons. warningwhen you grant other aws accounts access to your resources, be aware that the aws accounts can delegate their permissions to users under their accounts. this is known as cross-account access. for information about using cross-account access, see  in the iam user guide.  public access  warninguse caution when granting the everyone group public access to your s3 bucket. when you grant access to this group, anyone in the world can access your bucket. we highly recommend that you never grant any kind of public write access to your s3 bucket. s3 log delivery group to grant access to amazon s3 to write server access logs to the bucket, under s3 log delivery group, choose log delivery. if a bucket is set up as the target bucket to receive access logs, the bucket permissions must allow the log delivery group write access to the bucket. when you enable server access logging on a bucket, the amazon s3 console grants write access to the log delivery group for the target bucket that you choose to receive the logs. for more information about server access logging, see . you can also set bucket permissions when you are creating a bucket. for more information about setting permissions when creating a bucket, see .  
amazon s3 default encryption provides a way to set the default encryption behavior for an amazon s3 bucket. you can set default encryption on a bucket so that all objects are encrypted when they are stored in the bucket. the objects are encrypted using server-side encryption with either amazon s3-managed keys (sse-s3) or aws key management service (aws kms) customer master keys (cmks).  when you use server-side encryption, amazon s3 encrypts an object before saving it to disk in its data centers and decrypts it when you download the objects. for more information about protecting data using server-side encryption and encryption key management, see  in the amazon simple storage service developer guide. default encryption works with all existing and new amazon s3 buckets. without default encryption, to encrypt all objects stored in a bucket, you must include encryption information with every object storage request. you must also set up an amazon s3 bucket policy to reject storage requests that don't include encryption information.  there are no new charges for using default encryption for s3 buckets. requests to configure the default encryption feature incur standard amazon s3 request charges. for information about pricing, see . for sse-kms cmk storage, aws kms charges apply and are listed at .  to enable default encryption on an amazon s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose properties. choose default encryption. if you want to use keys that are managed by amazon s3 for default encryption, choose aes-256, and choose save.  for more information about using amazon s3 server-side encryption to encrypt your data, see  in the amazon simple storage service developer guide. importantyou might need to update your bucket policy when enabling default encryption. for more information, see  in the amazon simple storage service developer guide. if you want to use cmks that are stored in aws kms for default encryption, follow these steps: choose aws-kms. to choose a customer-managed aws kms cmk that you have created, use one of these methods: in the list that appears, choose the aws kms cmk.in the list that appears, choose custom kms arn, and then enter the amazon resource name of the aws kms cmk. importantyou can use only kms cmks that are enabled in the same aws region as the bucket. the s3 console only lists 100 kms cmks per region. if you have more than 100 cmks in the same region, you can only see the first 100 cmks in the s3 console. to use a kms cmk that is not listed in the console, choose custom kms arn, and enter the kms cmk arn.when you use an aws kms cmk for server-side encryption in amazon s3, you must choose a symmetric cmk. amazon s3 only supports symmetric cmks and not asymmetric cmks. for more information, see  in the aws key management service developer guide. importantif you use the aws kms option for your default encryption configuration, you are subject to the rps (requests per second) limits of aws kms. for more information about aws kms limits and how to request a limit increase, see . for more information about creating an aws kms cmk, see  in the aws key management service developer guide. for more information about using aws kms with amazon s3, see  in the amazon simple storage service developer guide. choose save.  in the amazon simple storage service developer guide
this section explains how to use the amazon s3 console to see the different versions of an object. a versioning-enabled bucket can have many versions of the same object:, one current (latest) version and zero or more noncurrent (previous) versions. amazon s3 assigns each object a unique version id. for information about enabling versioning, see .  if a bucket is versioning-enabled, amazon s3 creates another version of an object under the following conditions:  if you upload an object that has the same name as an object that already exists in the bucket, amazon s3 creates another version of the object instead of replacing the existing object. if you update any object properties after you upload the object to the bucket, such as changing the storage details or other metadata , amazon s3 creates a new object version in the bucket. for more information about versioning support in amazon s3, see  and  in the amazon simple storage service developer guide. to see multiple versions of an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. to see a list of the versions of the objects in the bucket, choose show. for each object version, the console shows a unique version id, the date and time the object version was created, and other properties. (objects stored in your bucket before you set the versioning state have a version id of null.)  to list the objects without the versions, choose hide. you also can view, download, and delete object versions in the object overview panel. for more information, see . importantyou can undelete an object only if it was deleted as the latest (current) version. you can't undelete a previous version of an object that was deleted. for more information, see  and  in the amazon simple storage service developer guide. 
this section explains how to use the amazon s3 console to add a cross-origin resource sharing (cors) configuration to an s3 bucket. cors allows client web applications that are loaded in one domain to interact with resources in another domain.  to configure your bucket to allow cross-origin requests, you add cors configuration to the bucket. a cors configuration is an xml document that defines rules that identify the origins that you will allow to access your bucket, the operations (http methods) supported for each origin, and other operation-specific information. for more information about cors and examples, see  in the amazon simple storage service developer guide. when you enable cors on the bucket, the access control lists (acls) and other access permission policies continue to apply. to add a cors configuration to an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to create a bucket policy for. choose permissions, and then choose cors configuration. in the cors configuration editor text box, type or copy and paste a new cors configuration, or edit an existing configuration. the cors configuration is an xml file. the text that you type in the editor must be valid xml. for more information, see  choose save. noteamazon s3 displays the amazon resource name (arn) for the bucket next to the cors configuration editor title. for more information about arns, see  in the amazon web services general reference. 
you can use lifecycle policies to define actions that you want amazon s3 to take during an object's lifetime (for example, transition objects to another storage class, archive them, or delete them after a specified period of time). you can define a lifecycle policy for all objects or a subset of objects in the bucket by using a shared prefix (objects names that begin with a common string) or a tag.  using a lifecycle policy, you can define actions specific to current and noncurrent object versions. for more information, see  and  and  in the amazon simple storage service developer guide. to create a lifecycle policy sign in to the aws management console and open the amazon s3 console at . in the buckets list, choose the name of the bucket that you want to create a lifecycle policy for. choose the management tab, and choose add lifecycle rule. in the lifecycle rule dialog box, type a name for your rule.  the name must be unique within the bucket.  choose the scope of the lifecycle rule: all objects with a specific prefix or tag or all the objects in the bucket. to apply this lifecycle rule to all objects with a specific prefix or tag, choose limit the scope to specific prefixes or tags. in the add prefix or tag filter box, type the prefix or tag name, and press enter.  for more information about object name prefixes, see  in the amazon simple storage service developer guide.for more information about object tags, see  in the amazon simple storage service developer guide.to apply this lifecycle rule to all objects in the bucket, choose apply to all objects in the bucket.choose next. the storage class transition page opens. when you configure your storage class transitions, you define the rules to transition objects to the standard-ia, one zone-ia, glacier, and deep archive storage classes. for more information, see  in the amazon simple storage service developer guide. you can define transitions for current or previous object versions or for both current and previous versions. versioning enables you to keep multiple versions of an object in one bucket. for more information about versioning, see . choose the versions for which you want to define transitions, current or noncurrent: to define transitions that are applied to the current verion of the object, choose current version. to define transitions that are applied to all previous versions of the object, choose previous versions. in the screenshot below, current version is chosen. to add a transition:  for a current version, under for current object versions, choose add transition. for a non-current version, under for non-current object versions, choose add transition. for each transition that you add, choose one of the following: transition to standard-ia after.transition to intelligent-tiering after. transition to one zone-ia after. transition to glacier after.transition to glacier deep archive after. importantwhen you choose the glacier or glacier deep archive storage class, your objects remain in amazon s3. you cannot access them directly through the separate amazon s3 glacier service. for more information, see . in the days after creation box, enter the number of days after the creation of the object that you want the transition to be applied (for example, 30 or 100 days). when you are done configuring transitions, choose next. under configure expiration, for this example, choose both current version and previous versions.  importantin a non-versioned bucket the expiration action results in amazon s3 permanently removing the object. for more information about lifecycle actions, see  in the amazon simple storage service developer guide. choose expire current version of object, and then enter the number of days after object creation to delete the object (for example, 395 days). if you choose this expire option, you cannot choose the option to clean up expired delete markers.  choose permanently delete previous versions, and then enter the number of days after an object becomes a previous version to permanently delete the object (for example, 465 days). to clean up incomplete multipart uploads, we recommend that you choose clean up incomplete multipart uploads and enter the number of days after the multipart upload initiation that you want to end and clean up incomplete multipart uploads(for example, 7 days).  for more information about multipart uploads, see  in the amazon simple storage service developer guide. choose next. for review, verify the settings for your rule. if you need to make changes, choose previous. otherwise, choose save.  if the rule does not contain any errors, it is enabled and you can see it on the lifecycle page. 
with s3 object lock, you can store objects in amazon s3 using a write-once-read-many (worm) model. you can use s3 object lock to prevent an object from being deleted or overwritten for a fixed amount of time or indefinitely. for information about object locking using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. before you lock any objects, you have to enable a bucket to use s3 object lock. you enable object lock when you create a bucket. after you enable object lock on a bucket, you can lock objects in that bucket. when you create a bucket with object lock enabled, you can't disable object lock or suspend versioning for that bucket.  for information about creating a bucket with s3 object lock enabled, see . to lock an amazon s3 object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. in the name list, choose the name of the object that you want to lock. choose properties. choose object lock. choose a retention mode. you can change the retain until date. you can also choose to enable a legal hold. for more information, see  in the amazon simple storage service developer guide. choose save. 
welcome to the amazon simple storage service console user guide for the amazon simple storage service (amazon s3) console.  amazon s3 provides virtually limitless storage on the internet. this guide explains how you can manage buckets, objects, and folders in amazon s3 by using the aws management console, a browser-based graphical user interface for interacting with aws services.  for detailed conceptual information about how amazon s3 works, see  in the amazon simple storage service developer guide. the developer guide also has detailed information about amazon s3 features and code examples to support those features. topics 
this topic describes how to enable server access logging for an amazon s3 bucket using the aws management console. for information about how to enable logging programmatically and details about how logs are delivered, see  in the amazon simple storage service developer guide. by default, amazon simple storage service (amazon s3) doesn't collect server access logs. when you enable logging, amazon s3 delivers access logs for a source bucket to a target bucket that you choose. the target bucket must be in the same aws region as the source bucket and must not have a default retention period configuration.  server access logging provides detailed records for the requests that are made to an s3 bucket. server access logs are useful for many applications. for example, access log information can be useful in security and access audits. it can also help you learn about your customer base and understand your amazon s3 bill.  an access log record contains details about the requests that are made to a bucket. this information can include the request type, the resources that are specified in the request, and the time and date that the request was processed. for more information, see  in the amazon simple storage service developer guide. importantthere is no extra charge for enabling server access logging on an amazon s3 bucket. however, any log files that the system delivers to you will accrue the usual charges for storage. (you can delete the log files at any time.) we do not assess data transfer charges for log file delivery, but we do charge the normal data transfer rate for accessing the log files. to enable server access logging for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to enable server access logging for. choose properties. choose server access logging. choose enable logging. for target, choose the name of the bucket that you want to receive the log record objects.  the target bucket must be in the same region as the source bucket and must not have a default retention period configuration. (optional) for target prefix, type a key name prefix for log objects, so that all of the log object names begin with the same string. choose save. you can view the logs in the target bucket. if you specified a prefix, the prefix shows as a folder in the target bucket in the console. after you enable server access logging, it might take a few hours before the logs are delivered to the target bucket. for more information about how and when logs are delivered, see  in the amazon simple storage service developer guide. more info  
this topic describes how to set or change the type of encryption an object using the amazon s3 console. noteif you change an object's encryption, a new object is created to replace the old one. if s3 versioning is enabled, a new version of the object is created, and the existing object becomes an older version. the role that changes the property also becomes the owner of the new object or (object version).  to add or change encryption for an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object that you want to add or change encryption for. choose properties, and then choose encryption. the encryption dialog box opens, giving you three choices for object encryption: none‐ no object encryption.aes-256 ‐ server-side encryption with amazon s3 managed keys (sse-s3).aws‐kms ‐ server-side encryption with aws key management service (aws kms) customer master keys (sse-kms).if you want to remove encryption from an object that already has encryption settings, choose none and then choose save. if you want to encrypt your object using keys that are managed by amazon s3, follow these steps: chooseaes-256.  for more information about using amazon s3 server-side encryption to encrypt your data, see  in the amazon simple storage service developer guide. choose save. if you want to encrypt your object using aws kms, follow these steps: choose aws-kms. choose an aws kms customer master key (cmk). the list shows  that you have created and your aws managed cmk for amazon s3. for more information about creating a customer managed aws kms cmk, see  in the aws key management service developer guide.  importantthe amazon s3 console lists only 100 aws kms cmks per aws region. if you have more than 100 cmks in the same region, you can see only the first 100 cmks in the s3 console. to use a kms cmk that is not listed in the console, choose custom kms arn, and enter the kms cmk arn. choose save. importantto encrypt objects in the bucket, you can use only cmks that are enabled in the same aws region as the bucket. amazon s3 only supports symmetric cmks. amazon s3 does not support asymmetric cmks. for more information, see . to give an external account the ability to use an object that is protected by an aws kms cmk, follow these steps:  choose aws-kms. enter the amazon resource name (arn) for the external account. choose save. administrators of an external account that have usage permissions to an object protected by your aws kms cmk can further restrict access by creating a resource-level aws identity and access management (iam) policy.  in the amazon simple storage service developer guide
this section explains how to manage and use your amazon s3 access points using the aws management console. each access point is associated with a single amazon s3 bucket. before you begin, navigate to the list of your access points for a bucket as described in the following procedure. to find a list of access points for a bucket sign in to the aws management console and open the amazon s3 console at . in the s3 buckets section, select the bucket whose access points you want to manage. on the bucket detail page, choose the access points tab. from the access points tab, you can view an access point's configuration details, edit an access point's policy, use an access point to access your bucket, or delete an access point. the following procedures explain how to perform each of these tasks. to view access point configuration details navigate to the access points tab for your bucket. locate the access point whose configuration you want to view. you can browse for the access point in the access point list, or you can search for a specific access point using the search by name field. choose the name of the access point whose configuration you want to view. noteto view an access point's configuration, choose (click on) the name of the access point, not the option button next to the access point name. to edit an access point policy navigate to the access points tab for your bucket. select the option button next to the name of the access point whose policy you want to edit. choose edit access point policy. enter the policy in the text field. the console automatically displays the amazon resource name (arn) for the access point, which you can use in the policy. you can also choose policy generator to use the aws policy generator to help construct the policy. choose save. to use an access point to access your bucket navigate to the access points tab for your bucket. select the option button next to the name of the access point you want to use. choose use this access point. the console displays a label above the name of your bucket that shows the access point that you're currently using. while you're using the access point, you can only perform the object operations that are allowed by the access point permissions. notethe console view always shows all objects in the bucket. using an access point as described in this procedure restricts the operations you can perform on those objects, but not whether you can see that they exist in the bucket. to exit the access point view of your bucket, choose exit access point. notethe s3 management console doesn't support using virtual private cloud (vpc) access points to access bucket resources. to access bucket resources from a vpc access point, use the aws cli, aws sdks, or amazon s3 rest apis. to delete an access point navigate to the access points tab for your bucket. select the option button next to the name of the access point that you want to delete. choose delete. confirm that you want to delete your access point by entering its name in the text field that appears, and choose confirm. 
this section explains how to use the amazon s3 console to view the object overview panel. this panel provides an overview of all the essential information for an object in one place. to see the overview panel for an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, select the check box next to the name of the object for which you want an overview. to download the object, choose download in the object overview panel. to copy the path of the object to the clipboard, choose copy path. if versioning is enabled on the bucket, choose latest versions to list the versions of the object. you can then choose the download icon to download an object version, or choose the trash can icon to delete an object version. importantyou can undelete an object only if it was deleted as the latest (current) version. you can't undelete a previous version of an object that was deleted. for more information, see  and  in the amazon simple storage service developer guide. 
this section explains how to use the amazon s3 console to download objects from an s3 bucket. data transfer fees apply when you download objects. for information about amazon s3 features, and pricing, see . importantif an object key name consists of a single period (.), or two periods (..), you can’t download the object using the amazon s3 console. to download an object with a key name of “.” or “..”, you must use the aws cli, aws sdks, or rest api. for more information about naming objects, see  in the amazon simple storage service developer guide. you can download a single object per request using the amazon s3 console. to . to download an object from an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to download an object from. you can download an object from an s3 bucket in any of the following ways: in the name list, select the check box next to the object you want to download, and then choose download on the object description page that appears.choose the name of the object that you want to download. on the overview page, choose download. choose the name of the object that you want to download and then choose download as on the overview page.choose the name of the object that you want to download. choose latest version and then choose the download icon.
each object in amazon simple storage service (amazon s3) has a set of name-value pairs that provides metadata about the object. metadata is additional information about the object. some metadata is set by amazon s3 when you upload the object, for example, and . you can also set some metadata when you upload the object, or you can add it later. this section explains how to use the amazon s3 console to add metadata to an s3 object. object metadata is a set of name-value (key-value) pairs. for example, the metadata for content length, , is the name (key) and the size of the object in bytes (value). for more information about object metadata, see  in the amazon simple storage service developer guide. there are two kinds of metadata for an s3 object, amazon s3 system metadata and user-defined metadata: system metadata–there are two categories of system metadata. metadata such as the  date is controlled by the system. only amazon s3 can modify the value. there is also system metadata that you control, for example, the storage class configured for the object. user-defined metadata–you can define your own custom metadata, called user-defined metadata. you can assign user-defined metadata to an object when you upload the object or after the object has been uploaded. user-defined metadata is stored with the object and is returned when you download the object. amazon s3 does not process user-defined metadata. the following topics describe how to add metadata to an object using the amazon s3 console. noteif you change an object's metadata, a new object will be created and will replace the old one. if s3 versioning is enabled, a new version of the object is created, and the existing object becomes an older version. the role that changes the property also becomes the owner of the new object or (object version). topics you can configure some system metadata for an s3 object. for a list of system-defined metadata and whether you can modify their values, see  in the amazon simple storage service developer guide. to add system metadata to an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object that you want to add metadata to. choose properties, and then choose metadata. choose add metadata, and then choose a key from the select a key menu. depending on which key you chose, choose a value from the select a value menu or type a value. choose save. you can assign user-defined metadata to an object. user-defined metadata must begin with the prefix "", otherwise amazon s3 will not set the key value pair as you define it. you define custom metadata by adding a name that you choose to the  key. this creates a custom key. for example, if you add the custom name , the metadata key would be .  user-defined metadata can be as large as 2 kb. both keys and their values must conform to us-ascii standards. for more information, see  in the amazon simple storage service developer guide. to add user-defined metadata to an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object that you want to add metadata to. choose properties, and then choose metadata. choose add metadata, and then choose the  key from the select a key menu. any metadata starting with the prefix  is user-defined metadata. type a custom name following the  key. for example, for the custom name , the metadata key would be . enter a value for the custom key, and then choose save. 
amazon s3 inventory provides a flat file list of your objects and metadata, which is a scheduled alternative to the amazon s3 synchronous  api operation. amazon s3 inventory provides comma-separated values (csv) or  or  output files that list your objects and their corresponding metadata on a daily or weekly basis for an s3 bucket or for objects that share a prefix (objects that have names that begin with the same string). for more information, see  in the amazon simple storage service developer guide. to configure inventory noteit may take up to 48 hours to deliver the first report. sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket for which you want to configure amazon s3 inventory. choose the management tab, and then choose inventory. choose add new. type a name for the inventory and set it up as follows: optionally, add a prefix for your filter to inventory only objects whose names begin with the same string.choose the destination bucket where you want reports to be saved. the destination bucket must be in the same aws region as the bucket for which you are setting up the inventory. the destination bucket can be in a different aws account. optionally, choose a prefix for the destination bucket.choose how frequently to generate the inventory.under advanced settings, you can set the following: choose either the csv, orc, or parquet output file format for your inventory. for more information about these formats, see  in the amazon simple storage service developer guide. to include all versions of the objects in the inventory, choose include all versions in the object versions list. by default, the inventory includes only the current versions of the objects. for optional fields, select one or more of the following to add to the inventory report: size – object size in bytes.last modified date – the object creation date or the last modified date, whichever is the latest.storage class – the storage class used for storing the object. etag – the entity tag is a hash of the object. the etag reflects changes only to the contents of an object, and not its metadata. the etag may or may not be an md5 digest of the object data. whether it is depends on how the object was created and how it is encrypted.multipart upload – specifies that the object was uploaded as a multipart upload. for more information, see  in the amazon simple storage service developer guide.replication status – the replication status of the object. for more information, see .encryption status – the server-side encryption used to encrypt the object. for more information, see  in the amazon simple storage service developer guide.s3 object lock configurations – the object lock status of the object, including the following settings:  retention mode – the level of protection applied to the object, either governance or compliance.retain until date – the date until which the locked object cannot be deleted.legal hold status – the legal hold status of the locked object. for information about s3 object lock, see  in the amazon simple storage service developer guide. for more information about the contents of an inventory report, see  in the amazon simple storage service developer guide. for encryption, choose a server-side encryption option to encrypt the inventory report, or choose none: none – do not encrypt the inventory report.aes-256 – encrypt the inventory report using server-side encryption with amazon s3-managed keys (sse-s3). amazon s3 server-side encryption uses 256-bit advanced encryption standard (aes-256). for more information, see  in the amazon simple storage service developer guide. aws-kms – encrypt the report using server-side encryption with aws key management service (aws kms) customer master keys (cmks). for more information, see  in the amazon simple storage service developer guide.  noteto encrypt the inventory list file with sse-kms, you must grant amazon s3 permission to use the aws kms cmk. for instructions, see  .choose save. amazon s3 creates a bucket policy on the destination bucket that grants amazon s3 write permission. this allows amazon s3 to write data for the inventory reports to the bucket.  if an error occurs when you try to create the bucket policy, you are given instructions on how to fix it. for example, if you choose a destination bucket in another aws account and don't have permissions to read and write to the bucket policy, you see the following message.   in this case, the destination bucket owner must add the displayed bucket policy to the destination bucket. if the policy is not added to the destination bucket, you won't get an inventory report because amazon s3 doesn’t have permission to write to the destination bucket. if the source bucket is owned by a different account than that of the current user, the correct account id of the source bucket must be substituted in the policy. for more information, see  in the amazon simple storage service developer guide. to grant amazon s3 permission to encrypt using a customer managed aws key management service (aws kms) customer master key (cmk), you must use a key policy. to update your key policy so that you can use an aws kms customer managed cmk to encrypt the inventory file, follow the steps below. to grant permissions to encrypt using your aws kms cmk using the aws account that owns the customer managed cmk, sign into the aws management console. open the aws kms console at . to change the aws region, use the region selector in the upper-right corner of the page. in the left navigation pane, choose customer managed keys. under customer managed keys, choose the customer managed cmk that you want to use to encrypt the inventory file. under key policy, choose switch to policy view. to update the key policy, choose edit. under edit key policy, add the following key policy to the existing key policy. choose save changes. for more information about creating customer managed cmks aws kms and using key policies, see the following links in the aws key management service developer guide: more info  
for more detailed information about configuring a redirect in amazon s3, see  in the amazon simple storage service developer guide. you can redirect all requests for a website endpoint for a bucket to another host. if you redirect all requests, any request made to the website endpoint is redirected to the specified host name.  for example, if your root domain is , and you want to serve requests for both  and , you can create two buckets named  and . then, maintain the content in the  bucket, and configure the other  bucket to redirect all requests to the  bucket. for more information, see . to redirect requests for a bucket website endpoint open the amazon s3 console at . choose the name of the bucket that you have configured as a static website (for example, ). choose properties. choose static website hosting. choose redirect requests.  in the target bucket or domain box, enter the bucket or domain that you want to redirect to. for example, if you are redirecting to a root domain address, you would enter example.com. in the protocol box, enter the protocol for the redirected requests (http or https). if you do not specify a protocol, the protocol of the original request is used. choose save. 
in the amazon s3 console, you can move an object to a bucket or a folder.  to move an object sign in to the aws management console and open the amazon s3 console at . in the buckets list, choose your bucket name. select the check box beside the object that you want to move. choose actions, and choose move. choose the move destination: if you want to move your object to a bucket, select the bucket.if you want to move your object to a folder in a bucket, choose the bucket name, and then choose the folder.under review, confirm the move details, and choose move. amazon s3 moves your object to the destination. 
this section explains how to use the amazon simple storage service (amazon s3) console to manage access permissions for an amazon s3 object by using access control lists (acls). acls are resource-based access policies that grant access permissions to buckets and objects. for more information about managing access permissions with resource-based policies, see  in the amazon simple storage service developer guide. bucket and object permissions are independent of each other. an object does not inherit the permissions from its bucket. for example, if you create a bucket and grant write access to a user, you can't access that user’s objects unless the user explicitly grants you access. you can grant permissions to other aws accounts or predefined groups. the user or group that you grant permissions to is called the grantee. by default, the owner, which is the aws account that created the bucket, has full permissions. each permission you grant for a user or a group adds an entry in the acl that is associated with the object. the acl lists grants, which identify the grantee and the permission granted. for more information about acls, see  in the amazon simple storage service developer guide. to set permissions for an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object for which you want to set permissions. choose permissions. you can manage object access permissions for the following:  1.  access for object owner  access for other aws accounts to grant permissions to an aws user from a different aws account, under access for other aws accounts, choose add account. in the enter an id field, enter the canonical id of the aws user that you want to grant object permissions to. for information about finding a canonical id, see  in the amazon web services general reference. you can add as many as 99 users. select the check boxes for the permissions that you want to grant to the user, and then choose save. to display information about the permissions, choose the help icons. public access  warninguse caution when granting the everyone group anonymous access to your amazon s3 objects. when you grant access to this group, anyone in the world can access your object. if you need to grant access to everyone, we highly recommend that you only grant permissions to read objects.we highly recommend that you do not grant the everyone group write object permissions. doing so allows anyone to overwrite the acl permissions for the object. you can also set object permissions when you upload objects. for more information about setting permissions when uploading objects, see .  
this section describes how to use the amazon s3 console to create a folder. to create a folder sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to create a folder in. choose create folder. enter a name for the folder (for example, favorite-pics). choose the encryption setting for the folder object, and then choose save. if you don't want to use encryption with your folder, choose none.  if you want to encrypt your object using keys that are managed by amazon s3, follow these steps: choose aes-256.  for more information about using amazon s3 server-side encryption to encrypt your data, see  in the amazon simple storage service developer guide. if you want to encrypt your object using aws kms, follow these steps: choose aws-kms. choose an aws kms customer master key (cmk). the list shows  that you have created and your aws managed cmk for amazon s3. for more information about creating a customer managed aws kms cmk, see  in the aws key management service developer guide.  importantto encrypt objects in the bucket, you can use only the cmks that are enabled in the same aws region as the bucket. amazon s3 only supports symmetric cmks and does not support asymmetric cmks. for more information, see .the amazon s3 console only lists 100 kms cmks per region. if you have more than 100 cmks in the same region, you can see only the first 100 cmks in the s3 console. to use anaws kms kms cmk that is not listed in the console, choose custom kms arn, and enter the kms cmk arn. choose save. 
to upload your data (photos, videos, documents etc.) to amazon s3, you must first create an s3 bucket in one of the aws regions. you can then upload an unlimited number of data objects to the bucket. the data that you store in amazon s3 consists of objects. every object resides within a bucket that you create in a specific aws region. every object that you store in amazon s3 resides in a bucket.  objects stored in a region never leave the region unless you explicitly transfer them to another region. for example, objects stored in the europe (ireland) region never leave it. the objects stored in an aws region physically remain in that region. amazon s3 does not keep copies of objects or move them to any other region. however, you can access the objects from anywhere, as long as you have necessary permissions to do so. before you can upload an object into amazon s3, you must have write permissions to a bucket. objects can be any file type: images, backups, data, movies, etc. you can have an unlimited number of objects in a bucket. the maximum size of file you can upload by using the amazon s3 console is 160 gb. to upload a file larger than 160 gb, use the aws cli, aws sdk, or amazon s3 rest api. for more information, see  in the amazon simple storage service developer guide. the following topics explain how to use the amazon s3 console to upload, delete, and manage objects. noteif you rename an object, or change any of the properties; storage class,  enryption, metadata, a new object is created to replace the old one. if s3 versioning is enabled, a new version of the object is created, and the existing object becomes an older version. the role that changes the property also becomes the owner of the new object or (object version). topics 
this section explains how to use the console to view the properties of an object. to view the properties of an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object you want to view the properties for. choose properties. on the properties page, you can configure the following properties for the object. noteif you change the properties; storage class,  enryption, metadata, a new object is created to replace the old one. if s3 versioning is enabled, a new version of the object is created, and the existing object becomes an older version. the role that changes the property also becomes the owner of the new object or (object version). storage class – each object in amazon s3 has a storage class associated with it. the storage class that you choose to use depends on how frequently you access the object. the default storage class for s3 objects is standard. you choose which storage class to use when you upload an object. for more information about storage classes, see  in the amazon simple storage service developer guide. to change the storage class after you upload an object, choose storage class. choose the storage class that you want, and then choose save. encryption – you can encrypt your s3 objects. for more information, see .  metadata – each object in amazon s3 has a set of name-value pairs that represents its metadata. for information on adding metadata to an s3 object, see . tags – you can add tags to an s3 object. for more information, see . 
amazon s3 block public access prevents the application of any settings that allow public access to data within s3 buckets. this section describes how to edit block public access settings for all the s3 buckets in your aws account. for information about blocking public using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. to edit block public access settings for all the s3 buckets in an aws account sign in to the aws management console and open the amazon s3 console at . choose block public access (account settings). choose edit to change the block public access settings for all the buckets in your aws account. choose the settings that you want to change, and then choose save. when you're asked for confirmation, enter confirm. then choose confirm to save your changes. 
 you can use amazon s3 access points to manage access to your s3 objects. amazon s3 access points are named network endpoints that are attached to buckets that you can use to perform s3 object operations, such as uploading and retrieving objects. a bucket can have up to 1,000 access points attached, and each access point enforces distinct permissions and network controls to give you fine-grained control over access to your s3 objects.  for more information about amazon s3 access points, see  in the amazon simple storage service developer guide. the following topics explain how to use the s3 management console to create, manage, and use amazon s3 access points. topics 
you can host a static website on amazon s3. on a static website, individual webpages include static content. a static website might also contain client-side scripts. by contrast, a dynamic website relies on server-side processing, including server-side scripts such as php, jsp, or asp.net. amazon s3 does not support server-side scripting. you can use the following quick procedures to configure an s3 bucket for static website hosting in the amazon s3 console. for more information and detailed walkthroughs, see  in the amazon simple storage service developer guide. topics sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to enable static website hosting for. choose properties. choose static website hosting. choose use this bucket to host a website. in index document, enter the name of the index document, typically .  when you configure a bucket for website hosting, you must specify an index document. amazon s3 returns this index document when requests are made to the root domain or any of the subfolders. for more information, see  in the amazon simple storage service developer guide. (optional) if you want to provide your own custom error document for 4xx class errors, in error document, enter the custom error document filename.  if you do not specify a custom error document and an error occurs, amazon s3 returns a default html error document. for more information, see  in the amazon simple storage service developer guide. (optional) if you want to specify advanced redirection rules, in the edit redirection rules box, enter xml to describe the rules. for example, you can conditionally route requests according to specific object key names or prefixes in the request. for more information, see  in the amazon simple storage service developer guide. choose save. upload the index document to your bucket. for step-by-step instructions on uploading an object to an s3 bucket, see .  upload other files for your website, including optional custom error documents. in the next section, you set the permissions required to access your bucket as a static website.. by default, amazon s3 blocks public access to your account and buckets. if you want to use a bucket to host a static website, you can use these steps to edit your block public access settings.  warningbefore you complete this step, review  to ensure that you understand and accept the risks involved with allowing public access. when you turn off block public access settings to make your bucket public, anyone on the internet can access your bucket. we recommend that you block all public access to your buckets. open the amazon s3 console at . choose the name of the bucket that you have configured as a static website. choose permissions. choose edit. clear block all public access, and choose save. warningbefore you complete this step, review  to ensure you understand and accept the risks involved with allowing public access. when you turn off block public access settings to make your bucket public, anyone on the internet can access your bucket. we recommend that you block all public access to your buckets. in the confirmation box, enter confirm, and then choose confirm. under s3 buckets, the access for your bucket updates to objects can be public. you can now add a bucket policy to make the objects in the bucket publicly readable. if the access still displays as bucket and objects not public, you might have to  for your account before adding a bucket policy. after you edit s3 block public access settings, you can add a bucket policy to grant public read access to your bucket. when you grant public read access, anyone on the internet can access your bucket. importantthe following policy is an example only and allows full access to the contents of your bucket. before you proceed with this step, review  to ensure that you understand the best practices for securing the files in your s3 bucket and risks involved in granting public access. under buckets, choose the name of your bucket. choose permissions. choose bucket policy. to grant public read access for your website, copy the following bucket policy, and paste it in the bucket policy editor. update the  to include your bucket name. in the preceding example bucket policy, example.com is the bucket name. to use this bucket policy with your own bucket, you must update this name to match your bucket name. choose save. a warning appears indicating that the bucket has public access. in bucket policy, a public label appears. if you see an error that says , confirm that the bucket name in the bucket policy matches your bucket name. for information about adding a bucket policy, see  if you get an error - access denied warning and the bucket policy editor does not allow you to save the bucket policy, check your account-level and bucket-level block public access settings to confirm that you allow public access to the bucket. once you configure your bucket as a static website and set permissions, you can access your website through an amazon s3 website endpoint. for more information, see  in the amazon simple storage service developer guide. for a complete list of amazon s3 website endpoints, see  in the amazon web services general reference. 
this section describes how to configure advanced s3 bucket property settings for object replication, event notification, and transfer acceleration. topics 
there are three types of amazon cloudwatch metrics for amazon s3: storage metrics are reported once per day and are provided to all customers at no additional cost.replication metrics are available 15 minutes after enabling a replication rule with s3 replication time control (s3 rtc). for more information, see request metrics are available at 1-minute intervals after some latency to process, and the metrics are billed at the standard cloudwatch rate. for more information about cloudwatch metrics for amazon s3, see  in the amazon simple storage service developer guide. to get request metrics, you must opt into them by configuring them on the aws management console or using the amazon s3 api. to configure request metrics on a bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the objects you want request metrics for. choose the management tab, and then choose metrics. choose requests. in the left pane, choose the edit icon next to the name of the bucket. select the request metrics check box. this also enables data transfer metrics. choose save. you have now created a metrics configuration for all the objects in an amazon s3 bucket. about 15 minutes after cloudwatch begins tracking these request metrics, you can see graphs for the metrics on the amazon s3 or cloudwatch console.  you can also define a filter so that the metrics are only collected and reported on a subset of objects in the bucket. for more information, see  
to upload your data (photos, videos, documents etc.) to amazon s3, you must first create an s3 bucket in one of the aws regions. you can then upload your data objects to the bucket. every object you store in amazon s3 resides in a bucket. you can use buckets to group related objects in the same way that you use a directory to group files in a file system.  amazon s3 creates buckets in the aws region that you specify. you can choose any aws region that is geographically close to you to optimize latency, minimize costs, or address regulatory requirements. for example, if you reside in europe, you might find it advantageous to create buckets in the europe (ireland) or europe (frankfurt) regions. for a list of amazon s3 aws regions, see  in the amazon web services general reference. you are not charged for creating a bucket. you are only charged for storing objects in the bucket and for transferring objects out of the bucket. for more information about pricing, see . amazon s3 bucket names are globally unique, regardless of the aws region in which you create the bucket. you specify the name at the time you create the bucket. for bucket naming guidelines, see  in the amazon simple storage service developer guide. the following topics explain how to use the amazon s3 console to create, delete, and manage buckets. topics 
amazon s3 provides a set of tools to help you manage your s3 batch operations jobs after you create them. for more information about managing s3 batch operations, see  in the amazon simple storage service developer guide.    in the amazon simple storage service developer guide in the amazon simple storage service developer guide in the amazon simple storage service developer guide
you can change the display language of the aws management console. several languages are supported. to change the console language sign in to the aws management console and open the amazon s3 console at . scroll down until you see the bar at the bottom of the window, and then choose the language on the left side of the bar. choose the language that you want from the menu. this will change the langauge for the entire aws management console. 
amazon simple storage service (amazon s3) transfer acceleration enables fast, easy, and secure file transfers between your client and an s3 bucket over long distances. this topic describes how to enable amazon s3 transfer acceleration for a bucket. for more information, see  in the amazon simple storage service developer guide. to enable transfer acceleration for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the buckets list, choose the name of the bucket that you want to enable transfer acceleration for. choose properties. under advanced settings, choose transfer acceleration. (optional) if you want to compare accelerated and non-accelerated upload speeds, to open the , choose the want to compare your data transfer speed by region?. the speed comparison tool uses multipart upload to transfer a file from your browser to various aws regions with and without amazon s3 transfer acceleration. you can compare the upload speed for direct uploads and transfer accelerated uploads by region.  choose enabled, and choose save. endpoint displays the transfer acceleration endpoint for your bucket. you use this endpoint to access accelerated data transfers to and from your bucket. if you suspend transfer acceleration, the accelerate endpoint no longer works. 
this section describes how to enable an aws cloudtrail trail to log data events for objects in an s3 bucket by using the amazon s3 console. cloudtrail supports logging amazon s3 object-level api operations such as , , and . these events are called data events. by default, cloudtrail trails don't log data events, but you can configure trails to log data events for s3 buckets that you specify, or to log data events for all the amazon s3 buckets in your aws account. for more information, see . cloudtrail does not populate data events in the cloudtrail event history. additionally, not all bucket-level actions are populated in the cloudtrail event history. for more information, see  to configure a trail to log data events for an s3 bucket, you can use either the aws cloudtrail console or the amazon s3 console. if you are configuring a trail to log data events for all the amazon s3 buckets in your aws account, it's easier to use the cloudtrail console. for information about using the cloudtrail console to configure a trail to log s3 data events, see  in the aws cloudtrail user guide.  importantadditional charges apply for data events. for more information, see .  the following procedure shows how to use the amazon s3 console to enable a cloudtrail trail to log data events for an s3 bucket. to enable cloudtrail data events logging for objects in an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket. choose properties. choose object-level logging. choose an existing cloudtrail trail in the drop-down menu.  the trail you select must be in the same aws region as your bucket, so the drop-down list contains only trails that are in the same region as the bucket or trails that were created for all regions.  if you need to create a trail, choose the cloudtrail console link to go to the cloudtrail console. for information about how to create trails in the cloudtrail console, see  in the aws cloudtrail user guide. under events, choose one of the following: read to specify that you want cloudtrail to log amazon s3 read apis such as . write to log amazon s3 write apis such as . read and write to log both read and write object apis.for a list of supported data events that cloudtrail logs for amazon s3 objects, see  in the amazon simple storage service developer guide. choose create to enable object-level logging for the bucket. to disable object-level logging for the bucket, you must go to the cloudtrail console and remove the bucket name from the trail's data events. noteif you use the cloudtrail console or the amazon s3 console to configure a trail to log data events for an s3 bucket, the amazon s3 console shows that object-level logging is enabled for the bucket.  for information about enabling object-level logging when you create an s3 bucket, see .  in the amazon simple storage service developer guide  in the aws cloudtrail user guide
this topic explains how to view the properties for an s3 bucket. to view the properties for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to view the properties for. choose properties. on the properties page, you can configure the following properties for the bucket. versioning – versioning enables you to keep multiple versions of an object in one bucket. by default, versioning is disabled for a new bucket. for information about enabling versioning, see . server access logging – server access logging provides detailed records for the requests that are made to your bucket. by default, amazon s3 does not collect server access logs. for information about enabling server access logging, see . static website hosting – you can host a static website on amazon s3. to enable static website hosting, choose static website hosting and then specify the settings you want to use. for more information, see . object-level logging – object-level logging records object-level api activity by using cloudtrail data events. for information about enabling object-level logging, see . tags – with aws cost allocation, you can use bucket tags to annotate billing for your use of a bucket. a tag is a key-value pair that represents a label that you assign to a bucket. to add tags, choose tags, and then choose add tag. for more information, see  in the amazon simple storage service developer guide.  transfer acceleration – amazon s3 transfer acceleration enables fast, easy, and secure transfers of files over long distances between your client and an s3 bucket. for information about enabling transfer acceleration, see . events – you can enable certain amazon s3 bucket events to send a notification message to a destination whenever the events occur. to enable events, choose events and then specify the settings you want to use. for more information, see .  requester pays – you can enable requester pays so that the requester (instead of the bucket owner) pays for requests and data transfers. for more information, see  in the amazon simple storage service developer guide.  
in the amazon s3 console, you can copy an object to a bucket or a folder. for more information, see  in the amazon simple storage service developer guide. to copy an object sign in to the aws management console and open the amazon s3 console at . in the buckets list, choose your bucket name. on the overview tab, select the check box beside the object that you want to copy. choose actions, and choose copy. choose the copy destination: if you want to copy your object to a bucket, select the bucket.if you want to copy your object to a folder in a bucket, choose the bucket name, and then choose the folder.under review, confirm the copy details, and choose copy. amazon s3 copies your object to the destination. 
amazon s3 block public access prevents the application of any settings that allow public access to data within s3 buckets. this section describes how to edit block public access settings for one or more s3 buckets. for information about blocking public access using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. topics follow these steps if you need to change the public access settings for a single s3 bucket. to edit the amazon s3 block public access settings for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose permissions. choose edit to change the public access settings for the bucket. for more information about the four amazon s3 block public access settings, see  in the amazon simple storage service developer guide. choose the setting that you want to change, and then choose save. when you're asked for confirmation, enter confirm. then choose confirm to save your changes. follow these steps if you need to change the public access settings for more than one s3 bucket. to edit the amazon s3 block public access settings for multiple s3 buckets sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the buckets that you want, and then choose edit public access settings. choose the setting that you want to change, and then choose save. when you're asked for confirmation, enter confirm. then choose confirm to save your changes. you can change amazon s3 block public access settings when you create a bucket. for more information, see .  
s3 batch operations performs large-scale batch operations on amazon s3 objects. you can use s3 batch operations to copy objects, set object tags or access control lists (acls), initiate object restores from amazon s3 glacier, or invoke an aws lambda function to perform custom actions using your objects. you can perform these operations on a custom list of objects, or you can use an amazon s3 inventory report to make generating even the largest lists of objects easy. s3 batch operations use the same amazon s3 apis that you already use, so you'll find the interface familiar. for information about performing s3 batch operations using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide.  the following topics explain how to use the amazon s3 console to configure and run batch operations. topics 
this section explains how to configure amazon s3 storage management tools. topics 
before you can enable event notifications for your bucket you must set up one of the following destination types: an amazon sns topicamazon simple notification service (amazon sns) is a web service that coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. you can use the amazon sns console to create an amazon sns topic that your notifications can be sent to. the amazon sns topic must be in the same region as your amazon s3 bucket. for information about creating an amazon sns topic, see  in the amazon simple notification service developer guide.before you can use the amazon sns topic that you create as an event notification destination, you need the following:   the amazon resource name (arn) for the amazon sns topica valid amazon sns topic subscription (the topic subscribers are notified when a message is published to your amazon sns topic)a permissions policy that you set up in the amazon sns console (as shown in the following example) an amazon sqs queueyou can use the amazon sqs console to create an amazon sqs queue that your notifications can be sent to. the amazon sqs queue must be in the same region as your amazon s3 bucket. for information about creating an amazon sqs queue, see  in the amazon simple queue service developer guide.before you can use the amazon sqs queue as an event notification destination, you need the following:   the amazon resource name (arn) for the amazon sqs topica permissions policy that you set up in the amazon sqs console (as shown in the following example) a lambda functionyou can use the aws lambda console to create a lambda function. the lambda function must be in the same region as your s3 bucket. for information about creating a lambda function, see the .before you can use the lambda function as an event notification destination, you must have the name or the arn of a lambda function to set up the lambda function as a event notification destination.  warningif your notification ends up writing to the bucket that triggers the notification, this could cause an execution loop. for example, if the bucket triggers a lambda function each time an object is uploaded, and the function uploads an object to the bucket, then the function indirectly triggers itself. to avoid this, use two buckets, or configure the trigger to only apply to a prefix used for incoming objects.for more information and an example of using amazon s3 notifications with aws lambda, see  in the aws lambda developer guide.  
replication is the automatic, asynchronous copying of objects across buckets in the same or different aws regions. it replicates newly created objects and object updates from a source bucket to a specified destination bucket.  you use the amazon s3 console to add replication rules to the source bucket. replication rules define the source bucket objects to replicate and the destination bucket where the replicated objects are stored. for more information about replication, see  in the amazon simple storage service developer guide. you can manage replication rules on the replication page. you can add, view, enable, disable, delete, and change the priority of the replication rules. for information about adding replication rules to a bucket, see . to manage the replication rules for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose management, and then choose replication. you change the replication rules in the following ways. to change settings that affect all the replication rules in the bucket, choose edit global settings. you can change the destination bucket, and the iam role. if needed, you can copy the required bucket policy for cross-account destination buckets. to change a replication rule, select the rule and choose edit, which starts the replication wizard to help you make the change. for information about using the wizard, see .to enable or disable a replication rule, select the rule, choose more, and in the drop-down list, choose enable rule or disable rule. you can also disable, enable, or delete all the rules in the bucket from the more drop-down list.to change the rule priorities, choose edit priorities. you can then change the priority for each rule under the priority column heading. choose save to save your changes. you set rule priorities to avoid conflicts caused by objects that are included in the scope of more than one rule. in the case of overlapping rules, amazon s3 uses the rule priority to determine which rule to apply. the higher the number, the higher the priority. for more information about rule priority, see  in the amazon simple storage service developer guide.  in the amazon simple storage service developer guide
for the latest aws terminology, see the  in the aws general reference. 
this section explains how to use the amazon simple storage service (amazon s3) console to add a new bucket policy or edit an existing bucket policy. a bucket policy is a resource-based aws identity and access management (iam) policy. you add a bucket policy to a bucket to grant other aws accounts or iam users access permissions for the bucket and the objects in it. object permissions apply only to the objects that the bucket owner creates. for more information about bucket policies, see  in the amazon simple storage service developer guide.  for examples of amazon s3 bucket policies, see  in the amazon simple storage service developer guide.  to create or edit a bucket policy sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to create a bucket policy for or whose bucket policy you want to edit. choose permissions, and then choose bucket policy. in the bucket policy editor text box, type or copy and paste a new bucket policy, or edit an existing policy. the bucket policy is a json file. the text you type in the editor must be valid json. choose save. noteamazon s3 displays the amazon resource name (arn) for the bucket next to the bucket policy editor title. for more information about arns, see  in the amazon web services general reference.directly below the bucket policy editor text box is a link to the policy generator, which you can use to create a bucket policy. 
before you can upload data to amazon s3, you must create a bucket in one of the aws regions to store your data. after you create a bucket, you can upload an unlimited number of data objects to the bucket.  the aws account that creates the bucket owns it. by default, you can create up to 100 buckets in each of your aws accounts. if you need additional buckets, you can increase your account bucket quota to a maximum of 1,000 buckets by submitting a service quota increase. for information about how to increase your bucket quota, see  in the aws general reference.  buckets have configuration properties, including geographical region, access settings for the objects in the bucket, and other metadata.  to create a bucket sign in to the aws management console and open the amazon s3 console at . choose create bucket. the create bucket wizard opens. in bucket name, enter a dns-compliant name for your bucket. the bucket name must: be unique across all of amazon s3.be between 3 and 63 characters long.not contain uppercase characters.start with a lowercase letter or number.after you create the bucket, you can't change its name. for information about naming buckets, see  in the amazon simple storage service developer guide. importantavoid including sensitive information, such as account numbers, in the bucket name. the bucket name is visible in the urls that point to the objects in the bucket. in region, choose the aws region where you want the bucket to reside.  choose a region close to you to minimize latency and costs and address regulatory requirements. objects stored in a region never leave that region unless you explicitly transfer them to another region. for a list of amazon s3 aws regions, see  in the amazon web services general reference. in bucket settings for block public access, choose the block public access settings that you want to apply to the bucket.  we recommend that you leave all settings enabled unless you know you need to turn one or more of them off for your use case, such as to host a public website. block public access settings that you enable for the bucket will also be enabled for all access points that you create on the bucket. for more information about blocking public access, see  in the amazon simple storage service developer guide. (optional) if you want to enable s3 object lock: choose advanced settings, and read the message that appears. importantyou can only enable s3 object lock for a bucket when you create it. if you enable object lock for the bucket, you can't disable it later. enabling object lock also enables versioning for the bucket. after you enable object lock for the bucket, you must configure the object lock settings before any objects in the bucket will be protected. for more information about configuring protection for objects, see . if you want to enable object lock, enter enable in the text box and choose confirm. for more information about the s3 object lock feature, see  in the amazon simple storage service developer guide. choose create bucket. 
this section explains how to use the amazon s3 console to delete folders from an s3 bucket.  for information about amazon s3 features and pricing, see . to delete folders from an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to delete folders from. in the name list, select the check box next to the folders and objects that you want to delete, choose more, and then choose delete. in the delete objects dialog box, verify that the names of the folders you selected for deletion are listed and then choose delete. 
latest documentation update: march 27, 2019 the following table describes the important changes in each release of the amazon simple storage service console user guide from june 19, 2018, onward. for notification about updates to this documentation, you can subscribe to an rss feed. the following table describes the important changes in each release of the amazon simple storage service console user guide before june 19, 2018.  
amazon s3 block public access prevents the application of any settings that allow public access to data within s3 buckets. you can configure block public access settings for an individual s3 bucket or for all the buckets in your account. for information about blocking public access using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. the following topics explain how to use the amazon s3 console to configure block public access settings:  the following sections explain viewing bucket access status and searching by access types. the list buckets view shows whether your bucket is publicly accessible. amazon s3 labels the permissions for a bucket as follows: public – everyone has access to one or more of the following: list objects, write objects, read and write permissions. objects can be public – the bucket is not public, but anyone with the appropriate permissions can grant public access to objects. buckets and objects not public – the bucket and objects do not have any public access.only authorized users of this account – access is isolated to iam users and roles in this account and aws service principals because there is a policy that grants public access.the access column shows the access status of the listed buckets.  you can also filter bucket searches by access type. choose an access type from the drop-down list that is next to the search for buckets bar.   
this section explains how to create an amazon s3 access point using the aws management console. for information about creating access points using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. an access point is associated with exactly one amazon s3 bucket. before you begin, make sure that you have created a bucket that you want to use with this access point. for more information about creating buckets, see . to create an access point sign in to the aws management console and open the amazon s3 console at . in the s3 buckets section, select the bucket that you want to attach this access point to. on the bucket detail page, choose the access points tab. choose create access point. enter your desired name for the access point in the access point name field. choose a network access type. if you choose virtual private cloud (vpc), enter the vpc id that you want to use with the access point. for more information about network access type for access points, see  in the amazon simple storage service developer guide. select the block public access settings that you want to apply to the access point. all block public access settings are enabled by default for new access points, and we recommend that you leave all settings enabled unless you know you have a specific need to disable any of them. amazon s3 currently doesn't support changing an access point's block public access settings after the access point has been created. for more information about using amazon s3 block public access with access points, see  in the amazon simple storage service developer guide. (optional) specify the access point policy. the console automatically displays the amazon resource name (arn) for the access point, which you can use in the policy. choose create access point. 
this section explains how to use the amazon s3 console to delete objects. because all objects in your s3 bucket incur storage costs, you should delete objects that you no longer need. if you are collecting log files, for example, it's a good idea to delete them when they're no longer needed. you can set up a lifecycle rule to automatically delete objects such as log files. for information about amazon s3 features and pricing, see . to delete objects from an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to delete an object from. you can delete objects from an s3 bucket in any of the following ways: in the name list, select the check box next to the objects and folders that you want to delete, choose actions, and then choose delete from the drop-down menu. in the delete objects dialog box, verify that the name(s) of the object(s) and/or folder(s) you selected for deletion are listed and then choose delete. or, choose the name of the object that you want to delete, choose latest version, and then choose the trash can icon.
by using the amazon s3 analytics storage class analysis tool, you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. storage class analysis observes data access patterns to help you determine when to transition less frequently accessed standard storage to the standard_ia (ia, for infrequent access) storage class. for more information about standard_ia, see the  and  in the amazon simple storage service developer guide. importantstorage class analysis does not give recommendations for transitions to the onezone_ia or s3 glacier storage classes. for more information about analytics, see  in the amazon simple storage service developer guide. to configure storage class analysis sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket for which you want to configure storage class analysis. choose the management tab, and then choose analytics. choose add. type a name for the filter. if you want to analyze the whole bucket, leave the prefix / tags field empty. in the prefix / tags  field, type text for the prefix or tag for the objects that you want to analyze, or choose from the dropdown list that appears when you start typing. if you chose tag, enter a value for the tag. you can enter one prefix and multiple tags. optionally, you can choose export data to export analysis reports to a comma-separated values (.csv) flat file. choose a destination bucket where the file can be stored. you can type a prefix for the destination bucket. the destination bucket must be in the same aws region as the bucket for which you are setting up the analysis. the destination bucket can be in a different aws account. choose save. amazon s3 creates a bucket policy on the destination bucket that grants amazon s3 write permission. this allow it to write the export data to the bucket.  if an error occurs when you try to create the bucket policy, you'll be given instructions on how to fix it. for example, if you chose a destination bucket in another aws account and do not have permissions to read and write to the bucket policy, you'll see the following message. you must have the destination bucket owner add the displayed bucket policy to the destination bucket. if the policy is not added to the destination bucket you won’t get the export data because amazon s3 doesn’t have permission to write to the destination bucket. if the source bucket is owned by a different account than that of the current user, then the correct account id of the source bucket must be substituted in the policy.  for information about the exported data and how the filter works, see  in the amazon simple storage service developer guide. more info  
there are three types of amazon cloudwatch metrics for amazon s3: storage metrics, request metrics, and replication metrics. replication metrics are available 15 minutes after a replication rule with s3 replication time control (s3 rtc) has been enabled. replication metrics are billed at the standard amazon cloudwatch rate. they are turned on automatically when you enable replication with s3 rtc using the aws management console or the amazon s3 api. replication metrics track the rule ids of the replication configuration. a replication rule id can be specific to a prefix, a tag, or a combination of both. for more information about s3 replication time control (s3 rtc), see  in the amazon simple storage service developer guide. for more information about cloudwatch metrics for amazon s3, see  in the amazon simple storage service developer guide. to view replication metrics sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the objects you want replication metrics for. choose the management tab, and then choose metrics. choose replication.   in the rule ids list in the left pane, select the rule ids that you want. if you have several rule ids to choose from, you can search for the ids that you want. after choosing the rule ids that you want, choose display graphs below the rule ids selection box. you can then view the replication metrics replication latency (in seconds), operations pending replication, and bytes pending replication for the rules that you selected. amazon cloudwatch begins reporting replication metrics 15 minutes after you enable s3 rtc on the respective replication rule. you can view replication metrics on the amazon s3 or cloudwatch console. for information, see  in the amazon simple storage service developer guide. 
amazon s3 has a flat structure instead of a hierarchy like you would typically see in a file system. however, for the sake of organizational simplicity, the amazon s3 console supports a folder concept as a way to group objects. in amazon s3, the folder is a naming prefix for an object or group of objects. for more information, see  we recommend blocking all public access to your amazon s3 folders and buckets unless you specifically require a public folder or bucket. when you make a folder public, anyone on the internet can view all the objects that are grouped in that folder. in the amazon s3 console, you can make a folder public. you can also make a folder public by creating a bucket policy that limits access by prefix. for more information, see .  warningafter you make a folder public in the amazon s3 console, you can't make it private again. instead, you must set permissions on each individual object in the public folder so that the objects have no public access. for more information, see  
access analyzer for s3 alerts you to s3 buckets that are configured to allow access to anyone on the internet or other aws accounts, including aws accounts outside of your organization. for each public or shared bucket, you receive findings into the source and level of public or shared access. for example, access analyzer for s3 might show that a bucket has read or write access provided through a bucket access control list (acl), a bucket policy, or an access point policy. armed with this knowledge, you can take immediate and precise corrective action to restore your bucket access to what you intended.  when reviewing an at-risk bucket in access analyzer for s3, you can block all public access to the bucket with a single click. we recommend that you block all access to your buckets unless you require public access to support a specific use case. before you block all public access, ensure that your applications will continue to work correctly without public access. for more information, see  in the amazon simple storage service developer guide. you can also drill down into bucket-level permission settings to configure granular levels of access. for specific and verified use cases that require public access, such as static website hosting, public downloads, or cross-account sharing, you can acknowledge and record your intent for the bucket to remain public or shared by archiving the findings for the bucket. you can revisit and modify these bucket configurations at any time. you can also download your findings as a csv report for auditing purposes. access analyzer for s3 is available at no extra cost on the amazon s3 console. access analyzer for s3 is powered by aws identity and access management (iam) access analyzer. to use access analyzer for s3 in the amazon s3 console, you must visit the iam console and enable iam access analyzer on a per-region basis.  for more information about iam access analyzer, see  in the iam user guide. for more information about access analyzer for s3, review the following sections. importantaccess analyzer for s3 requires an account-level analyzer. to use access analyzer for s3, you must visit iam access analyzer and create an analyzer that has an account as the zone of trust. for more information, see  in iam user guide. when a bucket policy or bucket acl is added or modified, access analyzer generates and updates findings based on the change within 30 minutes. findings related to account level block public access settings may not be generated or updated for up to 6 hours after you change the settings. topics access analyzer for s3 provides findings for buckets that can be accessed outside your aws account. buckets that are listed under buckets with public access can be accessed by anyone on the internet. if access analyzer for s3 identifies public buckets, you also see a warning at the top of the page that shows you the number of public buckets in your region. buckets listed under buckets with access from other aws accounts — including third-party aws accounts are shared conditionally with other aws accounts, including accounts outside of your organization.  for each bucket, access analyzer for s3 provides the following information: bucket namediscovered by access analyzer ‐ when access analyzer for s3 discovered the public or shared bucket access.shared through ‐ how the bucket is shared—through a bucket policy, a bucket acl, or an access point policy. a bucket can be shared through both policies and acls. if you want to find and review the source for your bucket access, you can use the information in this column as a starting point for taking immediate and precise corrective action. status ‐ the status of the bucket finding. access analyzer for s3 displays findings for all public and shared buckets. active ‐ finding has not been reviewed. archived ‐ finding has been reviewed and confirmed as intended. all ‐ all findings for buckets that are public or shared with other aws accounts, including aws accounts outside of your organization.access level ‐ access permissions granted for the bucket:list ‐ list resources.read ‐ read but not edit resource contents and attributes.write ‐ create, delete, or modify resources.permissions ‐ grant or modify resource permissions.tagging ‐ update tags associated with the resource.to use access analyzer for s3, you must complete the following prerequisite steps. grant the required permissions. for more information, see  in the iam user guide. visit iam to create an account-level analyzer for each region where you want to use access analyzer. access analyzer for s3 requires an account-level analyzer. to use access analyzer for s3, you must create an analyzer that has an account as the zone of trust. for more information, see  in iam user guide. if you want to block all access to a bucket in a single click, you can use the block all public access button in access analyzer for s3. when you block all public access to a bucket, no public access is granted. we recommend that you block all public access to your buckets unless you require public access to support a specific and verified use case. before you block all public access, ensure that your applications will continue to work correctly without public access. if you don't want to block all public access to your bucket, you can edit your block public access settings on the amazon s3 console to configure granular levels of access to your buckets. for more information, see  in the amazon simple storage service developer guide. in rare events, access analyzer for s3 might report no findings for a bucket that an amazon s3 block public access evaluation reports as public. this happens because amazon s3 block public access reviews policies for current actions and any potential actions that might be added in the future, leading to a bucket becoming public. on the other hand, access analyzer for s3 only analyzes the current actions specified for the amazon s3 service in the evaluation of access status. to block all public access to a bucket using access analyzer for s3 sign in to the aws management console and open the amazon s3 console at . in the navigation pane on the left, under dashboards, choose access analyzer for s3. in access analyzer for s3, choose a bucket. choose block all public access. to confirm your intent to block all public access to the bucket, in block all public access (bucket settings), enter confirm. amazon s3 blocks all public access to your bucket. the status of the bucket finding updates to resolved, and the bucket disappears from the access analyzer for s3 listing. if you want to review resolved buckets, open iam access analyzer on the iam console. if you did not intend to grant access to the public or other aws accounts, including accounts outside of your organization, you can modify the bucket acl, bucket policy, or access point policy to remove the access to the bucket. the shared through column shows all sources of bucket access: bucket policy, bucket acl, and/or access point policy. to review and change a bucket policy, a bucket acl, or an access point policy open the amazon s3 console at . in the navigation pane, choose access analyzer for s3. to see whether public access or shared access is granted through a bucket policy, a bucket acl, or an access point policy, look in the shared through column. under bucket name, choose the name of the bucket with the bucket policy, bucket acl, or access point policy that you want to change or review. if you want to change or view a bucket acl: choose permissions. choose access control list. review your bucket acl, and make changes as required. for more information, see  if you want to change or review a bucket policy: choose permissions. choose bucket policy. review or change your bucket policy as required. for more information, see  if you want to review or change an access point policy: choose access points. choose the access point name. review or change access as required.  for more information, see . if you edit or remove a bucket acl, a bucket policy, or an access point policy to remove public or shared access, the status for the bucket findings updates to resolved. the resolved bucket findings disappear from the access analyzer for s3 listing, but you can view them in iam access analyzer. if a bucket grants access to the public or other aws accounts, including accounts outside of your organization, to support a specific use case (for example, a static website, public downloads, or cross-account sharing), you can archive the finding for the bucket. when you archive bucket findings, you acknowledge and record your intent for the bucket to remain public or shared. archived bucket findings remain in your access analyzer for s3 listing so that you always know which buckets are public or shared. to archive bucket findings in access analyzer for s3 open the amazon s3 console at . in the navigation pane, choose access analyzer for s3. in access analyzer for s3, choose an active bucket. to acknowledge your intent for this bucket to be accessed by the public or other aws accounts, including accounts outside of your organization, choose archive. enter confirm, and choose archive. after you archive findings, you can always revisit them and change their status back to active, indicating that the bucket requires another review.  to activate an archived bucket finding in access analyzer for s3 open the amazon s3 console at . in the navigation pane, choose access analyzer for s3. choose the archived bucket findings. choose mark as active. if you need to see more information about a bucket, you can open the bucket finding details in iam access analyzer on the iam console. to view finding details in access analyzer for s3 open the amazon s3 console at . in the navigation pane, choose access analyzer for s3. in access analyzer for s3, choose a bucket. choose view details. the finding details open in iam access analyzer on the iam console.  you can download your bucket findings as a csv report that you can use for auditing purposes. the report includes the same information that you see in access analyzer for s3 on the amazon s3 console. to download a report open the amazon s3 console at . in the navigation pane on the left, choose access analyzer for s3. in the region filter, choose the region. access analyzer for s3 updates to shows buckets for the chosen region. choose download report. a csv report is generated and saved to your computer. 
there are three types of amazon cloudwatch metrics for amazon s3: storage metrics, request metrics and replication. storage metrics are reported once per day and are provided to all customers at no additional cost. request metrics are available at 1 minute intervals after some latency to process, and metrics are billed at the standard cloudwatch rate. to get request metrics, you must opt into them by configuring them in the console or with the amazon s3 api. for more conceptual information about cloudwatch metrics for amazon s3, see  in the amazon simple storage service developer guide. to filter request metrics on a subset of objects in a bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that has the objects you want to get request metrics for. choose the management tab. and then choose metrics. choose requests. from filters in the left-side pane, choose add. provide a name for this metrics configuration. provide one or more prefixes or tags, separated by commas, in prefix /tags that you want to monitor. from the drop down, select whether the value you provided is a tag or a prefix. choose save. you have now created a metrics configuration for request metrics on a subset of the objects in an amazon s3 bucket. about 15 minutes after cloudwatch begins tracking these request metrics, you can see graphs for the metrics in both the amazon s3 or cloudwatch consoles. you can also request metrics at the bucket level. for information, see  
in amazon s3, buckets and objects are the primary resources, and objects are stored in buckets. amazon s3 has a flat structure instead of a hierarchy like you would see in a file system. however, for the sake of organizational simplicity, the amazon s3 console supports the folder concept as a means of grouping objects. amazon s3 does this by using a shared name prefix for objects (that is, objects have names that begin with a common string). object names are also referred to as key names. for example, you can create a folder on the console named  and store an object named  in it. the object is then stored with the key name , where  is the prefix. here are two more examples:  if you have three objects in your bucket—, , and —the console will show a folder named . if you open the folder in the console, you will see three objects: , , and .if you have an object named , the console will show you a folder named  containing the folder  and the object .topics you can have folders within folders, but not buckets within buckets. you can upload and copy objects directly into a folder. folders can be created, deleted, and made public, but they cannot be renamed. objects can be copied from one folder to another.  importantthe amazon s3 console treats all objects that have a forward slash ("/") character as the last (trailing) character in the key name as a folder, for example . you can't upload an object that has a key name with a trailing "/" character using the amazon s3 console. however, you can upload objects that are named with a trailing "/" with the amazon s3 api by using the aws cli, aws sdks, or rest api.an object that is named with a trailing "/" appears as a folder in the amazon s3 console. the amazon s3 console does not display the content and metadata for such an object. when you use the console to copy an object named with a trailing "/", a new folder is created in the destination location, but the object's data and metadata are not copied.  
you can enable certain amazon s3 bucket events to send a notification message to a destination whenever the events occur. this section explains how to use the amazon s3 console to enable event notifications. for information about using event notifications with the aws sdks and the amazon s3 rest apis, see  in the amazon simple storage service developer guide.  topics when you configure event notifications for a bucket, you must specify the type of events for which you want to receive notifications. for a complete list of event types, see  section in the amazon simple storage service developer guide.  in the amazon s3 console, you have the following options for configuring event notifications. you can choose a single option or multiple options. object creationobjectcreated (all) – receive a notification when an object is created in your bucket for any of the following object creation actions: put, post, copy, and multipart upload completed.put, post, copy, and multipart upload completed – receive a notification for one of these specific object creation actions.object deletion objectdelete (all) – receive a notification any time an object in your bucket is deleted.delete marker created – receive a notification when a delete marker is created for a versioned object.  for information about deleting versioned objects, see . for information about object versioning, see  and . object restoration from the s3 glacier storage class restore initiated – receive a notification for initiation of object restoration.restore completed – receive a notification for completion of object restoration.reduced redundancy storage (rrs) object lost eventsobject in rss lost – receive a notification that an object of the rrs storage class has been lostobjects eligible for replication using amazon s3 replication time controlreplication time missed threshold – receive a notification that an object failed to replicate.replication time completed after threshold – receive a notification that an object exceeded the 15-minute threshold for replication.replication time not tracked – receive a notification that an object replicated after the 15-minute threshold.replication time failed – receive a notification that an object that was eligible for replication is no longer being tracked by replication metrics.notewhen you delete the last object from a folder, amazon s3 can generate an object creation event. when there are multiple objects with the same prefix with a trailing slash (/) as part of their names, those objects are shown as being part of a folder in the amazon s3 console. the name of the folder is formed from the characters preceding the trailing slash (/).when you delete all the objects listed under that folder, no actual object is available to represent the empty folder. under such circumstances, the amazon s3 console creates a zero-byte object to represent that folder. if you enabled event notification for the creation of objects, the zero-byte object creation action that is taken by the console triggers an object creation event.the amazon s3 console displays a folder under the following circumstances:when a zero-byte object has a trailing slash (/) in its name. in this case, there is an actual amazon s3 object of 0 bytes that represents a folder. if the object has a slash (/) within its name. in this case, there isn't an actual object representing the folder. when you configure event notifications for a bucket, you also choose the notification destination. event notification messages can be sent to the following destinations: amazon simple notification service (amazon sns) topic – coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. for information about the amazon sns topic format, see .amazon simple queue service (amazon sqs) queue – offers reliable and scalable hosted queues for storing messages as they travel between computers. for information about amazon sqs, see  in the amazon simple queue service developer guide.aws lambda  – invoke a lambda function and provide the event message as an argument. when you create a lambda function, you package up and upload your custom code to aws lambda. aws lambda uses the aws infrastructure to run the code on your behalf. for information about using lambda with amazon s3, see  in the aws lambda developer guide.for more information about granting the amazon s3 service principal the permissions required to publish event notifications to a destination, see  in the amazon s3 developer guide. warningif your notification ends up writing to the bucket that triggers the notification, this could cause an execution loop. for example, if the bucket triggers a lambda function each time an object is uploaded, and the function uploads an object to the bucket, then the function indirectly triggers itself. to avoid this, use two buckets, or configure the trigger to only apply to a prefix used for incoming objects.for more information and an example of using amazon s3 notifications with aws lambda, see  in the aws lambda developer guide.  before you can enable event notifications for your bucket, you must set up one of the destination types. for more information, see  to enable and configure event notifications for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to enable events for. choose properties. under advanced settings, choose events. choose add notification. in name, enter a descriptive name for your event notification.  if you don't enter a name, a guid is generated and used for the name. under events, select one or more events.  for a listing of the event types, see . to filter event notifications by a prefix or suffix, enter a prefix or a suffix.  for example, you can set up a prefix filter so that you receive notifications only when files are added to a specific folder (for example, ). for more information, see . choose the event notification destination: sns topic, sqs queue, or lambda function.  for a description of the destinations, see . when you choose your send to destination, a box appears for you to enter your specific sns, sqs, or lambda function destination. in the example image below, the send to location is sns topic, and you can see a sns box for the sns topic name. in the box that appears, choose or enter the destination sns, sqs, or lambda function.  you can choose or enter the sns, sqs, or lambda function name, or you can choose to the destination amazon resource name (arn). the example screenshot below shows the add sns topic arn option. if you chose add arn, enter the sns topic, sqs queue, or lambda function arn. choose save.  amazon s3 sends a test message to the event notification destination. 
you can empty a bucket, which deletes all of the objects in the bucket without deleting the bucket. when you empty a bucket with versioning enabled, all versions of all the objects in the bucket are deleted. for more information, see  and  in the amazon simple storage service developer guide. to empty an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, select the option next to the name of the bucket that you want to empty and then choose empty. on the empty bucket page, confirm that you want to empty the bucket by entering the bucket name into the text field, and then choose empty. (optional) monitor the progress of the bucket emptying process on the empty bucket: status page. 
this topic explains how to use the aws management console to upload one or more files or entire folders to an amazon s3 bucket. before you can upload files and folders to an amazon s3 bucket, you need write permissions for the bucket. for more information about access permissions, see . for information about uploading files programmatically, see  in the amazon simple storage service developer guide.  when you upload a file to amazon s3, it is stored as an s3 object. objects consist of the file data and metadata that describes the object. you can have an unlimited number of objects in a bucket. you can upload any file type—images, backups, data, movies, etc.—into an s3 bucket. the maximum size of a file that you can upload by using the amazon s3 console is 160 gb. to upload a file larger than 160 gb, use the aws cli, aws sdk, or amazon s3 rest api. for more information, see  in the amazon simple storage service developer guide.  you can upload files by dragging and dropping or by pointing and clicking. to upload folders, you must drag and drop them. drag and drop functionality is supported only for the chrome and firefox browsers. for information about which chrome and firefox browser versions are supported, see .  when you upload a folder, amazon s3 uploads all of the files and subfolders from the specified folder to your bucket. it then assigns an object key name that is a combination of the uploaded file name and the folder name. for example, if you upload a folder called  that contains two files,  and , amazon s3 uploads the files and then assigns the corresponding key names,  and . the key names include the folder name as a prefix. the amazon s3 console displays only the part of the key name that follows the last “/”. for example, within an images folder the  and  objects are displayed as  and a . if you upload individual files and you have a folder open in the amazon s3 console, when amazon s3 uploads the files, it includes the name of the open folder as the prefix of the key names. for example, if you have a folder named  open in the amazon s3 console and you upload a file named , the key name is . however, the object is displayed in the console as  in the  folder. if you upload individual files and you do not have a folder open in the amazon s3 console, when amazon s3 uploads the files, it assigns only the file name as the key name. for example, if you upload a file named , the key name is . for more information on key names, see  in the amazon simple storage service developer guide. if you upload an object with a key name that already exists in a versioning-enabled bucket, amazon s3 creates another version of the object instead of replacing the existing object. for more information about versioning, see . topics if you are using the chrome or firefox browsers, you can choose the folders and files to upload, and then drag and drop them into the destination bucket. dragging and dropping is the only way that you can upload folders. to upload folders and files to an s3 bucket by using drag and drop sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to upload your folders or files to. in a window other than the console window, select the files and folders that you want to upload. then drag and drop your selections into the console window that lists the objects in the destination bucket. the files you chose are listed in the upload dialog box. in the upload dialog box, do one of the following:  drag and drop more files and folders to the console window that displays the upload dialog box. to add more files, you can also choose add more files. this option works only for files, not folders. to immediately upload the listed files and folders without granting or removing permissions for specific users or setting public permissions for all of the files that you're uploading, choose upload. for information about object access permissions, see .  to set permissions or properties for the files that you are uploading, choose next. on the set permissions page, under manage users you can change the permissions for the aws account owner. the owner refers to the aws account root user, and not an aws identity and access management (iam) user. for more information about the root user, see .  choose add account to grant access to another aws account. for more information about granting permissions to another aws account, see . under manage public permissions you can grant read access to your objects to the general public (everyone in the world), for all of the files that you're uploading. granting public read access is applicable to a small subset of use cases such as when buckets are used for websites. we recommend that you do not change the default setting of do not grant public read access to this object(s). you can always make changes to object permissions after you upload the object. for information about object access permissions, see .  when you're done configuring permissions, choose next. on the set properties page, choose the storage class and encryption method to use for the files that you are uploading. you can also add or modify metadata.  choose a storage class for the files you're uploading. for more information about storage classes, see  in the amazon simple storage service developer guide. choose the type of encryption for the files that you're uploading. if you don't want to encrypt them, choose none.  to encrypt the uploaded files using keys that are managed by amazon s3, choose amazon s3 master-key. for more information, see  in the amazon simple storage service developer guide. to encrypt the uploaded files using the aws key management service (aws kms), choose aws kms master-key. then choose a customer master key (cmk) from the list of aws kms cmks. noteto encrypt objects in a bucket, you can use only cmks that are available in the same aws region as the bucket.  you can give an external account the ability to use an object that is protected by an aws kms cmk. to do this, select custom kms arn from the list and enter the amazon resource name (arn) for the external account. administrators of an external account that have usage permissions to an object protected by your aws kms cmk can further restrict access by creating a resource-level iam policy.  for more information about creating an aws kms cmk, see  in the aws key management service developer guide. for more information about protecting data with aws kms, see  in the amazon simple storage service developer guide. metadata for amazon s3 objects is represented by a name-value (key-value) pair. there are two kinds of metadata: system-defined metadata and user-defined metadata. if you want to add amazon s3 system-defined metadata to all of the objects you are uploading, for header, select a header. you can select common http headers, such as content-type and content-disposition. type a value for the header, and then choose save. for a list of system-defined metadata and information about whether you can add the value, see  in the amazon simple storage service developer guide. any metadata starting with prefix  is treated as user-defined metadata. user-defined metadata is stored with the object, and is returned when you download the object.  to add user-defined metadata to all of the objects that you are uploading, type  plus a custom metadata name in the header field. type a value for the header, and then choose save. both the keys and their values must conform to us-ascii standards. user-defined metadata can be as large as 2 kb. for more information about user-defined metadata, see  in the amazon simple storage service developer guide. object tagging gives you a way to categorize storage. each tag is a key-value pair. key and tag values are case sensitive. you can have up to 10 tags per object.  to add tags to all of the objects that you are uploading, type a tag name in the key field. type a value for the tag, and then choose save. a tag key can be up to 128 unicode characters in length and tag values can be up to 255 unicode characters in length. for more information about object tags, see  in the amazon simple storage service developer guide. choose next. on the upload review page, verify that your settings are correct, and then choose upload. to make changes, choose previous. to see the progress of the upload, choose in progress at the bottom of the browser window. to see a history of your uploads and other operations, choose success. this procedure explains how to upload files into an s3 bucket by choosing upload. to upload files to an s3 bucket by pointing and clicking sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to upload your files to. choose upload. in the upload dialog box, choose add files. choose one or more files to upload, and then choose open. after you see the files that you chose listed in the upload dialog box, do one of the following:  to add more files, choose add more files. to immediately upload the listed files, choose upload.  to set permissions or properties for the files that you are uploading, choose next. to set permissions and properties, start with step 5 of . .
you can delete an empty bucket, and when you're using the aws management console, you can delete a bucket that contains objects. if you delete a bucket that contains objects, all the objects in the bucket are permanently deleted.  when you delete a bucket with versioning enabled, all versions of all the objects in the bucket are permanently deleted. for more information about versioning, see  in the amazon simple storage service developer guide. before deleting a bucket, consider the following: bucket names are unique. if you delete a bucket, another aws user can use the name. when you delete a bucket that contains objects, all the objects in the bucket are permanently deleted, including objects that transitioned to the  storage class.if the bucket hosts a static website, and you created and configured an amazon route 53 hosted zone as described in : you must clean up the route 53 hosted zone settings that are related to the bucket as described in .if the bucket receives log data from elastic load balancing (elb): we recommend that you stop the delivery of elb logs to the bucket before deleting it. after you delete the bucket, if another user creates a bucket using the same name, your log data could potentially be delivered to that bucket. for information about elb access logs, see  in the user guide for classic load balancers and  in the user guide for application load balancers.importantif you want to continue to use the same bucket name, don't delete the bucket. we recommend that you empty the bucket and keep it. after a bucket is deleted, the name becomes available to reuse, but the name might not be available for you to reuse for various reasons. for example, it might take some time before the name can be reused, and some other account could create a bucket with that name before you do. to delete an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the buckets list, select the option next to the name of the bucket that you want to delete and then choose delete at the top of the page. on the delete bucket page, confirm that you want to delete the bucket by entering the bucket name into the text field, and then choose delete bucket. noteif the bucket contains any objects, empty the bucket before deleting it by selecting the empty bucket configuration link in the this bucket is not empty error alert and following the instructions on the empty bucket page. then return to the delete bucket page and delete the bucket. 
replication is the automatic, asynchronous copying of objects across buckets in the same or different aws regions. replication copies newly created objects and object updates from a source bucket to a destination bucket. for more information about replication concepts and how to use replication with the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide.  replication requires versioning to be enabled on both the source and destination buckets. to review the full list of requirements, see  in the amazon simple storage service developer guide. for more information about versioning, see  the object replicas in the destination bucket are exact replicas of the objects in the source bucket. they have the same key names and the same metadata—for example, creation time, owner, user-defined metadata, version id, access control list (acl), and storage class. optionally, you can explicitly specify a different storage class for object replicas. and regardless of who owns the source bucket or the source object, you can choose to change replica ownership to the aws account that owns the destination bucket. for more information, see  in the amazon simple storage service developer guide.  you can use s3 replication time control (s3 rtc) to replicate your data in the same aws region or across different aws regions in a predictable timeframe. s3 rtc replicates 99.99 percent of new objects stored in amazon s3 within 15 minutes and most objects within seconds. for more information, see  in the amazon simple storage service developer guide. note about replication and lifecycle rulesmetadata for an object remains identical between original objects and replica objects. lifecycle rules abide by the creation time of the original object, and not by when the replicated object becomes available in the destination bucket. however, lifecycle does not act on objects that are pending replication until replication is complete. you use the amazon s3 console to add replication rules to the source bucket. replication rules define which source bucket objects to replicate and the destination bucket where the replicated objects are stored. you can create a rule to replicate all the objects in a bucket or a subset of objects with a specific key name prefix, one or more object tags, or both. a destination bucket can be in the same aws account as the source bucket, or it can be in a different account. if you specify an object version id to delete, amazon s3 deletes that object version in the source bucket. but it doesn't replicate the deletion in the destination bucket. in other words, it doesn't delete the same object version from the destination bucket. this protects data from malicious deletions. if the destination bucket is in a different account from the source bucket, you must add a bucket policy to the destination bucket to grant the owner of the source bucket account permission to replicate objects in the destination bucket. the amazon s3 console builds this required bucket policy for you to copy and add to the destination bucket in the other account.  when you add a replication rule to a bucket, the rule is enabled by default, so it starts working as soon as you save it.  topics follow these steps to configure a replication rule when the destination bucket is in the same aws account as the source bucket. sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose management, choose replication, and then choose add rule. in the replication rule wizard, under set source, you have the following options for setting the replication source: to replicate the whole bucket, choose entire bucket bucket-name. to replicate all objects that have the same prefix (for example, all objects that have names that begin with the string ), choose prefix or tags. enter a prefix in the box, choose the prefix from the drop-down list, and then press enter. if you enter a prefix that is the name of a folder, you must use / (forward slash) as the last character (for example, ). for more information about prefixes, see  in the amazon simple storage service developer guide.to replicate all objects with one or more object tags, enter a tag in the box, choose the tag from the drop-down list, and then press enter. enter a tag value and then press enter. repeat the procedure to add another tag. you can combine a prefix and tags. for more information about object tags, see  in the amazon simple storage service developer guide.the new schema supports prefix and tag filtering and the prioritization of rules. for more information about the new schema, see  in the amazon simple storage service developer guide. the developer guide describes the xml used with the amazon s3 api that works behind the user interface. in the developer guide, the new schema is described as replication configuration xml v2. to replicate objects in the source bucket that are encrypted with aws key management service (aws kms), under replication criteria, select replicate objects encrypted with aws kms. under choose one or more keys for decrypting source objects are the source aws kms customer master keys (cmks) that you allow replication to use. all source cmks are included by default. you can choose to narrow the cmk selection.  objects encrypted by aws kms cmks that you do not select are not replicated. a cmk or a group of cmks is chosen for you, but you can choose the cmks if you want. for information about using aws kms with replication, see  in the amazon simple storage service developer guide. importantwhen you replicate objects that are encrypted with aws kms, the aws kms request rate doubles in the source region and increases in the destination region by the same amount. these increased call rates to aws kms are due to the way that data is re-encrypted using the customer master key (cmk) that you define for the replication destination region. aws kms has a request rate limit that is per calling account per region. for information about the limit defaults, see  in the aws key management service developer guide.if your current amazon s3 put object request rate during replication is more than half the default aws kms rate limit for your account, we recommend that you request an increase to your aws kms request rate limit. to request an increase, create a case in the aws support center at . for example, suppose that your current put object request rate is 1,000 requests per second and you use aws kms to encrypt your objects. in this case, we recommend that you ask aws support to increase your aws kms rate limit to 2,500 requests per second, in both your source and destination regions (if different), to ensure that there is no throttling by aws kms.to see your put object request rate in the source bucket, view  in the amazon cloudwatch request metrics for amazon s3. for information about viewing cloudwatch metrics, see  choose next. to choose a destination bucket from the account that you're currently using, on the set destination page, under destination bucket, choose buckets in this account. enter the name of the destination bucket for the replication, or choose a name in the drop-down list. if you want to choose a destination bucket from a different aws account, see . if versioning is not enabled on the destination bucket, you get a warning message that contains an enable versioning button. choose this button to enable versioning on the bucket. if you chose to replicate objects encrypted with aws kms, under destination encryption settings, enter the amazon resource name (arn) of the aws kms cmk to use to encrypt the replicas in the destination bucket. you can find the arn for your aws kms cmk in the iam console, under encryption keys.  or, you can choose a cmk name from the drop-down list. for more information about creating an aws kms cmk, see  in the aws key management service developer guide. importantthe amazon s3 console lists only 100 aws kms cmks per aws region. if you have more than 100 cmks in the same region, you can see only the first 100 cmks in the s3 console. to use a kms cmk that is not listed in the console, choose custom kms arn, and enter the kms cmk arn. if you want to replicate your data into a specific storage class in the destination bucket, on the set destination page, under destination options, select change the storage class for the replicated object(s). then choose the storage class that you want to use for the replicated objects in the destination bucket. if you don't select this option, the storage class for replicated objects is the same class as the original objects.  similarly, if you want to change object ownership in the destination bucket, choose change object ownership to the destination bucket owner. for more information about this option, see .  if you want to enable s3 replication time control (s3 rtc) in your replication configuration, select replication time control. notewhen you use s3 rtc, additional per-gb data transfer fees and cloudwatch metrics fees apply. choose next. set up an aws identity and access management (iam) role that amazon s3 can assume to replicate objects on your behalf. to set up an iam role, on the configure options page, under select role, do one of the following: we highly recommend that you choose create new role to have amazon s3 create a new iam role for you. when you save the rule, a new policy is generated for the iam role that matches the source and destination buckets that you choose. the name of the generated role is based on the bucket names and uses the following naming convention: replication_role_for_source-bucket_to_destination-bucket.you can choose to use an existing iam role. if you do, you must choose a role that grants amazon s3 the necessary permissions for replication. replication fails if this role does not grant amazon s3 sufficient permissions to follow your replication rule.  importantwhen you add a replication rule to a bucket, you must have the  permission to be able to pass the iam role that grants amazon s3 replication permissions. for more information, see  in the iam user guide.under rule name, enter a name for your rule to help identify the rule later. the name is required and must be unique within the bucket. if the bucket has existing replication rules, you are instructed to set a priority for the rule. you must set a priority for the rule to avoid conflicts caused by objects that are included in the scope of more than one rule. in the case of overlapping rules, amazon s3 uses the rule priority to determine which rule to apply. the higher the number, the higher the priority. for more information about rule priority, see  in the amazon simple storage service developer guide. under status, enabled is selected by default. an enabled rule starts to work as soon as you save it. if you want to enable the rule later, select disabled. choose next. on the review page, review your replication rule. if it looks correct, choose save. otherwise, choose previous to edit the rule before saving it.  after you save your rule, you can edit, enable, disable, or delete your rule on the replication page. follow these steps to configure a replication rule when the destination bucket is in a different aws account than the source bucket. sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose management, choose replication, and then choose add rule. if you have never created a replication rule before, start with . on the replication rule wizard set destination page, under destination bucket, choose buckets in another account. then enter the name of the destination bucket and the account id from a different aws account. choose save. after you save the destination bucket name and account id, you might get a warning message telling you to add a bucket policy to the destination bucket so that amazon s3 can verify whether versioning is enabled on the bucket. you are presented with a bucket policy in a few steps that you can copy and add to the destination bucket in the other account. for information about adding a bucket policy to an s3 bucket and versioning, see  and  if you chose to replicate objects encrypted with aws kms, under destination encryption settings, enter the amazon resource name (arn) aws kms cmk to use to encrypt the replicas in the destination bucket.  for more information about creating an aws kms cmk, see  in the aws key management service developer guide. on the set destination page, under destination options: to replicate your data into a specific storage class in the destination bucket, select change the storage class for the replicated object(s). then choose the storage class that you want to use for the replicated objects in the destination bucket. if you don't select this option, the storage class for replicated objects is the same class as the original objects.to change the object ownership of the replica objects to the destination bucket owner, select change object ownership to destination owner. this option enables you to separate object ownership of the replicated data from the source. if asked, type the account id of the destination bucket. when you select this option, regardless of who owns the source bucket or the source object, the aws account that owns the destination bucket is granted full permission to replica objects. for more information, see  in the amazon simple storage service developer guide. if you want to add s3 replication time control (s3 rtc) to your replication configuration, select replication time control. notewhen you use s3 rtc, additional per-gb data transfer fees and cloudwatch metrics fees apply.choose next. set up an aws identity and access management (iam) role that amazon s3 can assume to perform replication of objects on your behalf. to set up an iam role, on the configure options page, under select role, do one of the following: we highly recommend that you choose create new role to have amazon s3 create a new iam role for you. when you save the rule, a new policy is generated for the iam role that matches the source and destination buckets that you choose. the name of the generated role is based on the bucket names and uses the following naming convention: replication_role_for_source-bucket_to_destination-bucket.you can choose to use an existing iam role. if you do, you must choose a role that allows amazon s3 to replicate objects from the source bucket to the destination bucket on your behalf.a bucket policy is provided on the configure options page that you can copy and add to the destination bucket in the other account. for information about adding a bucket policy to an s3 bucket, see  if you chose to replicate objects encrypted with aws kms, an aws kms key policy is provided on the configure options page. you can copy this policy to add to the key policy for the aws kms cmk that you are using. the key policy grants the source bucket owner permission to use the cmk. for information about updating the key policy, see . on the review page, review your replication rule. if it looks correct, choose save. otherwise, choose previous to edit the rule before saving it.  after you save your rule, you can edit, enable, disable, or delete your rule on the replication page. follow the instructions given on the replication page under the warning message, the replication rule is saved, but additional settings are required in the destination account. sign out of the aws account that you are currently in, and then sign in to the destination account.  importantreplication fails until you sign in to the destination account and complete the following steps. after you sign in to the destination account, choose the management tab, choose replication, and then choose receive objects on the actions menu.  on the receive objects page, you can do the following: enable versioning on the destination bucket.apply the bucket policy provided by amazon s3 to the destination bucket.copy the aws kms key policy that you need to update the aws kms cmk that is being used to encrypt the replica objects in the destination bucket. for information about updating the key policy, see .you must grant permissions to the account of the source bucket owner to encrypt using your aws kms cmk with a key policy. the following procedure describes how to use the aws identity and access management (iam) console to modify the key policy for the aws kms cmk that is being used to encrypt the replica objects in the destination bucket. to grant permissions to encrypt using your aws kms cmk sign in to the aws management console using the aws account that owns the aws kms cmk. open the aws kms console at . choose the alias of the cmk that you want to encrypt with. in the key policy section of the page, choose switch to policy view. choose edit to edit key policy. using the key policy editor, insert the key policy provided by amazon s3 into the existing key policy, and then choose save changes. you might want to add the policy to the end of the existing policy.  for more information about creating and editing aws kms cmks, see  in the aws key management service developer guide.   in the amazon simple storage service developer guide
this section describes how to create a s3 batch operations job. for information about performing batch operations using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. to create a batch job sign in to the aws management console and open the amazon s3 console at . choose batch operations on the navigation pane of the amazon s3 console. choose create job. choose the region where you want to create your job. under manifest format, choose the type of manifest object to use. if you choose s3 inventory report, enter the path to the manifest.json object that amazon s3 generated as part of the csv-formatted inventory report, and optionally the version id for the manifest object if you want to use a version other than the most recent.if you choose csv, enter the path to a csv-formatted manifest object. the manifest object must follow the format described in the console. you can optionally include the version id for the manifest object if you want to use a version other than the most recent.choose next under operation, choose the operation that you want to perform on all objects listed in the manifest. fill out the information for the operation you chose and then choose next. fill out the information for configure additional options and then choose next. for review, verify the settings. if you need to make changes, choose previous. otherwise, choose create job.  in the amazon simple storage service developer guide in the amazon simple storage service developer guide in the amazon simple storage service developer guide
*copyright &copy; 2020 amazon web services, inc. and/or its affiliates. all rights reserved.* amazon's trademarks and trade dress may not be used in       connection with any product or service that is not amazon's,       in any manner that is likely to cause confusion among customers,       or in any manner that disparages or discredits amazon. all other       trademarks not owned by amazon are the property of their respective      owners, who may or may not be affiliated with, connected to, or       sponsored by amazon. 
versioning enables you to keep multiple versions of an object in one bucket. this section describes how to enable object versioning on a bucket. for more information about versioning support in amazon s3, see  and  in the amazon simple storage service developer guide. to enable or disable versioning on an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to enable versioning for. choose properties. choose versioning. choose enable versioning or suspend versioning, and then choose save. noteyou can use aws multi-factor authentication (mfa) with versioning. when you use mfa with versioning, you must provide your aws account’s access keys and a valid code from the account’s mfa device in order to permanently delete an object version or suspend or reactivate versioning. to use mfa with versioning, you enable . however, you cannot enable  using the aws management console. you must use the aws cli or api. for more information, see . 
this section explains how to use the amazon s3 console to recover (undelete) deleted objects. to be able to undelete a deleted object, you must have had versioning enabled on the bucket that contains the object before the object was deleted. for information about enabling versioning, see . when you delete an object in a versioning-enabled bucket, all versions remain in the bucket and amazon s3 creates a delete marker for the object. to undelete the object, you must delete this delete marker. for more information about versioning and delete markers, see  in the amazon simple storage service developer guide. to recover deleted objects from an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. to see a list of the versions of the objects in the bucket, select show. you'll be able to see the delete markers for deleted objects.  to undelete an object, you must delete the delete marker. select the check box next to the delete marker of the object to recover, and then choose delete from the actions menu. then, choose hide and you'll see the undeleted object listed.  in the amazon simple storage service developer guide
this section explains how to use the amazon s3 console to restore an object that has been archived to the s3 glacier or s3 glacier deep archive storage classes. objects stored in the s3 glacier or s3 glacier deep archive are not immediately accessible. to access an object in this class, you must restore a temporary copy of it to its s3 bucket for the duration (number of days) that you specify. for information about the s3 glacier or s3 glacier deep archive storage classes, see  in the amazon simple storage service developer guide.  when you restore an archive, you pay for both the archive and the restored copy. because there is a storage cost for the copy, restore objects only for the duration you need them. if you want a permanent copy of the object, create a copy of it in your s3 bucket. for information about amazon s3 features and pricing, see . after restoring an object, you can download it from the overview page. for more information, see . topics the following are the available retrieval options when restoring an archived object:   - expedited retrievals allow you to quickly access your data stored in the s3 glacier storage class when occasional urgent requests for a subset of archives are required. for all but the largest archived objects (250 mb+), data accessed using expedited retrievals is typically made available within 1–5 minutes. provisioned capacity ensures that retrieval capacity for expedited retrievals is available when you need it. for more information, see . expedited retrievals and provisioned capacity are not available for objects stored in the s3 glacier deep archive storage class. - standard retrievals allow you to access any of your archived objects within several hours. this is the default option for the s3 glacier and s3 glacier deep archive retrieval requests that do not specify the retrieval option. standard retrievals typically finish within 3–5 hours for objects stored in the s3 glacier storage class. they typically finish within 12 hours for objects stored in the s3 glacier deep archive storage class.  - bulk retrievals are the lowest-cost retrieval option in amazon s3 glacier, enabling you to retrieve large amounts, even petabytes, of data inexpensively. bulk retrievals typically finish within 5–12 hours for objects stored in the s3 glacier storage class. they typically finish within 48 hours for objects stored in the s3 glacier deep archive storage class.for more information about retrieval options, see  in the amazon simple storage service developer guide. this topic explains how to use the amazon s3 console to restore an object that has been archived to the s3 glacier or s3 glacier deep archive storage classes. (the console uses the names glacier and glacier deep archive for these storage classes.) to restore archived s3 objects sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the objects that you want to restore. in the name list, select the object or objects that you want to restore, choose actions, and then choose restore. in the initiate restore dialog box, type the number of days that you want your archived data to be accessible.  choose one of the following retrieval options from the retrieval options menu. choose bulk retrieval or standard retrieval, and then choose restore. choose expedited retrieval (available only for the glacier storage class).provisioned capacity is only available for the glacier storage class. if you have provisioned capacity, choose restore to start a provisioned retrieval. if you have provisioned capacity, all of your expedited retrievals are served by your provisioned capacity. for more information about provisioned capacity, see .  if you don't have provisioned capacity and you don't want to buy it, choose restore. if you don't have provisioned capacity, but you want to buy it, choose add capacity unit, and then choose buy. when you get the purchase succeeded message, choose restore to start provisioned retrieval.you can upgrade the speed of your restoration while it is in progress. to upgrade an in-progress restore to a faster tier in the name list, select one or more of the objects that you are restoring, choose actions, and then choose restore from glacier. for information about checking the restoration status of an object, see . choose the tier that you want to upgrade to and then choose restore. for more information about upgrading to a faster restore tier, see see  in the amazon simple storage service developer guide.  to check the progress of the restoration, see the object overview panel. for information about the overview panel, see .  the overview section shows that restoration is in progress.  when the temporary copy of the object is available, the object's overview section shows the restoration expiry date. this is when amazon s3 will remove the restored copy of your archive.   restored objects are stored only for the number of days that you specify. if you want a permanent copy of the object, create a copy of it in your amazon s3 bucket.  amazon s3 calculates the expiry date by adding the number of days that you specify to the time you request to restore the object, and then rounding to the next day at midnight utc. this calculation applies to the initial restoration of the object and to any extensions to availability that you request. for example, if an object was restored on 10/15/2012 10:30 am utc and the number of days that you specified is 3, then the object is available until 10/19/2012 00:00 utc. if, on 10/16/2012 11:00 am utc you change the number of days that you want it to be accessible to 1, then amazon s3 makes the restored object available until 10/18/2012 00:00 utc. after restoring an object, you can download it from the overview page. for more information, see .   in the amazon s3 developer guide. in the aws cli command reference.
object tagging gives you a way to categorize storage. this topic explains how to use the console to add tags to an s3 object after the object has been uploaded. for information about adding tags to an object when the object is being uploaded, see .  each tag is a key-value pair that adheres to the following rules: you can associate up to 10 tags with an object. tags associated with an object must have unique tag keys.a tag key can be up to 128 unicode characters in length and tag values can be up to 255 unicode characters in length.key and tag values are case sensitive.  for more information about object tags, see  in the amazon simple storage service developer guide. to add tags to an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object you want to add tags to. choose properties. choose tags and then choose add tag. each tag is a key-value pair. type a key and a value. then choose add tag to add another tag or choose save.  you can enter up to 10 tags for an object. 
this section explains how to use the amazon simple storage service (amazon s3) console to grant access permissions to your buckets and objects. it also explains how to use amazon s3 block public access to prevent the application of any settings that allow public access to data within s3 buckets.   buckets and objects are amazon s3 resources.  you grant access permissions to your buckets and objects by using resource-based access policies. you can associate an access policy with a resource. an access policy describes who has access to resources. the resource owner is the aws account that creates the resource. for more information about resource ownership and access policies, see  in the amazon simple storage service developer guide.  bucket access permissions specify which users are allowed access to the objects in a bucket and which types of access they have. object access permissions specify which users are allowed access to the object and which types of access they have. for example, one user might have only read permission, while another might have read and write permissions. bucket and object permissions are independent of each other. an object does not inherit the permissions from its bucket. for example, if you create a bucket and grant write access to a user, you can't access that user’s objects unless the user explicitly grants you access.  to grant access to your buckets and objects to other aws accounts and to the general public, you use resource-based access policies known as access control lists (acls).  a bucket policy is a resource-based aws identity and access management (iam) policy that grants other aws accounts or iam users access to an s3 bucket. bucket policies supplement, and in many cases, replace acl-based access policies.  for more information about using iam with amazon s3, see  in the amazon simple storage service developer guide.  for more in-depth information about managing access permissions, see  in the amazon simple storage service developer guide. this section also explains how to use the amazon s3 console to add a cross-origin resource sharing (cors) configuration to an s3 bucket. cors allows client web applications that are loaded in one domain to interact with resources in another domain. topics 
this section explains how to use the amazon simple storage service (amazon s3) console to manage access permissions for s3 buckets by using access control lists (acls). acls are resource-based access policies that grant access permissions to buckets and objects. for more information about managing access permissions with resource-based policies, see  in the amazon simple storage service developer guide. you can grant permissions to other aws account users or to predefined groups. the user or group that you are granting permissions to is called the grantee. by default, the owner, which is the aws account that created the bucket, has full permissions. each permission you grant for a user or group adds an entry in the acl that is associated with the bucket. the acl lists grants, which identify the grantee and the permission granted. for more information about acls, see  in the amazon simple storage service developer guide. to set acl access permissions for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to set permissions for. choose permissions, and then choose access control list. you can manage bucket access permissions for the following:  1.  access for your aws account root user  access for other aws accounts to grant permissions to an aws user from a different aws account, under access for other aws accounts, choose add account. in the enter an id field, enter the canonical id of the aws user that you want to grant bucket permissions to. for information about finding a canonical id, see  in the amazon web services general reference. you can add as many as 99 users. select the check boxes next to the permissions that you want to grant to the user, and then choose save. to display information about the permissions, choose the help icons. warningwhen you grant other aws accounts access to your resources, be aware that the aws accounts can delegate their permissions to users under their accounts. this is known as cross-account access. for information about using cross-account access, see  in the iam user guide.  public access  warninguse caution when granting the everyone group public access to your s3 bucket. when you grant access to this group, anyone in the world can access your bucket. we highly recommend that you never grant any kind of public write access to your s3 bucket. s3 log delivery group to grant access to amazon s3 to write server access logs to the bucket, under s3 log delivery group, choose log delivery. if a bucket is set up as the target bucket to receive access logs, the bucket permissions must allow the log delivery group write access to the bucket. when you enable server access logging on a bucket, the amazon s3 console grants write access to the log delivery group for the target bucket that you choose to receive the logs. for more information about server access logging, see . you can also set bucket permissions when you are creating a bucket. for more information about setting permissions when creating a bucket, see .  
amazon s3 default encryption provides a way to set the default encryption behavior for an amazon s3 bucket. you can set default encryption on a bucket so that all objects are encrypted when they are stored in the bucket. the objects are encrypted using server-side encryption with either amazon s3-managed keys (sse-s3) or aws key management service (aws kms) customer master keys (cmks).  when you use server-side encryption, amazon s3 encrypts an object before saving it to disk in its data centers and decrypts it when you download the objects. for more information about protecting data using server-side encryption and encryption key management, see  in the amazon simple storage service developer guide. default encryption works with all existing and new amazon s3 buckets. without default encryption, to encrypt all objects stored in a bucket, you must include encryption information with every object storage request. you must also set up an amazon s3 bucket policy to reject storage requests that don't include encryption information.  there are no new charges for using default encryption for s3 buckets. requests to configure the default encryption feature incur standard amazon s3 request charges. for information about pricing, see . for sse-kms cmk storage, aws kms charges apply and are listed at .  to enable default encryption on an amazon s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose properties. choose default encryption. if you want to use keys that are managed by amazon s3 for default encryption, choose aes-256, and choose save.  for more information about using amazon s3 server-side encryption to encrypt your data, see  in the amazon simple storage service developer guide. importantyou might need to update your bucket policy when enabling default encryption. for more information, see  in the amazon simple storage service developer guide. if you want to use cmks that are stored in aws kms for default encryption, follow these steps: choose aws-kms. to choose a customer-managed aws kms cmk that you have created, use one of these methods: in the list that appears, choose the aws kms cmk.in the list that appears, choose custom kms arn, and then enter the amazon resource name of the aws kms cmk. importantyou can use only kms cmks that are enabled in the same aws region as the bucket. the s3 console only lists 100 kms cmks per region. if you have more than 100 cmks in the same region, you can only see the first 100 cmks in the s3 console. to use a kms cmk that is not listed in the console, choose custom kms arn, and enter the kms cmk arn.when you use an aws kms cmk for server-side encryption in amazon s3, you must choose a symmetric cmk. amazon s3 only supports symmetric cmks and not asymmetric cmks. for more information, see  in the aws key management service developer guide. importantif you use the aws kms option for your default encryption configuration, you are subject to the rps (requests per second) limits of aws kms. for more information about aws kms limits and how to request a limit increase, see . for more information about creating an aws kms cmk, see  in the aws key management service developer guide. for more information about using aws kms with amazon s3, see  in the amazon simple storage service developer guide. choose save.  in the amazon simple storage service developer guide
this section explains how to use the amazon s3 console to see the different versions of an object. a versioning-enabled bucket can have many versions of the same object:, one current (latest) version and zero or more noncurrent (previous) versions. amazon s3 assigns each object a unique version id. for information about enabling versioning, see .  if a bucket is versioning-enabled, amazon s3 creates another version of an object under the following conditions:  if you upload an object that has the same name as an object that already exists in the bucket, amazon s3 creates another version of the object instead of replacing the existing object. if you update any object properties after you upload the object to the bucket, such as changing the storage details or other metadata , amazon s3 creates a new object version in the bucket. for more information about versioning support in amazon s3, see  and  in the amazon simple storage service developer guide. to see multiple versions of an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. to see a list of the versions of the objects in the bucket, choose show. for each object version, the console shows a unique version id, the date and time the object version was created, and other properties. (objects stored in your bucket before you set the versioning state have a version id of null.)  to list the objects without the versions, choose hide. you also can view, download, and delete object versions in the object overview panel. for more information, see . importantyou can undelete an object only if it was deleted as the latest (current) version. you can't undelete a previous version of an object that was deleted. for more information, see  and  in the amazon simple storage service developer guide. 
this section explains how to use the amazon s3 console to add a cross-origin resource sharing (cors) configuration to an s3 bucket. cors allows client web applications that are loaded in one domain to interact with resources in another domain.  to configure your bucket to allow cross-origin requests, you add cors configuration to the bucket. a cors configuration is an xml document that defines rules that identify the origins that you will allow to access your bucket, the operations (http methods) supported for each origin, and other operation-specific information. for more information about cors and examples, see  in the amazon simple storage service developer guide. when you enable cors on the bucket, the access control lists (acls) and other access permission policies continue to apply. to add a cors configuration to an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to create a bucket policy for. choose permissions, and then choose cors configuration. in the cors configuration editor text box, type or copy and paste a new cors configuration, or edit an existing configuration. the cors configuration is an xml file. the text that you type in the editor must be valid xml. for more information, see  choose save. noteamazon s3 displays the amazon resource name (arn) for the bucket next to the cors configuration editor title. for more information about arns, see  in the amazon web services general reference. 
you can use lifecycle policies to define actions that you want amazon s3 to take during an object's lifetime (for example, transition objects to another storage class, archive them, or delete them after a specified period of time). you can define a lifecycle policy for all objects or a subset of objects in the bucket by using a shared prefix (objects names that begin with a common string) or a tag.  using a lifecycle policy, you can define actions specific to current and noncurrent object versions. for more information, see  and  and  in the amazon simple storage service developer guide. to create a lifecycle policy sign in to the aws management console and open the amazon s3 console at . in the buckets list, choose the name of the bucket that you want to create a lifecycle policy for. choose the management tab, and choose add lifecycle rule. in the lifecycle rule dialog box, type a name for your rule.  the name must be unique within the bucket.  choose the scope of the lifecycle rule: all objects with a specific prefix or tag or all the objects in the bucket. to apply this lifecycle rule to all objects with a specific prefix or tag, choose limit the scope to specific prefixes or tags. in the add prefix or tag filter box, type the prefix or tag name, and press enter.  for more information about object name prefixes, see  in the amazon simple storage service developer guide.for more information about object tags, see  in the amazon simple storage service developer guide.to apply this lifecycle rule to all objects in the bucket, choose apply to all objects in the bucket.choose next. the storage class transition page opens. when you configure your storage class transitions, you define the rules to transition objects to the standard-ia, one zone-ia, glacier, and deep archive storage classes. for more information, see  in the amazon simple storage service developer guide. you can define transitions for current or previous object versions or for both current and previous versions. versioning enables you to keep multiple versions of an object in one bucket. for more information about versioning, see . choose the versions for which you want to define transitions, current or noncurrent: to define transitions that are applied to the current verion of the object, choose current version. to define transitions that are applied to all previous versions of the object, choose previous versions. in the screenshot below, current version is chosen. to add a transition:  for a current version, under for current object versions, choose add transition. for a non-current version, under for non-current object versions, choose add transition. for each transition that you add, choose one of the following: transition to standard-ia after.transition to intelligent-tiering after. transition to one zone-ia after. transition to glacier after.transition to glacier deep archive after. importantwhen you choose the glacier or glacier deep archive storage class, your objects remain in amazon s3. you cannot access them directly through the separate amazon s3 glacier service. for more information, see . in the days after creation box, enter the number of days after the creation of the object that you want the transition to be applied (for example, 30 or 100 days). when you are done configuring transitions, choose next. under configure expiration, for this example, choose both current version and previous versions.  importantin a non-versioned bucket the expiration action results in amazon s3 permanently removing the object. for more information about lifecycle actions, see  in the amazon simple storage service developer guide. choose expire current version of object, and then enter the number of days after object creation to delete the object (for example, 395 days). if you choose this expire option, you cannot choose the option to clean up expired delete markers.  choose permanently delete previous versions, and then enter the number of days after an object becomes a previous version to permanently delete the object (for example, 465 days). to clean up incomplete multipart uploads, we recommend that you choose clean up incomplete multipart uploads and enter the number of days after the multipart upload initiation that you want to end and clean up incomplete multipart uploads(for example, 7 days).  for more information about multipart uploads, see  in the amazon simple storage service developer guide. choose next. for review, verify the settings for your rule. if you need to make changes, choose previous. otherwise, choose save.  if the rule does not contain any errors, it is enabled and you can see it on the lifecycle page. 
with s3 object lock, you can store objects in amazon s3 using a write-once-read-many (worm) model. you can use s3 object lock to prevent an object from being deleted or overwritten for a fixed amount of time or indefinitely. for information about object locking using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. before you lock any objects, you have to enable a bucket to use s3 object lock. you enable object lock when you create a bucket. after you enable object lock on a bucket, you can lock objects in that bucket. when you create a bucket with object lock enabled, you can't disable object lock or suspend versioning for that bucket.  for information about creating a bucket with s3 object lock enabled, see . to lock an amazon s3 object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. in the name list, choose the name of the object that you want to lock. choose properties. choose object lock. choose a retention mode. you can change the retain until date. you can also choose to enable a legal hold. for more information, see  in the amazon simple storage service developer guide. choose save. 
welcome to the amazon simple storage service console user guide for the amazon simple storage service (amazon s3) console.  amazon s3 provides virtually limitless storage on the internet. this guide explains how you can manage buckets, objects, and folders in amazon s3 by using the aws management console, a browser-based graphical user interface for interacting with aws services.  for detailed conceptual information about how amazon s3 works, see  in the amazon simple storage service developer guide. the developer guide also has detailed information about amazon s3 features and code examples to support those features. topics 
this topic describes how to enable server access logging for an amazon s3 bucket using the aws management console. for information about how to enable logging programmatically and details about how logs are delivered, see  in the amazon simple storage service developer guide. by default, amazon simple storage service (amazon s3) doesn't collect server access logs. when you enable logging, amazon s3 delivers access logs for a source bucket to a target bucket that you choose. the target bucket must be in the same aws region as the source bucket and must not have a default retention period configuration.  server access logging provides detailed records for the requests that are made to an s3 bucket. server access logs are useful for many applications. for example, access log information can be useful in security and access audits. it can also help you learn about your customer base and understand your amazon s3 bill.  an access log record contains details about the requests that are made to a bucket. this information can include the request type, the resources that are specified in the request, and the time and date that the request was processed. for more information, see  in the amazon simple storage service developer guide. importantthere is no extra charge for enabling server access logging on an amazon s3 bucket. however, any log files that the system delivers to you will accrue the usual charges for storage. (you can delete the log files at any time.) we do not assess data transfer charges for log file delivery, but we do charge the normal data transfer rate for accessing the log files. to enable server access logging for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to enable server access logging for. choose properties. choose server access logging. choose enable logging. for target, choose the name of the bucket that you want to receive the log record objects.  the target bucket must be in the same region as the source bucket and must not have a default retention period configuration. (optional) for target prefix, type a key name prefix for log objects, so that all of the log object names begin with the same string. choose save. you can view the logs in the target bucket. if you specified a prefix, the prefix shows as a folder in the target bucket in the console. after you enable server access logging, it might take a few hours before the logs are delivered to the target bucket. for more information about how and when logs are delivered, see  in the amazon simple storage service developer guide. more info  
this topic describes how to set or change the type of encryption an object using the amazon s3 console. noteif you change an object's encryption, a new object is created to replace the old one. if s3 versioning is enabled, a new version of the object is created, and the existing object becomes an older version. the role that changes the property also becomes the owner of the new object or (object version).  to add or change encryption for an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object that you want to add or change encryption for. choose properties, and then choose encryption. the encryption dialog box opens, giving you three choices for object encryption: none‐ no object encryption.aes-256 ‐ server-side encryption with amazon s3 managed keys (sse-s3).aws‐kms ‐ server-side encryption with aws key management service (aws kms) customer master keys (sse-kms).if you want to remove encryption from an object that already has encryption settings, choose none and then choose save. if you want to encrypt your object using keys that are managed by amazon s3, follow these steps: chooseaes-256.  for more information about using amazon s3 server-side encryption to encrypt your data, see  in the amazon simple storage service developer guide. choose save. if you want to encrypt your object using aws kms, follow these steps: choose aws-kms. choose an aws kms customer master key (cmk). the list shows  that you have created and your aws managed cmk for amazon s3. for more information about creating a customer managed aws kms cmk, see  in the aws key management service developer guide.  importantthe amazon s3 console lists only 100 aws kms cmks per aws region. if you have more than 100 cmks in the same region, you can see only the first 100 cmks in the s3 console. to use a kms cmk that is not listed in the console, choose custom kms arn, and enter the kms cmk arn. choose save. importantto encrypt objects in the bucket, you can use only cmks that are enabled in the same aws region as the bucket. amazon s3 only supports symmetric cmks. amazon s3 does not support asymmetric cmks. for more information, see . to give an external account the ability to use an object that is protected by an aws kms cmk, follow these steps:  choose aws-kms. enter the amazon resource name (arn) for the external account. choose save. administrators of an external account that have usage permissions to an object protected by your aws kms cmk can further restrict access by creating a resource-level aws identity and access management (iam) policy.  in the amazon simple storage service developer guide
this section explains how to manage and use your amazon s3 access points using the aws management console. each access point is associated with a single amazon s3 bucket. before you begin, navigate to the list of your access points for a bucket as described in the following procedure. to find a list of access points for a bucket sign in to the aws management console and open the amazon s3 console at . in the s3 buckets section, select the bucket whose access points you want to manage. on the bucket detail page, choose the access points tab. from the access points tab, you can view an access point's configuration details, edit an access point's policy, use an access point to access your bucket, or delete an access point. the following procedures explain how to perform each of these tasks. to view access point configuration details navigate to the access points tab for your bucket. locate the access point whose configuration you want to view. you can browse for the access point in the access point list, or you can search for a specific access point using the search by name field. choose the name of the access point whose configuration you want to view. noteto view an access point's configuration, choose (click on) the name of the access point, not the option button next to the access point name. to edit an access point policy navigate to the access points tab for your bucket. select the option button next to the name of the access point whose policy you want to edit. choose edit access point policy. enter the policy in the text field. the console automatically displays the amazon resource name (arn) for the access point, which you can use in the policy. you can also choose policy generator to use the aws policy generator to help construct the policy. choose save. to use an access point to access your bucket navigate to the access points tab for your bucket. select the option button next to the name of the access point you want to use. choose use this access point. the console displays a label above the name of your bucket that shows the access point that you're currently using. while you're using the access point, you can only perform the object operations that are allowed by the access point permissions. notethe console view always shows all objects in the bucket. using an access point as described in this procedure restricts the operations you can perform on those objects, but not whether you can see that they exist in the bucket. to exit the access point view of your bucket, choose exit access point. notethe s3 management console doesn't support using virtual private cloud (vpc) access points to access bucket resources. to access bucket resources from a vpc access point, use the aws cli, aws sdks, or amazon s3 rest apis. to delete an access point navigate to the access points tab for your bucket. select the option button next to the name of the access point that you want to delete. choose delete. confirm that you want to delete your access point by entering its name in the text field that appears, and choose confirm. 
this section explains how to use the amazon s3 console to view the object overview panel. this panel provides an overview of all the essential information for an object in one place. to see the overview panel for an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, select the check box next to the name of the object for which you want an overview. to download the object, choose download in the object overview panel. to copy the path of the object to the clipboard, choose copy path. if versioning is enabled on the bucket, choose latest versions to list the versions of the object. you can then choose the download icon to download an object version, or choose the trash can icon to delete an object version. importantyou can undelete an object only if it was deleted as the latest (current) version. you can't undelete a previous version of an object that was deleted. for more information, see  and  in the amazon simple storage service developer guide. 
this section explains how to use the amazon s3 console to download objects from an s3 bucket. data transfer fees apply when you download objects. for information about amazon s3 features, and pricing, see . importantif an object key name consists of a single period (.), or two periods (..), you can’t download the object using the amazon s3 console. to download an object with a key name of “.” or “..”, you must use the aws cli, aws sdks, or rest api. for more information about naming objects, see  in the amazon simple storage service developer guide. you can download a single object per request using the amazon s3 console. to . to download an object from an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to download an object from. you can download an object from an s3 bucket in any of the following ways: in the name list, select the check box next to the object you want to download, and then choose download on the object description page that appears.choose the name of the object that you want to download. on the overview page, choose download. choose the name of the object that you want to download and then choose download as on the overview page.choose the name of the object that you want to download. choose latest version and then choose the download icon.
each object in amazon simple storage service (amazon s3) has a set of name-value pairs that provides metadata about the object. metadata is additional information about the object. some metadata is set by amazon s3 when you upload the object, for example, and . you can also set some metadata when you upload the object, or you can add it later. this section explains how to use the amazon s3 console to add metadata to an s3 object. object metadata is a set of name-value (key-value) pairs. for example, the metadata for content length, , is the name (key) and the size of the object in bytes (value). for more information about object metadata, see  in the amazon simple storage service developer guide. there are two kinds of metadata for an s3 object, amazon s3 system metadata and user-defined metadata: system metadata–there are two categories of system metadata. metadata such as the  date is controlled by the system. only amazon s3 can modify the value. there is also system metadata that you control, for example, the storage class configured for the object. user-defined metadata–you can define your own custom metadata, called user-defined metadata. you can assign user-defined metadata to an object when you upload the object or after the object has been uploaded. user-defined metadata is stored with the object and is returned when you download the object. amazon s3 does not process user-defined metadata. the following topics describe how to add metadata to an object using the amazon s3 console. noteif you change an object's metadata, a new object will be created and will replace the old one. if s3 versioning is enabled, a new version of the object is created, and the existing object becomes an older version. the role that changes the property also becomes the owner of the new object or (object version). topics you can configure some system metadata for an s3 object. for a list of system-defined metadata and whether you can modify their values, see  in the amazon simple storage service developer guide. to add system metadata to an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object that you want to add metadata to. choose properties, and then choose metadata. choose add metadata, and then choose a key from the select a key menu. depending on which key you chose, choose a value from the select a value menu or type a value. choose save. you can assign user-defined metadata to an object. user-defined metadata must begin with the prefix "", otherwise amazon s3 will not set the key value pair as you define it. you define custom metadata by adding a name that you choose to the  key. this creates a custom key. for example, if you add the custom name , the metadata key would be .  user-defined metadata can be as large as 2 kb. both keys and their values must conform to us-ascii standards. for more information, see  in the amazon simple storage service developer guide. to add user-defined metadata to an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object that you want to add metadata to. choose properties, and then choose metadata. choose add metadata, and then choose the  key from the select a key menu. any metadata starting with the prefix  is user-defined metadata. type a custom name following the  key. for example, for the custom name , the metadata key would be . enter a value for the custom key, and then choose save. 
amazon s3 inventory provides a flat file list of your objects and metadata, which is a scheduled alternative to the amazon s3 synchronous  api operation. amazon s3 inventory provides comma-separated values (csv) or  or  output files that list your objects and their corresponding metadata on a daily or weekly basis for an s3 bucket or for objects that share a prefix (objects that have names that begin with the same string). for more information, see  in the amazon simple storage service developer guide. to configure inventory noteit may take up to 48 hours to deliver the first report. sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket for which you want to configure amazon s3 inventory. choose the management tab, and then choose inventory. choose add new. type a name for the inventory and set it up as follows: optionally, add a prefix for your filter to inventory only objects whose names begin with the same string.choose the destination bucket where you want reports to be saved. the destination bucket must be in the same aws region as the bucket for which you are setting up the inventory. the destination bucket can be in a different aws account. optionally, choose a prefix for the destination bucket.choose how frequently to generate the inventory.under advanced settings, you can set the following: choose either the csv, orc, or parquet output file format for your inventory. for more information about these formats, see  in the amazon simple storage service developer guide. to include all versions of the objects in the inventory, choose include all versions in the object versions list. by default, the inventory includes only the current versions of the objects. for optional fields, select one or more of the following to add to the inventory report: size – object size in bytes.last modified date – the object creation date or the last modified date, whichever is the latest.storage class – the storage class used for storing the object. etag – the entity tag is a hash of the object. the etag reflects changes only to the contents of an object, and not its metadata. the etag may or may not be an md5 digest of the object data. whether it is depends on how the object was created and how it is encrypted.multipart upload – specifies that the object was uploaded as a multipart upload. for more information, see  in the amazon simple storage service developer guide.replication status – the replication status of the object. for more information, see .encryption status – the server-side encryption used to encrypt the object. for more information, see  in the amazon simple storage service developer guide.s3 object lock configurations – the object lock status of the object, including the following settings:  retention mode – the level of protection applied to the object, either governance or compliance.retain until date – the date until which the locked object cannot be deleted.legal hold status – the legal hold status of the locked object. for information about s3 object lock, see  in the amazon simple storage service developer guide. for more information about the contents of an inventory report, see  in the amazon simple storage service developer guide. for encryption, choose a server-side encryption option to encrypt the inventory report, or choose none: none – do not encrypt the inventory report.aes-256 – encrypt the inventory report using server-side encryption with amazon s3-managed keys (sse-s3). amazon s3 server-side encryption uses 256-bit advanced encryption standard (aes-256). for more information, see  in the amazon simple storage service developer guide. aws-kms – encrypt the report using server-side encryption with aws key management service (aws kms) customer master keys (cmks). for more information, see  in the amazon simple storage service developer guide.  noteto encrypt the inventory list file with sse-kms, you must grant amazon s3 permission to use the aws kms cmk. for instructions, see  .choose save. amazon s3 creates a bucket policy on the destination bucket that grants amazon s3 write permission. this allows amazon s3 to write data for the inventory reports to the bucket.  if an error occurs when you try to create the bucket policy, you are given instructions on how to fix it. for example, if you choose a destination bucket in another aws account and don't have permissions to read and write to the bucket policy, you see the following message.   in this case, the destination bucket owner must add the displayed bucket policy to the destination bucket. if the policy is not added to the destination bucket, you won't get an inventory report because amazon s3 doesn’t have permission to write to the destination bucket. if the source bucket is owned by a different account than that of the current user, the correct account id of the source bucket must be substituted in the policy. for more information, see  in the amazon simple storage service developer guide. to grant amazon s3 permission to encrypt using a customer managed aws key management service (aws kms) customer master key (cmk), you must use a key policy. to update your key policy so that you can use an aws kms customer managed cmk to encrypt the inventory file, follow the steps below. to grant permissions to encrypt using your aws kms cmk using the aws account that owns the customer managed cmk, sign into the aws management console. open the aws kms console at . to change the aws region, use the region selector in the upper-right corner of the page. in the left navigation pane, choose customer managed keys. under customer managed keys, choose the customer managed cmk that you want to use to encrypt the inventory file. under key policy, choose switch to policy view. to update the key policy, choose edit. under edit key policy, add the following key policy to the existing key policy. choose save changes. for more information about creating customer managed cmks aws kms and using key policies, see the following links in the aws key management service developer guide: more info  
for more detailed information about configuring a redirect in amazon s3, see  in the amazon simple storage service developer guide. you can redirect all requests for a website endpoint for a bucket to another host. if you redirect all requests, any request made to the website endpoint is redirected to the specified host name.  for example, if your root domain is , and you want to serve requests for both  and , you can create two buckets named  and . then, maintain the content in the  bucket, and configure the other  bucket to redirect all requests to the  bucket. for more information, see . to redirect requests for a bucket website endpoint open the amazon s3 console at . choose the name of the bucket that you have configured as a static website (for example, ). choose properties. choose static website hosting. choose redirect requests.  in the target bucket or domain box, enter the bucket or domain that you want to redirect to. for example, if you are redirecting to a root domain address, you would enter example.com. in the protocol box, enter the protocol for the redirected requests (http or https). if you do not specify a protocol, the protocol of the original request is used. choose save. 
in the amazon s3 console, you can move an object to a bucket or a folder.  to move an object sign in to the aws management console and open the amazon s3 console at . in the buckets list, choose your bucket name. select the check box beside the object that you want to move. choose actions, and choose move. choose the move destination: if you want to move your object to a bucket, select the bucket.if you want to move your object to a folder in a bucket, choose the bucket name, and then choose the folder.under review, confirm the move details, and choose move. amazon s3 moves your object to the destination. 
this section explains how to use the amazon simple storage service (amazon s3) console to manage access permissions for an amazon s3 object by using access control lists (acls). acls are resource-based access policies that grant access permissions to buckets and objects. for more information about managing access permissions with resource-based policies, see  in the amazon simple storage service developer guide. bucket and object permissions are independent of each other. an object does not inherit the permissions from its bucket. for example, if you create a bucket and grant write access to a user, you can't access that user’s objects unless the user explicitly grants you access. you can grant permissions to other aws accounts or predefined groups. the user or group that you grant permissions to is called the grantee. by default, the owner, which is the aws account that created the bucket, has full permissions. each permission you grant for a user or a group adds an entry in the acl that is associated with the object. the acl lists grants, which identify the grantee and the permission granted. for more information about acls, see  in the amazon simple storage service developer guide. to set permissions for an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object for which you want to set permissions. choose permissions. you can manage object access permissions for the following:  1.  access for object owner  access for other aws accounts to grant permissions to an aws user from a different aws account, under access for other aws accounts, choose add account. in the enter an id field, enter the canonical id of the aws user that you want to grant object permissions to. for information about finding a canonical id, see  in the amazon web services general reference. you can add as many as 99 users. select the check boxes for the permissions that you want to grant to the user, and then choose save. to display information about the permissions, choose the help icons. public access  warninguse caution when granting the everyone group anonymous access to your amazon s3 objects. when you grant access to this group, anyone in the world can access your object. if you need to grant access to everyone, we highly recommend that you only grant permissions to read objects.we highly recommend that you do not grant the everyone group write object permissions. doing so allows anyone to overwrite the acl permissions for the object. you can also set object permissions when you upload objects. for more information about setting permissions when uploading objects, see .  
this section describes how to use the amazon s3 console to create a folder. to create a folder sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to create a folder in. choose create folder. enter a name for the folder (for example, favorite-pics). choose the encryption setting for the folder object, and then choose save. if you don't want to use encryption with your folder, choose none.  if you want to encrypt your object using keys that are managed by amazon s3, follow these steps: choose aes-256.  for more information about using amazon s3 server-side encryption to encrypt your data, see  in the amazon simple storage service developer guide. if you want to encrypt your object using aws kms, follow these steps: choose aws-kms. choose an aws kms customer master key (cmk). the list shows  that you have created and your aws managed cmk for amazon s3. for more information about creating a customer managed aws kms cmk, see  in the aws key management service developer guide.  importantto encrypt objects in the bucket, you can use only the cmks that are enabled in the same aws region as the bucket. amazon s3 only supports symmetric cmks and does not support asymmetric cmks. for more information, see .the amazon s3 console only lists 100 kms cmks per region. if you have more than 100 cmks in the same region, you can see only the first 100 cmks in the s3 console. to use anaws kms kms cmk that is not listed in the console, choose custom kms arn, and enter the kms cmk arn. choose save. 
to upload your data (photos, videos, documents etc.) to amazon s3, you must first create an s3 bucket in one of the aws regions. you can then upload an unlimited number of data objects to the bucket. the data that you store in amazon s3 consists of objects. every object resides within a bucket that you create in a specific aws region. every object that you store in amazon s3 resides in a bucket.  objects stored in a region never leave the region unless you explicitly transfer them to another region. for example, objects stored in the europe (ireland) region never leave it. the objects stored in an aws region physically remain in that region. amazon s3 does not keep copies of objects or move them to any other region. however, you can access the objects from anywhere, as long as you have necessary permissions to do so. before you can upload an object into amazon s3, you must have write permissions to a bucket. objects can be any file type: images, backups, data, movies, etc. you can have an unlimited number of objects in a bucket. the maximum size of file you can upload by using the amazon s3 console is 160 gb. to upload a file larger than 160 gb, use the aws cli, aws sdk, or amazon s3 rest api. for more information, see  in the amazon simple storage service developer guide. the following topics explain how to use the amazon s3 console to upload, delete, and manage objects. noteif you rename an object, or change any of the properties; storage class,  enryption, metadata, a new object is created to replace the old one. if s3 versioning is enabled, a new version of the object is created, and the existing object becomes an older version. the role that changes the property also becomes the owner of the new object or (object version). topics 
this section explains how to use the console to view the properties of an object. to view the properties of an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object you want to view the properties for. choose properties. on the properties page, you can configure the following properties for the object. noteif you change the properties; storage class,  enryption, metadata, a new object is created to replace the old one. if s3 versioning is enabled, a new version of the object is created, and the existing object becomes an older version. the role that changes the property also becomes the owner of the new object or (object version). storage class – each object in amazon s3 has a storage class associated with it. the storage class that you choose to use depends on how frequently you access the object. the default storage class for s3 objects is standard. you choose which storage class to use when you upload an object. for more information about storage classes, see  in the amazon simple storage service developer guide. to change the storage class after you upload an object, choose storage class. choose the storage class that you want, and then choose save. encryption – you can encrypt your s3 objects. for more information, see .  metadata – each object in amazon s3 has a set of name-value pairs that represents its metadata. for information on adding metadata to an s3 object, see . tags – you can add tags to an s3 object. for more information, see . 
amazon s3 block public access prevents the application of any settings that allow public access to data within s3 buckets. this section describes how to edit block public access settings for all the s3 buckets in your aws account. for information about blocking public using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. to edit block public access settings for all the s3 buckets in an aws account sign in to the aws management console and open the amazon s3 console at . choose block public access (account settings). choose edit to change the block public access settings for all the buckets in your aws account. choose the settings that you want to change, and then choose save. when you're asked for confirmation, enter confirm. then choose confirm to save your changes. 
 you can use amazon s3 access points to manage access to your s3 objects. amazon s3 access points are named network endpoints that are attached to buckets that you can use to perform s3 object operations, such as uploading and retrieving objects. a bucket can have up to 1,000 access points attached, and each access point enforces distinct permissions and network controls to give you fine-grained control over access to your s3 objects.  for more information about amazon s3 access points, see  in the amazon simple storage service developer guide. the following topics explain how to use the s3 management console to create, manage, and use amazon s3 access points. topics 
you can host a static website on amazon s3. on a static website, individual webpages include static content. a static website might also contain client-side scripts. by contrast, a dynamic website relies on server-side processing, including server-side scripts such as php, jsp, or asp.net. amazon s3 does not support server-side scripting. you can use the following quick procedures to configure an s3 bucket for static website hosting in the amazon s3 console. for more information and detailed walkthroughs, see  in the amazon simple storage service developer guide. topics sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to enable static website hosting for. choose properties. choose static website hosting. choose use this bucket to host a website. in index document, enter the name of the index document, typically .  when you configure a bucket for website hosting, you must specify an index document. amazon s3 returns this index document when requests are made to the root domain or any of the subfolders. for more information, see  in the amazon simple storage service developer guide. (optional) if you want to provide your own custom error document for 4xx class errors, in error document, enter the custom error document filename.  if you do not specify a custom error document and an error occurs, amazon s3 returns a default html error document. for more information, see  in the amazon simple storage service developer guide. (optional) if you want to specify advanced redirection rules, in the edit redirection rules box, enter xml to describe the rules. for example, you can conditionally route requests according to specific object key names or prefixes in the request. for more information, see  in the amazon simple storage service developer guide. choose save. upload the index document to your bucket. for step-by-step instructions on uploading an object to an s3 bucket, see .  upload other files for your website, including optional custom error documents. in the next section, you set the permissions required to access your bucket as a static website.. by default, amazon s3 blocks public access to your account and buckets. if you want to use a bucket to host a static website, you can use these steps to edit your block public access settings.  warningbefore you complete this step, review  to ensure that you understand and accept the risks involved with allowing public access. when you turn off block public access settings to make your bucket public, anyone on the internet can access your bucket. we recommend that you block all public access to your buckets. open the amazon s3 console at . choose the name of the bucket that you have configured as a static website. choose permissions. choose edit. clear block all public access, and choose save. warningbefore you complete this step, review  to ensure you understand and accept the risks involved with allowing public access. when you turn off block public access settings to make your bucket public, anyone on the internet can access your bucket. we recommend that you block all public access to your buckets. in the confirmation box, enter confirm, and then choose confirm. under s3 buckets, the access for your bucket updates to objects can be public. you can now add a bucket policy to make the objects in the bucket publicly readable. if the access still displays as bucket and objects not public, you might have to  for your account before adding a bucket policy. after you edit s3 block public access settings, you can add a bucket policy to grant public read access to your bucket. when you grant public read access, anyone on the internet can access your bucket. importantthe following policy is an example only and allows full access to the contents of your bucket. before you proceed with this step, review  to ensure that you understand the best practices for securing the files in your s3 bucket and risks involved in granting public access. under buckets, choose the name of your bucket. choose permissions. choose bucket policy. to grant public read access for your website, copy the following bucket policy, and paste it in the bucket policy editor. update the  to include your bucket name. in the preceding example bucket policy, example.com is the bucket name. to use this bucket policy with your own bucket, you must update this name to match your bucket name. choose save. a warning appears indicating that the bucket has public access. in bucket policy, a public label appears. if you see an error that says , confirm that the bucket name in the bucket policy matches your bucket name. for information about adding a bucket policy, see  if you get an error - access denied warning and the bucket policy editor does not allow you to save the bucket policy, check your account-level and bucket-level block public access settings to confirm that you allow public access to the bucket. once you configure your bucket as a static website and set permissions, you can access your website through an amazon s3 website endpoint. for more information, see  in the amazon simple storage service developer guide. for a complete list of amazon s3 website endpoints, see  in the amazon web services general reference. 
this section describes how to configure advanced s3 bucket property settings for object replication, event notification, and transfer acceleration. topics 
there are three types of amazon cloudwatch metrics for amazon s3: storage metrics are reported once per day and are provided to all customers at no additional cost.replication metrics are available 15 minutes after enabling a replication rule with s3 replication time control (s3 rtc). for more information, see request metrics are available at 1-minute intervals after some latency to process, and the metrics are billed at the standard cloudwatch rate. for more information about cloudwatch metrics for amazon s3, see  in the amazon simple storage service developer guide. to get request metrics, you must opt into them by configuring them on the aws management console or using the amazon s3 api. to configure request metrics on a bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the objects you want request metrics for. choose the management tab, and then choose metrics. choose requests. in the left pane, choose the edit icon next to the name of the bucket. select the request metrics check box. this also enables data transfer metrics. choose save. you have now created a metrics configuration for all the objects in an amazon s3 bucket. about 15 minutes after cloudwatch begins tracking these request metrics, you can see graphs for the metrics on the amazon s3 or cloudwatch console.  you can also define a filter so that the metrics are only collected and reported on a subset of objects in the bucket. for more information, see  
to upload your data (photos, videos, documents etc.) to amazon s3, you must first create an s3 bucket in one of the aws regions. you can then upload your data objects to the bucket. every object you store in amazon s3 resides in a bucket. you can use buckets to group related objects in the same way that you use a directory to group files in a file system.  amazon s3 creates buckets in the aws region that you specify. you can choose any aws region that is geographically close to you to optimize latency, minimize costs, or address regulatory requirements. for example, if you reside in europe, you might find it advantageous to create buckets in the europe (ireland) or europe (frankfurt) regions. for a list of amazon s3 aws regions, see  in the amazon web services general reference. you are not charged for creating a bucket. you are only charged for storing objects in the bucket and for transferring objects out of the bucket. for more information about pricing, see . amazon s3 bucket names are globally unique, regardless of the aws region in which you create the bucket. you specify the name at the time you create the bucket. for bucket naming guidelines, see  in the amazon simple storage service developer guide. the following topics explain how to use the amazon s3 console to create, delete, and manage buckets. topics 
amazon s3 provides a set of tools to help you manage your s3 batch operations jobs after you create them. for more information about managing s3 batch operations, see  in the amazon simple storage service developer guide.    in the amazon simple storage service developer guide in the amazon simple storage service developer guide in the amazon simple storage service developer guide
you can change the display language of the aws management console. several languages are supported. to change the console language sign in to the aws management console and open the amazon s3 console at . scroll down until you see the bar at the bottom of the window, and then choose the language on the left side of the bar. choose the language that you want from the menu. this will change the langauge for the entire aws management console. 
amazon simple storage service (amazon s3) transfer acceleration enables fast, easy, and secure file transfers between your client and an s3 bucket over long distances. this topic describes how to enable amazon s3 transfer acceleration for a bucket. for more information, see  in the amazon simple storage service developer guide. to enable transfer acceleration for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the buckets list, choose the name of the bucket that you want to enable transfer acceleration for. choose properties. under advanced settings, choose transfer acceleration. (optional) if you want to compare accelerated and non-accelerated upload speeds, to open the , choose the want to compare your data transfer speed by region?. the speed comparison tool uses multipart upload to transfer a file from your browser to various aws regions with and without amazon s3 transfer acceleration. you can compare the upload speed for direct uploads and transfer accelerated uploads by region.  choose enabled, and choose save. endpoint displays the transfer acceleration endpoint for your bucket. you use this endpoint to access accelerated data transfers to and from your bucket. if you suspend transfer acceleration, the accelerate endpoint no longer works. 
this section describes how to enable an aws cloudtrail trail to log data events for objects in an s3 bucket by using the amazon s3 console. cloudtrail supports logging amazon s3 object-level api operations such as , , and . these events are called data events. by default, cloudtrail trails don't log data events, but you can configure trails to log data events for s3 buckets that you specify, or to log data events for all the amazon s3 buckets in your aws account. for more information, see . cloudtrail does not populate data events in the cloudtrail event history. additionally, not all bucket-level actions are populated in the cloudtrail event history. for more information, see  to configure a trail to log data events for an s3 bucket, you can use either the aws cloudtrail console or the amazon s3 console. if you are configuring a trail to log data events for all the amazon s3 buckets in your aws account, it's easier to use the cloudtrail console. for information about using the cloudtrail console to configure a trail to log s3 data events, see  in the aws cloudtrail user guide.  importantadditional charges apply for data events. for more information, see .  the following procedure shows how to use the amazon s3 console to enable a cloudtrail trail to log data events for an s3 bucket. to enable cloudtrail data events logging for objects in an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket. choose properties. choose object-level logging. choose an existing cloudtrail trail in the drop-down menu.  the trail you select must be in the same aws region as your bucket, so the drop-down list contains only trails that are in the same region as the bucket or trails that were created for all regions.  if you need to create a trail, choose the cloudtrail console link to go to the cloudtrail console. for information about how to create trails in the cloudtrail console, see  in the aws cloudtrail user guide. under events, choose one of the following: read to specify that you want cloudtrail to log amazon s3 read apis such as . write to log amazon s3 write apis such as . read and write to log both read and write object apis.for a list of supported data events that cloudtrail logs for amazon s3 objects, see  in the amazon simple storage service developer guide. choose create to enable object-level logging for the bucket. to disable object-level logging for the bucket, you must go to the cloudtrail console and remove the bucket name from the trail's data events. noteif you use the cloudtrail console or the amazon s3 console to configure a trail to log data events for an s3 bucket, the amazon s3 console shows that object-level logging is enabled for the bucket.  for information about enabling object-level logging when you create an s3 bucket, see .  in the amazon simple storage service developer guide  in the aws cloudtrail user guide
this topic explains how to view the properties for an s3 bucket. to view the properties for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to view the properties for. choose properties. on the properties page, you can configure the following properties for the bucket. versioning – versioning enables you to keep multiple versions of an object in one bucket. by default, versioning is disabled for a new bucket. for information about enabling versioning, see . server access logging – server access logging provides detailed records for the requests that are made to your bucket. by default, amazon s3 does not collect server access logs. for information about enabling server access logging, see . static website hosting – you can host a static website on amazon s3. to enable static website hosting, choose static website hosting and then specify the settings you want to use. for more information, see . object-level logging – object-level logging records object-level api activity by using cloudtrail data events. for information about enabling object-level logging, see . tags – with aws cost allocation, you can use bucket tags to annotate billing for your use of a bucket. a tag is a key-value pair that represents a label that you assign to a bucket. to add tags, choose tags, and then choose add tag. for more information, see  in the amazon simple storage service developer guide.  transfer acceleration – amazon s3 transfer acceleration enables fast, easy, and secure transfers of files over long distances between your client and an s3 bucket. for information about enabling transfer acceleration, see . events – you can enable certain amazon s3 bucket events to send a notification message to a destination whenever the events occur. to enable events, choose events and then specify the settings you want to use. for more information, see .  requester pays – you can enable requester pays so that the requester (instead of the bucket owner) pays for requests and data transfers. for more information, see  in the amazon simple storage service developer guide.  
in the amazon s3 console, you can copy an object to a bucket or a folder. for more information, see  in the amazon simple storage service developer guide. to copy an object sign in to the aws management console and open the amazon s3 console at . in the buckets list, choose your bucket name. on the overview tab, select the check box beside the object that you want to copy. choose actions, and choose copy. choose the copy destination: if you want to copy your object to a bucket, select the bucket.if you want to copy your object to a folder in a bucket, choose the bucket name, and then choose the folder.under review, confirm the copy details, and choose copy. amazon s3 copies your object to the destination. 
amazon s3 block public access prevents the application of any settings that allow public access to data within s3 buckets. this section describes how to edit block public access settings for one or more s3 buckets. for information about blocking public access using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. topics follow these steps if you need to change the public access settings for a single s3 bucket. to edit the amazon s3 block public access settings for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose permissions. choose edit to change the public access settings for the bucket. for more information about the four amazon s3 block public access settings, see  in the amazon simple storage service developer guide. choose the setting that you want to change, and then choose save. when you're asked for confirmation, enter confirm. then choose confirm to save your changes. follow these steps if you need to change the public access settings for more than one s3 bucket. to edit the amazon s3 block public access settings for multiple s3 buckets sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the buckets that you want, and then choose edit public access settings. choose the setting that you want to change, and then choose save. when you're asked for confirmation, enter confirm. then choose confirm to save your changes. you can change amazon s3 block public access settings when you create a bucket. for more information, see .  
s3 batch operations performs large-scale batch operations on amazon s3 objects. you can use s3 batch operations to copy objects, set object tags or access control lists (acls), initiate object restores from amazon s3 glacier, or invoke an aws lambda function to perform custom actions using your objects. you can perform these operations on a custom list of objects, or you can use an amazon s3 inventory report to make generating even the largest lists of objects easy. s3 batch operations use the same amazon s3 apis that you already use, so you'll find the interface familiar. for information about performing s3 batch operations using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide.  the following topics explain how to use the amazon s3 console to configure and run batch operations. topics 
this section explains how to configure amazon s3 storage management tools. topics 
before you can enable event notifications for your bucket you must set up one of the following destination types: an amazon sns topicamazon simple notification service (amazon sns) is a web service that coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. you can use the amazon sns console to create an amazon sns topic that your notifications can be sent to. the amazon sns topic must be in the same region as your amazon s3 bucket. for information about creating an amazon sns topic, see  in the amazon simple notification service developer guide.before you can use the amazon sns topic that you create as an event notification destination, you need the following:   the amazon resource name (arn) for the amazon sns topica valid amazon sns topic subscription (the topic subscribers are notified when a message is published to your amazon sns topic)a permissions policy that you set up in the amazon sns console (as shown in the following example) an amazon sqs queueyou can use the amazon sqs console to create an amazon sqs queue that your notifications can be sent to. the amazon sqs queue must be in the same region as your amazon s3 bucket. for information about creating an amazon sqs queue, see  in the amazon simple queue service developer guide.before you can use the amazon sqs queue as an event notification destination, you need the following:   the amazon resource name (arn) for the amazon sqs topica permissions policy that you set up in the amazon sqs console (as shown in the following example) a lambda functionyou can use the aws lambda console to create a lambda function. the lambda function must be in the same region as your s3 bucket. for information about creating a lambda function, see the .before you can use the lambda function as an event notification destination, you must have the name or the arn of a lambda function to set up the lambda function as a event notification destination.  warningif your notification ends up writing to the bucket that triggers the notification, this could cause an execution loop. for example, if the bucket triggers a lambda function each time an object is uploaded, and the function uploads an object to the bucket, then the function indirectly triggers itself. to avoid this, use two buckets, or configure the trigger to only apply to a prefix used for incoming objects.for more information and an example of using amazon s3 notifications with aws lambda, see  in the aws lambda developer guide.  
replication is the automatic, asynchronous copying of objects across buckets in the same or different aws regions. it replicates newly created objects and object updates from a source bucket to a specified destination bucket.  you use the amazon s3 console to add replication rules to the source bucket. replication rules define the source bucket objects to replicate and the destination bucket where the replicated objects are stored. for more information about replication, see  in the amazon simple storage service developer guide. you can manage replication rules on the replication page. you can add, view, enable, disable, delete, and change the priority of the replication rules. for information about adding replication rules to a bucket, see . to manage the replication rules for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose management, and then choose replication. you change the replication rules in the following ways. to change settings that affect all the replication rules in the bucket, choose edit global settings. you can change the destination bucket, and the iam role. if needed, you can copy the required bucket policy for cross-account destination buckets. to change a replication rule, select the rule and choose edit, which starts the replication wizard to help you make the change. for information about using the wizard, see .to enable or disable a replication rule, select the rule, choose more, and in the drop-down list, choose enable rule or disable rule. you can also disable, enable, or delete all the rules in the bucket from the more drop-down list.to change the rule priorities, choose edit priorities. you can then change the priority for each rule under the priority column heading. choose save to save your changes. you set rule priorities to avoid conflicts caused by objects that are included in the scope of more than one rule. in the case of overlapping rules, amazon s3 uses the rule priority to determine which rule to apply. the higher the number, the higher the priority. for more information about rule priority, see  in the amazon simple storage service developer guide.  in the amazon simple storage service developer guide
for the latest aws terminology, see the  in the aws general reference. 
this section explains how to use the amazon simple storage service (amazon s3) console to add a new bucket policy or edit an existing bucket policy. a bucket policy is a resource-based aws identity and access management (iam) policy. you add a bucket policy to a bucket to grant other aws accounts or iam users access permissions for the bucket and the objects in it. object permissions apply only to the objects that the bucket owner creates. for more information about bucket policies, see  in the amazon simple storage service developer guide.  for examples of amazon s3 bucket policies, see  in the amazon simple storage service developer guide.  to create or edit a bucket policy sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to create a bucket policy for or whose bucket policy you want to edit. choose permissions, and then choose bucket policy. in the bucket policy editor text box, type or copy and paste a new bucket policy, or edit an existing policy. the bucket policy is a json file. the text you type in the editor must be valid json. choose save. noteamazon s3 displays the amazon resource name (arn) for the bucket next to the bucket policy editor title. for more information about arns, see  in the amazon web services general reference.directly below the bucket policy editor text box is a link to the policy generator, which you can use to create a bucket policy. 
before you can upload data to amazon s3, you must create a bucket in one of the aws regions to store your data. after you create a bucket, you can upload an unlimited number of data objects to the bucket.  the aws account that creates the bucket owns it. by default, you can create up to 100 buckets in each of your aws accounts. if you need additional buckets, you can increase your account bucket quota to a maximum of 1,000 buckets by submitting a service quota increase. for information about how to increase your bucket quota, see  in the aws general reference.  buckets have configuration properties, including geographical region, access settings for the objects in the bucket, and other metadata.  to create a bucket sign in to the aws management console and open the amazon s3 console at . choose create bucket. the create bucket wizard opens. in bucket name, enter a dns-compliant name for your bucket. the bucket name must: be unique across all of amazon s3.be between 3 and 63 characters long.not contain uppercase characters.start with a lowercase letter or number.after you create the bucket, you can't change its name. for information about naming buckets, see  in the amazon simple storage service developer guide. importantavoid including sensitive information, such as account numbers, in the bucket name. the bucket name is visible in the urls that point to the objects in the bucket. in region, choose the aws region where you want the bucket to reside.  choose a region close to you to minimize latency and costs and address regulatory requirements. objects stored in a region never leave that region unless you explicitly transfer them to another region. for a list of amazon s3 aws regions, see  in the amazon web services general reference. in bucket settings for block public access, choose the block public access settings that you want to apply to the bucket.  we recommend that you leave all settings enabled unless you know you need to turn one or more of them off for your use case, such as to host a public website. block public access settings that you enable for the bucket will also be enabled for all access points that you create on the bucket. for more information about blocking public access, see  in the amazon simple storage service developer guide. (optional) if you want to enable s3 object lock: choose advanced settings, and read the message that appears. importantyou can only enable s3 object lock for a bucket when you create it. if you enable object lock for the bucket, you can't disable it later. enabling object lock also enables versioning for the bucket. after you enable object lock for the bucket, you must configure the object lock settings before any objects in the bucket will be protected. for more information about configuring protection for objects, see . if you want to enable object lock, enter enable in the text box and choose confirm. for more information about the s3 object lock feature, see  in the amazon simple storage service developer guide. choose create bucket. 
this section explains how to use the amazon s3 console to delete folders from an s3 bucket.  for information about amazon s3 features and pricing, see . to delete folders from an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to delete folders from. in the name list, select the check box next to the folders and objects that you want to delete, choose more, and then choose delete. in the delete objects dialog box, verify that the names of the folders you selected for deletion are listed and then choose delete. 
latest documentation update: march 27, 2019 the following table describes the important changes in each release of the amazon simple storage service console user guide from june 19, 2018, onward. for notification about updates to this documentation, you can subscribe to an rss feed. the following table describes the important changes in each release of the amazon simple storage service console user guide before june 19, 2018.  
amazon s3 block public access prevents the application of any settings that allow public access to data within s3 buckets. you can configure block public access settings for an individual s3 bucket or for all the buckets in your account. for information about blocking public access using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. the following topics explain how to use the amazon s3 console to configure block public access settings:  the following sections explain viewing bucket access status and searching by access types. the list buckets view shows whether your bucket is publicly accessible. amazon s3 labels the permissions for a bucket as follows: public – everyone has access to one or more of the following: list objects, write objects, read and write permissions. objects can be public – the bucket is not public, but anyone with the appropriate permissions can grant public access to objects. buckets and objects not public – the bucket and objects do not have any public access.only authorized users of this account – access is isolated to iam users and roles in this account and aws service principals because there is a policy that grants public access.the access column shows the access status of the listed buckets.  you can also filter bucket searches by access type. choose an access type from the drop-down list that is next to the search for buckets bar.   
this section explains how to create an amazon s3 access point using the aws management console. for information about creating access points using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. an access point is associated with exactly one amazon s3 bucket. before you begin, make sure that you have created a bucket that you want to use with this access point. for more information about creating buckets, see . to create an access point sign in to the aws management console and open the amazon s3 console at . in the s3 buckets section, select the bucket that you want to attach this access point to. on the bucket detail page, choose the access points tab. choose create access point. enter your desired name for the access point in the access point name field. choose a network access type. if you choose virtual private cloud (vpc), enter the vpc id that you want to use with the access point. for more information about network access type for access points, see  in the amazon simple storage service developer guide. select the block public access settings that you want to apply to the access point. all block public access settings are enabled by default for new access points, and we recommend that you leave all settings enabled unless you know you have a specific need to disable any of them. amazon s3 currently doesn't support changing an access point's block public access settings after the access point has been created. for more information about using amazon s3 block public access with access points, see  in the amazon simple storage service developer guide. (optional) specify the access point policy. the console automatically displays the amazon resource name (arn) for the access point, which you can use in the policy. choose create access point. 
this section explains how to use the amazon s3 console to delete objects. because all objects in your s3 bucket incur storage costs, you should delete objects that you no longer need. if you are collecting log files, for example, it's a good idea to delete them when they're no longer needed. you can set up a lifecycle rule to automatically delete objects such as log files. for information about amazon s3 features and pricing, see . to delete objects from an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to delete an object from. you can delete objects from an s3 bucket in any of the following ways: in the name list, select the check box next to the objects and folders that you want to delete, choose actions, and then choose delete from the drop-down menu. in the delete objects dialog box, verify that the name(s) of the object(s) and/or folder(s) you selected for deletion are listed and then choose delete. or, choose the name of the object that you want to delete, choose latest version, and then choose the trash can icon.
by using the amazon s3 analytics storage class analysis tool, you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. storage class analysis observes data access patterns to help you determine when to transition less frequently accessed standard storage to the standard_ia (ia, for infrequent access) storage class. for more information about standard_ia, see the  and  in the amazon simple storage service developer guide. importantstorage class analysis does not give recommendations for transitions to the onezone_ia or s3 glacier storage classes. for more information about analytics, see  in the amazon simple storage service developer guide. to configure storage class analysis sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket for which you want to configure storage class analysis. choose the management tab, and then choose analytics. choose add. type a name for the filter. if you want to analyze the whole bucket, leave the prefix / tags field empty. in the prefix / tags  field, type text for the prefix or tag for the objects that you want to analyze, or choose from the dropdown list that appears when you start typing. if you chose tag, enter a value for the tag. you can enter one prefix and multiple tags. optionally, you can choose export data to export analysis reports to a comma-separated values (.csv) flat file. choose a destination bucket where the file can be stored. you can type a prefix for the destination bucket. the destination bucket must be in the same aws region as the bucket for which you are setting up the analysis. the destination bucket can be in a different aws account. choose save. amazon s3 creates a bucket policy on the destination bucket that grants amazon s3 write permission. this allow it to write the export data to the bucket.  if an error occurs when you try to create the bucket policy, you'll be given instructions on how to fix it. for example, if you chose a destination bucket in another aws account and do not have permissions to read and write to the bucket policy, you'll see the following message. you must have the destination bucket owner add the displayed bucket policy to the destination bucket. if the policy is not added to the destination bucket you won’t get the export data because amazon s3 doesn’t have permission to write to the destination bucket. if the source bucket is owned by a different account than that of the current user, then the correct account id of the source bucket must be substituted in the policy.  for information about the exported data and how the filter works, see  in the amazon simple storage service developer guide. more info  
there are three types of amazon cloudwatch metrics for amazon s3: storage metrics, request metrics, and replication metrics. replication metrics are available 15 minutes after a replication rule with s3 replication time control (s3 rtc) has been enabled. replication metrics are billed at the standard amazon cloudwatch rate. they are turned on automatically when you enable replication with s3 rtc using the aws management console or the amazon s3 api. replication metrics track the rule ids of the replication configuration. a replication rule id can be specific to a prefix, a tag, or a combination of both. for more information about s3 replication time control (s3 rtc), see  in the amazon simple storage service developer guide. for more information about cloudwatch metrics for amazon s3, see  in the amazon simple storage service developer guide. to view replication metrics sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the objects you want replication metrics for. choose the management tab, and then choose metrics. choose replication.   in the rule ids list in the left pane, select the rule ids that you want. if you have several rule ids to choose from, you can search for the ids that you want. after choosing the rule ids that you want, choose display graphs below the rule ids selection box. you can then view the replication metrics replication latency (in seconds), operations pending replication, and bytes pending replication for the rules that you selected. amazon cloudwatch begins reporting replication metrics 15 minutes after you enable s3 rtc on the respective replication rule. you can view replication metrics on the amazon s3 or cloudwatch console. for information, see  in the amazon simple storage service developer guide. 
amazon s3 has a flat structure instead of a hierarchy like you would typically see in a file system. however, for the sake of organizational simplicity, the amazon s3 console supports a folder concept as a way to group objects. in amazon s3, the folder is a naming prefix for an object or group of objects. for more information, see  we recommend blocking all public access to your amazon s3 folders and buckets unless you specifically require a public folder or bucket. when you make a folder public, anyone on the internet can view all the objects that are grouped in that folder. in the amazon s3 console, you can make a folder public. you can also make a folder public by creating a bucket policy that limits access by prefix. for more information, see .  warningafter you make a folder public in the amazon s3 console, you can't make it private again. instead, you must set permissions on each individual object in the public folder so that the objects have no public access. for more information, see  
access analyzer for s3 alerts you to s3 buckets that are configured to allow access to anyone on the internet or other aws accounts, including aws accounts outside of your organization. for each public or shared bucket, you receive findings into the source and level of public or shared access. for example, access analyzer for s3 might show that a bucket has read or write access provided through a bucket access control list (acl), a bucket policy, or an access point policy. armed with this knowledge, you can take immediate and precise corrective action to restore your bucket access to what you intended.  when reviewing an at-risk bucket in access analyzer for s3, you can block all public access to the bucket with a single click. we recommend that you block all access to your buckets unless you require public access to support a specific use case. before you block all public access, ensure that your applications will continue to work correctly without public access. for more information, see  in the amazon simple storage service developer guide. you can also drill down into bucket-level permission settings to configure granular levels of access. for specific and verified use cases that require public access, such as static website hosting, public downloads, or cross-account sharing, you can acknowledge and record your intent for the bucket to remain public or shared by archiving the findings for the bucket. you can revisit and modify these bucket configurations at any time. you can also download your findings as a csv report for auditing purposes. access analyzer for s3 is available at no extra cost on the amazon s3 console. access analyzer for s3 is powered by aws identity and access management (iam) access analyzer. to use access analyzer for s3 in the amazon s3 console, you must visit the iam console and enable iam access analyzer on a per-region basis.  for more information about iam access analyzer, see  in the iam user guide. for more information about access analyzer for s3, review the following sections. importantaccess analyzer for s3 requires an account-level analyzer. to use access analyzer for s3, you must visit iam access analyzer and create an analyzer that has an account as the zone of trust. for more information, see  in iam user guide. when a bucket policy or bucket acl is added or modified, access analyzer generates and updates findings based on the change within 30 minutes. findings related to account level block public access settings may not be generated or updated for up to 6 hours after you change the settings. topics access analyzer for s3 provides findings for buckets that can be accessed outside your aws account. buckets that are listed under buckets with public access can be accessed by anyone on the internet. if access analyzer for s3 identifies public buckets, you also see a warning at the top of the page that shows you the number of public buckets in your region. buckets listed under buckets with access from other aws accounts — including third-party aws accounts are shared conditionally with other aws accounts, including accounts outside of your organization.  for each bucket, access analyzer for s3 provides the following information: bucket namediscovered by access analyzer ‐ when access analyzer for s3 discovered the public or shared bucket access.shared through ‐ how the bucket is shared—through a bucket policy, a bucket acl, or an access point policy. a bucket can be shared through both policies and acls. if you want to find and review the source for your bucket access, you can use the information in this column as a starting point for taking immediate and precise corrective action. status ‐ the status of the bucket finding. access analyzer for s3 displays findings for all public and shared buckets. active ‐ finding has not been reviewed. archived ‐ finding has been reviewed and confirmed as intended. all ‐ all findings for buckets that are public or shared with other aws accounts, including aws accounts outside of your organization.access level ‐ access permissions granted for the bucket:list ‐ list resources.read ‐ read but not edit resource contents and attributes.write ‐ create, delete, or modify resources.permissions ‐ grant or modify resource permissions.tagging ‐ update tags associated with the resource.to use access analyzer for s3, you must complete the following prerequisite steps. grant the required permissions. for more information, see  in the iam user guide. visit iam to create an account-level analyzer for each region where you want to use access analyzer. access analyzer for s3 requires an account-level analyzer. to use access analyzer for s3, you must create an analyzer that has an account as the zone of trust. for more information, see  in iam user guide. if you want to block all access to a bucket in a single click, you can use the block all public access button in access analyzer for s3. when you block all public access to a bucket, no public access is granted. we recommend that you block all public access to your buckets unless you require public access to support a specific and verified use case. before you block all public access, ensure that your applications will continue to work correctly without public access. if you don't want to block all public access to your bucket, you can edit your block public access settings on the amazon s3 console to configure granular levels of access to your buckets. for more information, see  in the amazon simple storage service developer guide. in rare events, access analyzer for s3 might report no findings for a bucket that an amazon s3 block public access evaluation reports as public. this happens because amazon s3 block public access reviews policies for current actions and any potential actions that might be added in the future, leading to a bucket becoming public. on the other hand, access analyzer for s3 only analyzes the current actions specified for the amazon s3 service in the evaluation of access status. to block all public access to a bucket using access analyzer for s3 sign in to the aws management console and open the amazon s3 console at . in the navigation pane on the left, under dashboards, choose access analyzer for s3. in access analyzer for s3, choose a bucket. choose block all public access. to confirm your intent to block all public access to the bucket, in block all public access (bucket settings), enter confirm. amazon s3 blocks all public access to your bucket. the status of the bucket finding updates to resolved, and the bucket disappears from the access analyzer for s3 listing. if you want to review resolved buckets, open iam access analyzer on the iam console. if you did not intend to grant access to the public or other aws accounts, including accounts outside of your organization, you can modify the bucket acl, bucket policy, or access point policy to remove the access to the bucket. the shared through column shows all sources of bucket access: bucket policy, bucket acl, and/or access point policy. to review and change a bucket policy, a bucket acl, or an access point policy open the amazon s3 console at . in the navigation pane, choose access analyzer for s3. to see whether public access or shared access is granted through a bucket policy, a bucket acl, or an access point policy, look in the shared through column. under bucket name, choose the name of the bucket with the bucket policy, bucket acl, or access point policy that you want to change or review. if you want to change or view a bucket acl: choose permissions. choose access control list. review your bucket acl, and make changes as required. for more information, see  if you want to change or review a bucket policy: choose permissions. choose bucket policy. review or change your bucket policy as required. for more information, see  if you want to review or change an access point policy: choose access points. choose the access point name. review or change access as required.  for more information, see . if you edit or remove a bucket acl, a bucket policy, or an access point policy to remove public or shared access, the status for the bucket findings updates to resolved. the resolved bucket findings disappear from the access analyzer for s3 listing, but you can view them in iam access analyzer. if a bucket grants access to the public or other aws accounts, including accounts outside of your organization, to support a specific use case (for example, a static website, public downloads, or cross-account sharing), you can archive the finding for the bucket. when you archive bucket findings, you acknowledge and record your intent for the bucket to remain public or shared. archived bucket findings remain in your access analyzer for s3 listing so that you always know which buckets are public or shared. to archive bucket findings in access analyzer for s3 open the amazon s3 console at . in the navigation pane, choose access analyzer for s3. in access analyzer for s3, choose an active bucket. to acknowledge your intent for this bucket to be accessed by the public or other aws accounts, including accounts outside of your organization, choose archive. enter confirm, and choose archive. after you archive findings, you can always revisit them and change their status back to active, indicating that the bucket requires another review.  to activate an archived bucket finding in access analyzer for s3 open the amazon s3 console at . in the navigation pane, choose access analyzer for s3. choose the archived bucket findings. choose mark as active. if you need to see more information about a bucket, you can open the bucket finding details in iam access analyzer on the iam console. to view finding details in access analyzer for s3 open the amazon s3 console at . in the navigation pane, choose access analyzer for s3. in access analyzer for s3, choose a bucket. choose view details. the finding details open in iam access analyzer on the iam console.  you can download your bucket findings as a csv report that you can use for auditing purposes. the report includes the same information that you see in access analyzer for s3 on the amazon s3 console. to download a report open the amazon s3 console at . in the navigation pane on the left, choose access analyzer for s3. in the region filter, choose the region. access analyzer for s3 updates to shows buckets for the chosen region. choose download report. a csv report is generated and saved to your computer. 
there are three types of amazon cloudwatch metrics for amazon s3: storage metrics, request metrics and replication. storage metrics are reported once per day and are provided to all customers at no additional cost. request metrics are available at 1 minute intervals after some latency to process, and metrics are billed at the standard cloudwatch rate. to get request metrics, you must opt into them by configuring them in the console or with the amazon s3 api. for more conceptual information about cloudwatch metrics for amazon s3, see  in the amazon simple storage service developer guide. to filter request metrics on a subset of objects in a bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that has the objects you want to get request metrics for. choose the management tab. and then choose metrics. choose requests. from filters in the left-side pane, choose add. provide a name for this metrics configuration. provide one or more prefixes or tags, separated by commas, in prefix /tags that you want to monitor. from the drop down, select whether the value you provided is a tag or a prefix. choose save. you have now created a metrics configuration for request metrics on a subset of the objects in an amazon s3 bucket. about 15 minutes after cloudwatch begins tracking these request metrics, you can see graphs for the metrics in both the amazon s3 or cloudwatch consoles. you can also request metrics at the bucket level. for information, see  
in amazon s3, buckets and objects are the primary resources, and objects are stored in buckets. amazon s3 has a flat structure instead of a hierarchy like you would see in a file system. however, for the sake of organizational simplicity, the amazon s3 console supports the folder concept as a means of grouping objects. amazon s3 does this by using a shared name prefix for objects (that is, objects have names that begin with a common string). object names are also referred to as key names. for example, you can create a folder on the console named  and store an object named  in it. the object is then stored with the key name , where  is the prefix. here are two more examples:  if you have three objects in your bucket—, , and —the console will show a folder named . if you open the folder in the console, you will see three objects: , , and .if you have an object named , the console will show you a folder named  containing the folder  and the object .topics you can have folders within folders, but not buckets within buckets. you can upload and copy objects directly into a folder. folders can be created, deleted, and made public, but they cannot be renamed. objects can be copied from one folder to another.  importantthe amazon s3 console treats all objects that have a forward slash ("/") character as the last (trailing) character in the key name as a folder, for example . you can't upload an object that has a key name with a trailing "/" character using the amazon s3 console. however, you can upload objects that are named with a trailing "/" with the amazon s3 api by using the aws cli, aws sdks, or rest api.an object that is named with a trailing "/" appears as a folder in the amazon s3 console. the amazon s3 console does not display the content and metadata for such an object. when you use the console to copy an object named with a trailing "/", a new folder is created in the destination location, but the object's data and metadata are not copied.  
you can enable certain amazon s3 bucket events to send a notification message to a destination whenever the events occur. this section explains how to use the amazon s3 console to enable event notifications. for information about using event notifications with the aws sdks and the amazon s3 rest apis, see  in the amazon simple storage service developer guide.  topics when you configure event notifications for a bucket, you must specify the type of events for which you want to receive notifications. for a complete list of event types, see  section in the amazon simple storage service developer guide.  in the amazon s3 console, you have the following options for configuring event notifications. you can choose a single option or multiple options. object creationobjectcreated (all) – receive a notification when an object is created in your bucket for any of the following object creation actions: put, post, copy, and multipart upload completed.put, post, copy, and multipart upload completed – receive a notification for one of these specific object creation actions.object deletion objectdelete (all) – receive a notification any time an object in your bucket is deleted.delete marker created – receive a notification when a delete marker is created for a versioned object.  for information about deleting versioned objects, see . for information about object versioning, see  and . object restoration from the s3 glacier storage class restore initiated – receive a notification for initiation of object restoration.restore completed – receive a notification for completion of object restoration.reduced redundancy storage (rrs) object lost eventsobject in rss lost – receive a notification that an object of the rrs storage class has been lostobjects eligible for replication using amazon s3 replication time controlreplication time missed threshold – receive a notification that an object failed to replicate.replication time completed after threshold – receive a notification that an object exceeded the 15-minute threshold for replication.replication time not tracked – receive a notification that an object replicated after the 15-minute threshold.replication time failed – receive a notification that an object that was eligible for replication is no longer being tracked by replication metrics.notewhen you delete the last object from a folder, amazon s3 can generate an object creation event. when there are multiple objects with the same prefix with a trailing slash (/) as part of their names, those objects are shown as being part of a folder in the amazon s3 console. the name of the folder is formed from the characters preceding the trailing slash (/).when you delete all the objects listed under that folder, no actual object is available to represent the empty folder. under such circumstances, the amazon s3 console creates a zero-byte object to represent that folder. if you enabled event notification for the creation of objects, the zero-byte object creation action that is taken by the console triggers an object creation event.the amazon s3 console displays a folder under the following circumstances:when a zero-byte object has a trailing slash (/) in its name. in this case, there is an actual amazon s3 object of 0 bytes that represents a folder. if the object has a slash (/) within its name. in this case, there isn't an actual object representing the folder. when you configure event notifications for a bucket, you also choose the notification destination. event notification messages can be sent to the following destinations: amazon simple notification service (amazon sns) topic – coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. for information about the amazon sns topic format, see .amazon simple queue service (amazon sqs) queue – offers reliable and scalable hosted queues for storing messages as they travel between computers. for information about amazon sqs, see  in the amazon simple queue service developer guide.aws lambda  – invoke a lambda function and provide the event message as an argument. when you create a lambda function, you package up and upload your custom code to aws lambda. aws lambda uses the aws infrastructure to run the code on your behalf. for information about using lambda with amazon s3, see  in the aws lambda developer guide.for more information about granting the amazon s3 service principal the permissions required to publish event notifications to a destination, see  in the amazon s3 developer guide. warningif your notification ends up writing to the bucket that triggers the notification, this could cause an execution loop. for example, if the bucket triggers a lambda function each time an object is uploaded, and the function uploads an object to the bucket, then the function indirectly triggers itself. to avoid this, use two buckets, or configure the trigger to only apply to a prefix used for incoming objects.for more information and an example of using amazon s3 notifications with aws lambda, see  in the aws lambda developer guide.  before you can enable event notifications for your bucket, you must set up one of the destination types. for more information, see  to enable and configure event notifications for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to enable events for. choose properties. under advanced settings, choose events. choose add notification. in name, enter a descriptive name for your event notification.  if you don't enter a name, a guid is generated and used for the name. under events, select one or more events.  for a listing of the event types, see . to filter event notifications by a prefix or suffix, enter a prefix or a suffix.  for example, you can set up a prefix filter so that you receive notifications only when files are added to a specific folder (for example, ). for more information, see . choose the event notification destination: sns topic, sqs queue, or lambda function.  for a description of the destinations, see . when you choose your send to destination, a box appears for you to enter your specific sns, sqs, or lambda function destination. in the example image below, the send to location is sns topic, and you can see a sns box for the sns topic name. in the box that appears, choose or enter the destination sns, sqs, or lambda function.  you can choose or enter the sns, sqs, or lambda function name, or you can choose to the destination amazon resource name (arn). the example screenshot below shows the add sns topic arn option. if you chose add arn, enter the sns topic, sqs queue, or lambda function arn. choose save.  amazon s3 sends a test message to the event notification destination. 
you can empty a bucket, which deletes all of the objects in the bucket without deleting the bucket. when you empty a bucket with versioning enabled, all versions of all the objects in the bucket are deleted. for more information, see  and  in the amazon simple storage service developer guide. to empty an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, select the option next to the name of the bucket that you want to empty and then choose empty. on the empty bucket page, confirm that you want to empty the bucket by entering the bucket name into the text field, and then choose empty. (optional) monitor the progress of the bucket emptying process on the empty bucket: status page. 
this topic explains how to use the aws management console to upload one or more files or entire folders to an amazon s3 bucket. before you can upload files and folders to an amazon s3 bucket, you need write permissions for the bucket. for more information about access permissions, see . for information about uploading files programmatically, see  in the amazon simple storage service developer guide.  when you upload a file to amazon s3, it is stored as an s3 object. objects consist of the file data and metadata that describes the object. you can have an unlimited number of objects in a bucket. you can upload any file type—images, backups, data, movies, etc.—into an s3 bucket. the maximum size of a file that you can upload by using the amazon s3 console is 160 gb. to upload a file larger than 160 gb, use the aws cli, aws sdk, or amazon s3 rest api. for more information, see  in the amazon simple storage service developer guide.  you can upload files by dragging and dropping or by pointing and clicking. to upload folders, you must drag and drop them. drag and drop functionality is supported only for the chrome and firefox browsers. for information about which chrome and firefox browser versions are supported, see .  when you upload a folder, amazon s3 uploads all of the files and subfolders from the specified folder to your bucket. it then assigns an object key name that is a combination of the uploaded file name and the folder name. for example, if you upload a folder called  that contains two files,  and , amazon s3 uploads the files and then assigns the corresponding key names,  and . the key names include the folder name as a prefix. the amazon s3 console displays only the part of the key name that follows the last “/”. for example, within an images folder the  and  objects are displayed as  and a . if you upload individual files and you have a folder open in the amazon s3 console, when amazon s3 uploads the files, it includes the name of the open folder as the prefix of the key names. for example, if you have a folder named  open in the amazon s3 console and you upload a file named , the key name is . however, the object is displayed in the console as  in the  folder. if you upload individual files and you do not have a folder open in the amazon s3 console, when amazon s3 uploads the files, it assigns only the file name as the key name. for example, if you upload a file named , the key name is . for more information on key names, see  in the amazon simple storage service developer guide. if you upload an object with a key name that already exists in a versioning-enabled bucket, amazon s3 creates another version of the object instead of replacing the existing object. for more information about versioning, see . topics if you are using the chrome or firefox browsers, you can choose the folders and files to upload, and then drag and drop them into the destination bucket. dragging and dropping is the only way that you can upload folders. to upload folders and files to an s3 bucket by using drag and drop sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to upload your folders or files to. in a window other than the console window, select the files and folders that you want to upload. then drag and drop your selections into the console window that lists the objects in the destination bucket. the files you chose are listed in the upload dialog box. in the upload dialog box, do one of the following:  drag and drop more files and folders to the console window that displays the upload dialog box. to add more files, you can also choose add more files. this option works only for files, not folders. to immediately upload the listed files and folders without granting or removing permissions for specific users or setting public permissions for all of the files that you're uploading, choose upload. for information about object access permissions, see .  to set permissions or properties for the files that you are uploading, choose next. on the set permissions page, under manage users you can change the permissions for the aws account owner. the owner refers to the aws account root user, and not an aws identity and access management (iam) user. for more information about the root user, see .  choose add account to grant access to another aws account. for more information about granting permissions to another aws account, see . under manage public permissions you can grant read access to your objects to the general public (everyone in the world), for all of the files that you're uploading. granting public read access is applicable to a small subset of use cases such as when buckets are used for websites. we recommend that you do not change the default setting of do not grant public read access to this object(s). you can always make changes to object permissions after you upload the object. for information about object access permissions, see .  when you're done configuring permissions, choose next. on the set properties page, choose the storage class and encryption method to use for the files that you are uploading. you can also add or modify metadata.  choose a storage class for the files you're uploading. for more information about storage classes, see  in the amazon simple storage service developer guide. choose the type of encryption for the files that you're uploading. if you don't want to encrypt them, choose none.  to encrypt the uploaded files using keys that are managed by amazon s3, choose amazon s3 master-key. for more information, see  in the amazon simple storage service developer guide. to encrypt the uploaded files using the aws key management service (aws kms), choose aws kms master-key. then choose a customer master key (cmk) from the list of aws kms cmks. noteto encrypt objects in a bucket, you can use only cmks that are available in the same aws region as the bucket.  you can give an external account the ability to use an object that is protected by an aws kms cmk. to do this, select custom kms arn from the list and enter the amazon resource name (arn) for the external account. administrators of an external account that have usage permissions to an object protected by your aws kms cmk can further restrict access by creating a resource-level iam policy.  for more information about creating an aws kms cmk, see  in the aws key management service developer guide. for more information about protecting data with aws kms, see  in the amazon simple storage service developer guide. metadata for amazon s3 objects is represented by a name-value (key-value) pair. there are two kinds of metadata: system-defined metadata and user-defined metadata. if you want to add amazon s3 system-defined metadata to all of the objects you are uploading, for header, select a header. you can select common http headers, such as content-type and content-disposition. type a value for the header, and then choose save. for a list of system-defined metadata and information about whether you can add the value, see  in the amazon simple storage service developer guide. any metadata starting with prefix  is treated as user-defined metadata. user-defined metadata is stored with the object, and is returned when you download the object.  to add user-defined metadata to all of the objects that you are uploading, type  plus a custom metadata name in the header field. type a value for the header, and then choose save. both the keys and their values must conform to us-ascii standards. user-defined metadata can be as large as 2 kb. for more information about user-defined metadata, see  in the amazon simple storage service developer guide. object tagging gives you a way to categorize storage. each tag is a key-value pair. key and tag values are case sensitive. you can have up to 10 tags per object.  to add tags to all of the objects that you are uploading, type a tag name in the key field. type a value for the tag, and then choose save. a tag key can be up to 128 unicode characters in length and tag values can be up to 255 unicode characters in length. for more information about object tags, see  in the amazon simple storage service developer guide. choose next. on the upload review page, verify that your settings are correct, and then choose upload. to make changes, choose previous. to see the progress of the upload, choose in progress at the bottom of the browser window. to see a history of your uploads and other operations, choose success. this procedure explains how to upload files into an s3 bucket by choosing upload. to upload files to an s3 bucket by pointing and clicking sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to upload your files to. choose upload. in the upload dialog box, choose add files. choose one or more files to upload, and then choose open. after you see the files that you chose listed in the upload dialog box, do one of the following:  to add more files, choose add more files. to immediately upload the listed files, choose upload.  to set permissions or properties for the files that you are uploading, choose next. to set permissions and properties, start with step 5 of . .
you can delete an empty bucket, and when you're using the aws management console, you can delete a bucket that contains objects. if you delete a bucket that contains objects, all the objects in the bucket are permanently deleted.  when you delete a bucket with versioning enabled, all versions of all the objects in the bucket are permanently deleted. for more information about versioning, see  in the amazon simple storage service developer guide. before deleting a bucket, consider the following: bucket names are unique. if you delete a bucket, another aws user can use the name. when you delete a bucket that contains objects, all the objects in the bucket are permanently deleted, including objects that transitioned to the  storage class.if the bucket hosts a static website, and you created and configured an amazon route 53 hosted zone as described in : you must clean up the route 53 hosted zone settings that are related to the bucket as described in .if the bucket receives log data from elastic load balancing (elb): we recommend that you stop the delivery of elb logs to the bucket before deleting it. after you delete the bucket, if another user creates a bucket using the same name, your log data could potentially be delivered to that bucket. for information about elb access logs, see  in the user guide for classic load balancers and  in the user guide for application load balancers.importantif you want to continue to use the same bucket name, don't delete the bucket. we recommend that you empty the bucket and keep it. after a bucket is deleted, the name becomes available to reuse, but the name might not be available for you to reuse for various reasons. for example, it might take some time before the name can be reused, and some other account could create a bucket with that name before you do. to delete an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the buckets list, select the option next to the name of the bucket that you want to delete and then choose delete at the top of the page. on the delete bucket page, confirm that you want to delete the bucket by entering the bucket name into the text field, and then choose delete bucket. noteif the bucket contains any objects, empty the bucket before deleting it by selecting the empty bucket configuration link in the this bucket is not empty error alert and following the instructions on the empty bucket page. then return to the delete bucket page and delete the bucket. 
replication is the automatic, asynchronous copying of objects across buckets in the same or different aws regions. replication copies newly created objects and object updates from a source bucket to a destination bucket. for more information about replication concepts and how to use replication with the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide.  replication requires versioning to be enabled on both the source and destination buckets. to review the full list of requirements, see  in the amazon simple storage service developer guide. for more information about versioning, see  the object replicas in the destination bucket are exact replicas of the objects in the source bucket. they have the same key names and the same metadata—for example, creation time, owner, user-defined metadata, version id, access control list (acl), and storage class. optionally, you can explicitly specify a different storage class for object replicas. and regardless of who owns the source bucket or the source object, you can choose to change replica ownership to the aws account that owns the destination bucket. for more information, see  in the amazon simple storage service developer guide.  you can use s3 replication time control (s3 rtc) to replicate your data in the same aws region or across different aws regions in a predictable timeframe. s3 rtc replicates 99.99 percent of new objects stored in amazon s3 within 15 minutes and most objects within seconds. for more information, see  in the amazon simple storage service developer guide. note about replication and lifecycle rulesmetadata for an object remains identical between original objects and replica objects. lifecycle rules abide by the creation time of the original object, and not by when the replicated object becomes available in the destination bucket. however, lifecycle does not act on objects that are pending replication until replication is complete. you use the amazon s3 console to add replication rules to the source bucket. replication rules define which source bucket objects to replicate and the destination bucket where the replicated objects are stored. you can create a rule to replicate all the objects in a bucket or a subset of objects with a specific key name prefix, one or more object tags, or both. a destination bucket can be in the same aws account as the source bucket, or it can be in a different account. if you specify an object version id to delete, amazon s3 deletes that object version in the source bucket. but it doesn't replicate the deletion in the destination bucket. in other words, it doesn't delete the same object version from the destination bucket. this protects data from malicious deletions. if the destination bucket is in a different account from the source bucket, you must add a bucket policy to the destination bucket to grant the owner of the source bucket account permission to replicate objects in the destination bucket. the amazon s3 console builds this required bucket policy for you to copy and add to the destination bucket in the other account.  when you add a replication rule to a bucket, the rule is enabled by default, so it starts working as soon as you save it.  topics follow these steps to configure a replication rule when the destination bucket is in the same aws account as the source bucket. sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose management, choose replication, and then choose add rule. in the replication rule wizard, under set source, you have the following options for setting the replication source: to replicate the whole bucket, choose entire bucket bucket-name. to replicate all objects that have the same prefix (for example, all objects that have names that begin with the string ), choose prefix or tags. enter a prefix in the box, choose the prefix from the drop-down list, and then press enter. if you enter a prefix that is the name of a folder, you must use / (forward slash) as the last character (for example, ). for more information about prefixes, see  in the amazon simple storage service developer guide.to replicate all objects with one or more object tags, enter a tag in the box, choose the tag from the drop-down list, and then press enter. enter a tag value and then press enter. repeat the procedure to add another tag. you can combine a prefix and tags. for more information about object tags, see  in the amazon simple storage service developer guide.the new schema supports prefix and tag filtering and the prioritization of rules. for more information about the new schema, see  in the amazon simple storage service developer guide. the developer guide describes the xml used with the amazon s3 api that works behind the user interface. in the developer guide, the new schema is described as replication configuration xml v2. to replicate objects in the source bucket that are encrypted with aws key management service (aws kms), under replication criteria, select replicate objects encrypted with aws kms. under choose one or more keys for decrypting source objects are the source aws kms customer master keys (cmks) that you allow replication to use. all source cmks are included by default. you can choose to narrow the cmk selection.  objects encrypted by aws kms cmks that you do not select are not replicated. a cmk or a group of cmks is chosen for you, but you can choose the cmks if you want. for information about using aws kms with replication, see  in the amazon simple storage service developer guide. importantwhen you replicate objects that are encrypted with aws kms, the aws kms request rate doubles in the source region and increases in the destination region by the same amount. these increased call rates to aws kms are due to the way that data is re-encrypted using the customer master key (cmk) that you define for the replication destination region. aws kms has a request rate limit that is per calling account per region. for information about the limit defaults, see  in the aws key management service developer guide.if your current amazon s3 put object request rate during replication is more than half the default aws kms rate limit for your account, we recommend that you request an increase to your aws kms request rate limit. to request an increase, create a case in the aws support center at . for example, suppose that your current put object request rate is 1,000 requests per second and you use aws kms to encrypt your objects. in this case, we recommend that you ask aws support to increase your aws kms rate limit to 2,500 requests per second, in both your source and destination regions (if different), to ensure that there is no throttling by aws kms.to see your put object request rate in the source bucket, view  in the amazon cloudwatch request metrics for amazon s3. for information about viewing cloudwatch metrics, see  choose next. to choose a destination bucket from the account that you're currently using, on the set destination page, under destination bucket, choose buckets in this account. enter the name of the destination bucket for the replication, or choose a name in the drop-down list. if you want to choose a destination bucket from a different aws account, see . if versioning is not enabled on the destination bucket, you get a warning message that contains an enable versioning button. choose this button to enable versioning on the bucket. if you chose to replicate objects encrypted with aws kms, under destination encryption settings, enter the amazon resource name (arn) of the aws kms cmk to use to encrypt the replicas in the destination bucket. you can find the arn for your aws kms cmk in the iam console, under encryption keys.  or, you can choose a cmk name from the drop-down list. for more information about creating an aws kms cmk, see  in the aws key management service developer guide. importantthe amazon s3 console lists only 100 aws kms cmks per aws region. if you have more than 100 cmks in the same region, you can see only the first 100 cmks in the s3 console. to use a kms cmk that is not listed in the console, choose custom kms arn, and enter the kms cmk arn. if you want to replicate your data into a specific storage class in the destination bucket, on the set destination page, under destination options, select change the storage class for the replicated object(s). then choose the storage class that you want to use for the replicated objects in the destination bucket. if you don't select this option, the storage class for replicated objects is the same class as the original objects.  similarly, if you want to change object ownership in the destination bucket, choose change object ownership to the destination bucket owner. for more information about this option, see .  if you want to enable s3 replication time control (s3 rtc) in your replication configuration, select replication time control. notewhen you use s3 rtc, additional per-gb data transfer fees and cloudwatch metrics fees apply. choose next. set up an aws identity and access management (iam) role that amazon s3 can assume to replicate objects on your behalf. to set up an iam role, on the configure options page, under select role, do one of the following: we highly recommend that you choose create new role to have amazon s3 create a new iam role for you. when you save the rule, a new policy is generated for the iam role that matches the source and destination buckets that you choose. the name of the generated role is based on the bucket names and uses the following naming convention: replication_role_for_source-bucket_to_destination-bucket.you can choose to use an existing iam role. if you do, you must choose a role that grants amazon s3 the necessary permissions for replication. replication fails if this role does not grant amazon s3 sufficient permissions to follow your replication rule.  importantwhen you add a replication rule to a bucket, you must have the  permission to be able to pass the iam role that grants amazon s3 replication permissions. for more information, see  in the iam user guide.under rule name, enter a name for your rule to help identify the rule later. the name is required and must be unique within the bucket. if the bucket has existing replication rules, you are instructed to set a priority for the rule. you must set a priority for the rule to avoid conflicts caused by objects that are included in the scope of more than one rule. in the case of overlapping rules, amazon s3 uses the rule priority to determine which rule to apply. the higher the number, the higher the priority. for more information about rule priority, see  in the amazon simple storage service developer guide. under status, enabled is selected by default. an enabled rule starts to work as soon as you save it. if you want to enable the rule later, select disabled. choose next. on the review page, review your replication rule. if it looks correct, choose save. otherwise, choose previous to edit the rule before saving it.  after you save your rule, you can edit, enable, disable, or delete your rule on the replication page. follow these steps to configure a replication rule when the destination bucket is in a different aws account than the source bucket. sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose management, choose replication, and then choose add rule. if you have never created a replication rule before, start with . on the replication rule wizard set destination page, under destination bucket, choose buckets in another account. then enter the name of the destination bucket and the account id from a different aws account. choose save. after you save the destination bucket name and account id, you might get a warning message telling you to add a bucket policy to the destination bucket so that amazon s3 can verify whether versioning is enabled on the bucket. you are presented with a bucket policy in a few steps that you can copy and add to the destination bucket in the other account. for information about adding a bucket policy to an s3 bucket and versioning, see  and  if you chose to replicate objects encrypted with aws kms, under destination encryption settings, enter the amazon resource name (arn) aws kms cmk to use to encrypt the replicas in the destination bucket.  for more information about creating an aws kms cmk, see  in the aws key management service developer guide. on the set destination page, under destination options: to replicate your data into a specific storage class in the destination bucket, select change the storage class for the replicated object(s). then choose the storage class that you want to use for the replicated objects in the destination bucket. if you don't select this option, the storage class for replicated objects is the same class as the original objects.to change the object ownership of the replica objects to the destination bucket owner, select change object ownership to destination owner. this option enables you to separate object ownership of the replicated data from the source. if asked, type the account id of the destination bucket. when you select this option, regardless of who owns the source bucket or the source object, the aws account that owns the destination bucket is granted full permission to replica objects. for more information, see  in the amazon simple storage service developer guide. if you want to add s3 replication time control (s3 rtc) to your replication configuration, select replication time control. notewhen you use s3 rtc, additional per-gb data transfer fees and cloudwatch metrics fees apply.choose next. set up an aws identity and access management (iam) role that amazon s3 can assume to perform replication of objects on your behalf. to set up an iam role, on the configure options page, under select role, do one of the following: we highly recommend that you choose create new role to have amazon s3 create a new iam role for you. when you save the rule, a new policy is generated for the iam role that matches the source and destination buckets that you choose. the name of the generated role is based on the bucket names and uses the following naming convention: replication_role_for_source-bucket_to_destination-bucket.you can choose to use an existing iam role. if you do, you must choose a role that allows amazon s3 to replicate objects from the source bucket to the destination bucket on your behalf.a bucket policy is provided on the configure options page that you can copy and add to the destination bucket in the other account. for information about adding a bucket policy to an s3 bucket, see  if you chose to replicate objects encrypted with aws kms, an aws kms key policy is provided on the configure options page. you can copy this policy to add to the key policy for the aws kms cmk that you are using. the key policy grants the source bucket owner permission to use the cmk. for information about updating the key policy, see . on the review page, review your replication rule. if it looks correct, choose save. otherwise, choose previous to edit the rule before saving it.  after you save your rule, you can edit, enable, disable, or delete your rule on the replication page. follow the instructions given on the replication page under the warning message, the replication rule is saved, but additional settings are required in the destination account. sign out of the aws account that you are currently in, and then sign in to the destination account.  importantreplication fails until you sign in to the destination account and complete the following steps. after you sign in to the destination account, choose the management tab, choose replication, and then choose receive objects on the actions menu.  on the receive objects page, you can do the following: enable versioning on the destination bucket.apply the bucket policy provided by amazon s3 to the destination bucket.copy the aws kms key policy that you need to update the aws kms cmk that is being used to encrypt the replica objects in the destination bucket. for information about updating the key policy, see .you must grant permissions to the account of the source bucket owner to encrypt using your aws kms cmk with a key policy. the following procedure describes how to use the aws identity and access management (iam) console to modify the key policy for the aws kms cmk that is being used to encrypt the replica objects in the destination bucket. to grant permissions to encrypt using your aws kms cmk sign in to the aws management console using the aws account that owns the aws kms cmk. open the aws kms console at . choose the alias of the cmk that you want to encrypt with. in the key policy section of the page, choose switch to policy view. choose edit to edit key policy. using the key policy editor, insert the key policy provided by amazon s3 into the existing key policy, and then choose save changes. you might want to add the policy to the end of the existing policy.  for more information about creating and editing aws kms cmks, see  in the aws key management service developer guide.   in the amazon simple storage service developer guide
this section describes how to create a s3 batch operations job. for information about performing batch operations using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. to create a batch job sign in to the aws management console and open the amazon s3 console at . choose batch operations on the navigation pane of the amazon s3 console. choose create job. choose the region where you want to create your job. under manifest format, choose the type of manifest object to use. if you choose s3 inventory report, enter the path to the manifest.json object that amazon s3 generated as part of the csv-formatted inventory report, and optionally the version id for the manifest object if you want to use a version other than the most recent.if you choose csv, enter the path to a csv-formatted manifest object. the manifest object must follow the format described in the console. you can optionally include the version id for the manifest object if you want to use a version other than the most recent.choose next under operation, choose the operation that you want to perform on all objects listed in the manifest. fill out the information for the operation you chose and then choose next. fill out the information for configure additional options and then choose next. for review, verify the settings. if you need to make changes, choose previous. otherwise, choose create job.  in the amazon simple storage service developer guide in the amazon simple storage service developer guide in the amazon simple storage service developer guide
*copyright &copy; 2020 amazon web services, inc. and/or its affiliates. all rights reserved.* amazon's trademarks and trade dress may not be used in       connection with any product or service that is not amazon's,       in any manner that is likely to cause confusion among customers,       or in any manner that disparages or discredits amazon. all other       trademarks not owned by amazon are the property of their respective      owners, who may or may not be affiliated with, connected to, or       sponsored by amazon. 
versioning enables you to keep multiple versions of an object in one bucket. this section describes how to enable object versioning on a bucket. for more information about versioning support in amazon s3, see  and  in the amazon simple storage service developer guide. to enable or disable versioning on an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to enable versioning for. choose properties. choose versioning. choose enable versioning or suspend versioning, and then choose save. noteyou can use aws multi-factor authentication (mfa) with versioning. when you use mfa with versioning, you must provide your aws account’s access keys and a valid code from the account’s mfa device in order to permanently delete an object version or suspend or reactivate versioning. to use mfa with versioning, you enable . however, you cannot enable  using the aws management console. you must use the aws cli or api. for more information, see . 
this section explains how to use the amazon s3 console to recover (undelete) deleted objects. to be able to undelete a deleted object, you must have had versioning enabled on the bucket that contains the object before the object was deleted. for information about enabling versioning, see . when you delete an object in a versioning-enabled bucket, all versions remain in the bucket and amazon s3 creates a delete marker for the object. to undelete the object, you must delete this delete marker. for more information about versioning and delete markers, see  in the amazon simple storage service developer guide. to recover deleted objects from an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. to see a list of the versions of the objects in the bucket, select show. you'll be able to see the delete markers for deleted objects.  to undelete an object, you must delete the delete marker. select the check box next to the delete marker of the object to recover, and then choose delete from the actions menu. then, choose hide and you'll see the undeleted object listed.  in the amazon simple storage service developer guide
this section explains how to use the amazon s3 console to restore an object that has been archived to the s3 glacier or s3 glacier deep archive storage classes. objects stored in the s3 glacier or s3 glacier deep archive are not immediately accessible. to access an object in this class, you must restore a temporary copy of it to its s3 bucket for the duration (number of days) that you specify. for information about the s3 glacier or s3 glacier deep archive storage classes, see  in the amazon simple storage service developer guide.  when you restore an archive, you pay for both the archive and the restored copy. because there is a storage cost for the copy, restore objects only for the duration you need them. if you want a permanent copy of the object, create a copy of it in your s3 bucket. for information about amazon s3 features and pricing, see . after restoring an object, you can download it from the overview page. for more information, see . topics the following are the available retrieval options when restoring an archived object:   - expedited retrievals allow you to quickly access your data stored in the s3 glacier storage class when occasional urgent requests for a subset of archives are required. for all but the largest archived objects (250 mb+), data accessed using expedited retrievals is typically made available within 1–5 minutes. provisioned capacity ensures that retrieval capacity for expedited retrievals is available when you need it. for more information, see . expedited retrievals and provisioned capacity are not available for objects stored in the s3 glacier deep archive storage class. - standard retrievals allow you to access any of your archived objects within several hours. this is the default option for the s3 glacier and s3 glacier deep archive retrieval requests that do not specify the retrieval option. standard retrievals typically finish within 3–5 hours for objects stored in the s3 glacier storage class. they typically finish within 12 hours for objects stored in the s3 glacier deep archive storage class.  - bulk retrievals are the lowest-cost retrieval option in amazon s3 glacier, enabling you to retrieve large amounts, even petabytes, of data inexpensively. bulk retrievals typically finish within 5–12 hours for objects stored in the s3 glacier storage class. they typically finish within 48 hours for objects stored in the s3 glacier deep archive storage class.for more information about retrieval options, see  in the amazon simple storage service developer guide. this topic explains how to use the amazon s3 console to restore an object that has been archived to the s3 glacier or s3 glacier deep archive storage classes. (the console uses the names glacier and glacier deep archive for these storage classes.) to restore archived s3 objects sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the objects that you want to restore. in the name list, select the object or objects that you want to restore, choose actions, and then choose restore. in the initiate restore dialog box, type the number of days that you want your archived data to be accessible.  choose one of the following retrieval options from the retrieval options menu. choose bulk retrieval or standard retrieval, and then choose restore. choose expedited retrieval (available only for the glacier storage class).provisioned capacity is only available for the glacier storage class. if you have provisioned capacity, choose restore to start a provisioned retrieval. if you have provisioned capacity, all of your expedited retrievals are served by your provisioned capacity. for more information about provisioned capacity, see .  if you don't have provisioned capacity and you don't want to buy it, choose restore. if you don't have provisioned capacity, but you want to buy it, choose add capacity unit, and then choose buy. when you get the purchase succeeded message, choose restore to start provisioned retrieval.you can upgrade the speed of your restoration while it is in progress. to upgrade an in-progress restore to a faster tier in the name list, select one or more of the objects that you are restoring, choose actions, and then choose restore from glacier. for information about checking the restoration status of an object, see . choose the tier that you want to upgrade to and then choose restore. for more information about upgrading to a faster restore tier, see see  in the amazon simple storage service developer guide.  to check the progress of the restoration, see the object overview panel. for information about the overview panel, see .  the overview section shows that restoration is in progress.  when the temporary copy of the object is available, the object's overview section shows the restoration expiry date. this is when amazon s3 will remove the restored copy of your archive.   restored objects are stored only for the number of days that you specify. if you want a permanent copy of the object, create a copy of it in your amazon s3 bucket.  amazon s3 calculates the expiry date by adding the number of days that you specify to the time you request to restore the object, and then rounding to the next day at midnight utc. this calculation applies to the initial restoration of the object and to any extensions to availability that you request. for example, if an object was restored on 10/15/2012 10:30 am utc and the number of days that you specified is 3, then the object is available until 10/19/2012 00:00 utc. if, on 10/16/2012 11:00 am utc you change the number of days that you want it to be accessible to 1, then amazon s3 makes the restored object available until 10/18/2012 00:00 utc. after restoring an object, you can download it from the overview page. for more information, see .   in the amazon s3 developer guide. in the aws cli command reference.
object tagging gives you a way to categorize storage. this topic explains how to use the console to add tags to an s3 object after the object has been uploaded. for information about adding tags to an object when the object is being uploaded, see .  each tag is a key-value pair that adheres to the following rules: you can associate up to 10 tags with an object. tags associated with an object must have unique tag keys.a tag key can be up to 128 unicode characters in length and tag values can be up to 255 unicode characters in length.key and tag values are case sensitive.  for more information about object tags, see  in the amazon simple storage service developer guide. to add tags to an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object you want to add tags to. choose properties. choose tags and then choose add tag. each tag is a key-value pair. type a key and a value. then choose add tag to add another tag or choose save.  you can enter up to 10 tags for an object. 
this section explains how to use the amazon simple storage service (amazon s3) console to grant access permissions to your buckets and objects. it also explains how to use amazon s3 block public access to prevent the application of any settings that allow public access to data within s3 buckets.   buckets and objects are amazon s3 resources.  you grant access permissions to your buckets and objects by using resource-based access policies. you can associate an access policy with a resource. an access policy describes who has access to resources. the resource owner is the aws account that creates the resource. for more information about resource ownership and access policies, see  in the amazon simple storage service developer guide.  bucket access permissions specify which users are allowed access to the objects in a bucket and which types of access they have. object access permissions specify which users are allowed access to the object and which types of access they have. for example, one user might have only read permission, while another might have read and write permissions. bucket and object permissions are independent of each other. an object does not inherit the permissions from its bucket. for example, if you create a bucket and grant write access to a user, you can't access that user’s objects unless the user explicitly grants you access.  to grant access to your buckets and objects to other aws accounts and to the general public, you use resource-based access policies known as access control lists (acls).  a bucket policy is a resource-based aws identity and access management (iam) policy that grants other aws accounts or iam users access to an s3 bucket. bucket policies supplement, and in many cases, replace acl-based access policies.  for more information about using iam with amazon s3, see  in the amazon simple storage service developer guide.  for more in-depth information about managing access permissions, see  in the amazon simple storage service developer guide. this section also explains how to use the amazon s3 console to add a cross-origin resource sharing (cors) configuration to an s3 bucket. cors allows client web applications that are loaded in one domain to interact with resources in another domain. topics 
this section explains how to use the amazon simple storage service (amazon s3) console to manage access permissions for s3 buckets by using access control lists (acls). acls are resource-based access policies that grant access permissions to buckets and objects. for more information about managing access permissions with resource-based policies, see  in the amazon simple storage service developer guide. you can grant permissions to other aws account users or to predefined groups. the user or group that you are granting permissions to is called the grantee. by default, the owner, which is the aws account that created the bucket, has full permissions. each permission you grant for a user or group adds an entry in the acl that is associated with the bucket. the acl lists grants, which identify the grantee and the permission granted. for more information about acls, see  in the amazon simple storage service developer guide. to set acl access permissions for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to set permissions for. choose permissions, and then choose access control list. you can manage bucket access permissions for the following:  1.  access for your aws account root user  access for other aws accounts to grant permissions to an aws user from a different aws account, under access for other aws accounts, choose add account. in the enter an id field, enter the canonical id of the aws user that you want to grant bucket permissions to. for information about finding a canonical id, see  in the amazon web services general reference. you can add as many as 99 users. select the check boxes next to the permissions that you want to grant to the user, and then choose save. to display information about the permissions, choose the help icons. warningwhen you grant other aws accounts access to your resources, be aware that the aws accounts can delegate their permissions to users under their accounts. this is known as cross-account access. for information about using cross-account access, see  in the iam user guide.  public access  warninguse caution when granting the everyone group public access to your s3 bucket. when you grant access to this group, anyone in the world can access your bucket. we highly recommend that you never grant any kind of public write access to your s3 bucket. s3 log delivery group to grant access to amazon s3 to write server access logs to the bucket, under s3 log delivery group, choose log delivery. if a bucket is set up as the target bucket to receive access logs, the bucket permissions must allow the log delivery group write access to the bucket. when you enable server access logging on a bucket, the amazon s3 console grants write access to the log delivery group for the target bucket that you choose to receive the logs. for more information about server access logging, see . you can also set bucket permissions when you are creating a bucket. for more information about setting permissions when creating a bucket, see .  
amazon s3 default encryption provides a way to set the default encryption behavior for an amazon s3 bucket. you can set default encryption on a bucket so that all objects are encrypted when they are stored in the bucket. the objects are encrypted using server-side encryption with either amazon s3-managed keys (sse-s3) or aws key management service (aws kms) customer master keys (cmks).  when you use server-side encryption, amazon s3 encrypts an object before saving it to disk in its data centers and decrypts it when you download the objects. for more information about protecting data using server-side encryption and encryption key management, see  in the amazon simple storage service developer guide. default encryption works with all existing and new amazon s3 buckets. without default encryption, to encrypt all objects stored in a bucket, you must include encryption information with every object storage request. you must also set up an amazon s3 bucket policy to reject storage requests that don't include encryption information.  there are no new charges for using default encryption for s3 buckets. requests to configure the default encryption feature incur standard amazon s3 request charges. for information about pricing, see . for sse-kms cmk storage, aws kms charges apply and are listed at .  to enable default encryption on an amazon s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose properties. choose default encryption. if you want to use keys that are managed by amazon s3 for default encryption, choose aes-256, and choose save.  for more information about using amazon s3 server-side encryption to encrypt your data, see  in the amazon simple storage service developer guide. importantyou might need to update your bucket policy when enabling default encryption. for more information, see  in the amazon simple storage service developer guide. if you want to use cmks that are stored in aws kms for default encryption, follow these steps: choose aws-kms. to choose a customer-managed aws kms cmk that you have created, use one of these methods: in the list that appears, choose the aws kms cmk.in the list that appears, choose custom kms arn, and then enter the amazon resource name of the aws kms cmk. importantyou can use only kms cmks that are enabled in the same aws region as the bucket. the s3 console only lists 100 kms cmks per region. if you have more than 100 cmks in the same region, you can only see the first 100 cmks in the s3 console. to use a kms cmk that is not listed in the console, choose custom kms arn, and enter the kms cmk arn.when you use an aws kms cmk for server-side encryption in amazon s3, you must choose a symmetric cmk. amazon s3 only supports symmetric cmks and not asymmetric cmks. for more information, see  in the aws key management service developer guide. importantif you use the aws kms option for your default encryption configuration, you are subject to the rps (requests per second) limits of aws kms. for more information about aws kms limits and how to request a limit increase, see . for more information about creating an aws kms cmk, see  in the aws key management service developer guide. for more information about using aws kms with amazon s3, see  in the amazon simple storage service developer guide. choose save.  in the amazon simple storage service developer guide
this section explains how to use the amazon s3 console to see the different versions of an object. a versioning-enabled bucket can have many versions of the same object:, one current (latest) version and zero or more noncurrent (previous) versions. amazon s3 assigns each object a unique version id. for information about enabling versioning, see .  if a bucket is versioning-enabled, amazon s3 creates another version of an object under the following conditions:  if you upload an object that has the same name as an object that already exists in the bucket, amazon s3 creates another version of the object instead of replacing the existing object. if you update any object properties after you upload the object to the bucket, such as changing the storage details or other metadata , amazon s3 creates a new object version in the bucket. for more information about versioning support in amazon s3, see  and  in the amazon simple storage service developer guide. to see multiple versions of an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. to see a list of the versions of the objects in the bucket, choose show. for each object version, the console shows a unique version id, the date and time the object version was created, and other properties. (objects stored in your bucket before you set the versioning state have a version id of null.)  to list the objects without the versions, choose hide. you also can view, download, and delete object versions in the object overview panel. for more information, see . importantyou can undelete an object only if it was deleted as the latest (current) version. you can't undelete a previous version of an object that was deleted. for more information, see  and  in the amazon simple storage service developer guide. 
this section explains how to use the amazon s3 console to add a cross-origin resource sharing (cors) configuration to an s3 bucket. cors allows client web applications that are loaded in one domain to interact with resources in another domain.  to configure your bucket to allow cross-origin requests, you add cors configuration to the bucket. a cors configuration is an xml document that defines rules that identify the origins that you will allow to access your bucket, the operations (http methods) supported for each origin, and other operation-specific information. for more information about cors and examples, see  in the amazon simple storage service developer guide. when you enable cors on the bucket, the access control lists (acls) and other access permission policies continue to apply. to add a cors configuration to an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to create a bucket policy for. choose permissions, and then choose cors configuration. in the cors configuration editor text box, type or copy and paste a new cors configuration, or edit an existing configuration. the cors configuration is an xml file. the text that you type in the editor must be valid xml. for more information, see  choose save. noteamazon s3 displays the amazon resource name (arn) for the bucket next to the cors configuration editor title. for more information about arns, see  in the amazon web services general reference. 
you can use lifecycle policies to define actions that you want amazon s3 to take during an object's lifetime (for example, transition objects to another storage class, archive them, or delete them after a specified period of time). you can define a lifecycle policy for all objects or a subset of objects in the bucket by using a shared prefix (objects names that begin with a common string) or a tag.  using a lifecycle policy, you can define actions specific to current and noncurrent object versions. for more information, see  and  and  in the amazon simple storage service developer guide. to create a lifecycle policy sign in to the aws management console and open the amazon s3 console at . in the buckets list, choose the name of the bucket that you want to create a lifecycle policy for. choose the management tab, and choose add lifecycle rule. in the lifecycle rule dialog box, type a name for your rule.  the name must be unique within the bucket.  choose the scope of the lifecycle rule: all objects with a specific prefix or tag or all the objects in the bucket. to apply this lifecycle rule to all objects with a specific prefix or tag, choose limit the scope to specific prefixes or tags. in the add prefix or tag filter box, type the prefix or tag name, and press enter.  for more information about object name prefixes, see  in the amazon simple storage service developer guide.for more information about object tags, see  in the amazon simple storage service developer guide.to apply this lifecycle rule to all objects in the bucket, choose apply to all objects in the bucket.choose next. the storage class transition page opens. when you configure your storage class transitions, you define the rules to transition objects to the standard-ia, one zone-ia, glacier, and deep archive storage classes. for more information, see  in the amazon simple storage service developer guide. you can define transitions for current or previous object versions or for both current and previous versions. versioning enables you to keep multiple versions of an object in one bucket. for more information about versioning, see . choose the versions for which you want to define transitions, current or noncurrent: to define transitions that are applied to the current verion of the object, choose current version. to define transitions that are applied to all previous versions of the object, choose previous versions. in the screenshot below, current version is chosen. to add a transition:  for a current version, under for current object versions, choose add transition. for a non-current version, under for non-current object versions, choose add transition. for each transition that you add, choose one of the following: transition to standard-ia after.transition to intelligent-tiering after. transition to one zone-ia after. transition to glacier after.transition to glacier deep archive after. importantwhen you choose the glacier or glacier deep archive storage class, your objects remain in amazon s3. you cannot access them directly through the separate amazon s3 glacier service. for more information, see . in the days after creation box, enter the number of days after the creation of the object that you want the transition to be applied (for example, 30 or 100 days). when you are done configuring transitions, choose next. under configure expiration, for this example, choose both current version and previous versions.  importantin a non-versioned bucket the expiration action results in amazon s3 permanently removing the object. for more information about lifecycle actions, see  in the amazon simple storage service developer guide. choose expire current version of object, and then enter the number of days after object creation to delete the object (for example, 395 days). if you choose this expire option, you cannot choose the option to clean up expired delete markers.  choose permanently delete previous versions, and then enter the number of days after an object becomes a previous version to permanently delete the object (for example, 465 days). to clean up incomplete multipart uploads, we recommend that you choose clean up incomplete multipart uploads and enter the number of days after the multipart upload initiation that you want to end and clean up incomplete multipart uploads(for example, 7 days).  for more information about multipart uploads, see  in the amazon simple storage service developer guide. choose next. for review, verify the settings for your rule. if you need to make changes, choose previous. otherwise, choose save.  if the rule does not contain any errors, it is enabled and you can see it on the lifecycle page. 
with s3 object lock, you can store objects in amazon s3 using a write-once-read-many (worm) model. you can use s3 object lock to prevent an object from being deleted or overwritten for a fixed amount of time or indefinitely. for information about object locking using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. before you lock any objects, you have to enable a bucket to use s3 object lock. you enable object lock when you create a bucket. after you enable object lock on a bucket, you can lock objects in that bucket. when you create a bucket with object lock enabled, you can't disable object lock or suspend versioning for that bucket.  for information about creating a bucket with s3 object lock enabled, see . to lock an amazon s3 object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. in the name list, choose the name of the object that you want to lock. choose properties. choose object lock. choose a retention mode. you can change the retain until date. you can also choose to enable a legal hold. for more information, see  in the amazon simple storage service developer guide. choose save. 
welcome to the amazon simple storage service console user guide for the amazon simple storage service (amazon s3) console.  amazon s3 provides virtually limitless storage on the internet. this guide explains how you can manage buckets, objects, and folders in amazon s3 by using the aws management console, a browser-based graphical user interface for interacting with aws services.  for detailed conceptual information about how amazon s3 works, see  in the amazon simple storage service developer guide. the developer guide also has detailed information about amazon s3 features and code examples to support those features. topics 
this topic describes how to enable server access logging for an amazon s3 bucket using the aws management console. for information about how to enable logging programmatically and details about how logs are delivered, see  in the amazon simple storage service developer guide. by default, amazon simple storage service (amazon s3) doesn't collect server access logs. when you enable logging, amazon s3 delivers access logs for a source bucket to a target bucket that you choose. the target bucket must be in the same aws region as the source bucket and must not have a default retention period configuration.  server access logging provides detailed records for the requests that are made to an s3 bucket. server access logs are useful for many applications. for example, access log information can be useful in security and access audits. it can also help you learn about your customer base and understand your amazon s3 bill.  an access log record contains details about the requests that are made to a bucket. this information can include the request type, the resources that are specified in the request, and the time and date that the request was processed. for more information, see  in the amazon simple storage service developer guide. importantthere is no extra charge for enabling server access logging on an amazon s3 bucket. however, any log files that the system delivers to you will accrue the usual charges for storage. (you can delete the log files at any time.) we do not assess data transfer charges for log file delivery, but we do charge the normal data transfer rate for accessing the log files. to enable server access logging for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to enable server access logging for. choose properties. choose server access logging. choose enable logging. for target, choose the name of the bucket that you want to receive the log record objects.  the target bucket must be in the same region as the source bucket and must not have a default retention period configuration. (optional) for target prefix, type a key name prefix for log objects, so that all of the log object names begin with the same string. choose save. you can view the logs in the target bucket. if you specified a prefix, the prefix shows as a folder in the target bucket in the console. after you enable server access logging, it might take a few hours before the logs are delivered to the target bucket. for more information about how and when logs are delivered, see  in the amazon simple storage service developer guide. more info  
this topic describes how to set or change the type of encryption an object using the amazon s3 console. noteif you change an object's encryption, a new object is created to replace the old one. if s3 versioning is enabled, a new version of the object is created, and the existing object becomes an older version. the role that changes the property also becomes the owner of the new object or (object version).  to add or change encryption for an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object that you want to add or change encryption for. choose properties, and then choose encryption. the encryption dialog box opens, giving you three choices for object encryption: none‐ no object encryption.aes-256 ‐ server-side encryption with amazon s3 managed keys (sse-s3).aws‐kms ‐ server-side encryption with aws key management service (aws kms) customer master keys (sse-kms).if you want to remove encryption from an object that already has encryption settings, choose none and then choose save. if you want to encrypt your object using keys that are managed by amazon s3, follow these steps: chooseaes-256.  for more information about using amazon s3 server-side encryption to encrypt your data, see  in the amazon simple storage service developer guide. choose save. if you want to encrypt your object using aws kms, follow these steps: choose aws-kms. choose an aws kms customer master key (cmk). the list shows  that you have created and your aws managed cmk for amazon s3. for more information about creating a customer managed aws kms cmk, see  in the aws key management service developer guide.  importantthe amazon s3 console lists only 100 aws kms cmks per aws region. if you have more than 100 cmks in the same region, you can see only the first 100 cmks in the s3 console. to use a kms cmk that is not listed in the console, choose custom kms arn, and enter the kms cmk arn. choose save. importantto encrypt objects in the bucket, you can use only cmks that are enabled in the same aws region as the bucket. amazon s3 only supports symmetric cmks. amazon s3 does not support asymmetric cmks. for more information, see . to give an external account the ability to use an object that is protected by an aws kms cmk, follow these steps:  choose aws-kms. enter the amazon resource name (arn) for the external account. choose save. administrators of an external account that have usage permissions to an object protected by your aws kms cmk can further restrict access by creating a resource-level aws identity and access management (iam) policy.  in the amazon simple storage service developer guide
this section explains how to manage and use your amazon s3 access points using the aws management console. each access point is associated with a single amazon s3 bucket. before you begin, navigate to the list of your access points for a bucket as described in the following procedure. to find a list of access points for a bucket sign in to the aws management console and open the amazon s3 console at . in the s3 buckets section, select the bucket whose access points you want to manage. on the bucket detail page, choose the access points tab. from the access points tab, you can view an access point's configuration details, edit an access point's policy, use an access point to access your bucket, or delete an access point. the following procedures explain how to perform each of these tasks. to view access point configuration details navigate to the access points tab for your bucket. locate the access point whose configuration you want to view. you can browse for the access point in the access point list, or you can search for a specific access point using the search by name field. choose the name of the access point whose configuration you want to view. noteto view an access point's configuration, choose (click on) the name of the access point, not the option button next to the access point name. to edit an access point policy navigate to the access points tab for your bucket. select the option button next to the name of the access point whose policy you want to edit. choose edit access point policy. enter the policy in the text field. the console automatically displays the amazon resource name (arn) for the access point, which you can use in the policy. you can also choose policy generator to use the aws policy generator to help construct the policy. choose save. to use an access point to access your bucket navigate to the access points tab for your bucket. select the option button next to the name of the access point you want to use. choose use this access point. the console displays a label above the name of your bucket that shows the access point that you're currently using. while you're using the access point, you can only perform the object operations that are allowed by the access point permissions. notethe console view always shows all objects in the bucket. using an access point as described in this procedure restricts the operations you can perform on those objects, but not whether you can see that they exist in the bucket. to exit the access point view of your bucket, choose exit access point. notethe s3 management console doesn't support using virtual private cloud (vpc) access points to access bucket resources. to access bucket resources from a vpc access point, use the aws cli, aws sdks, or amazon s3 rest apis. to delete an access point navigate to the access points tab for your bucket. select the option button next to the name of the access point that you want to delete. choose delete. confirm that you want to delete your access point by entering its name in the text field that appears, and choose confirm. 
this section explains how to use the amazon s3 console to view the object overview panel. this panel provides an overview of all the essential information for an object in one place. to see the overview panel for an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, select the check box next to the name of the object for which you want an overview. to download the object, choose download in the object overview panel. to copy the path of the object to the clipboard, choose copy path. if versioning is enabled on the bucket, choose latest versions to list the versions of the object. you can then choose the download icon to download an object version, or choose the trash can icon to delete an object version. importantyou can undelete an object only if it was deleted as the latest (current) version. you can't undelete a previous version of an object that was deleted. for more information, see  and  in the amazon simple storage service developer guide. 
this section explains how to use the amazon s3 console to download objects from an s3 bucket. data transfer fees apply when you download objects. for information about amazon s3 features, and pricing, see . importantif an object key name consists of a single period (.), or two periods (..), you can’t download the object using the amazon s3 console. to download an object with a key name of “.” or “..”, you must use the aws cli, aws sdks, or rest api. for more information about naming objects, see  in the amazon simple storage service developer guide. you can download a single object per request using the amazon s3 console. to . to download an object from an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to download an object from. you can download an object from an s3 bucket in any of the following ways: in the name list, select the check box next to the object you want to download, and then choose download on the object description page that appears.choose the name of the object that you want to download. on the overview page, choose download. choose the name of the object that you want to download and then choose download as on the overview page.choose the name of the object that you want to download. choose latest version and then choose the download icon.
each object in amazon simple storage service (amazon s3) has a set of name-value pairs that provides metadata about the object. metadata is additional information about the object. some metadata is set by amazon s3 when you upload the object, for example, and . you can also set some metadata when you upload the object, or you can add it later. this section explains how to use the amazon s3 console to add metadata to an s3 object. object metadata is a set of name-value (key-value) pairs. for example, the metadata for content length, , is the name (key) and the size of the object in bytes (value). for more information about object metadata, see  in the amazon simple storage service developer guide. there are two kinds of metadata for an s3 object, amazon s3 system metadata and user-defined metadata: system metadata–there are two categories of system metadata. metadata such as the  date is controlled by the system. only amazon s3 can modify the value. there is also system metadata that you control, for example, the storage class configured for the object. user-defined metadata–you can define your own custom metadata, called user-defined metadata. you can assign user-defined metadata to an object when you upload the object or after the object has been uploaded. user-defined metadata is stored with the object and is returned when you download the object. amazon s3 does not process user-defined metadata. the following topics describe how to add metadata to an object using the amazon s3 console. noteif you change an object's metadata, a new object will be created and will replace the old one. if s3 versioning is enabled, a new version of the object is created, and the existing object becomes an older version. the role that changes the property also becomes the owner of the new object or (object version). topics you can configure some system metadata for an s3 object. for a list of system-defined metadata and whether you can modify their values, see  in the amazon simple storage service developer guide. to add system metadata to an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object that you want to add metadata to. choose properties, and then choose metadata. choose add metadata, and then choose a key from the select a key menu. depending on which key you chose, choose a value from the select a value menu or type a value. choose save. you can assign user-defined metadata to an object. user-defined metadata must begin with the prefix "", otherwise amazon s3 will not set the key value pair as you define it. you define custom metadata by adding a name that you choose to the  key. this creates a custom key. for example, if you add the custom name , the metadata key would be .  user-defined metadata can be as large as 2 kb. both keys and their values must conform to us-ascii standards. for more information, see  in the amazon simple storage service developer guide. to add user-defined metadata to an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object that you want to add metadata to. choose properties, and then choose metadata. choose add metadata, and then choose the  key from the select a key menu. any metadata starting with the prefix  is user-defined metadata. type a custom name following the  key. for example, for the custom name , the metadata key would be . enter a value for the custom key, and then choose save. 
amazon s3 inventory provides a flat file list of your objects and metadata, which is a scheduled alternative to the amazon s3 synchronous  api operation. amazon s3 inventory provides comma-separated values (csv) or  or  output files that list your objects and their corresponding metadata on a daily or weekly basis for an s3 bucket or for objects that share a prefix (objects that have names that begin with the same string). for more information, see  in the amazon simple storage service developer guide. to configure inventory noteit may take up to 48 hours to deliver the first report. sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket for which you want to configure amazon s3 inventory. choose the management tab, and then choose inventory. choose add new. type a name for the inventory and set it up as follows: optionally, add a prefix for your filter to inventory only objects whose names begin with the same string.choose the destination bucket where you want reports to be saved. the destination bucket must be in the same aws region as the bucket for which you are setting up the inventory. the destination bucket can be in a different aws account. optionally, choose a prefix for the destination bucket.choose how frequently to generate the inventory.under advanced settings, you can set the following: choose either the csv, orc, or parquet output file format for your inventory. for more information about these formats, see  in the amazon simple storage service developer guide. to include all versions of the objects in the inventory, choose include all versions in the object versions list. by default, the inventory includes only the current versions of the objects. for optional fields, select one or more of the following to add to the inventory report: size – object size in bytes.last modified date – the object creation date or the last modified date, whichever is the latest.storage class – the storage class used for storing the object. etag – the entity tag is a hash of the object. the etag reflects changes only to the contents of an object, and not its metadata. the etag may or may not be an md5 digest of the object data. whether it is depends on how the object was created and how it is encrypted.multipart upload – specifies that the object was uploaded as a multipart upload. for more information, see  in the amazon simple storage service developer guide.replication status – the replication status of the object. for more information, see .encryption status – the server-side encryption used to encrypt the object. for more information, see  in the amazon simple storage service developer guide.s3 object lock configurations – the object lock status of the object, including the following settings:  retention mode – the level of protection applied to the object, either governance or compliance.retain until date – the date until which the locked object cannot be deleted.legal hold status – the legal hold status of the locked object. for information about s3 object lock, see  in the amazon simple storage service developer guide. for more information about the contents of an inventory report, see  in the amazon simple storage service developer guide. for encryption, choose a server-side encryption option to encrypt the inventory report, or choose none: none – do not encrypt the inventory report.aes-256 – encrypt the inventory report using server-side encryption with amazon s3-managed keys (sse-s3). amazon s3 server-side encryption uses 256-bit advanced encryption standard (aes-256). for more information, see  in the amazon simple storage service developer guide. aws-kms – encrypt the report using server-side encryption with aws key management service (aws kms) customer master keys (cmks). for more information, see  in the amazon simple storage service developer guide.  noteto encrypt the inventory list file with sse-kms, you must grant amazon s3 permission to use the aws kms cmk. for instructions, see  .choose save. amazon s3 creates a bucket policy on the destination bucket that grants amazon s3 write permission. this allows amazon s3 to write data for the inventory reports to the bucket.  if an error occurs when you try to create the bucket policy, you are given instructions on how to fix it. for example, if you choose a destination bucket in another aws account and don't have permissions to read and write to the bucket policy, you see the following message.   in this case, the destination bucket owner must add the displayed bucket policy to the destination bucket. if the policy is not added to the destination bucket, you won't get an inventory report because amazon s3 doesn’t have permission to write to the destination bucket. if the source bucket is owned by a different account than that of the current user, the correct account id of the source bucket must be substituted in the policy. for more information, see  in the amazon simple storage service developer guide. to grant amazon s3 permission to encrypt using a customer managed aws key management service (aws kms) customer master key (cmk), you must use a key policy. to update your key policy so that you can use an aws kms customer managed cmk to encrypt the inventory file, follow the steps below. to grant permissions to encrypt using your aws kms cmk using the aws account that owns the customer managed cmk, sign into the aws management console. open the aws kms console at . to change the aws region, use the region selector in the upper-right corner of the page. in the left navigation pane, choose customer managed keys. under customer managed keys, choose the customer managed cmk that you want to use to encrypt the inventory file. under key policy, choose switch to policy view. to update the key policy, choose edit. under edit key policy, add the following key policy to the existing key policy. choose save changes. for more information about creating customer managed cmks aws kms and using key policies, see the following links in the aws key management service developer guide: more info  
for more detailed information about configuring a redirect in amazon s3, see  in the amazon simple storage service developer guide. you can redirect all requests for a website endpoint for a bucket to another host. if you redirect all requests, any request made to the website endpoint is redirected to the specified host name.  for example, if your root domain is , and you want to serve requests for both  and , you can create two buckets named  and . then, maintain the content in the  bucket, and configure the other  bucket to redirect all requests to the  bucket. for more information, see . to redirect requests for a bucket website endpoint open the amazon s3 console at . choose the name of the bucket that you have configured as a static website (for example, ). choose properties. choose static website hosting. choose redirect requests.  in the target bucket or domain box, enter the bucket or domain that you want to redirect to. for example, if you are redirecting to a root domain address, you would enter example.com. in the protocol box, enter the protocol for the redirected requests (http or https). if you do not specify a protocol, the protocol of the original request is used. choose save. 
in the amazon s3 console, you can move an object to a bucket or a folder.  to move an object sign in to the aws management console and open the amazon s3 console at . in the buckets list, choose your bucket name. select the check box beside the object that you want to move. choose actions, and choose move. choose the move destination: if you want to move your object to a bucket, select the bucket.if you want to move your object to a folder in a bucket, choose the bucket name, and then choose the folder.under review, confirm the move details, and choose move. amazon s3 moves your object to the destination. 
this section explains how to use the amazon simple storage service (amazon s3) console to manage access permissions for an amazon s3 object by using access control lists (acls). acls are resource-based access policies that grant access permissions to buckets and objects. for more information about managing access permissions with resource-based policies, see  in the amazon simple storage service developer guide. bucket and object permissions are independent of each other. an object does not inherit the permissions from its bucket. for example, if you create a bucket and grant write access to a user, you can't access that user’s objects unless the user explicitly grants you access. you can grant permissions to other aws accounts or predefined groups. the user or group that you grant permissions to is called the grantee. by default, the owner, which is the aws account that created the bucket, has full permissions. each permission you grant for a user or a group adds an entry in the acl that is associated with the object. the acl lists grants, which identify the grantee and the permission granted. for more information about acls, see  in the amazon simple storage service developer guide. to set permissions for an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object for which you want to set permissions. choose permissions. you can manage object access permissions for the following:  1.  access for object owner  access for other aws accounts to grant permissions to an aws user from a different aws account, under access for other aws accounts, choose add account. in the enter an id field, enter the canonical id of the aws user that you want to grant object permissions to. for information about finding a canonical id, see  in the amazon web services general reference. you can add as many as 99 users. select the check boxes for the permissions that you want to grant to the user, and then choose save. to display information about the permissions, choose the help icons. public access  warninguse caution when granting the everyone group anonymous access to your amazon s3 objects. when you grant access to this group, anyone in the world can access your object. if you need to grant access to everyone, we highly recommend that you only grant permissions to read objects.we highly recommend that you do not grant the everyone group write object permissions. doing so allows anyone to overwrite the acl permissions for the object. you can also set object permissions when you upload objects. for more information about setting permissions when uploading objects, see .  
this section describes how to use the amazon s3 console to create a folder. to create a folder sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to create a folder in. choose create folder. enter a name for the folder (for example, favorite-pics). choose the encryption setting for the folder object, and then choose save. if you don't want to use encryption with your folder, choose none.  if you want to encrypt your object using keys that are managed by amazon s3, follow these steps: choose aes-256.  for more information about using amazon s3 server-side encryption to encrypt your data, see  in the amazon simple storage service developer guide. if you want to encrypt your object using aws kms, follow these steps: choose aws-kms. choose an aws kms customer master key (cmk). the list shows  that you have created and your aws managed cmk for amazon s3. for more information about creating a customer managed aws kms cmk, see  in the aws key management service developer guide.  importantto encrypt objects in the bucket, you can use only the cmks that are enabled in the same aws region as the bucket. amazon s3 only supports symmetric cmks and does not support asymmetric cmks. for more information, see .the amazon s3 console only lists 100 kms cmks per region. if you have more than 100 cmks in the same region, you can see only the first 100 cmks in the s3 console. to use anaws kms kms cmk that is not listed in the console, choose custom kms arn, and enter the kms cmk arn. choose save. 
to upload your data (photos, videos, documents etc.) to amazon s3, you must first create an s3 bucket in one of the aws regions. you can then upload an unlimited number of data objects to the bucket. the data that you store in amazon s3 consists of objects. every object resides within a bucket that you create in a specific aws region. every object that you store in amazon s3 resides in a bucket.  objects stored in a region never leave the region unless you explicitly transfer them to another region. for example, objects stored in the europe (ireland) region never leave it. the objects stored in an aws region physically remain in that region. amazon s3 does not keep copies of objects or move them to any other region. however, you can access the objects from anywhere, as long as you have necessary permissions to do so. before you can upload an object into amazon s3, you must have write permissions to a bucket. objects can be any file type: images, backups, data, movies, etc. you can have an unlimited number of objects in a bucket. the maximum size of file you can upload by using the amazon s3 console is 160 gb. to upload a file larger than 160 gb, use the aws cli, aws sdk, or amazon s3 rest api. for more information, see  in the amazon simple storage service developer guide. the following topics explain how to use the amazon s3 console to upload, delete, and manage objects. noteif you rename an object, or change any of the properties; storage class,  enryption, metadata, a new object is created to replace the old one. if s3 versioning is enabled, a new version of the object is created, and the existing object becomes an older version. the role that changes the property also becomes the owner of the new object or (object version). topics 
this section explains how to use the console to view the properties of an object. to view the properties of an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object you want to view the properties for. choose properties. on the properties page, you can configure the following properties for the object. noteif you change the properties; storage class,  enryption, metadata, a new object is created to replace the old one. if s3 versioning is enabled, a new version of the object is created, and the existing object becomes an older version. the role that changes the property also becomes the owner of the new object or (object version). storage class – each object in amazon s3 has a storage class associated with it. the storage class that you choose to use depends on how frequently you access the object. the default storage class for s3 objects is standard. you choose which storage class to use when you upload an object. for more information about storage classes, see  in the amazon simple storage service developer guide. to change the storage class after you upload an object, choose storage class. choose the storage class that you want, and then choose save. encryption – you can encrypt your s3 objects. for more information, see .  metadata – each object in amazon s3 has a set of name-value pairs that represents its metadata. for information on adding metadata to an s3 object, see . tags – you can add tags to an s3 object. for more information, see . 
amazon s3 block public access prevents the application of any settings that allow public access to data within s3 buckets. this section describes how to edit block public access settings for all the s3 buckets in your aws account. for information about blocking public using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. to edit block public access settings for all the s3 buckets in an aws account sign in to the aws management console and open the amazon s3 console at . choose block public access (account settings). choose edit to change the block public access settings for all the buckets in your aws account. choose the settings that you want to change, and then choose save. when you're asked for confirmation, enter confirm. then choose confirm to save your changes. 
 you can use amazon s3 access points to manage access to your s3 objects. amazon s3 access points are named network endpoints that are attached to buckets that you can use to perform s3 object operations, such as uploading and retrieving objects. a bucket can have up to 1,000 access points attached, and each access point enforces distinct permissions and network controls to give you fine-grained control over access to your s3 objects.  for more information about amazon s3 access points, see  in the amazon simple storage service developer guide. the following topics explain how to use the s3 management console to create, manage, and use amazon s3 access points. topics 
you can host a static website on amazon s3. on a static website, individual webpages include static content. a static website might also contain client-side scripts. by contrast, a dynamic website relies on server-side processing, including server-side scripts such as php, jsp, or asp.net. amazon s3 does not support server-side scripting. you can use the following quick procedures to configure an s3 bucket for static website hosting in the amazon s3 console. for more information and detailed walkthroughs, see  in the amazon simple storage service developer guide. topics sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to enable static website hosting for. choose properties. choose static website hosting. choose use this bucket to host a website. in index document, enter the name of the index document, typically .  when you configure a bucket for website hosting, you must specify an index document. amazon s3 returns this index document when requests are made to the root domain or any of the subfolders. for more information, see  in the amazon simple storage service developer guide. (optional) if you want to provide your own custom error document for 4xx class errors, in error document, enter the custom error document filename.  if you do not specify a custom error document and an error occurs, amazon s3 returns a default html error document. for more information, see  in the amazon simple storage service developer guide. (optional) if you want to specify advanced redirection rules, in the edit redirection rules box, enter xml to describe the rules. for example, you can conditionally route requests according to specific object key names or prefixes in the request. for more information, see  in the amazon simple storage service developer guide. choose save. upload the index document to your bucket. for step-by-step instructions on uploading an object to an s3 bucket, see .  upload other files for your website, including optional custom error documents. in the next section, you set the permissions required to access your bucket as a static website.. by default, amazon s3 blocks public access to your account and buckets. if you want to use a bucket to host a static website, you can use these steps to edit your block public access settings.  warningbefore you complete this step, review  to ensure that you understand and accept the risks involved with allowing public access. when you turn off block public access settings to make your bucket public, anyone on the internet can access your bucket. we recommend that you block all public access to your buckets. open the amazon s3 console at . choose the name of the bucket that you have configured as a static website. choose permissions. choose edit. clear block all public access, and choose save. warningbefore you complete this step, review  to ensure you understand and accept the risks involved with allowing public access. when you turn off block public access settings to make your bucket public, anyone on the internet can access your bucket. we recommend that you block all public access to your buckets. in the confirmation box, enter confirm, and then choose confirm. under s3 buckets, the access for your bucket updates to objects can be public. you can now add a bucket policy to make the objects in the bucket publicly readable. if the access still displays as bucket and objects not public, you might have to  for your account before adding a bucket policy. after you edit s3 block public access settings, you can add a bucket policy to grant public read access to your bucket. when you grant public read access, anyone on the internet can access your bucket. importantthe following policy is an example only and allows full access to the contents of your bucket. before you proceed with this step, review  to ensure that you understand the best practices for securing the files in your s3 bucket and risks involved in granting public access. under buckets, choose the name of your bucket. choose permissions. choose bucket policy. to grant public read access for your website, copy the following bucket policy, and paste it in the bucket policy editor. update the  to include your bucket name. in the preceding example bucket policy, example.com is the bucket name. to use this bucket policy with your own bucket, you must update this name to match your bucket name. choose save. a warning appears indicating that the bucket has public access. in bucket policy, a public label appears. if you see an error that says , confirm that the bucket name in the bucket policy matches your bucket name. for information about adding a bucket policy, see  if you get an error - access denied warning and the bucket policy editor does not allow you to save the bucket policy, check your account-level and bucket-level block public access settings to confirm that you allow public access to the bucket. once you configure your bucket as a static website and set permissions, you can access your website through an amazon s3 website endpoint. for more information, see  in the amazon simple storage service developer guide. for a complete list of amazon s3 website endpoints, see  in the amazon web services general reference. 
this section describes how to configure advanced s3 bucket property settings for object replication, event notification, and transfer acceleration. topics 
there are three types of amazon cloudwatch metrics for amazon s3: storage metrics are reported once per day and are provided to all customers at no additional cost.replication metrics are available 15 minutes after enabling a replication rule with s3 replication time control (s3 rtc). for more information, see request metrics are available at 1-minute intervals after some latency to process, and the metrics are billed at the standard cloudwatch rate. for more information about cloudwatch metrics for amazon s3, see  in the amazon simple storage service developer guide. to get request metrics, you must opt into them by configuring them on the aws management console or using the amazon s3 api. to configure request metrics on a bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the objects you want request metrics for. choose the management tab, and then choose metrics. choose requests. in the left pane, choose the edit icon next to the name of the bucket. select the request metrics check box. this also enables data transfer metrics. choose save. you have now created a metrics configuration for all the objects in an amazon s3 bucket. about 15 minutes after cloudwatch begins tracking these request metrics, you can see graphs for the metrics on the amazon s3 or cloudwatch console.  you can also define a filter so that the metrics are only collected and reported on a subset of objects in the bucket. for more information, see  
to upload your data (photos, videos, documents etc.) to amazon s3, you must first create an s3 bucket in one of the aws regions. you can then upload your data objects to the bucket. every object you store in amazon s3 resides in a bucket. you can use buckets to group related objects in the same way that you use a directory to group files in a file system.  amazon s3 creates buckets in the aws region that you specify. you can choose any aws region that is geographically close to you to optimize latency, minimize costs, or address regulatory requirements. for example, if you reside in europe, you might find it advantageous to create buckets in the europe (ireland) or europe (frankfurt) regions. for a list of amazon s3 aws regions, see  in the amazon web services general reference. you are not charged for creating a bucket. you are only charged for storing objects in the bucket and for transferring objects out of the bucket. for more information about pricing, see . amazon s3 bucket names are globally unique, regardless of the aws region in which you create the bucket. you specify the name at the time you create the bucket. for bucket naming guidelines, see  in the amazon simple storage service developer guide. the following topics explain how to use the amazon s3 console to create, delete, and manage buckets. topics 
amazon s3 provides a set of tools to help you manage your s3 batch operations jobs after you create them. for more information about managing s3 batch operations, see  in the amazon simple storage service developer guide.    in the amazon simple storage service developer guide in the amazon simple storage service developer guide in the amazon simple storage service developer guide
you can change the display language of the aws management console. several languages are supported. to change the console language sign in to the aws management console and open the amazon s3 console at . scroll down until you see the bar at the bottom of the window, and then choose the language on the left side of the bar. choose the language that you want from the menu. this will change the langauge for the entire aws management console. 
amazon simple storage service (amazon s3) transfer acceleration enables fast, easy, and secure file transfers between your client and an s3 bucket over long distances. this topic describes how to enable amazon s3 transfer acceleration for a bucket. for more information, see  in the amazon simple storage service developer guide. to enable transfer acceleration for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the buckets list, choose the name of the bucket that you want to enable transfer acceleration for. choose properties. under advanced settings, choose transfer acceleration. (optional) if you want to compare accelerated and non-accelerated upload speeds, to open the , choose the want to compare your data transfer speed by region?. the speed comparison tool uses multipart upload to transfer a file from your browser to various aws regions with and without amazon s3 transfer acceleration. you can compare the upload speed for direct uploads and transfer accelerated uploads by region.  choose enabled, and choose save. endpoint displays the transfer acceleration endpoint for your bucket. you use this endpoint to access accelerated data transfers to and from your bucket. if you suspend transfer acceleration, the accelerate endpoint no longer works. 
this section describes how to enable an aws cloudtrail trail to log data events for objects in an s3 bucket by using the amazon s3 console. cloudtrail supports logging amazon s3 object-level api operations such as , , and . these events are called data events. by default, cloudtrail trails don't log data events, but you can configure trails to log data events for s3 buckets that you specify, or to log data events for all the amazon s3 buckets in your aws account. for more information, see . cloudtrail does not populate data events in the cloudtrail event history. additionally, not all bucket-level actions are populated in the cloudtrail event history. for more information, see  to configure a trail to log data events for an s3 bucket, you can use either the aws cloudtrail console or the amazon s3 console. if you are configuring a trail to log data events for all the amazon s3 buckets in your aws account, it's easier to use the cloudtrail console. for information about using the cloudtrail console to configure a trail to log s3 data events, see  in the aws cloudtrail user guide.  importantadditional charges apply for data events. for more information, see .  the following procedure shows how to use the amazon s3 console to enable a cloudtrail trail to log data events for an s3 bucket. to enable cloudtrail data events logging for objects in an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket. choose properties. choose object-level logging. choose an existing cloudtrail trail in the drop-down menu.  the trail you select must be in the same aws region as your bucket, so the drop-down list contains only trails that are in the same region as the bucket or trails that were created for all regions.  if you need to create a trail, choose the cloudtrail console link to go to the cloudtrail console. for information about how to create trails in the cloudtrail console, see  in the aws cloudtrail user guide. under events, choose one of the following: read to specify that you want cloudtrail to log amazon s3 read apis such as . write to log amazon s3 write apis such as . read and write to log both read and write object apis.for a list of supported data events that cloudtrail logs for amazon s3 objects, see  in the amazon simple storage service developer guide. choose create to enable object-level logging for the bucket. to disable object-level logging for the bucket, you must go to the cloudtrail console and remove the bucket name from the trail's data events. noteif you use the cloudtrail console or the amazon s3 console to configure a trail to log data events for an s3 bucket, the amazon s3 console shows that object-level logging is enabled for the bucket.  for information about enabling object-level logging when you create an s3 bucket, see .  in the amazon simple storage service developer guide  in the aws cloudtrail user guide
this topic explains how to view the properties for an s3 bucket. to view the properties for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to view the properties for. choose properties. on the properties page, you can configure the following properties for the bucket. versioning – versioning enables you to keep multiple versions of an object in one bucket. by default, versioning is disabled for a new bucket. for information about enabling versioning, see . server access logging – server access logging provides detailed records for the requests that are made to your bucket. by default, amazon s3 does not collect server access logs. for information about enabling server access logging, see . static website hosting – you can host a static website on amazon s3. to enable static website hosting, choose static website hosting and then specify the settings you want to use. for more information, see . object-level logging – object-level logging records object-level api activity by using cloudtrail data events. for information about enabling object-level logging, see . tags – with aws cost allocation, you can use bucket tags to annotate billing for your use of a bucket. a tag is a key-value pair that represents a label that you assign to a bucket. to add tags, choose tags, and then choose add tag. for more information, see  in the amazon simple storage service developer guide.  transfer acceleration – amazon s3 transfer acceleration enables fast, easy, and secure transfers of files over long distances between your client and an s3 bucket. for information about enabling transfer acceleration, see . events – you can enable certain amazon s3 bucket events to send a notification message to a destination whenever the events occur. to enable events, choose events and then specify the settings you want to use. for more information, see .  requester pays – you can enable requester pays so that the requester (instead of the bucket owner) pays for requests and data transfers. for more information, see  in the amazon simple storage service developer guide.  
in the amazon s3 console, you can copy an object to a bucket or a folder. for more information, see  in the amazon simple storage service developer guide. to copy an object sign in to the aws management console and open the amazon s3 console at . in the buckets list, choose your bucket name. on the overview tab, select the check box beside the object that you want to copy. choose actions, and choose copy. choose the copy destination: if you want to copy your object to a bucket, select the bucket.if you want to copy your object to a folder in a bucket, choose the bucket name, and then choose the folder.under review, confirm the copy details, and choose copy. amazon s3 copies your object to the destination. 
amazon s3 block public access prevents the application of any settings that allow public access to data within s3 buckets. this section describes how to edit block public access settings for one or more s3 buckets. for information about blocking public access using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. topics follow these steps if you need to change the public access settings for a single s3 bucket. to edit the amazon s3 block public access settings for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose permissions. choose edit to change the public access settings for the bucket. for more information about the four amazon s3 block public access settings, see  in the amazon simple storage service developer guide. choose the setting that you want to change, and then choose save. when you're asked for confirmation, enter confirm. then choose confirm to save your changes. follow these steps if you need to change the public access settings for more than one s3 bucket. to edit the amazon s3 block public access settings for multiple s3 buckets sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the buckets that you want, and then choose edit public access settings. choose the setting that you want to change, and then choose save. when you're asked for confirmation, enter confirm. then choose confirm to save your changes. you can change amazon s3 block public access settings when you create a bucket. for more information, see .  
s3 batch operations performs large-scale batch operations on amazon s3 objects. you can use s3 batch operations to copy objects, set object tags or access control lists (acls), initiate object restores from amazon s3 glacier, or invoke an aws lambda function to perform custom actions using your objects. you can perform these operations on a custom list of objects, or you can use an amazon s3 inventory report to make generating even the largest lists of objects easy. s3 batch operations use the same amazon s3 apis that you already use, so you'll find the interface familiar. for information about performing s3 batch operations using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide.  the following topics explain how to use the amazon s3 console to configure and run batch operations. topics 
this section explains how to configure amazon s3 storage management tools. topics 
before you can enable event notifications for your bucket you must set up one of the following destination types: an amazon sns topicamazon simple notification service (amazon sns) is a web service that coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. you can use the amazon sns console to create an amazon sns topic that your notifications can be sent to. the amazon sns topic must be in the same region as your amazon s3 bucket. for information about creating an amazon sns topic, see  in the amazon simple notification service developer guide.before you can use the amazon sns topic that you create as an event notification destination, you need the following:   the amazon resource name (arn) for the amazon sns topica valid amazon sns topic subscription (the topic subscribers are notified when a message is published to your amazon sns topic)a permissions policy that you set up in the amazon sns console (as shown in the following example) an amazon sqs queueyou can use the amazon sqs console to create an amazon sqs queue that your notifications can be sent to. the amazon sqs queue must be in the same region as your amazon s3 bucket. for information about creating an amazon sqs queue, see  in the amazon simple queue service developer guide.before you can use the amazon sqs queue as an event notification destination, you need the following:   the amazon resource name (arn) for the amazon sqs topica permissions policy that you set up in the amazon sqs console (as shown in the following example) a lambda functionyou can use the aws lambda console to create a lambda function. the lambda function must be in the same region as your s3 bucket. for information about creating a lambda function, see the .before you can use the lambda function as an event notification destination, you must have the name or the arn of a lambda function to set up the lambda function as a event notification destination.  warningif your notification ends up writing to the bucket that triggers the notification, this could cause an execution loop. for example, if the bucket triggers a lambda function each time an object is uploaded, and the function uploads an object to the bucket, then the function indirectly triggers itself. to avoid this, use two buckets, or configure the trigger to only apply to a prefix used for incoming objects.for more information and an example of using amazon s3 notifications with aws lambda, see  in the aws lambda developer guide.  
replication is the automatic, asynchronous copying of objects across buckets in the same or different aws regions. it replicates newly created objects and object updates from a source bucket to a specified destination bucket.  you use the amazon s3 console to add replication rules to the source bucket. replication rules define the source bucket objects to replicate and the destination bucket where the replicated objects are stored. for more information about replication, see  in the amazon simple storage service developer guide. you can manage replication rules on the replication page. you can add, view, enable, disable, delete, and change the priority of the replication rules. for information about adding replication rules to a bucket, see . to manage the replication rules for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose management, and then choose replication. you change the replication rules in the following ways. to change settings that affect all the replication rules in the bucket, choose edit global settings. you can change the destination bucket, and the iam role. if needed, you can copy the required bucket policy for cross-account destination buckets. to change a replication rule, select the rule and choose edit, which starts the replication wizard to help you make the change. for information about using the wizard, see .to enable or disable a replication rule, select the rule, choose more, and in the drop-down list, choose enable rule or disable rule. you can also disable, enable, or delete all the rules in the bucket from the more drop-down list.to change the rule priorities, choose edit priorities. you can then change the priority for each rule under the priority column heading. choose save to save your changes. you set rule priorities to avoid conflicts caused by objects that are included in the scope of more than one rule. in the case of overlapping rules, amazon s3 uses the rule priority to determine which rule to apply. the higher the number, the higher the priority. for more information about rule priority, see  in the amazon simple storage service developer guide.  in the amazon simple storage service developer guide
for the latest aws terminology, see the  in the aws general reference. 
this section explains how to use the amazon simple storage service (amazon s3) console to add a new bucket policy or edit an existing bucket policy. a bucket policy is a resource-based aws identity and access management (iam) policy. you add a bucket policy to a bucket to grant other aws accounts or iam users access permissions for the bucket and the objects in it. object permissions apply only to the objects that the bucket owner creates. for more information about bucket policies, see  in the amazon simple storage service developer guide.  for examples of amazon s3 bucket policies, see  in the amazon simple storage service developer guide.  to create or edit a bucket policy sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to create a bucket policy for or whose bucket policy you want to edit. choose permissions, and then choose bucket policy. in the bucket policy editor text box, type or copy and paste a new bucket policy, or edit an existing policy. the bucket policy is a json file. the text you type in the editor must be valid json. choose save. noteamazon s3 displays the amazon resource name (arn) for the bucket next to the bucket policy editor title. for more information about arns, see  in the amazon web services general reference.directly below the bucket policy editor text box is a link to the policy generator, which you can use to create a bucket policy. 
before you can upload data to amazon s3, you must create a bucket in one of the aws regions to store your data. after you create a bucket, you can upload an unlimited number of data objects to the bucket.  the aws account that creates the bucket owns it. by default, you can create up to 100 buckets in each of your aws accounts. if you need additional buckets, you can increase your account bucket quota to a maximum of 1,000 buckets by submitting a service quota increase. for information about how to increase your bucket quota, see  in the aws general reference.  buckets have configuration properties, including geographical region, access settings for the objects in the bucket, and other metadata.  to create a bucket sign in to the aws management console and open the amazon s3 console at . choose create bucket. the create bucket wizard opens. in bucket name, enter a dns-compliant name for your bucket. the bucket name must: be unique across all of amazon s3.be between 3 and 63 characters long.not contain uppercase characters.start with a lowercase letter or number.after you create the bucket, you can't change its name. for information about naming buckets, see  in the amazon simple storage service developer guide. importantavoid including sensitive information, such as account numbers, in the bucket name. the bucket name is visible in the urls that point to the objects in the bucket. in region, choose the aws region where you want the bucket to reside.  choose a region close to you to minimize latency and costs and address regulatory requirements. objects stored in a region never leave that region unless you explicitly transfer them to another region. for a list of amazon s3 aws regions, see  in the amazon web services general reference. in bucket settings for block public access, choose the block public access settings that you want to apply to the bucket.  we recommend that you leave all settings enabled unless you know you need to turn one or more of them off for your use case, such as to host a public website. block public access settings that you enable for the bucket will also be enabled for all access points that you create on the bucket. for more information about blocking public access, see  in the amazon simple storage service developer guide. (optional) if you want to enable s3 object lock: choose advanced settings, and read the message that appears. importantyou can only enable s3 object lock for a bucket when you create it. if you enable object lock for the bucket, you can't disable it later. enabling object lock also enables versioning for the bucket. after you enable object lock for the bucket, you must configure the object lock settings before any objects in the bucket will be protected. for more information about configuring protection for objects, see . if you want to enable object lock, enter enable in the text box and choose confirm. for more information about the s3 object lock feature, see  in the amazon simple storage service developer guide. choose create bucket. 
this section explains how to use the amazon s3 console to delete folders from an s3 bucket.  for information about amazon s3 features and pricing, see . to delete folders from an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to delete folders from. in the name list, select the check box next to the folders and objects that you want to delete, choose more, and then choose delete. in the delete objects dialog box, verify that the names of the folders you selected for deletion are listed and then choose delete. 
latest documentation update: march 27, 2019 the following table describes the important changes in each release of the amazon simple storage service console user guide from june 19, 2018, onward. for notification about updates to this documentation, you can subscribe to an rss feed. the following table describes the important changes in each release of the amazon simple storage service console user guide before june 19, 2018.  
amazon s3 block public access prevents the application of any settings that allow public access to data within s3 buckets. you can configure block public access settings for an individual s3 bucket or for all the buckets in your account. for information about blocking public access using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. the following topics explain how to use the amazon s3 console to configure block public access settings:  the following sections explain viewing bucket access status and searching by access types. the list buckets view shows whether your bucket is publicly accessible. amazon s3 labels the permissions for a bucket as follows: public – everyone has access to one or more of the following: list objects, write objects, read and write permissions. objects can be public – the bucket is not public, but anyone with the appropriate permissions can grant public access to objects. buckets and objects not public – the bucket and objects do not have any public access.only authorized users of this account – access is isolated to iam users and roles in this account and aws service principals because there is a policy that grants public access.the access column shows the access status of the listed buckets.  you can also filter bucket searches by access type. choose an access type from the drop-down list that is next to the search for buckets bar.   
this section explains how to create an amazon s3 access point using the aws management console. for information about creating access points using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. an access point is associated with exactly one amazon s3 bucket. before you begin, make sure that you have created a bucket that you want to use with this access point. for more information about creating buckets, see . to create an access point sign in to the aws management console and open the amazon s3 console at . in the s3 buckets section, select the bucket that you want to attach this access point to. on the bucket detail page, choose the access points tab. choose create access point. enter your desired name for the access point in the access point name field. choose a network access type. if you choose virtual private cloud (vpc), enter the vpc id that you want to use with the access point. for more information about network access type for access points, see  in the amazon simple storage service developer guide. select the block public access settings that you want to apply to the access point. all block public access settings are enabled by default for new access points, and we recommend that you leave all settings enabled unless you know you have a specific need to disable any of them. amazon s3 currently doesn't support changing an access point's block public access settings after the access point has been created. for more information about using amazon s3 block public access with access points, see  in the amazon simple storage service developer guide. (optional) specify the access point policy. the console automatically displays the amazon resource name (arn) for the access point, which you can use in the policy. choose create access point. 
this section explains how to use the amazon s3 console to delete objects. because all objects in your s3 bucket incur storage costs, you should delete objects that you no longer need. if you are collecting log files, for example, it's a good idea to delete them when they're no longer needed. you can set up a lifecycle rule to automatically delete objects such as log files. for information about amazon s3 features and pricing, see . to delete objects from an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to delete an object from. you can delete objects from an s3 bucket in any of the following ways: in the name list, select the check box next to the objects and folders that you want to delete, choose actions, and then choose delete from the drop-down menu. in the delete objects dialog box, verify that the name(s) of the object(s) and/or folder(s) you selected for deletion are listed and then choose delete. or, choose the name of the object that you want to delete, choose latest version, and then choose the trash can icon.
by using the amazon s3 analytics storage class analysis tool, you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. storage class analysis observes data access patterns to help you determine when to transition less frequently accessed standard storage to the standard_ia (ia, for infrequent access) storage class. for more information about standard_ia, see the  and  in the amazon simple storage service developer guide. importantstorage class analysis does not give recommendations for transitions to the onezone_ia or s3 glacier storage classes. for more information about analytics, see  in the amazon simple storage service developer guide. to configure storage class analysis sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket for which you want to configure storage class analysis. choose the management tab, and then choose analytics. choose add. type a name for the filter. if you want to analyze the whole bucket, leave the prefix / tags field empty. in the prefix / tags  field, type text for the prefix or tag for the objects that you want to analyze, or choose from the dropdown list that appears when you start typing. if you chose tag, enter a value for the tag. you can enter one prefix and multiple tags. optionally, you can choose export data to export analysis reports to a comma-separated values (.csv) flat file. choose a destination bucket where the file can be stored. you can type a prefix for the destination bucket. the destination bucket must be in the same aws region as the bucket for which you are setting up the analysis. the destination bucket can be in a different aws account. choose save. amazon s3 creates a bucket policy on the destination bucket that grants amazon s3 write permission. this allow it to write the export data to the bucket.  if an error occurs when you try to create the bucket policy, you'll be given instructions on how to fix it. for example, if you chose a destination bucket in another aws account and do not have permissions to read and write to the bucket policy, you'll see the following message. you must have the destination bucket owner add the displayed bucket policy to the destination bucket. if the policy is not added to the destination bucket you won’t get the export data because amazon s3 doesn’t have permission to write to the destination bucket. if the source bucket is owned by a different account than that of the current user, then the correct account id of the source bucket must be substituted in the policy.  for information about the exported data and how the filter works, see  in the amazon simple storage service developer guide. more info  
there are three types of amazon cloudwatch metrics for amazon s3: storage metrics, request metrics, and replication metrics. replication metrics are available 15 minutes after a replication rule with s3 replication time control (s3 rtc) has been enabled. replication metrics are billed at the standard amazon cloudwatch rate. they are turned on automatically when you enable replication with s3 rtc using the aws management console or the amazon s3 api. replication metrics track the rule ids of the replication configuration. a replication rule id can be specific to a prefix, a tag, or a combination of both. for more information about s3 replication time control (s3 rtc), see  in the amazon simple storage service developer guide. for more information about cloudwatch metrics for amazon s3, see  in the amazon simple storage service developer guide. to view replication metrics sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the objects you want replication metrics for. choose the management tab, and then choose metrics. choose replication.   in the rule ids list in the left pane, select the rule ids that you want. if you have several rule ids to choose from, you can search for the ids that you want. after choosing the rule ids that you want, choose display graphs below the rule ids selection box. you can then view the replication metrics replication latency (in seconds), operations pending replication, and bytes pending replication for the rules that you selected. amazon cloudwatch begins reporting replication metrics 15 minutes after you enable s3 rtc on the respective replication rule. you can view replication metrics on the amazon s3 or cloudwatch console. for information, see  in the amazon simple storage service developer guide. 
amazon s3 has a flat structure instead of a hierarchy like you would typically see in a file system. however, for the sake of organizational simplicity, the amazon s3 console supports a folder concept as a way to group objects. in amazon s3, the folder is a naming prefix for an object or group of objects. for more information, see  we recommend blocking all public access to your amazon s3 folders and buckets unless you specifically require a public folder or bucket. when you make a folder public, anyone on the internet can view all the objects that are grouped in that folder. in the amazon s3 console, you can make a folder public. you can also make a folder public by creating a bucket policy that limits access by prefix. for more information, see .  warningafter you make a folder public in the amazon s3 console, you can't make it private again. instead, you must set permissions on each individual object in the public folder so that the objects have no public access. for more information, see  
access analyzer for s3 alerts you to s3 buckets that are configured to allow access to anyone on the internet or other aws accounts, including aws accounts outside of your organization. for each public or shared bucket, you receive findings into the source and level of public or shared access. for example, access analyzer for s3 might show that a bucket has read or write access provided through a bucket access control list (acl), a bucket policy, or an access point policy. armed with this knowledge, you can take immediate and precise corrective action to restore your bucket access to what you intended.  when reviewing an at-risk bucket in access analyzer for s3, you can block all public access to the bucket with a single click. we recommend that you block all access to your buckets unless you require public access to support a specific use case. before you block all public access, ensure that your applications will continue to work correctly without public access. for more information, see  in the amazon simple storage service developer guide. you can also drill down into bucket-level permission settings to configure granular levels of access. for specific and verified use cases that require public access, such as static website hosting, public downloads, or cross-account sharing, you can acknowledge and record your intent for the bucket to remain public or shared by archiving the findings for the bucket. you can revisit and modify these bucket configurations at any time. you can also download your findings as a csv report for auditing purposes. access analyzer for s3 is available at no extra cost on the amazon s3 console. access analyzer for s3 is powered by aws identity and access management (iam) access analyzer. to use access analyzer for s3 in the amazon s3 console, you must visit the iam console and enable iam access analyzer on a per-region basis.  for more information about iam access analyzer, see  in the iam user guide. for more information about access analyzer for s3, review the following sections. importantaccess analyzer for s3 requires an account-level analyzer. to use access analyzer for s3, you must visit iam access analyzer and create an analyzer that has an account as the zone of trust. for more information, see  in iam user guide. when a bucket policy or bucket acl is added or modified, access analyzer generates and updates findings based on the change within 30 minutes. findings related to account level block public access settings may not be generated or updated for up to 6 hours after you change the settings. topics access analyzer for s3 provides findings for buckets that can be accessed outside your aws account. buckets that are listed under buckets with public access can be accessed by anyone on the internet. if access analyzer for s3 identifies public buckets, you also see a warning at the top of the page that shows you the number of public buckets in your region. buckets listed under buckets with access from other aws accounts — including third-party aws accounts are shared conditionally with other aws accounts, including accounts outside of your organization.  for each bucket, access analyzer for s3 provides the following information: bucket namediscovered by access analyzer ‐ when access analyzer for s3 discovered the public or shared bucket access.shared through ‐ how the bucket is shared—through a bucket policy, a bucket acl, or an access point policy. a bucket can be shared through both policies and acls. if you want to find and review the source for your bucket access, you can use the information in this column as a starting point for taking immediate and precise corrective action. status ‐ the status of the bucket finding. access analyzer for s3 displays findings for all public and shared buckets. active ‐ finding has not been reviewed. archived ‐ finding has been reviewed and confirmed as intended. all ‐ all findings for buckets that are public or shared with other aws accounts, including aws accounts outside of your organization.access level ‐ access permissions granted for the bucket:list ‐ list resources.read ‐ read but not edit resource contents and attributes.write ‐ create, delete, or modify resources.permissions ‐ grant or modify resource permissions.tagging ‐ update tags associated with the resource.to use access analyzer for s3, you must complete the following prerequisite steps. grant the required permissions. for more information, see  in the iam user guide. visit iam to create an account-level analyzer for each region where you want to use access analyzer. access analyzer for s3 requires an account-level analyzer. to use access analyzer for s3, you must create an analyzer that has an account as the zone of trust. for more information, see  in iam user guide. if you want to block all access to a bucket in a single click, you can use the block all public access button in access analyzer for s3. when you block all public access to a bucket, no public access is granted. we recommend that you block all public access to your buckets unless you require public access to support a specific and verified use case. before you block all public access, ensure that your applications will continue to work correctly without public access. if you don't want to block all public access to your bucket, you can edit your block public access settings on the amazon s3 console to configure granular levels of access to your buckets. for more information, see  in the amazon simple storage service developer guide. in rare events, access analyzer for s3 might report no findings for a bucket that an amazon s3 block public access evaluation reports as public. this happens because amazon s3 block public access reviews policies for current actions and any potential actions that might be added in the future, leading to a bucket becoming public. on the other hand, access analyzer for s3 only analyzes the current actions specified for the amazon s3 service in the evaluation of access status. to block all public access to a bucket using access analyzer for s3 sign in to the aws management console and open the amazon s3 console at . in the navigation pane on the left, under dashboards, choose access analyzer for s3. in access analyzer for s3, choose a bucket. choose block all public access. to confirm your intent to block all public access to the bucket, in block all public access (bucket settings), enter confirm. amazon s3 blocks all public access to your bucket. the status of the bucket finding updates to resolved, and the bucket disappears from the access analyzer for s3 listing. if you want to review resolved buckets, open iam access analyzer on the iam console. if you did not intend to grant access to the public or other aws accounts, including accounts outside of your organization, you can modify the bucket acl, bucket policy, or access point policy to remove the access to the bucket. the shared through column shows all sources of bucket access: bucket policy, bucket acl, and/or access point policy. to review and change a bucket policy, a bucket acl, or an access point policy open the amazon s3 console at . in the navigation pane, choose access analyzer for s3. to see whether public access or shared access is granted through a bucket policy, a bucket acl, or an access point policy, look in the shared through column. under bucket name, choose the name of the bucket with the bucket policy, bucket acl, or access point policy that you want to change or review. if you want to change or view a bucket acl: choose permissions. choose access control list. review your bucket acl, and make changes as required. for more information, see  if you want to change or review a bucket policy: choose permissions. choose bucket policy. review or change your bucket policy as required. for more information, see  if you want to review or change an access point policy: choose access points. choose the access point name. review or change access as required.  for more information, see . if you edit or remove a bucket acl, a bucket policy, or an access point policy to remove public or shared access, the status for the bucket findings updates to resolved. the resolved bucket findings disappear from the access analyzer for s3 listing, but you can view them in iam access analyzer. if a bucket grants access to the public or other aws accounts, including accounts outside of your organization, to support a specific use case (for example, a static website, public downloads, or cross-account sharing), you can archive the finding for the bucket. when you archive bucket findings, you acknowledge and record your intent for the bucket to remain public or shared. archived bucket findings remain in your access analyzer for s3 listing so that you always know which buckets are public or shared. to archive bucket findings in access analyzer for s3 open the amazon s3 console at . in the navigation pane, choose access analyzer for s3. in access analyzer for s3, choose an active bucket. to acknowledge your intent for this bucket to be accessed by the public or other aws accounts, including accounts outside of your organization, choose archive. enter confirm, and choose archive. after you archive findings, you can always revisit them and change their status back to active, indicating that the bucket requires another review.  to activate an archived bucket finding in access analyzer for s3 open the amazon s3 console at . in the navigation pane, choose access analyzer for s3. choose the archived bucket findings. choose mark as active. if you need to see more information about a bucket, you can open the bucket finding details in iam access analyzer on the iam console. to view finding details in access analyzer for s3 open the amazon s3 console at . in the navigation pane, choose access analyzer for s3. in access analyzer for s3, choose a bucket. choose view details. the finding details open in iam access analyzer on the iam console.  you can download your bucket findings as a csv report that you can use for auditing purposes. the report includes the same information that you see in access analyzer for s3 on the amazon s3 console. to download a report open the amazon s3 console at . in the navigation pane on the left, choose access analyzer for s3. in the region filter, choose the region. access analyzer for s3 updates to shows buckets for the chosen region. choose download report. a csv report is generated and saved to your computer. 
there are three types of amazon cloudwatch metrics for amazon s3: storage metrics, request metrics and replication. storage metrics are reported once per day and are provided to all customers at no additional cost. request metrics are available at 1 minute intervals after some latency to process, and metrics are billed at the standard cloudwatch rate. to get request metrics, you must opt into them by configuring them in the console or with the amazon s3 api. for more conceptual information about cloudwatch metrics for amazon s3, see  in the amazon simple storage service developer guide. to filter request metrics on a subset of objects in a bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that has the objects you want to get request metrics for. choose the management tab. and then choose metrics. choose requests. from filters in the left-side pane, choose add. provide a name for this metrics configuration. provide one or more prefixes or tags, separated by commas, in prefix /tags that you want to monitor. from the drop down, select whether the value you provided is a tag or a prefix. choose save. you have now created a metrics configuration for request metrics on a subset of the objects in an amazon s3 bucket. about 15 minutes after cloudwatch begins tracking these request metrics, you can see graphs for the metrics in both the amazon s3 or cloudwatch consoles. you can also request metrics at the bucket level. for information, see  
in amazon s3, buckets and objects are the primary resources, and objects are stored in buckets. amazon s3 has a flat structure instead of a hierarchy like you would see in a file system. however, for the sake of organizational simplicity, the amazon s3 console supports the folder concept as a means of grouping objects. amazon s3 does this by using a shared name prefix for objects (that is, objects have names that begin with a common string). object names are also referred to as key names. for example, you can create a folder on the console named  and store an object named  in it. the object is then stored with the key name , where  is the prefix. here are two more examples:  if you have three objects in your bucket—, , and —the console will show a folder named . if you open the folder in the console, you will see three objects: , , and .if you have an object named , the console will show you a folder named  containing the folder  and the object .topics you can have folders within folders, but not buckets within buckets. you can upload and copy objects directly into a folder. folders can be created, deleted, and made public, but they cannot be renamed. objects can be copied from one folder to another.  importantthe amazon s3 console treats all objects that have a forward slash ("/") character as the last (trailing) character in the key name as a folder, for example . you can't upload an object that has a key name with a trailing "/" character using the amazon s3 console. however, you can upload objects that are named with a trailing "/" with the amazon s3 api by using the aws cli, aws sdks, or rest api.an object that is named with a trailing "/" appears as a folder in the amazon s3 console. the amazon s3 console does not display the content and metadata for such an object. when you use the console to copy an object named with a trailing "/", a new folder is created in the destination location, but the object's data and metadata are not copied.  
you can enable certain amazon s3 bucket events to send a notification message to a destination whenever the events occur. this section explains how to use the amazon s3 console to enable event notifications. for information about using event notifications with the aws sdks and the amazon s3 rest apis, see  in the amazon simple storage service developer guide.  topics when you configure event notifications for a bucket, you must specify the type of events for which you want to receive notifications. for a complete list of event types, see  section in the amazon simple storage service developer guide.  in the amazon s3 console, you have the following options for configuring event notifications. you can choose a single option or multiple options. object creationobjectcreated (all) – receive a notification when an object is created in your bucket for any of the following object creation actions: put, post, copy, and multipart upload completed.put, post, copy, and multipart upload completed – receive a notification for one of these specific object creation actions.object deletion objectdelete (all) – receive a notification any time an object in your bucket is deleted.delete marker created – receive a notification when a delete marker is created for a versioned object.  for information about deleting versioned objects, see . for information about object versioning, see  and . object restoration from the s3 glacier storage class restore initiated – receive a notification for initiation of object restoration.restore completed – receive a notification for completion of object restoration.reduced redundancy storage (rrs) object lost eventsobject in rss lost – receive a notification that an object of the rrs storage class has been lostobjects eligible for replication using amazon s3 replication time controlreplication time missed threshold – receive a notification that an object failed to replicate.replication time completed after threshold – receive a notification that an object exceeded the 15-minute threshold for replication.replication time not tracked – receive a notification that an object replicated after the 15-minute threshold.replication time failed – receive a notification that an object that was eligible for replication is no longer being tracked by replication metrics.notewhen you delete the last object from a folder, amazon s3 can generate an object creation event. when there are multiple objects with the same prefix with a trailing slash (/) as part of their names, those objects are shown as being part of a folder in the amazon s3 console. the name of the folder is formed from the characters preceding the trailing slash (/).when you delete all the objects listed under that folder, no actual object is available to represent the empty folder. under such circumstances, the amazon s3 console creates a zero-byte object to represent that folder. if you enabled event notification for the creation of objects, the zero-byte object creation action that is taken by the console triggers an object creation event.the amazon s3 console displays a folder under the following circumstances:when a zero-byte object has a trailing slash (/) in its name. in this case, there is an actual amazon s3 object of 0 bytes that represents a folder. if the object has a slash (/) within its name. in this case, there isn't an actual object representing the folder. when you configure event notifications for a bucket, you also choose the notification destination. event notification messages can be sent to the following destinations: amazon simple notification service (amazon sns) topic – coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. for information about the amazon sns topic format, see .amazon simple queue service (amazon sqs) queue – offers reliable and scalable hosted queues for storing messages as they travel between computers. for information about amazon sqs, see  in the amazon simple queue service developer guide.aws lambda  – invoke a lambda function and provide the event message as an argument. when you create a lambda function, you package up and upload your custom code to aws lambda. aws lambda uses the aws infrastructure to run the code on your behalf. for information about using lambda with amazon s3, see  in the aws lambda developer guide.for more information about granting the amazon s3 service principal the permissions required to publish event notifications to a destination, see  in the amazon s3 developer guide. warningif your notification ends up writing to the bucket that triggers the notification, this could cause an execution loop. for example, if the bucket triggers a lambda function each time an object is uploaded, and the function uploads an object to the bucket, then the function indirectly triggers itself. to avoid this, use two buckets, or configure the trigger to only apply to a prefix used for incoming objects.for more information and an example of using amazon s3 notifications with aws lambda, see  in the aws lambda developer guide.  before you can enable event notifications for your bucket, you must set up one of the destination types. for more information, see  to enable and configure event notifications for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to enable events for. choose properties. under advanced settings, choose events. choose add notification. in name, enter a descriptive name for your event notification.  if you don't enter a name, a guid is generated and used for the name. under events, select one or more events.  for a listing of the event types, see . to filter event notifications by a prefix or suffix, enter a prefix or a suffix.  for example, you can set up a prefix filter so that you receive notifications only when files are added to a specific folder (for example, ). for more information, see . choose the event notification destination: sns topic, sqs queue, or lambda function.  for a description of the destinations, see . when you choose your send to destination, a box appears for you to enter your specific sns, sqs, or lambda function destination. in the example image below, the send to location is sns topic, and you can see a sns box for the sns topic name. in the box that appears, choose or enter the destination sns, sqs, or lambda function.  you can choose or enter the sns, sqs, or lambda function name, or you can choose to the destination amazon resource name (arn). the example screenshot below shows the add sns topic arn option. if you chose add arn, enter the sns topic, sqs queue, or lambda function arn. choose save.  amazon s3 sends a test message to the event notification destination. 
you can empty a bucket, which deletes all of the objects in the bucket without deleting the bucket. when you empty a bucket with versioning enabled, all versions of all the objects in the bucket are deleted. for more information, see  and  in the amazon simple storage service developer guide. to empty an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, select the option next to the name of the bucket that you want to empty and then choose empty. on the empty bucket page, confirm that you want to empty the bucket by entering the bucket name into the text field, and then choose empty. (optional) monitor the progress of the bucket emptying process on the empty bucket: status page. 
this topic explains how to use the aws management console to upload one or more files or entire folders to an amazon s3 bucket. before you can upload files and folders to an amazon s3 bucket, you need write permissions for the bucket. for more information about access permissions, see . for information about uploading files programmatically, see  in the amazon simple storage service developer guide.  when you upload a file to amazon s3, it is stored as an s3 object. objects consist of the file data and metadata that describes the object. you can have an unlimited number of objects in a bucket. you can upload any file type—images, backups, data, movies, etc.—into an s3 bucket. the maximum size of a file that you can upload by using the amazon s3 console is 160 gb. to upload a file larger than 160 gb, use the aws cli, aws sdk, or amazon s3 rest api. for more information, see  in the amazon simple storage service developer guide.  you can upload files by dragging and dropping or by pointing and clicking. to upload folders, you must drag and drop them. drag and drop functionality is supported only for the chrome and firefox browsers. for information about which chrome and firefox browser versions are supported, see .  when you upload a folder, amazon s3 uploads all of the files and subfolders from the specified folder to your bucket. it then assigns an object key name that is a combination of the uploaded file name and the folder name. for example, if you upload a folder called  that contains two files,  and , amazon s3 uploads the files and then assigns the corresponding key names,  and . the key names include the folder name as a prefix. the amazon s3 console displays only the part of the key name that follows the last “/”. for example, within an images folder the  and  objects are displayed as  and a . if you upload individual files and you have a folder open in the amazon s3 console, when amazon s3 uploads the files, it includes the name of the open folder as the prefix of the key names. for example, if you have a folder named  open in the amazon s3 console and you upload a file named , the key name is . however, the object is displayed in the console as  in the  folder. if you upload individual files and you do not have a folder open in the amazon s3 console, when amazon s3 uploads the files, it assigns only the file name as the key name. for example, if you upload a file named , the key name is . for more information on key names, see  in the amazon simple storage service developer guide. if you upload an object with a key name that already exists in a versioning-enabled bucket, amazon s3 creates another version of the object instead of replacing the existing object. for more information about versioning, see . topics if you are using the chrome or firefox browsers, you can choose the folders and files to upload, and then drag and drop them into the destination bucket. dragging and dropping is the only way that you can upload folders. to upload folders and files to an s3 bucket by using drag and drop sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to upload your folders or files to. in a window other than the console window, select the files and folders that you want to upload. then drag and drop your selections into the console window that lists the objects in the destination bucket. the files you chose are listed in the upload dialog box. in the upload dialog box, do one of the following:  drag and drop more files and folders to the console window that displays the upload dialog box. to add more files, you can also choose add more files. this option works only for files, not folders. to immediately upload the listed files and folders without granting or removing permissions for specific users or setting public permissions for all of the files that you're uploading, choose upload. for information about object access permissions, see .  to set permissions or properties for the files that you are uploading, choose next. on the set permissions page, under manage users you can change the permissions for the aws account owner. the owner refers to the aws account root user, and not an aws identity and access management (iam) user. for more information about the root user, see .  choose add account to grant access to another aws account. for more information about granting permissions to another aws account, see . under manage public permissions you can grant read access to your objects to the general public (everyone in the world), for all of the files that you're uploading. granting public read access is applicable to a small subset of use cases such as when buckets are used for websites. we recommend that you do not change the default setting of do not grant public read access to this object(s). you can always make changes to object permissions after you upload the object. for information about object access permissions, see .  when you're done configuring permissions, choose next. on the set properties page, choose the storage class and encryption method to use for the files that you are uploading. you can also add or modify metadata.  choose a storage class for the files you're uploading. for more information about storage classes, see  in the amazon simple storage service developer guide. choose the type of encryption for the files that you're uploading. if you don't want to encrypt them, choose none.  to encrypt the uploaded files using keys that are managed by amazon s3, choose amazon s3 master-key. for more information, see  in the amazon simple storage service developer guide. to encrypt the uploaded files using the aws key management service (aws kms), choose aws kms master-key. then choose a customer master key (cmk) from the list of aws kms cmks. noteto encrypt objects in a bucket, you can use only cmks that are available in the same aws region as the bucket.  you can give an external account the ability to use an object that is protected by an aws kms cmk. to do this, select custom kms arn from the list and enter the amazon resource name (arn) for the external account. administrators of an external account that have usage permissions to an object protected by your aws kms cmk can further restrict access by creating a resource-level iam policy.  for more information about creating an aws kms cmk, see  in the aws key management service developer guide. for more information about protecting data with aws kms, see  in the amazon simple storage service developer guide. metadata for amazon s3 objects is represented by a name-value (key-value) pair. there are two kinds of metadata: system-defined metadata and user-defined metadata. if you want to add amazon s3 system-defined metadata to all of the objects you are uploading, for header, select a header. you can select common http headers, such as content-type and content-disposition. type a value for the header, and then choose save. for a list of system-defined metadata and information about whether you can add the value, see  in the amazon simple storage service developer guide. any metadata starting with prefix  is treated as user-defined metadata. user-defined metadata is stored with the object, and is returned when you download the object.  to add user-defined metadata to all of the objects that you are uploading, type  plus a custom metadata name in the header field. type a value for the header, and then choose save. both the keys and their values must conform to us-ascii standards. user-defined metadata can be as large as 2 kb. for more information about user-defined metadata, see  in the amazon simple storage service developer guide. object tagging gives you a way to categorize storage. each tag is a key-value pair. key and tag values are case sensitive. you can have up to 10 tags per object.  to add tags to all of the objects that you are uploading, type a tag name in the key field. type a value for the tag, and then choose save. a tag key can be up to 128 unicode characters in length and tag values can be up to 255 unicode characters in length. for more information about object tags, see  in the amazon simple storage service developer guide. choose next. on the upload review page, verify that your settings are correct, and then choose upload. to make changes, choose previous. to see the progress of the upload, choose in progress at the bottom of the browser window. to see a history of your uploads and other operations, choose success. this procedure explains how to upload files into an s3 bucket by choosing upload. to upload files to an s3 bucket by pointing and clicking sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to upload your files to. choose upload. in the upload dialog box, choose add files. choose one or more files to upload, and then choose open. after you see the files that you chose listed in the upload dialog box, do one of the following:  to add more files, choose add more files. to immediately upload the listed files, choose upload.  to set permissions or properties for the files that you are uploading, choose next. to set permissions and properties, start with step 5 of . .
you can delete an empty bucket, and when you're using the aws management console, you can delete a bucket that contains objects. if you delete a bucket that contains objects, all the objects in the bucket are permanently deleted.  when you delete a bucket with versioning enabled, all versions of all the objects in the bucket are permanently deleted. for more information about versioning, see  in the amazon simple storage service developer guide. before deleting a bucket, consider the following: bucket names are unique. if you delete a bucket, another aws user can use the name. when you delete a bucket that contains objects, all the objects in the bucket are permanently deleted, including objects that transitioned to the  storage class.if the bucket hosts a static website, and you created and configured an amazon route 53 hosted zone as described in : you must clean up the route 53 hosted zone settings that are related to the bucket as described in .if the bucket receives log data from elastic load balancing (elb): we recommend that you stop the delivery of elb logs to the bucket before deleting it. after you delete the bucket, if another user creates a bucket using the same name, your log data could potentially be delivered to that bucket. for information about elb access logs, see  in the user guide for classic load balancers and  in the user guide for application load balancers.importantif you want to continue to use the same bucket name, don't delete the bucket. we recommend that you empty the bucket and keep it. after a bucket is deleted, the name becomes available to reuse, but the name might not be available for you to reuse for various reasons. for example, it might take some time before the name can be reused, and some other account could create a bucket with that name before you do. to delete an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the buckets list, select the option next to the name of the bucket that you want to delete and then choose delete at the top of the page. on the delete bucket page, confirm that you want to delete the bucket by entering the bucket name into the text field, and then choose delete bucket. noteif the bucket contains any objects, empty the bucket before deleting it by selecting the empty bucket configuration link in the this bucket is not empty error alert and following the instructions on the empty bucket page. then return to the delete bucket page and delete the bucket. 
replication is the automatic, asynchronous copying of objects across buckets in the same or different aws regions. replication copies newly created objects and object updates from a source bucket to a destination bucket. for more information about replication concepts and how to use replication with the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide.  replication requires versioning to be enabled on both the source and destination buckets. to review the full list of requirements, see  in the amazon simple storage service developer guide. for more information about versioning, see  the object replicas in the destination bucket are exact replicas of the objects in the source bucket. they have the same key names and the same metadata—for example, creation time, owner, user-defined metadata, version id, access control list (acl), and storage class. optionally, you can explicitly specify a different storage class for object replicas. and regardless of who owns the source bucket or the source object, you can choose to change replica ownership to the aws account that owns the destination bucket. for more information, see  in the amazon simple storage service developer guide.  you can use s3 replication time control (s3 rtc) to replicate your data in the same aws region or across different aws regions in a predictable timeframe. s3 rtc replicates 99.99 percent of new objects stored in amazon s3 within 15 minutes and most objects within seconds. for more information, see  in the amazon simple storage service developer guide. note about replication and lifecycle rulesmetadata for an object remains identical between original objects and replica objects. lifecycle rules abide by the creation time of the original object, and not by when the replicated object becomes available in the destination bucket. however, lifecycle does not act on objects that are pending replication until replication is complete. you use the amazon s3 console to add replication rules to the source bucket. replication rules define which source bucket objects to replicate and the destination bucket where the replicated objects are stored. you can create a rule to replicate all the objects in a bucket or a subset of objects with a specific key name prefix, one or more object tags, or both. a destination bucket can be in the same aws account as the source bucket, or it can be in a different account. if you specify an object version id to delete, amazon s3 deletes that object version in the source bucket. but it doesn't replicate the deletion in the destination bucket. in other words, it doesn't delete the same object version from the destination bucket. this protects data from malicious deletions. if the destination bucket is in a different account from the source bucket, you must add a bucket policy to the destination bucket to grant the owner of the source bucket account permission to replicate objects in the destination bucket. the amazon s3 console builds this required bucket policy for you to copy and add to the destination bucket in the other account.  when you add a replication rule to a bucket, the rule is enabled by default, so it starts working as soon as you save it.  topics follow these steps to configure a replication rule when the destination bucket is in the same aws account as the source bucket. sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose management, choose replication, and then choose add rule. in the replication rule wizard, under set source, you have the following options for setting the replication source: to replicate the whole bucket, choose entire bucket bucket-name. to replicate all objects that have the same prefix (for example, all objects that have names that begin with the string ), choose prefix or tags. enter a prefix in the box, choose the prefix from the drop-down list, and then press enter. if you enter a prefix that is the name of a folder, you must use / (forward slash) as the last character (for example, ). for more information about prefixes, see  in the amazon simple storage service developer guide.to replicate all objects with one or more object tags, enter a tag in the box, choose the tag from the drop-down list, and then press enter. enter a tag value and then press enter. repeat the procedure to add another tag. you can combine a prefix and tags. for more information about object tags, see  in the amazon simple storage service developer guide.the new schema supports prefix and tag filtering and the prioritization of rules. for more information about the new schema, see  in the amazon simple storage service developer guide. the developer guide describes the xml used with the amazon s3 api that works behind the user interface. in the developer guide, the new schema is described as replication configuration xml v2. to replicate objects in the source bucket that are encrypted with aws key management service (aws kms), under replication criteria, select replicate objects encrypted with aws kms. under choose one or more keys for decrypting source objects are the source aws kms customer master keys (cmks) that you allow replication to use. all source cmks are included by default. you can choose to narrow the cmk selection.  objects encrypted by aws kms cmks that you do not select are not replicated. a cmk or a group of cmks is chosen for you, but you can choose the cmks if you want. for information about using aws kms with replication, see  in the amazon simple storage service developer guide. importantwhen you replicate objects that are encrypted with aws kms, the aws kms request rate doubles in the source region and increases in the destination region by the same amount. these increased call rates to aws kms are due to the way that data is re-encrypted using the customer master key (cmk) that you define for the replication destination region. aws kms has a request rate limit that is per calling account per region. for information about the limit defaults, see  in the aws key management service developer guide.if your current amazon s3 put object request rate during replication is more than half the default aws kms rate limit for your account, we recommend that you request an increase to your aws kms request rate limit. to request an increase, create a case in the aws support center at . for example, suppose that your current put object request rate is 1,000 requests per second and you use aws kms to encrypt your objects. in this case, we recommend that you ask aws support to increase your aws kms rate limit to 2,500 requests per second, in both your source and destination regions (if different), to ensure that there is no throttling by aws kms.to see your put object request rate in the source bucket, view  in the amazon cloudwatch request metrics for amazon s3. for information about viewing cloudwatch metrics, see  choose next. to choose a destination bucket from the account that you're currently using, on the set destination page, under destination bucket, choose buckets in this account. enter the name of the destination bucket for the replication, or choose a name in the drop-down list. if you want to choose a destination bucket from a different aws account, see . if versioning is not enabled on the destination bucket, you get a warning message that contains an enable versioning button. choose this button to enable versioning on the bucket. if you chose to replicate objects encrypted with aws kms, under destination encryption settings, enter the amazon resource name (arn) of the aws kms cmk to use to encrypt the replicas in the destination bucket. you can find the arn for your aws kms cmk in the iam console, under encryption keys.  or, you can choose a cmk name from the drop-down list. for more information about creating an aws kms cmk, see  in the aws key management service developer guide. importantthe amazon s3 console lists only 100 aws kms cmks per aws region. if you have more than 100 cmks in the same region, you can see only the first 100 cmks in the s3 console. to use a kms cmk that is not listed in the console, choose custom kms arn, and enter the kms cmk arn. if you want to replicate your data into a specific storage class in the destination bucket, on the set destination page, under destination options, select change the storage class for the replicated object(s). then choose the storage class that you want to use for the replicated objects in the destination bucket. if you don't select this option, the storage class for replicated objects is the same class as the original objects.  similarly, if you want to change object ownership in the destination bucket, choose change object ownership to the destination bucket owner. for more information about this option, see .  if you want to enable s3 replication time control (s3 rtc) in your replication configuration, select replication time control. notewhen you use s3 rtc, additional per-gb data transfer fees and cloudwatch metrics fees apply. choose next. set up an aws identity and access management (iam) role that amazon s3 can assume to replicate objects on your behalf. to set up an iam role, on the configure options page, under select role, do one of the following: we highly recommend that you choose create new role to have amazon s3 create a new iam role for you. when you save the rule, a new policy is generated for the iam role that matches the source and destination buckets that you choose. the name of the generated role is based on the bucket names and uses the following naming convention: replication_role_for_source-bucket_to_destination-bucket.you can choose to use an existing iam role. if you do, you must choose a role that grants amazon s3 the necessary permissions for replication. replication fails if this role does not grant amazon s3 sufficient permissions to follow your replication rule.  importantwhen you add a replication rule to a bucket, you must have the  permission to be able to pass the iam role that grants amazon s3 replication permissions. for more information, see  in the iam user guide.under rule name, enter a name for your rule to help identify the rule later. the name is required and must be unique within the bucket. if the bucket has existing replication rules, you are instructed to set a priority for the rule. you must set a priority for the rule to avoid conflicts caused by objects that are included in the scope of more than one rule. in the case of overlapping rules, amazon s3 uses the rule priority to determine which rule to apply. the higher the number, the higher the priority. for more information about rule priority, see  in the amazon simple storage service developer guide. under status, enabled is selected by default. an enabled rule starts to work as soon as you save it. if you want to enable the rule later, select disabled. choose next. on the review page, review your replication rule. if it looks correct, choose save. otherwise, choose previous to edit the rule before saving it.  after you save your rule, you can edit, enable, disable, or delete your rule on the replication page. follow these steps to configure a replication rule when the destination bucket is in a different aws account than the source bucket. sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose management, choose replication, and then choose add rule. if you have never created a replication rule before, start with . on the replication rule wizard set destination page, under destination bucket, choose buckets in another account. then enter the name of the destination bucket and the account id from a different aws account. choose save. after you save the destination bucket name and account id, you might get a warning message telling you to add a bucket policy to the destination bucket so that amazon s3 can verify whether versioning is enabled on the bucket. you are presented with a bucket policy in a few steps that you can copy and add to the destination bucket in the other account. for information about adding a bucket policy to an s3 bucket and versioning, see  and  if you chose to replicate objects encrypted with aws kms, under destination encryption settings, enter the amazon resource name (arn) aws kms cmk to use to encrypt the replicas in the destination bucket.  for more information about creating an aws kms cmk, see  in the aws key management service developer guide. on the set destination page, under destination options: to replicate your data into a specific storage class in the destination bucket, select change the storage class for the replicated object(s). then choose the storage class that you want to use for the replicated objects in the destination bucket. if you don't select this option, the storage class for replicated objects is the same class as the original objects.to change the object ownership of the replica objects to the destination bucket owner, select change object ownership to destination owner. this option enables you to separate object ownership of the replicated data from the source. if asked, type the account id of the destination bucket. when you select this option, regardless of who owns the source bucket or the source object, the aws account that owns the destination bucket is granted full permission to replica objects. for more information, see  in the amazon simple storage service developer guide. if you want to add s3 replication time control (s3 rtc) to your replication configuration, select replication time control. notewhen you use s3 rtc, additional per-gb data transfer fees and cloudwatch metrics fees apply.choose next. set up an aws identity and access management (iam) role that amazon s3 can assume to perform replication of objects on your behalf. to set up an iam role, on the configure options page, under select role, do one of the following: we highly recommend that you choose create new role to have amazon s3 create a new iam role for you. when you save the rule, a new policy is generated for the iam role that matches the source and destination buckets that you choose. the name of the generated role is based on the bucket names and uses the following naming convention: replication_role_for_source-bucket_to_destination-bucket.you can choose to use an existing iam role. if you do, you must choose a role that allows amazon s3 to replicate objects from the source bucket to the destination bucket on your behalf.a bucket policy is provided on the configure options page that you can copy and add to the destination bucket in the other account. for information about adding a bucket policy to an s3 bucket, see  if you chose to replicate objects encrypted with aws kms, an aws kms key policy is provided on the configure options page. you can copy this policy to add to the key policy for the aws kms cmk that you are using. the key policy grants the source bucket owner permission to use the cmk. for information about updating the key policy, see . on the review page, review your replication rule. if it looks correct, choose save. otherwise, choose previous to edit the rule before saving it.  after you save your rule, you can edit, enable, disable, or delete your rule on the replication page. follow the instructions given on the replication page under the warning message, the replication rule is saved, but additional settings are required in the destination account. sign out of the aws account that you are currently in, and then sign in to the destination account.  importantreplication fails until you sign in to the destination account and complete the following steps. after you sign in to the destination account, choose the management tab, choose replication, and then choose receive objects on the actions menu.  on the receive objects page, you can do the following: enable versioning on the destination bucket.apply the bucket policy provided by amazon s3 to the destination bucket.copy the aws kms key policy that you need to update the aws kms cmk that is being used to encrypt the replica objects in the destination bucket. for information about updating the key policy, see .you must grant permissions to the account of the source bucket owner to encrypt using your aws kms cmk with a key policy. the following procedure describes how to use the aws identity and access management (iam) console to modify the key policy for the aws kms cmk that is being used to encrypt the replica objects in the destination bucket. to grant permissions to encrypt using your aws kms cmk sign in to the aws management console using the aws account that owns the aws kms cmk. open the aws kms console at . choose the alias of the cmk that you want to encrypt with. in the key policy section of the page, choose switch to policy view. choose edit to edit key policy. using the key policy editor, insert the key policy provided by amazon s3 into the existing key policy, and then choose save changes. you might want to add the policy to the end of the existing policy.  for more information about creating and editing aws kms cmks, see  in the aws key management service developer guide.   in the amazon simple storage service developer guide
this section describes how to create a s3 batch operations job. for information about performing batch operations using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. to create a batch job sign in to the aws management console and open the amazon s3 console at . choose batch operations on the navigation pane of the amazon s3 console. choose create job. choose the region where you want to create your job. under manifest format, choose the type of manifest object to use. if you choose s3 inventory report, enter the path to the manifest.json object that amazon s3 generated as part of the csv-formatted inventory report, and optionally the version id for the manifest object if you want to use a version other than the most recent.if you choose csv, enter the path to a csv-formatted manifest object. the manifest object must follow the format described in the console. you can optionally include the version id for the manifest object if you want to use a version other than the most recent.choose next under operation, choose the operation that you want to perform on all objects listed in the manifest. fill out the information for the operation you chose and then choose next. fill out the information for configure additional options and then choose next. for review, verify the settings. if you need to make changes, choose previous. otherwise, choose create job.  in the amazon simple storage service developer guide in the amazon simple storage service developer guide in the amazon simple storage service developer guide
*copyright &copy; 2020 amazon web services, inc. and/or its affiliates. all rights reserved.* amazon's trademarks and trade dress may not be used in       connection with any product or service that is not amazon's,       in any manner that is likely to cause confusion among customers,       or in any manner that disparages or discredits amazon. all other       trademarks not owned by amazon are the property of their respective      owners, who may or may not be affiliated with, connected to, or       sponsored by amazon. 
versioning enables you to keep multiple versions of an object in one bucket. this section describes how to enable object versioning on a bucket. for more information about versioning support in amazon s3, see  and  in the amazon simple storage service developer guide. to enable or disable versioning on an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to enable versioning for. choose properties. choose versioning. choose enable versioning or suspend versioning, and then choose save. noteyou can use aws multi-factor authentication (mfa) with versioning. when you use mfa with versioning, you must provide your aws account’s access keys and a valid code from the account’s mfa device in order to permanently delete an object version or suspend or reactivate versioning. to use mfa with versioning, you enable . however, you cannot enable  using the aws management console. you must use the aws cli or api. for more information, see . 
this section explains how to use the amazon s3 console to recover (undelete) deleted objects. to be able to undelete a deleted object, you must have had versioning enabled on the bucket that contains the object before the object was deleted. for information about enabling versioning, see . when you delete an object in a versioning-enabled bucket, all versions remain in the bucket and amazon s3 creates a delete marker for the object. to undelete the object, you must delete this delete marker. for more information about versioning and delete markers, see  in the amazon simple storage service developer guide. to recover deleted objects from an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. to see a list of the versions of the objects in the bucket, select show. you'll be able to see the delete markers for deleted objects.  to undelete an object, you must delete the delete marker. select the check box next to the delete marker of the object to recover, and then choose delete from the actions menu. then, choose hide and you'll see the undeleted object listed.  in the amazon simple storage service developer guide
this section explains how to use the amazon s3 console to restore an object that has been archived to the s3 glacier or s3 glacier deep archive storage classes. objects stored in the s3 glacier or s3 glacier deep archive are not immediately accessible. to access an object in this class, you must restore a temporary copy of it to its s3 bucket for the duration (number of days) that you specify. for information about the s3 glacier or s3 glacier deep archive storage classes, see  in the amazon simple storage service developer guide.  when you restore an archive, you pay for both the archive and the restored copy. because there is a storage cost for the copy, restore objects only for the duration you need them. if you want a permanent copy of the object, create a copy of it in your s3 bucket. for information about amazon s3 features and pricing, see . after restoring an object, you can download it from the overview page. for more information, see . topics the following are the available retrieval options when restoring an archived object:   - expedited retrievals allow you to quickly access your data stored in the s3 glacier storage class when occasional urgent requests for a subset of archives are required. for all but the largest archived objects (250 mb+), data accessed using expedited retrievals is typically made available within 1–5 minutes. provisioned capacity ensures that retrieval capacity for expedited retrievals is available when you need it. for more information, see . expedited retrievals and provisioned capacity are not available for objects stored in the s3 glacier deep archive storage class. - standard retrievals allow you to access any of your archived objects within several hours. this is the default option for the s3 glacier and s3 glacier deep archive retrieval requests that do not specify the retrieval option. standard retrievals typically finish within 3–5 hours for objects stored in the s3 glacier storage class. they typically finish within 12 hours for objects stored in the s3 glacier deep archive storage class.  - bulk retrievals are the lowest-cost retrieval option in amazon s3 glacier, enabling you to retrieve large amounts, even petabytes, of data inexpensively. bulk retrievals typically finish within 5–12 hours for objects stored in the s3 glacier storage class. they typically finish within 48 hours for objects stored in the s3 glacier deep archive storage class.for more information about retrieval options, see  in the amazon simple storage service developer guide. this topic explains how to use the amazon s3 console to restore an object that has been archived to the s3 glacier or s3 glacier deep archive storage classes. (the console uses the names glacier and glacier deep archive for these storage classes.) to restore archived s3 objects sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the objects that you want to restore. in the name list, select the object or objects that you want to restore, choose actions, and then choose restore. in the initiate restore dialog box, type the number of days that you want your archived data to be accessible.  choose one of the following retrieval options from the retrieval options menu. choose bulk retrieval or standard retrieval, and then choose restore. choose expedited retrieval (available only for the glacier storage class).provisioned capacity is only available for the glacier storage class. if you have provisioned capacity, choose restore to start a provisioned retrieval. if you have provisioned capacity, all of your expedited retrievals are served by your provisioned capacity. for more information about provisioned capacity, see .  if you don't have provisioned capacity and you don't want to buy it, choose restore. if you don't have provisioned capacity, but you want to buy it, choose add capacity unit, and then choose buy. when you get the purchase succeeded message, choose restore to start provisioned retrieval.you can upgrade the speed of your restoration while it is in progress. to upgrade an in-progress restore to a faster tier in the name list, select one or more of the objects that you are restoring, choose actions, and then choose restore from glacier. for information about checking the restoration status of an object, see . choose the tier that you want to upgrade to and then choose restore. for more information about upgrading to a faster restore tier, see see  in the amazon simple storage service developer guide.  to check the progress of the restoration, see the object overview panel. for information about the overview panel, see .  the overview section shows that restoration is in progress.  when the temporary copy of the object is available, the object's overview section shows the restoration expiry date. this is when amazon s3 will remove the restored copy of your archive.   restored objects are stored only for the number of days that you specify. if you want a permanent copy of the object, create a copy of it in your amazon s3 bucket.  amazon s3 calculates the expiry date by adding the number of days that you specify to the time you request to restore the object, and then rounding to the next day at midnight utc. this calculation applies to the initial restoration of the object and to any extensions to availability that you request. for example, if an object was restored on 10/15/2012 10:30 am utc and the number of days that you specified is 3, then the object is available until 10/19/2012 00:00 utc. if, on 10/16/2012 11:00 am utc you change the number of days that you want it to be accessible to 1, then amazon s3 makes the restored object available until 10/18/2012 00:00 utc. after restoring an object, you can download it from the overview page. for more information, see .   in the amazon s3 developer guide. in the aws cli command reference.
object tagging gives you a way to categorize storage. this topic explains how to use the console to add tags to an s3 object after the object has been uploaded. for information about adding tags to an object when the object is being uploaded, see .  each tag is a key-value pair that adheres to the following rules: you can associate up to 10 tags with an object. tags associated with an object must have unique tag keys.a tag key can be up to 128 unicode characters in length and tag values can be up to 255 unicode characters in length.key and tag values are case sensitive.  for more information about object tags, see  in the amazon simple storage service developer guide. to add tags to an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object you want to add tags to. choose properties. choose tags and then choose add tag. each tag is a key-value pair. type a key and a value. then choose add tag to add another tag or choose save.  you can enter up to 10 tags for an object. 
this section explains how to use the amazon simple storage service (amazon s3) console to grant access permissions to your buckets and objects. it also explains how to use amazon s3 block public access to prevent the application of any settings that allow public access to data within s3 buckets.   buckets and objects are amazon s3 resources.  you grant access permissions to your buckets and objects by using resource-based access policies. you can associate an access policy with a resource. an access policy describes who has access to resources. the resource owner is the aws account that creates the resource. for more information about resource ownership and access policies, see  in the amazon simple storage service developer guide.  bucket access permissions specify which users are allowed access to the objects in a bucket and which types of access they have. object access permissions specify which users are allowed access to the object and which types of access they have. for example, one user might have only read permission, while another might have read and write permissions. bucket and object permissions are independent of each other. an object does not inherit the permissions from its bucket. for example, if you create a bucket and grant write access to a user, you can't access that user’s objects unless the user explicitly grants you access.  to grant access to your buckets and objects to other aws accounts and to the general public, you use resource-based access policies known as access control lists (acls).  a bucket policy is a resource-based aws identity and access management (iam) policy that grants other aws accounts or iam users access to an s3 bucket. bucket policies supplement, and in many cases, replace acl-based access policies.  for more information about using iam with amazon s3, see  in the amazon simple storage service developer guide.  for more in-depth information about managing access permissions, see  in the amazon simple storage service developer guide. this section also explains how to use the amazon s3 console to add a cross-origin resource sharing (cors) configuration to an s3 bucket. cors allows client web applications that are loaded in one domain to interact with resources in another domain. topics 
this section explains how to use the amazon simple storage service (amazon s3) console to manage access permissions for s3 buckets by using access control lists (acls). acls are resource-based access policies that grant access permissions to buckets and objects. for more information about managing access permissions with resource-based policies, see  in the amazon simple storage service developer guide. you can grant permissions to other aws account users or to predefined groups. the user or group that you are granting permissions to is called the grantee. by default, the owner, which is the aws account that created the bucket, has full permissions. each permission you grant for a user or group adds an entry in the acl that is associated with the bucket. the acl lists grants, which identify the grantee and the permission granted. for more information about acls, see  in the amazon simple storage service developer guide. to set acl access permissions for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to set permissions for. choose permissions, and then choose access control list. you can manage bucket access permissions for the following:  1.  access for your aws account root user  access for other aws accounts to grant permissions to an aws user from a different aws account, under access for other aws accounts, choose add account. in the enter an id field, enter the canonical id of the aws user that you want to grant bucket permissions to. for information about finding a canonical id, see  in the amazon web services general reference. you can add as many as 99 users. select the check boxes next to the permissions that you want to grant to the user, and then choose save. to display information about the permissions, choose the help icons. warningwhen you grant other aws accounts access to your resources, be aware that the aws accounts can delegate their permissions to users under their accounts. this is known as cross-account access. for information about using cross-account access, see  in the iam user guide.  public access  warninguse caution when granting the everyone group public access to your s3 bucket. when you grant access to this group, anyone in the world can access your bucket. we highly recommend that you never grant any kind of public write access to your s3 bucket. s3 log delivery group to grant access to amazon s3 to write server access logs to the bucket, under s3 log delivery group, choose log delivery. if a bucket is set up as the target bucket to receive access logs, the bucket permissions must allow the log delivery group write access to the bucket. when you enable server access logging on a bucket, the amazon s3 console grants write access to the log delivery group for the target bucket that you choose to receive the logs. for more information about server access logging, see . you can also set bucket permissions when you are creating a bucket. for more information about setting permissions when creating a bucket, see .  
amazon s3 default encryption provides a way to set the default encryption behavior for an amazon s3 bucket. you can set default encryption on a bucket so that all objects are encrypted when they are stored in the bucket. the objects are encrypted using server-side encryption with either amazon s3-managed keys (sse-s3) or aws key management service (aws kms) customer master keys (cmks).  when you use server-side encryption, amazon s3 encrypts an object before saving it to disk in its data centers and decrypts it when you download the objects. for more information about protecting data using server-side encryption and encryption key management, see  in the amazon simple storage service developer guide. default encryption works with all existing and new amazon s3 buckets. without default encryption, to encrypt all objects stored in a bucket, you must include encryption information with every object storage request. you must also set up an amazon s3 bucket policy to reject storage requests that don't include encryption information.  there are no new charges for using default encryption for s3 buckets. requests to configure the default encryption feature incur standard amazon s3 request charges. for information about pricing, see . for sse-kms cmk storage, aws kms charges apply and are listed at .  to enable default encryption on an amazon s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose properties. choose default encryption. if you want to use keys that are managed by amazon s3 for default encryption, choose aes-256, and choose save.  for more information about using amazon s3 server-side encryption to encrypt your data, see  in the amazon simple storage service developer guide. importantyou might need to update your bucket policy when enabling default encryption. for more information, see  in the amazon simple storage service developer guide. if you want to use cmks that are stored in aws kms for default encryption, follow these steps: choose aws-kms. to choose a customer-managed aws kms cmk that you have created, use one of these methods: in the list that appears, choose the aws kms cmk.in the list that appears, choose custom kms arn, and then enter the amazon resource name of the aws kms cmk. importantyou can use only kms cmks that are enabled in the same aws region as the bucket. the s3 console only lists 100 kms cmks per region. if you have more than 100 cmks in the same region, you can only see the first 100 cmks in the s3 console. to use a kms cmk that is not listed in the console, choose custom kms arn, and enter the kms cmk arn.when you use an aws kms cmk for server-side encryption in amazon s3, you must choose a symmetric cmk. amazon s3 only supports symmetric cmks and not asymmetric cmks. for more information, see  in the aws key management service developer guide. importantif you use the aws kms option for your default encryption configuration, you are subject to the rps (requests per second) limits of aws kms. for more information about aws kms limits and how to request a limit increase, see . for more information about creating an aws kms cmk, see  in the aws key management service developer guide. for more information about using aws kms with amazon s3, see  in the amazon simple storage service developer guide. choose save.  in the amazon simple storage service developer guide
this section explains how to use the amazon s3 console to see the different versions of an object. a versioning-enabled bucket can have many versions of the same object:, one current (latest) version and zero or more noncurrent (previous) versions. amazon s3 assigns each object a unique version id. for information about enabling versioning, see .  if a bucket is versioning-enabled, amazon s3 creates another version of an object under the following conditions:  if you upload an object that has the same name as an object that already exists in the bucket, amazon s3 creates another version of the object instead of replacing the existing object. if you update any object properties after you upload the object to the bucket, such as changing the storage details or other metadata , amazon s3 creates a new object version in the bucket. for more information about versioning support in amazon s3, see  and  in the amazon simple storage service developer guide. to see multiple versions of an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. to see a list of the versions of the objects in the bucket, choose show. for each object version, the console shows a unique version id, the date and time the object version was created, and other properties. (objects stored in your bucket before you set the versioning state have a version id of null.)  to list the objects without the versions, choose hide. you also can view, download, and delete object versions in the object overview panel. for more information, see . importantyou can undelete an object only if it was deleted as the latest (current) version. you can't undelete a previous version of an object that was deleted. for more information, see  and  in the amazon simple storage service developer guide. 
this section explains how to use the amazon s3 console to add a cross-origin resource sharing (cors) configuration to an s3 bucket. cors allows client web applications that are loaded in one domain to interact with resources in another domain.  to configure your bucket to allow cross-origin requests, you add cors configuration to the bucket. a cors configuration is an xml document that defines rules that identify the origins that you will allow to access your bucket, the operations (http methods) supported for each origin, and other operation-specific information. for more information about cors and examples, see  in the amazon simple storage service developer guide. when you enable cors on the bucket, the access control lists (acls) and other access permission policies continue to apply. to add a cors configuration to an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to create a bucket policy for. choose permissions, and then choose cors configuration. in the cors configuration editor text box, type or copy and paste a new cors configuration, or edit an existing configuration. the cors configuration is an xml file. the text that you type in the editor must be valid xml. for more information, see  choose save. noteamazon s3 displays the amazon resource name (arn) for the bucket next to the cors configuration editor title. for more information about arns, see  in the amazon web services general reference. 
you can use lifecycle policies to define actions that you want amazon s3 to take during an object's lifetime (for example, transition objects to another storage class, archive them, or delete them after a specified period of time). you can define a lifecycle policy for all objects or a subset of objects in the bucket by using a shared prefix (objects names that begin with a common string) or a tag.  using a lifecycle policy, you can define actions specific to current and noncurrent object versions. for more information, see  and  and  in the amazon simple storage service developer guide. to create a lifecycle policy sign in to the aws management console and open the amazon s3 console at . in the buckets list, choose the name of the bucket that you want to create a lifecycle policy for. choose the management tab, and choose add lifecycle rule. in the lifecycle rule dialog box, type a name for your rule.  the name must be unique within the bucket.  choose the scope of the lifecycle rule: all objects with a specific prefix or tag or all the objects in the bucket. to apply this lifecycle rule to all objects with a specific prefix or tag, choose limit the scope to specific prefixes or tags. in the add prefix or tag filter box, type the prefix or tag name, and press enter.  for more information about object name prefixes, see  in the amazon simple storage service developer guide.for more information about object tags, see  in the amazon simple storage service developer guide.to apply this lifecycle rule to all objects in the bucket, choose apply to all objects in the bucket.choose next. the storage class transition page opens. when you configure your storage class transitions, you define the rules to transition objects to the standard-ia, one zone-ia, glacier, and deep archive storage classes. for more information, see  in the amazon simple storage service developer guide. you can define transitions for current or previous object versions or for both current and previous versions. versioning enables you to keep multiple versions of an object in one bucket. for more information about versioning, see . choose the versions for which you want to define transitions, current or noncurrent: to define transitions that are applied to the current verion of the object, choose current version. to define transitions that are applied to all previous versions of the object, choose previous versions. in the screenshot below, current version is chosen. to add a transition:  for a current version, under for current object versions, choose add transition. for a non-current version, under for non-current object versions, choose add transition. for each transition that you add, choose one of the following: transition to standard-ia after.transition to intelligent-tiering after. transition to one zone-ia after. transition to glacier after.transition to glacier deep archive after. importantwhen you choose the glacier or glacier deep archive storage class, your objects remain in amazon s3. you cannot access them directly through the separate amazon s3 glacier service. for more information, see . in the days after creation box, enter the number of days after the creation of the object that you want the transition to be applied (for example, 30 or 100 days). when you are done configuring transitions, choose next. under configure expiration, for this example, choose both current version and previous versions.  importantin a non-versioned bucket the expiration action results in amazon s3 permanently removing the object. for more information about lifecycle actions, see  in the amazon simple storage service developer guide. choose expire current version of object, and then enter the number of days after object creation to delete the object (for example, 395 days). if you choose this expire option, you cannot choose the option to clean up expired delete markers.  choose permanently delete previous versions, and then enter the number of days after an object becomes a previous version to permanently delete the object (for example, 465 days). to clean up incomplete multipart uploads, we recommend that you choose clean up incomplete multipart uploads and enter the number of days after the multipart upload initiation that you want to end and clean up incomplete multipart uploads(for example, 7 days).  for more information about multipart uploads, see  in the amazon simple storage service developer guide. choose next. for review, verify the settings for your rule. if you need to make changes, choose previous. otherwise, choose save.  if the rule does not contain any errors, it is enabled and you can see it on the lifecycle page. 
with s3 object lock, you can store objects in amazon s3 using a write-once-read-many (worm) model. you can use s3 object lock to prevent an object from being deleted or overwritten for a fixed amount of time or indefinitely. for information about object locking using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. before you lock any objects, you have to enable a bucket to use s3 object lock. you enable object lock when you create a bucket. after you enable object lock on a bucket, you can lock objects in that bucket. when you create a bucket with object lock enabled, you can't disable object lock or suspend versioning for that bucket.  for information about creating a bucket with s3 object lock enabled, see . to lock an amazon s3 object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. in the name list, choose the name of the object that you want to lock. choose properties. choose object lock. choose a retention mode. you can change the retain until date. you can also choose to enable a legal hold. for more information, see  in the amazon simple storage service developer guide. choose save. 
welcome to the amazon simple storage service console user guide for the amazon simple storage service (amazon s3) console.  amazon s3 provides virtually limitless storage on the internet. this guide explains how you can manage buckets, objects, and folders in amazon s3 by using the aws management console, a browser-based graphical user interface for interacting with aws services.  for detailed conceptual information about how amazon s3 works, see  in the amazon simple storage service developer guide. the developer guide also has detailed information about amazon s3 features and code examples to support those features. topics 
this topic describes how to enable server access logging for an amazon s3 bucket using the aws management console. for information about how to enable logging programmatically and details about how logs are delivered, see  in the amazon simple storage service developer guide. by default, amazon simple storage service (amazon s3) doesn't collect server access logs. when you enable logging, amazon s3 delivers access logs for a source bucket to a target bucket that you choose. the target bucket must be in the same aws region as the source bucket and must not have a default retention period configuration.  server access logging provides detailed records for the requests that are made to an s3 bucket. server access logs are useful for many applications. for example, access log information can be useful in security and access audits. it can also help you learn about your customer base and understand your amazon s3 bill.  an access log record contains details about the requests that are made to a bucket. this information can include the request type, the resources that are specified in the request, and the time and date that the request was processed. for more information, see  in the amazon simple storage service developer guide. importantthere is no extra charge for enabling server access logging on an amazon s3 bucket. however, any log files that the system delivers to you will accrue the usual charges for storage. (you can delete the log files at any time.) we do not assess data transfer charges for log file delivery, but we do charge the normal data transfer rate for accessing the log files. to enable server access logging for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to enable server access logging for. choose properties. choose server access logging. choose enable logging. for target, choose the name of the bucket that you want to receive the log record objects.  the target bucket must be in the same region as the source bucket and must not have a default retention period configuration. (optional) for target prefix, type a key name prefix for log objects, so that all of the log object names begin with the same string. choose save. you can view the logs in the target bucket. if you specified a prefix, the prefix shows as a folder in the target bucket in the console. after you enable server access logging, it might take a few hours before the logs are delivered to the target bucket. for more information about how and when logs are delivered, see  in the amazon simple storage service developer guide. more info  
this topic describes how to set or change the type of encryption an object using the amazon s3 console. noteif you change an object's encryption, a new object is created to replace the old one. if s3 versioning is enabled, a new version of the object is created, and the existing object becomes an older version. the role that changes the property also becomes the owner of the new object or (object version).  to add or change encryption for an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object that you want to add or change encryption for. choose properties, and then choose encryption. the encryption dialog box opens, giving you three choices for object encryption: none‐ no object encryption.aes-256 ‐ server-side encryption with amazon s3 managed keys (sse-s3).aws‐kms ‐ server-side encryption with aws key management service (aws kms) customer master keys (sse-kms).if you want to remove encryption from an object that already has encryption settings, choose none and then choose save. if you want to encrypt your object using keys that are managed by amazon s3, follow these steps: chooseaes-256.  for more information about using amazon s3 server-side encryption to encrypt your data, see  in the amazon simple storage service developer guide. choose save. if you want to encrypt your object using aws kms, follow these steps: choose aws-kms. choose an aws kms customer master key (cmk). the list shows  that you have created and your aws managed cmk for amazon s3. for more information about creating a customer managed aws kms cmk, see  in the aws key management service developer guide.  importantthe amazon s3 console lists only 100 aws kms cmks per aws region. if you have more than 100 cmks in the same region, you can see only the first 100 cmks in the s3 console. to use a kms cmk that is not listed in the console, choose custom kms arn, and enter the kms cmk arn. choose save. importantto encrypt objects in the bucket, you can use only cmks that are enabled in the same aws region as the bucket. amazon s3 only supports symmetric cmks. amazon s3 does not support asymmetric cmks. for more information, see . to give an external account the ability to use an object that is protected by an aws kms cmk, follow these steps:  choose aws-kms. enter the amazon resource name (arn) for the external account. choose save. administrators of an external account that have usage permissions to an object protected by your aws kms cmk can further restrict access by creating a resource-level aws identity and access management (iam) policy.  in the amazon simple storage service developer guide
this section explains how to manage and use your amazon s3 access points using the aws management console. each access point is associated with a single amazon s3 bucket. before you begin, navigate to the list of your access points for a bucket as described in the following procedure. to find a list of access points for a bucket sign in to the aws management console and open the amazon s3 console at . in the s3 buckets section, select the bucket whose access points you want to manage. on the bucket detail page, choose the access points tab. from the access points tab, you can view an access point's configuration details, edit an access point's policy, use an access point to access your bucket, or delete an access point. the following procedures explain how to perform each of these tasks. to view access point configuration details navigate to the access points tab for your bucket. locate the access point whose configuration you want to view. you can browse for the access point in the access point list, or you can search for a specific access point using the search by name field. choose the name of the access point whose configuration you want to view. noteto view an access point's configuration, choose (click on) the name of the access point, not the option button next to the access point name. to edit an access point policy navigate to the access points tab for your bucket. select the option button next to the name of the access point whose policy you want to edit. choose edit access point policy. enter the policy in the text field. the console automatically displays the amazon resource name (arn) for the access point, which you can use in the policy. you can also choose policy generator to use the aws policy generator to help construct the policy. choose save. to use an access point to access your bucket navigate to the access points tab for your bucket. select the option button next to the name of the access point you want to use. choose use this access point. the console displays a label above the name of your bucket that shows the access point that you're currently using. while you're using the access point, you can only perform the object operations that are allowed by the access point permissions. notethe console view always shows all objects in the bucket. using an access point as described in this procedure restricts the operations you can perform on those objects, but not whether you can see that they exist in the bucket. to exit the access point view of your bucket, choose exit access point. notethe s3 management console doesn't support using virtual private cloud (vpc) access points to access bucket resources. to access bucket resources from a vpc access point, use the aws cli, aws sdks, or amazon s3 rest apis. to delete an access point navigate to the access points tab for your bucket. select the option button next to the name of the access point that you want to delete. choose delete. confirm that you want to delete your access point by entering its name in the text field that appears, and choose confirm. 
this section explains how to use the amazon s3 console to view the object overview panel. this panel provides an overview of all the essential information for an object in one place. to see the overview panel for an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, select the check box next to the name of the object for which you want an overview. to download the object, choose download in the object overview panel. to copy the path of the object to the clipboard, choose copy path. if versioning is enabled on the bucket, choose latest versions to list the versions of the object. you can then choose the download icon to download an object version, or choose the trash can icon to delete an object version. importantyou can undelete an object only if it was deleted as the latest (current) version. you can't undelete a previous version of an object that was deleted. for more information, see  and  in the amazon simple storage service developer guide. 
this section explains how to use the amazon s3 console to download objects from an s3 bucket. data transfer fees apply when you download objects. for information about amazon s3 features, and pricing, see . importantif an object key name consists of a single period (.), or two periods (..), you can’t download the object using the amazon s3 console. to download an object with a key name of “.” or “..”, you must use the aws cli, aws sdks, or rest api. for more information about naming objects, see  in the amazon simple storage service developer guide. you can download a single object per request using the amazon s3 console. to . to download an object from an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to download an object from. you can download an object from an s3 bucket in any of the following ways: in the name list, select the check box next to the object you want to download, and then choose download on the object description page that appears.choose the name of the object that you want to download. on the overview page, choose download. choose the name of the object that you want to download and then choose download as on the overview page.choose the name of the object that you want to download. choose latest version and then choose the download icon.
each object in amazon simple storage service (amazon s3) has a set of name-value pairs that provides metadata about the object. metadata is additional information about the object. some metadata is set by amazon s3 when you upload the object, for example, and . you can also set some metadata when you upload the object, or you can add it later. this section explains how to use the amazon s3 console to add metadata to an s3 object. object metadata is a set of name-value (key-value) pairs. for example, the metadata for content length, , is the name (key) and the size of the object in bytes (value). for more information about object metadata, see  in the amazon simple storage service developer guide. there are two kinds of metadata for an s3 object, amazon s3 system metadata and user-defined metadata: system metadata–there are two categories of system metadata. metadata such as the  date is controlled by the system. only amazon s3 can modify the value. there is also system metadata that you control, for example, the storage class configured for the object. user-defined metadata–you can define your own custom metadata, called user-defined metadata. you can assign user-defined metadata to an object when you upload the object or after the object has been uploaded. user-defined metadata is stored with the object and is returned when you download the object. amazon s3 does not process user-defined metadata. the following topics describe how to add metadata to an object using the amazon s3 console. noteif you change an object's metadata, a new object will be created and will replace the old one. if s3 versioning is enabled, a new version of the object is created, and the existing object becomes an older version. the role that changes the property also becomes the owner of the new object or (object version). topics you can configure some system metadata for an s3 object. for a list of system-defined metadata and whether you can modify their values, see  in the amazon simple storage service developer guide. to add system metadata to an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object that you want to add metadata to. choose properties, and then choose metadata. choose add metadata, and then choose a key from the select a key menu. depending on which key you chose, choose a value from the select a value menu or type a value. choose save. you can assign user-defined metadata to an object. user-defined metadata must begin with the prefix "", otherwise amazon s3 will not set the key value pair as you define it. you define custom metadata by adding a name that you choose to the  key. this creates a custom key. for example, if you add the custom name , the metadata key would be .  user-defined metadata can be as large as 2 kb. both keys and their values must conform to us-ascii standards. for more information, see  in the amazon simple storage service developer guide. to add user-defined metadata to an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object that you want to add metadata to. choose properties, and then choose metadata. choose add metadata, and then choose the  key from the select a key menu. any metadata starting with the prefix  is user-defined metadata. type a custom name following the  key. for example, for the custom name , the metadata key would be . enter a value for the custom key, and then choose save. 
amazon s3 inventory provides a flat file list of your objects and metadata, which is a scheduled alternative to the amazon s3 synchronous  api operation. amazon s3 inventory provides comma-separated values (csv) or  or  output files that list your objects and their corresponding metadata on a daily or weekly basis for an s3 bucket or for objects that share a prefix (objects that have names that begin with the same string). for more information, see  in the amazon simple storage service developer guide. to configure inventory noteit may take up to 48 hours to deliver the first report. sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket for which you want to configure amazon s3 inventory. choose the management tab, and then choose inventory. choose add new. type a name for the inventory and set it up as follows: optionally, add a prefix for your filter to inventory only objects whose names begin with the same string.choose the destination bucket where you want reports to be saved. the destination bucket must be in the same aws region as the bucket for which you are setting up the inventory. the destination bucket can be in a different aws account. optionally, choose a prefix for the destination bucket.choose how frequently to generate the inventory.under advanced settings, you can set the following: choose either the csv, orc, or parquet output file format for your inventory. for more information about these formats, see  in the amazon simple storage service developer guide. to include all versions of the objects in the inventory, choose include all versions in the object versions list. by default, the inventory includes only the current versions of the objects. for optional fields, select one or more of the following to add to the inventory report: size – object size in bytes.last modified date – the object creation date or the last modified date, whichever is the latest.storage class – the storage class used for storing the object. etag – the entity tag is a hash of the object. the etag reflects changes only to the contents of an object, and not its metadata. the etag may or may not be an md5 digest of the object data. whether it is depends on how the object was created and how it is encrypted.multipart upload – specifies that the object was uploaded as a multipart upload. for more information, see  in the amazon simple storage service developer guide.replication status – the replication status of the object. for more information, see .encryption status – the server-side encryption used to encrypt the object. for more information, see  in the amazon simple storage service developer guide.s3 object lock configurations – the object lock status of the object, including the following settings:  retention mode – the level of protection applied to the object, either governance or compliance.retain until date – the date until which the locked object cannot be deleted.legal hold status – the legal hold status of the locked object. for information about s3 object lock, see  in the amazon simple storage service developer guide. for more information about the contents of an inventory report, see  in the amazon simple storage service developer guide. for encryption, choose a server-side encryption option to encrypt the inventory report, or choose none: none – do not encrypt the inventory report.aes-256 – encrypt the inventory report using server-side encryption with amazon s3-managed keys (sse-s3). amazon s3 server-side encryption uses 256-bit advanced encryption standard (aes-256). for more information, see  in the amazon simple storage service developer guide. aws-kms – encrypt the report using server-side encryption with aws key management service (aws kms) customer master keys (cmks). for more information, see  in the amazon simple storage service developer guide.  noteto encrypt the inventory list file with sse-kms, you must grant amazon s3 permission to use the aws kms cmk. for instructions, see  .choose save. amazon s3 creates a bucket policy on the destination bucket that grants amazon s3 write permission. this allows amazon s3 to write data for the inventory reports to the bucket.  if an error occurs when you try to create the bucket policy, you are given instructions on how to fix it. for example, if you choose a destination bucket in another aws account and don't have permissions to read and write to the bucket policy, you see the following message.   in this case, the destination bucket owner must add the displayed bucket policy to the destination bucket. if the policy is not added to the destination bucket, you won't get an inventory report because amazon s3 doesn’t have permission to write to the destination bucket. if the source bucket is owned by a different account than that of the current user, the correct account id of the source bucket must be substituted in the policy. for more information, see  in the amazon simple storage service developer guide. to grant amazon s3 permission to encrypt using a customer managed aws key management service (aws kms) customer master key (cmk), you must use a key policy. to update your key policy so that you can use an aws kms customer managed cmk to encrypt the inventory file, follow the steps below. to grant permissions to encrypt using your aws kms cmk using the aws account that owns the customer managed cmk, sign into the aws management console. open the aws kms console at . to change the aws region, use the region selector in the upper-right corner of the page. in the left navigation pane, choose customer managed keys. under customer managed keys, choose the customer managed cmk that you want to use to encrypt the inventory file. under key policy, choose switch to policy view. to update the key policy, choose edit. under edit key policy, add the following key policy to the existing key policy. choose save changes. for more information about creating customer managed cmks aws kms and using key policies, see the following links in the aws key management service developer guide: more info  
for more detailed information about configuring a redirect in amazon s3, see  in the amazon simple storage service developer guide. you can redirect all requests for a website endpoint for a bucket to another host. if you redirect all requests, any request made to the website endpoint is redirected to the specified host name.  for example, if your root domain is , and you want to serve requests for both  and , you can create two buckets named  and . then, maintain the content in the  bucket, and configure the other  bucket to redirect all requests to the  bucket. for more information, see . to redirect requests for a bucket website endpoint open the amazon s3 console at . choose the name of the bucket that you have configured as a static website (for example, ). choose properties. choose static website hosting. choose redirect requests.  in the target bucket or domain box, enter the bucket or domain that you want to redirect to. for example, if you are redirecting to a root domain address, you would enter example.com. in the protocol box, enter the protocol for the redirected requests (http or https). if you do not specify a protocol, the protocol of the original request is used. choose save. 
in the amazon s3 console, you can move an object to a bucket or a folder.  to move an object sign in to the aws management console and open the amazon s3 console at . in the buckets list, choose your bucket name. select the check box beside the object that you want to move. choose actions, and choose move. choose the move destination: if you want to move your object to a bucket, select the bucket.if you want to move your object to a folder in a bucket, choose the bucket name, and then choose the folder.under review, confirm the move details, and choose move. amazon s3 moves your object to the destination. 
this section explains how to use the amazon simple storage service (amazon s3) console to manage access permissions for an amazon s3 object by using access control lists (acls). acls are resource-based access policies that grant access permissions to buckets and objects. for more information about managing access permissions with resource-based policies, see  in the amazon simple storage service developer guide. bucket and object permissions are independent of each other. an object does not inherit the permissions from its bucket. for example, if you create a bucket and grant write access to a user, you can't access that user’s objects unless the user explicitly grants you access. you can grant permissions to other aws accounts or predefined groups. the user or group that you grant permissions to is called the grantee. by default, the owner, which is the aws account that created the bucket, has full permissions. each permission you grant for a user or a group adds an entry in the acl that is associated with the object. the acl lists grants, which identify the grantee and the permission granted. for more information about acls, see  in the amazon simple storage service developer guide. to set permissions for an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object for which you want to set permissions. choose permissions. you can manage object access permissions for the following:  1.  access for object owner  access for other aws accounts to grant permissions to an aws user from a different aws account, under access for other aws accounts, choose add account. in the enter an id field, enter the canonical id of the aws user that you want to grant object permissions to. for information about finding a canonical id, see  in the amazon web services general reference. you can add as many as 99 users. select the check boxes for the permissions that you want to grant to the user, and then choose save. to display information about the permissions, choose the help icons. public access  warninguse caution when granting the everyone group anonymous access to your amazon s3 objects. when you grant access to this group, anyone in the world can access your object. if you need to grant access to everyone, we highly recommend that you only grant permissions to read objects.we highly recommend that you do not grant the everyone group write object permissions. doing so allows anyone to overwrite the acl permissions for the object. you can also set object permissions when you upload objects. for more information about setting permissions when uploading objects, see .  
this section describes how to use the amazon s3 console to create a folder. to create a folder sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to create a folder in. choose create folder. enter a name for the folder (for example, favorite-pics). choose the encryption setting for the folder object, and then choose save. if you don't want to use encryption with your folder, choose none.  if you want to encrypt your object using keys that are managed by amazon s3, follow these steps: choose aes-256.  for more information about using amazon s3 server-side encryption to encrypt your data, see  in the amazon simple storage service developer guide. if you want to encrypt your object using aws kms, follow these steps: choose aws-kms. choose an aws kms customer master key (cmk). the list shows  that you have created and your aws managed cmk for amazon s3. for more information about creating a customer managed aws kms cmk, see  in the aws key management service developer guide.  importantto encrypt objects in the bucket, you can use only the cmks that are enabled in the same aws region as the bucket. amazon s3 only supports symmetric cmks and does not support asymmetric cmks. for more information, see .the amazon s3 console only lists 100 kms cmks per region. if you have more than 100 cmks in the same region, you can see only the first 100 cmks in the s3 console. to use anaws kms kms cmk that is not listed in the console, choose custom kms arn, and enter the kms cmk arn. choose save. 
to upload your data (photos, videos, documents etc.) to amazon s3, you must first create an s3 bucket in one of the aws regions. you can then upload an unlimited number of data objects to the bucket. the data that you store in amazon s3 consists of objects. every object resides within a bucket that you create in a specific aws region. every object that you store in amazon s3 resides in a bucket.  objects stored in a region never leave the region unless you explicitly transfer them to another region. for example, objects stored in the europe (ireland) region never leave it. the objects stored in an aws region physically remain in that region. amazon s3 does not keep copies of objects or move them to any other region. however, you can access the objects from anywhere, as long as you have necessary permissions to do so. before you can upload an object into amazon s3, you must have write permissions to a bucket. objects can be any file type: images, backups, data, movies, etc. you can have an unlimited number of objects in a bucket. the maximum size of file you can upload by using the amazon s3 console is 160 gb. to upload a file larger than 160 gb, use the aws cli, aws sdk, or amazon s3 rest api. for more information, see  in the amazon simple storage service developer guide. the following topics explain how to use the amazon s3 console to upload, delete, and manage objects. noteif you rename an object, or change any of the properties; storage class,  enryption, metadata, a new object is created to replace the old one. if s3 versioning is enabled, a new version of the object is created, and the existing object becomes an older version. the role that changes the property also becomes the owner of the new object or (object version). topics 
this section explains how to use the console to view the properties of an object. to view the properties of an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object you want to view the properties for. choose properties. on the properties page, you can configure the following properties for the object. noteif you change the properties; storage class,  enryption, metadata, a new object is created to replace the old one. if s3 versioning is enabled, a new version of the object is created, and the existing object becomes an older version. the role that changes the property also becomes the owner of the new object or (object version). storage class – each object in amazon s3 has a storage class associated with it. the storage class that you choose to use depends on how frequently you access the object. the default storage class for s3 objects is standard. you choose which storage class to use when you upload an object. for more information about storage classes, see  in the amazon simple storage service developer guide. to change the storage class after you upload an object, choose storage class. choose the storage class that you want, and then choose save. encryption – you can encrypt your s3 objects. for more information, see .  metadata – each object in amazon s3 has a set of name-value pairs that represents its metadata. for information on adding metadata to an s3 object, see . tags – you can add tags to an s3 object. for more information, see . 
amazon s3 block public access prevents the application of any settings that allow public access to data within s3 buckets. this section describes how to edit block public access settings for all the s3 buckets in your aws account. for information about blocking public using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. to edit block public access settings for all the s3 buckets in an aws account sign in to the aws management console and open the amazon s3 console at . choose block public access (account settings). choose edit to change the block public access settings for all the buckets in your aws account. choose the settings that you want to change, and then choose save. when you're asked for confirmation, enter confirm. then choose confirm to save your changes. 
 you can use amazon s3 access points to manage access to your s3 objects. amazon s3 access points are named network endpoints that are attached to buckets that you can use to perform s3 object operations, such as uploading and retrieving objects. a bucket can have up to 1,000 access points attached, and each access point enforces distinct permissions and network controls to give you fine-grained control over access to your s3 objects.  for more information about amazon s3 access points, see  in the amazon simple storage service developer guide. the following topics explain how to use the s3 management console to create, manage, and use amazon s3 access points. topics 
you can host a static website on amazon s3. on a static website, individual webpages include static content. a static website might also contain client-side scripts. by contrast, a dynamic website relies on server-side processing, including server-side scripts such as php, jsp, or asp.net. amazon s3 does not support server-side scripting. you can use the following quick procedures to configure an s3 bucket for static website hosting in the amazon s3 console. for more information and detailed walkthroughs, see  in the amazon simple storage service developer guide. topics sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to enable static website hosting for. choose properties. choose static website hosting. choose use this bucket to host a website. in index document, enter the name of the index document, typically .  when you configure a bucket for website hosting, you must specify an index document. amazon s3 returns this index document when requests are made to the root domain or any of the subfolders. for more information, see  in the amazon simple storage service developer guide. (optional) if you want to provide your own custom error document for 4xx class errors, in error document, enter the custom error document filename.  if you do not specify a custom error document and an error occurs, amazon s3 returns a default html error document. for more information, see  in the amazon simple storage service developer guide. (optional) if you want to specify advanced redirection rules, in the edit redirection rules box, enter xml to describe the rules. for example, you can conditionally route requests according to specific object key names or prefixes in the request. for more information, see  in the amazon simple storage service developer guide. choose save. upload the index document to your bucket. for step-by-step instructions on uploading an object to an s3 bucket, see .  upload other files for your website, including optional custom error documents. in the next section, you set the permissions required to access your bucket as a static website.. by default, amazon s3 blocks public access to your account and buckets. if you want to use a bucket to host a static website, you can use these steps to edit your block public access settings.  warningbefore you complete this step, review  to ensure that you understand and accept the risks involved with allowing public access. when you turn off block public access settings to make your bucket public, anyone on the internet can access your bucket. we recommend that you block all public access to your buckets. open the amazon s3 console at . choose the name of the bucket that you have configured as a static website. choose permissions. choose edit. clear block all public access, and choose save. warningbefore you complete this step, review  to ensure you understand and accept the risks involved with allowing public access. when you turn off block public access settings to make your bucket public, anyone on the internet can access your bucket. we recommend that you block all public access to your buckets. in the confirmation box, enter confirm, and then choose confirm. under s3 buckets, the access for your bucket updates to objects can be public. you can now add a bucket policy to make the objects in the bucket publicly readable. if the access still displays as bucket and objects not public, you might have to  for your account before adding a bucket policy. after you edit s3 block public access settings, you can add a bucket policy to grant public read access to your bucket. when you grant public read access, anyone on the internet can access your bucket. importantthe following policy is an example only and allows full access to the contents of your bucket. before you proceed with this step, review  to ensure that you understand the best practices for securing the files in your s3 bucket and risks involved in granting public access. under buckets, choose the name of your bucket. choose permissions. choose bucket policy. to grant public read access for your website, copy the following bucket policy, and paste it in the bucket policy editor. update the  to include your bucket name. in the preceding example bucket policy, example.com is the bucket name. to use this bucket policy with your own bucket, you must update this name to match your bucket name. choose save. a warning appears indicating that the bucket has public access. in bucket policy, a public label appears. if you see an error that says , confirm that the bucket name in the bucket policy matches your bucket name. for information about adding a bucket policy, see  if you get an error - access denied warning and the bucket policy editor does not allow you to save the bucket policy, check your account-level and bucket-level block public access settings to confirm that you allow public access to the bucket. once you configure your bucket as a static website and set permissions, you can access your website through an amazon s3 website endpoint. for more information, see  in the amazon simple storage service developer guide. for a complete list of amazon s3 website endpoints, see  in the amazon web services general reference. 
this section describes how to configure advanced s3 bucket property settings for object replication, event notification, and transfer acceleration. topics 
there are three types of amazon cloudwatch metrics for amazon s3: storage metrics are reported once per day and are provided to all customers at no additional cost.replication metrics are available 15 minutes after enabling a replication rule with s3 replication time control (s3 rtc). for more information, see request metrics are available at 1-minute intervals after some latency to process, and the metrics are billed at the standard cloudwatch rate. for more information about cloudwatch metrics for amazon s3, see  in the amazon simple storage service developer guide. to get request metrics, you must opt into them by configuring them on the aws management console or using the amazon s3 api. to configure request metrics on a bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the objects you want request metrics for. choose the management tab, and then choose metrics. choose requests. in the left pane, choose the edit icon next to the name of the bucket. select the request metrics check box. this also enables data transfer metrics. choose save. you have now created a metrics configuration for all the objects in an amazon s3 bucket. about 15 minutes after cloudwatch begins tracking these request metrics, you can see graphs for the metrics on the amazon s3 or cloudwatch console.  you can also define a filter so that the metrics are only collected and reported on a subset of objects in the bucket. for more information, see  
to upload your data (photos, videos, documents etc.) to amazon s3, you must first create an s3 bucket in one of the aws regions. you can then upload your data objects to the bucket. every object you store in amazon s3 resides in a bucket. you can use buckets to group related objects in the same way that you use a directory to group files in a file system.  amazon s3 creates buckets in the aws region that you specify. you can choose any aws region that is geographically close to you to optimize latency, minimize costs, or address regulatory requirements. for example, if you reside in europe, you might find it advantageous to create buckets in the europe (ireland) or europe (frankfurt) regions. for a list of amazon s3 aws regions, see  in the amazon web services general reference. you are not charged for creating a bucket. you are only charged for storing objects in the bucket and for transferring objects out of the bucket. for more information about pricing, see . amazon s3 bucket names are globally unique, regardless of the aws region in which you create the bucket. you specify the name at the time you create the bucket. for bucket naming guidelines, see  in the amazon simple storage service developer guide. the following topics explain how to use the amazon s3 console to create, delete, and manage buckets. topics 
amazon s3 provides a set of tools to help you manage your s3 batch operations jobs after you create them. for more information about managing s3 batch operations, see  in the amazon simple storage service developer guide.    in the amazon simple storage service developer guide in the amazon simple storage service developer guide in the amazon simple storage service developer guide
you can change the display language of the aws management console. several languages are supported. to change the console language sign in to the aws management console and open the amazon s3 console at . scroll down until you see the bar at the bottom of the window, and then choose the language on the left side of the bar. choose the language that you want from the menu. this will change the langauge for the entire aws management console. 
amazon simple storage service (amazon s3) transfer acceleration enables fast, easy, and secure file transfers between your client and an s3 bucket over long distances. this topic describes how to enable amazon s3 transfer acceleration for a bucket. for more information, see  in the amazon simple storage service developer guide. to enable transfer acceleration for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the buckets list, choose the name of the bucket that you want to enable transfer acceleration for. choose properties. under advanced settings, choose transfer acceleration. (optional) if you want to compare accelerated and non-accelerated upload speeds, to open the , choose the want to compare your data transfer speed by region?. the speed comparison tool uses multipart upload to transfer a file from your browser to various aws regions with and without amazon s3 transfer acceleration. you can compare the upload speed for direct uploads and transfer accelerated uploads by region.  choose enabled, and choose save. endpoint displays the transfer acceleration endpoint for your bucket. you use this endpoint to access accelerated data transfers to and from your bucket. if you suspend transfer acceleration, the accelerate endpoint no longer works. 
this section describes how to enable an aws cloudtrail trail to log data events for objects in an s3 bucket by using the amazon s3 console. cloudtrail supports logging amazon s3 object-level api operations such as , , and . these events are called data events. by default, cloudtrail trails don't log data events, but you can configure trails to log data events for s3 buckets that you specify, or to log data events for all the amazon s3 buckets in your aws account. for more information, see . cloudtrail does not populate data events in the cloudtrail event history. additionally, not all bucket-level actions are populated in the cloudtrail event history. for more information, see  to configure a trail to log data events for an s3 bucket, you can use either the aws cloudtrail console or the amazon s3 console. if you are configuring a trail to log data events for all the amazon s3 buckets in your aws account, it's easier to use the cloudtrail console. for information about using the cloudtrail console to configure a trail to log s3 data events, see  in the aws cloudtrail user guide.  importantadditional charges apply for data events. for more information, see .  the following procedure shows how to use the amazon s3 console to enable a cloudtrail trail to log data events for an s3 bucket. to enable cloudtrail data events logging for objects in an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket. choose properties. choose object-level logging. choose an existing cloudtrail trail in the drop-down menu.  the trail you select must be in the same aws region as your bucket, so the drop-down list contains only trails that are in the same region as the bucket or trails that were created for all regions.  if you need to create a trail, choose the cloudtrail console link to go to the cloudtrail console. for information about how to create trails in the cloudtrail console, see  in the aws cloudtrail user guide. under events, choose one of the following: read to specify that you want cloudtrail to log amazon s3 read apis such as . write to log amazon s3 write apis such as . read and write to log both read and write object apis.for a list of supported data events that cloudtrail logs for amazon s3 objects, see  in the amazon simple storage service developer guide. choose create to enable object-level logging for the bucket. to disable object-level logging for the bucket, you must go to the cloudtrail console and remove the bucket name from the trail's data events. noteif you use the cloudtrail console or the amazon s3 console to configure a trail to log data events for an s3 bucket, the amazon s3 console shows that object-level logging is enabled for the bucket.  for information about enabling object-level logging when you create an s3 bucket, see .  in the amazon simple storage service developer guide  in the aws cloudtrail user guide
this topic explains how to view the properties for an s3 bucket. to view the properties for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to view the properties for. choose properties. on the properties page, you can configure the following properties for the bucket. versioning – versioning enables you to keep multiple versions of an object in one bucket. by default, versioning is disabled for a new bucket. for information about enabling versioning, see . server access logging – server access logging provides detailed records for the requests that are made to your bucket. by default, amazon s3 does not collect server access logs. for information about enabling server access logging, see . static website hosting – you can host a static website on amazon s3. to enable static website hosting, choose static website hosting and then specify the settings you want to use. for more information, see . object-level logging – object-level logging records object-level api activity by using cloudtrail data events. for information about enabling object-level logging, see . tags – with aws cost allocation, you can use bucket tags to annotate billing for your use of a bucket. a tag is a key-value pair that represents a label that you assign to a bucket. to add tags, choose tags, and then choose add tag. for more information, see  in the amazon simple storage service developer guide.  transfer acceleration – amazon s3 transfer acceleration enables fast, easy, and secure transfers of files over long distances between your client and an s3 bucket. for information about enabling transfer acceleration, see . events – you can enable certain amazon s3 bucket events to send a notification message to a destination whenever the events occur. to enable events, choose events and then specify the settings you want to use. for more information, see .  requester pays – you can enable requester pays so that the requester (instead of the bucket owner) pays for requests and data transfers. for more information, see  in the amazon simple storage service developer guide.  
in the amazon s3 console, you can copy an object to a bucket or a folder. for more information, see  in the amazon simple storage service developer guide. to copy an object sign in to the aws management console and open the amazon s3 console at . in the buckets list, choose your bucket name. on the overview tab, select the check box beside the object that you want to copy. choose actions, and choose copy. choose the copy destination: if you want to copy your object to a bucket, select the bucket.if you want to copy your object to a folder in a bucket, choose the bucket name, and then choose the folder.under review, confirm the copy details, and choose copy. amazon s3 copies your object to the destination. 
amazon s3 block public access prevents the application of any settings that allow public access to data within s3 buckets. this section describes how to edit block public access settings for one or more s3 buckets. for information about blocking public access using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. topics follow these steps if you need to change the public access settings for a single s3 bucket. to edit the amazon s3 block public access settings for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose permissions. choose edit to change the public access settings for the bucket. for more information about the four amazon s3 block public access settings, see  in the amazon simple storage service developer guide. choose the setting that you want to change, and then choose save. when you're asked for confirmation, enter confirm. then choose confirm to save your changes. follow these steps if you need to change the public access settings for more than one s3 bucket. to edit the amazon s3 block public access settings for multiple s3 buckets sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the buckets that you want, and then choose edit public access settings. choose the setting that you want to change, and then choose save. when you're asked for confirmation, enter confirm. then choose confirm to save your changes. you can change amazon s3 block public access settings when you create a bucket. for more information, see .  
s3 batch operations performs large-scale batch operations on amazon s3 objects. you can use s3 batch operations to copy objects, set object tags or access control lists (acls), initiate object restores from amazon s3 glacier, or invoke an aws lambda function to perform custom actions using your objects. you can perform these operations on a custom list of objects, or you can use an amazon s3 inventory report to make generating even the largest lists of objects easy. s3 batch operations use the same amazon s3 apis that you already use, so you'll find the interface familiar. for information about performing s3 batch operations using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide.  the following topics explain how to use the amazon s3 console to configure and run batch operations. topics 
this section explains how to configure amazon s3 storage management tools. topics 
before you can enable event notifications for your bucket you must set up one of the following destination types: an amazon sns topicamazon simple notification service (amazon sns) is a web service that coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. you can use the amazon sns console to create an amazon sns topic that your notifications can be sent to. the amazon sns topic must be in the same region as your amazon s3 bucket. for information about creating an amazon sns topic, see  in the amazon simple notification service developer guide.before you can use the amazon sns topic that you create as an event notification destination, you need the following:   the amazon resource name (arn) for the amazon sns topica valid amazon sns topic subscription (the topic subscribers are notified when a message is published to your amazon sns topic)a permissions policy that you set up in the amazon sns console (as shown in the following example) an amazon sqs queueyou can use the amazon sqs console to create an amazon sqs queue that your notifications can be sent to. the amazon sqs queue must be in the same region as your amazon s3 bucket. for information about creating an amazon sqs queue, see  in the amazon simple queue service developer guide.before you can use the amazon sqs queue as an event notification destination, you need the following:   the amazon resource name (arn) for the amazon sqs topica permissions policy that you set up in the amazon sqs console (as shown in the following example) a lambda functionyou can use the aws lambda console to create a lambda function. the lambda function must be in the same region as your s3 bucket. for information about creating a lambda function, see the .before you can use the lambda function as an event notification destination, you must have the name or the arn of a lambda function to set up the lambda function as a event notification destination.  warningif your notification ends up writing to the bucket that triggers the notification, this could cause an execution loop. for example, if the bucket triggers a lambda function each time an object is uploaded, and the function uploads an object to the bucket, then the function indirectly triggers itself. to avoid this, use two buckets, or configure the trigger to only apply to a prefix used for incoming objects.for more information and an example of using amazon s3 notifications with aws lambda, see  in the aws lambda developer guide.  
replication is the automatic, asynchronous copying of objects across buckets in the same or different aws regions. it replicates newly created objects and object updates from a source bucket to a specified destination bucket.  you use the amazon s3 console to add replication rules to the source bucket. replication rules define the source bucket objects to replicate and the destination bucket where the replicated objects are stored. for more information about replication, see  in the amazon simple storage service developer guide. you can manage replication rules on the replication page. you can add, view, enable, disable, delete, and change the priority of the replication rules. for information about adding replication rules to a bucket, see . to manage the replication rules for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose management, and then choose replication. you change the replication rules in the following ways. to change settings that affect all the replication rules in the bucket, choose edit global settings. you can change the destination bucket, and the iam role. if needed, you can copy the required bucket policy for cross-account destination buckets. to change a replication rule, select the rule and choose edit, which starts the replication wizard to help you make the change. for information about using the wizard, see .to enable or disable a replication rule, select the rule, choose more, and in the drop-down list, choose enable rule or disable rule. you can also disable, enable, or delete all the rules in the bucket from the more drop-down list.to change the rule priorities, choose edit priorities. you can then change the priority for each rule under the priority column heading. choose save to save your changes. you set rule priorities to avoid conflicts caused by objects that are included in the scope of more than one rule. in the case of overlapping rules, amazon s3 uses the rule priority to determine which rule to apply. the higher the number, the higher the priority. for more information about rule priority, see  in the amazon simple storage service developer guide.  in the amazon simple storage service developer guide
for the latest aws terminology, see the  in the aws general reference. 
this section explains how to use the amazon simple storage service (amazon s3) console to add a new bucket policy or edit an existing bucket policy. a bucket policy is a resource-based aws identity and access management (iam) policy. you add a bucket policy to a bucket to grant other aws accounts or iam users access permissions for the bucket and the objects in it. object permissions apply only to the objects that the bucket owner creates. for more information about bucket policies, see  in the amazon simple storage service developer guide.  for examples of amazon s3 bucket policies, see  in the amazon simple storage service developer guide.  to create or edit a bucket policy sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to create a bucket policy for or whose bucket policy you want to edit. choose permissions, and then choose bucket policy. in the bucket policy editor text box, type or copy and paste a new bucket policy, or edit an existing policy. the bucket policy is a json file. the text you type in the editor must be valid json. choose save. noteamazon s3 displays the amazon resource name (arn) for the bucket next to the bucket policy editor title. for more information about arns, see  in the amazon web services general reference.directly below the bucket policy editor text box is a link to the policy generator, which you can use to create a bucket policy. 
before you can upload data to amazon s3, you must create a bucket in one of the aws regions to store your data. after you create a bucket, you can upload an unlimited number of data objects to the bucket.  the aws account that creates the bucket owns it. by default, you can create up to 100 buckets in each of your aws accounts. if you need additional buckets, you can increase your account bucket quota to a maximum of 1,000 buckets by submitting a service quota increase. for information about how to increase your bucket quota, see  in the aws general reference.  buckets have configuration properties, including geographical region, access settings for the objects in the bucket, and other metadata.  to create a bucket sign in to the aws management console and open the amazon s3 console at . choose create bucket. the create bucket wizard opens. in bucket name, enter a dns-compliant name for your bucket. the bucket name must: be unique across all of amazon s3.be between 3 and 63 characters long.not contain uppercase characters.start with a lowercase letter or number.after you create the bucket, you can't change its name. for information about naming buckets, see  in the amazon simple storage service developer guide. importantavoid including sensitive information, such as account numbers, in the bucket name. the bucket name is visible in the urls that point to the objects in the bucket. in region, choose the aws region where you want the bucket to reside.  choose a region close to you to minimize latency and costs and address regulatory requirements. objects stored in a region never leave that region unless you explicitly transfer them to another region. for a list of amazon s3 aws regions, see  in the amazon web services general reference. in bucket settings for block public access, choose the block public access settings that you want to apply to the bucket.  we recommend that you leave all settings enabled unless you know you need to turn one or more of them off for your use case, such as to host a public website. block public access settings that you enable for the bucket will also be enabled for all access points that you create on the bucket. for more information about blocking public access, see  in the amazon simple storage service developer guide. (optional) if you want to enable s3 object lock: choose advanced settings, and read the message that appears. importantyou can only enable s3 object lock for a bucket when you create it. if you enable object lock for the bucket, you can't disable it later. enabling object lock also enables versioning for the bucket. after you enable object lock for the bucket, you must configure the object lock settings before any objects in the bucket will be protected. for more information about configuring protection for objects, see . if you want to enable object lock, enter enable in the text box and choose confirm. for more information about the s3 object lock feature, see  in the amazon simple storage service developer guide. choose create bucket. 
this section explains how to use the amazon s3 console to delete folders from an s3 bucket.  for information about amazon s3 features and pricing, see . to delete folders from an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to delete folders from. in the name list, select the check box next to the folders and objects that you want to delete, choose more, and then choose delete. in the delete objects dialog box, verify that the names of the folders you selected for deletion are listed and then choose delete. 
latest documentation update: march 27, 2019 the following table describes the important changes in each release of the amazon simple storage service console user guide from june 19, 2018, onward. for notification about updates to this documentation, you can subscribe to an rss feed. the following table describes the important changes in each release of the amazon simple storage service console user guide before june 19, 2018.  
amazon s3 block public access prevents the application of any settings that allow public access to data within s3 buckets. you can configure block public access settings for an individual s3 bucket or for all the buckets in your account. for information about blocking public access using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. the following topics explain how to use the amazon s3 console to configure block public access settings:  the following sections explain viewing bucket access status and searching by access types. the list buckets view shows whether your bucket is publicly accessible. amazon s3 labels the permissions for a bucket as follows: public – everyone has access to one or more of the following: list objects, write objects, read and write permissions. objects can be public – the bucket is not public, but anyone with the appropriate permissions can grant public access to objects. buckets and objects not public – the bucket and objects do not have any public access.only authorized users of this account – access is isolated to iam users and roles in this account and aws service principals because there is a policy that grants public access.the access column shows the access status of the listed buckets.  you can also filter bucket searches by access type. choose an access type from the drop-down list that is next to the search for buckets bar.   
this section explains how to create an amazon s3 access point using the aws management console. for information about creating access points using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. an access point is associated with exactly one amazon s3 bucket. before you begin, make sure that you have created a bucket that you want to use with this access point. for more information about creating buckets, see . to create an access point sign in to the aws management console and open the amazon s3 console at . in the s3 buckets section, select the bucket that you want to attach this access point to. on the bucket detail page, choose the access points tab. choose create access point. enter your desired name for the access point in the access point name field. choose a network access type. if you choose virtual private cloud (vpc), enter the vpc id that you want to use with the access point. for more information about network access type for access points, see  in the amazon simple storage service developer guide. select the block public access settings that you want to apply to the access point. all block public access settings are enabled by default for new access points, and we recommend that you leave all settings enabled unless you know you have a specific need to disable any of them. amazon s3 currently doesn't support changing an access point's block public access settings after the access point has been created. for more information about using amazon s3 block public access with access points, see  in the amazon simple storage service developer guide. (optional) specify the access point policy. the console automatically displays the amazon resource name (arn) for the access point, which you can use in the policy. choose create access point. 
this section explains how to use the amazon s3 console to delete objects. because all objects in your s3 bucket incur storage costs, you should delete objects that you no longer need. if you are collecting log files, for example, it's a good idea to delete them when they're no longer needed. you can set up a lifecycle rule to automatically delete objects such as log files. for information about amazon s3 features and pricing, see . to delete objects from an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to delete an object from. you can delete objects from an s3 bucket in any of the following ways: in the name list, select the check box next to the objects and folders that you want to delete, choose actions, and then choose delete from the drop-down menu. in the delete objects dialog box, verify that the name(s) of the object(s) and/or folder(s) you selected for deletion are listed and then choose delete. or, choose the name of the object that you want to delete, choose latest version, and then choose the trash can icon.
by using the amazon s3 analytics storage class analysis tool, you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. storage class analysis observes data access patterns to help you determine when to transition less frequently accessed standard storage to the standard_ia (ia, for infrequent access) storage class. for more information about standard_ia, see the  and  in the amazon simple storage service developer guide. importantstorage class analysis does not give recommendations for transitions to the onezone_ia or s3 glacier storage classes. for more information about analytics, see  in the amazon simple storage service developer guide. to configure storage class analysis sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket for which you want to configure storage class analysis. choose the management tab, and then choose analytics. choose add. type a name for the filter. if you want to analyze the whole bucket, leave the prefix / tags field empty. in the prefix / tags  field, type text for the prefix or tag for the objects that you want to analyze, or choose from the dropdown list that appears when you start typing. if you chose tag, enter a value for the tag. you can enter one prefix and multiple tags. optionally, you can choose export data to export analysis reports to a comma-separated values (.csv) flat file. choose a destination bucket where the file can be stored. you can type a prefix for the destination bucket. the destination bucket must be in the same aws region as the bucket for which you are setting up the analysis. the destination bucket can be in a different aws account. choose save. amazon s3 creates a bucket policy on the destination bucket that grants amazon s3 write permission. this allow it to write the export data to the bucket.  if an error occurs when you try to create the bucket policy, you'll be given instructions on how to fix it. for example, if you chose a destination bucket in another aws account and do not have permissions to read and write to the bucket policy, you'll see the following message. you must have the destination bucket owner add the displayed bucket policy to the destination bucket. if the policy is not added to the destination bucket you won’t get the export data because amazon s3 doesn’t have permission to write to the destination bucket. if the source bucket is owned by a different account than that of the current user, then the correct account id of the source bucket must be substituted in the policy.  for information about the exported data and how the filter works, see  in the amazon simple storage service developer guide. more info  
there are three types of amazon cloudwatch metrics for amazon s3: storage metrics, request metrics, and replication metrics. replication metrics are available 15 minutes after a replication rule with s3 replication time control (s3 rtc) has been enabled. replication metrics are billed at the standard amazon cloudwatch rate. they are turned on automatically when you enable replication with s3 rtc using the aws management console or the amazon s3 api. replication metrics track the rule ids of the replication configuration. a replication rule id can be specific to a prefix, a tag, or a combination of both. for more information about s3 replication time control (s3 rtc), see  in the amazon simple storage service developer guide. for more information about cloudwatch metrics for amazon s3, see  in the amazon simple storage service developer guide. to view replication metrics sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the objects you want replication metrics for. choose the management tab, and then choose metrics. choose replication.   in the rule ids list in the left pane, select the rule ids that you want. if you have several rule ids to choose from, you can search for the ids that you want. after choosing the rule ids that you want, choose display graphs below the rule ids selection box. you can then view the replication metrics replication latency (in seconds), operations pending replication, and bytes pending replication for the rules that you selected. amazon cloudwatch begins reporting replication metrics 15 minutes after you enable s3 rtc on the respective replication rule. you can view replication metrics on the amazon s3 or cloudwatch console. for information, see  in the amazon simple storage service developer guide. 
amazon s3 has a flat structure instead of a hierarchy like you would typically see in a file system. however, for the sake of organizational simplicity, the amazon s3 console supports a folder concept as a way to group objects. in amazon s3, the folder is a naming prefix for an object or group of objects. for more information, see  we recommend blocking all public access to your amazon s3 folders and buckets unless you specifically require a public folder or bucket. when you make a folder public, anyone on the internet can view all the objects that are grouped in that folder. in the amazon s3 console, you can make a folder public. you can also make a folder public by creating a bucket policy that limits access by prefix. for more information, see .  warningafter you make a folder public in the amazon s3 console, you can't make it private again. instead, you must set permissions on each individual object in the public folder so that the objects have no public access. for more information, see  
access analyzer for s3 alerts you to s3 buckets that are configured to allow access to anyone on the internet or other aws accounts, including aws accounts outside of your organization. for each public or shared bucket, you receive findings into the source and level of public or shared access. for example, access analyzer for s3 might show that a bucket has read or write access provided through a bucket access control list (acl), a bucket policy, or an access point policy. armed with this knowledge, you can take immediate and precise corrective action to restore your bucket access to what you intended.  when reviewing an at-risk bucket in access analyzer for s3, you can block all public access to the bucket with a single click. we recommend that you block all access to your buckets unless you require public access to support a specific use case. before you block all public access, ensure that your applications will continue to work correctly without public access. for more information, see  in the amazon simple storage service developer guide. you can also drill down into bucket-level permission settings to configure granular levels of access. for specific and verified use cases that require public access, such as static website hosting, public downloads, or cross-account sharing, you can acknowledge and record your intent for the bucket to remain public or shared by archiving the findings for the bucket. you can revisit and modify these bucket configurations at any time. you can also download your findings as a csv report for auditing purposes. access analyzer for s3 is available at no extra cost on the amazon s3 console. access analyzer for s3 is powered by aws identity and access management (iam) access analyzer. to use access analyzer for s3 in the amazon s3 console, you must visit the iam console and enable iam access analyzer on a per-region basis.  for more information about iam access analyzer, see  in the iam user guide. for more information about access analyzer for s3, review the following sections. importantaccess analyzer for s3 requires an account-level analyzer. to use access analyzer for s3, you must visit iam access analyzer and create an analyzer that has an account as the zone of trust. for more information, see  in iam user guide. when a bucket policy or bucket acl is added or modified, access analyzer generates and updates findings based on the change within 30 minutes. findings related to account level block public access settings may not be generated or updated for up to 6 hours after you change the settings. topics access analyzer for s3 provides findings for buckets that can be accessed outside your aws account. buckets that are listed under buckets with public access can be accessed by anyone on the internet. if access analyzer for s3 identifies public buckets, you also see a warning at the top of the page that shows you the number of public buckets in your region. buckets listed under buckets with access from other aws accounts — including third-party aws accounts are shared conditionally with other aws accounts, including accounts outside of your organization.  for each bucket, access analyzer for s3 provides the following information: bucket namediscovered by access analyzer ‐ when access analyzer for s3 discovered the public or shared bucket access.shared through ‐ how the bucket is shared—through a bucket policy, a bucket acl, or an access point policy. a bucket can be shared through both policies and acls. if you want to find and review the source for your bucket access, you can use the information in this column as a starting point for taking immediate and precise corrective action. status ‐ the status of the bucket finding. access analyzer for s3 displays findings for all public and shared buckets. active ‐ finding has not been reviewed. archived ‐ finding has been reviewed and confirmed as intended. all ‐ all findings for buckets that are public or shared with other aws accounts, including aws accounts outside of your organization.access level ‐ access permissions granted for the bucket:list ‐ list resources.read ‐ read but not edit resource contents and attributes.write ‐ create, delete, or modify resources.permissions ‐ grant or modify resource permissions.tagging ‐ update tags associated with the resource.to use access analyzer for s3, you must complete the following prerequisite steps. grant the required permissions. for more information, see  in the iam user guide. visit iam to create an account-level analyzer for each region where you want to use access analyzer. access analyzer for s3 requires an account-level analyzer. to use access analyzer for s3, you must create an analyzer that has an account as the zone of trust. for more information, see  in iam user guide. if you want to block all access to a bucket in a single click, you can use the block all public access button in access analyzer for s3. when you block all public access to a bucket, no public access is granted. we recommend that you block all public access to your buckets unless you require public access to support a specific and verified use case. before you block all public access, ensure that your applications will continue to work correctly without public access. if you don't want to block all public access to your bucket, you can edit your block public access settings on the amazon s3 console to configure granular levels of access to your buckets. for more information, see  in the amazon simple storage service developer guide. in rare events, access analyzer for s3 might report no findings for a bucket that an amazon s3 block public access evaluation reports as public. this happens because amazon s3 block public access reviews policies for current actions and any potential actions that might be added in the future, leading to a bucket becoming public. on the other hand, access analyzer for s3 only analyzes the current actions specified for the amazon s3 service in the evaluation of access status. to block all public access to a bucket using access analyzer for s3 sign in to the aws management console and open the amazon s3 console at . in the navigation pane on the left, under dashboards, choose access analyzer for s3. in access analyzer for s3, choose a bucket. choose block all public access. to confirm your intent to block all public access to the bucket, in block all public access (bucket settings), enter confirm. amazon s3 blocks all public access to your bucket. the status of the bucket finding updates to resolved, and the bucket disappears from the access analyzer for s3 listing. if you want to review resolved buckets, open iam access analyzer on the iam console. if you did not intend to grant access to the public or other aws accounts, including accounts outside of your organization, you can modify the bucket acl, bucket policy, or access point policy to remove the access to the bucket. the shared through column shows all sources of bucket access: bucket policy, bucket acl, and/or access point policy. to review and change a bucket policy, a bucket acl, or an access point policy open the amazon s3 console at . in the navigation pane, choose access analyzer for s3. to see whether public access or shared access is granted through a bucket policy, a bucket acl, or an access point policy, look in the shared through column. under bucket name, choose the name of the bucket with the bucket policy, bucket acl, or access point policy that you want to change or review. if you want to change or view a bucket acl: choose permissions. choose access control list. review your bucket acl, and make changes as required. for more information, see  if you want to change or review a bucket policy: choose permissions. choose bucket policy. review or change your bucket policy as required. for more information, see  if you want to review or change an access point policy: choose access points. choose the access point name. review or change access as required.  for more information, see . if you edit or remove a bucket acl, a bucket policy, or an access point policy to remove public or shared access, the status for the bucket findings updates to resolved. the resolved bucket findings disappear from the access analyzer for s3 listing, but you can view them in iam access analyzer. if a bucket grants access to the public or other aws accounts, including accounts outside of your organization, to support a specific use case (for example, a static website, public downloads, or cross-account sharing), you can archive the finding for the bucket. when you archive bucket findings, you acknowledge and record your intent for the bucket to remain public or shared. archived bucket findings remain in your access analyzer for s3 listing so that you always know which buckets are public or shared. to archive bucket findings in access analyzer for s3 open the amazon s3 console at . in the navigation pane, choose access analyzer for s3. in access analyzer for s3, choose an active bucket. to acknowledge your intent for this bucket to be accessed by the public or other aws accounts, including accounts outside of your organization, choose archive. enter confirm, and choose archive. after you archive findings, you can always revisit them and change their status back to active, indicating that the bucket requires another review.  to activate an archived bucket finding in access analyzer for s3 open the amazon s3 console at . in the navigation pane, choose access analyzer for s3. choose the archived bucket findings. choose mark as active. if you need to see more information about a bucket, you can open the bucket finding details in iam access analyzer on the iam console. to view finding details in access analyzer for s3 open the amazon s3 console at . in the navigation pane, choose access analyzer for s3. in access analyzer for s3, choose a bucket. choose view details. the finding details open in iam access analyzer on the iam console.  you can download your bucket findings as a csv report that you can use for auditing purposes. the report includes the same information that you see in access analyzer for s3 on the amazon s3 console. to download a report open the amazon s3 console at . in the navigation pane on the left, choose access analyzer for s3. in the region filter, choose the region. access analyzer for s3 updates to shows buckets for the chosen region. choose download report. a csv report is generated and saved to your computer. 
there are three types of amazon cloudwatch metrics for amazon s3: storage metrics, request metrics and replication. storage metrics are reported once per day and are provided to all customers at no additional cost. request metrics are available at 1 minute intervals after some latency to process, and metrics are billed at the standard cloudwatch rate. to get request metrics, you must opt into them by configuring them in the console or with the amazon s3 api. for more conceptual information about cloudwatch metrics for amazon s3, see  in the amazon simple storage service developer guide. to filter request metrics on a subset of objects in a bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that has the objects you want to get request metrics for. choose the management tab. and then choose metrics. choose requests. from filters in the left-side pane, choose add. provide a name for this metrics configuration. provide one or more prefixes or tags, separated by commas, in prefix /tags that you want to monitor. from the drop down, select whether the value you provided is a tag or a prefix. choose save. you have now created a metrics configuration for request metrics on a subset of the objects in an amazon s3 bucket. about 15 minutes after cloudwatch begins tracking these request metrics, you can see graphs for the metrics in both the amazon s3 or cloudwatch consoles. you can also request metrics at the bucket level. for information, see  
in amazon s3, buckets and objects are the primary resources, and objects are stored in buckets. amazon s3 has a flat structure instead of a hierarchy like you would see in a file system. however, for the sake of organizational simplicity, the amazon s3 console supports the folder concept as a means of grouping objects. amazon s3 does this by using a shared name prefix for objects (that is, objects have names that begin with a common string). object names are also referred to as key names. for example, you can create a folder on the console named  and store an object named  in it. the object is then stored with the key name , where  is the prefix. here are two more examples:  if you have three objects in your bucket—, , and —the console will show a folder named . if you open the folder in the console, you will see three objects: , , and .if you have an object named , the console will show you a folder named  containing the folder  and the object .topics you can have folders within folders, but not buckets within buckets. you can upload and copy objects directly into a folder. folders can be created, deleted, and made public, but they cannot be renamed. objects can be copied from one folder to another.  importantthe amazon s3 console treats all objects that have a forward slash ("/") character as the last (trailing) character in the key name as a folder, for example . you can't upload an object that has a key name with a trailing "/" character using the amazon s3 console. however, you can upload objects that are named with a trailing "/" with the amazon s3 api by using the aws cli, aws sdks, or rest api.an object that is named with a trailing "/" appears as a folder in the amazon s3 console. the amazon s3 console does not display the content and metadata for such an object. when you use the console to copy an object named with a trailing "/", a new folder is created in the destination location, but the object's data and metadata are not copied.  
you can enable certain amazon s3 bucket events to send a notification message to a destination whenever the events occur. this section explains how to use the amazon s3 console to enable event notifications. for information about using event notifications with the aws sdks and the amazon s3 rest apis, see  in the amazon simple storage service developer guide.  topics when you configure event notifications for a bucket, you must specify the type of events for which you want to receive notifications. for a complete list of event types, see  section in the amazon simple storage service developer guide.  in the amazon s3 console, you have the following options for configuring event notifications. you can choose a single option or multiple options. object creationobjectcreated (all) – receive a notification when an object is created in your bucket for any of the following object creation actions: put, post, copy, and multipart upload completed.put, post, copy, and multipart upload completed – receive a notification for one of these specific object creation actions.object deletion objectdelete (all) – receive a notification any time an object in your bucket is deleted.delete marker created – receive a notification when a delete marker is created for a versioned object.  for information about deleting versioned objects, see . for information about object versioning, see  and . object restoration from the s3 glacier storage class restore initiated – receive a notification for initiation of object restoration.restore completed – receive a notification for completion of object restoration.reduced redundancy storage (rrs) object lost eventsobject in rss lost – receive a notification that an object of the rrs storage class has been lostobjects eligible for replication using amazon s3 replication time controlreplication time missed threshold – receive a notification that an object failed to replicate.replication time completed after threshold – receive a notification that an object exceeded the 15-minute threshold for replication.replication time not tracked – receive a notification that an object replicated after the 15-minute threshold.replication time failed – receive a notification that an object that was eligible for replication is no longer being tracked by replication metrics.notewhen you delete the last object from a folder, amazon s3 can generate an object creation event. when there are multiple objects with the same prefix with a trailing slash (/) as part of their names, those objects are shown as being part of a folder in the amazon s3 console. the name of the folder is formed from the characters preceding the trailing slash (/).when you delete all the objects listed under that folder, no actual object is available to represent the empty folder. under such circumstances, the amazon s3 console creates a zero-byte object to represent that folder. if you enabled event notification for the creation of objects, the zero-byte object creation action that is taken by the console triggers an object creation event.the amazon s3 console displays a folder under the following circumstances:when a zero-byte object has a trailing slash (/) in its name. in this case, there is an actual amazon s3 object of 0 bytes that represents a folder. if the object has a slash (/) within its name. in this case, there isn't an actual object representing the folder. when you configure event notifications for a bucket, you also choose the notification destination. event notification messages can be sent to the following destinations: amazon simple notification service (amazon sns) topic – coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. for information about the amazon sns topic format, see .amazon simple queue service (amazon sqs) queue – offers reliable and scalable hosted queues for storing messages as they travel between computers. for information about amazon sqs, see  in the amazon simple queue service developer guide.aws lambda  – invoke a lambda function and provide the event message as an argument. when you create a lambda function, you package up and upload your custom code to aws lambda. aws lambda uses the aws infrastructure to run the code on your behalf. for information about using lambda with amazon s3, see  in the aws lambda developer guide.for more information about granting the amazon s3 service principal the permissions required to publish event notifications to a destination, see  in the amazon s3 developer guide. warningif your notification ends up writing to the bucket that triggers the notification, this could cause an execution loop. for example, if the bucket triggers a lambda function each time an object is uploaded, and the function uploads an object to the bucket, then the function indirectly triggers itself. to avoid this, use two buckets, or configure the trigger to only apply to a prefix used for incoming objects.for more information and an example of using amazon s3 notifications with aws lambda, see  in the aws lambda developer guide.  before you can enable event notifications for your bucket, you must set up one of the destination types. for more information, see  to enable and configure event notifications for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to enable events for. choose properties. under advanced settings, choose events. choose add notification. in name, enter a descriptive name for your event notification.  if you don't enter a name, a guid is generated and used for the name. under events, select one or more events.  for a listing of the event types, see . to filter event notifications by a prefix or suffix, enter a prefix or a suffix.  for example, you can set up a prefix filter so that you receive notifications only when files are added to a specific folder (for example, ). for more information, see . choose the event notification destination: sns topic, sqs queue, or lambda function.  for a description of the destinations, see . when you choose your send to destination, a box appears for you to enter your specific sns, sqs, or lambda function destination. in the example image below, the send to location is sns topic, and you can see a sns box for the sns topic name. in the box that appears, choose or enter the destination sns, sqs, or lambda function.  you can choose or enter the sns, sqs, or lambda function name, or you can choose to the destination amazon resource name (arn). the example screenshot below shows the add sns topic arn option. if you chose add arn, enter the sns topic, sqs queue, or lambda function arn. choose save.  amazon s3 sends a test message to the event notification destination. 
you can empty a bucket, which deletes all of the objects in the bucket without deleting the bucket. when you empty a bucket with versioning enabled, all versions of all the objects in the bucket are deleted. for more information, see  and  in the amazon simple storage service developer guide. to empty an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, select the option next to the name of the bucket that you want to empty and then choose empty. on the empty bucket page, confirm that you want to empty the bucket by entering the bucket name into the text field, and then choose empty. (optional) monitor the progress of the bucket emptying process on the empty bucket: status page. 
this topic explains how to use the aws management console to upload one or more files or entire folders to an amazon s3 bucket. before you can upload files and folders to an amazon s3 bucket, you need write permissions for the bucket. for more information about access permissions, see . for information about uploading files programmatically, see  in the amazon simple storage service developer guide.  when you upload a file to amazon s3, it is stored as an s3 object. objects consist of the file data and metadata that describes the object. you can have an unlimited number of objects in a bucket. you can upload any file type—images, backups, data, movies, etc.—into an s3 bucket. the maximum size of a file that you can upload by using the amazon s3 console is 160 gb. to upload a file larger than 160 gb, use the aws cli, aws sdk, or amazon s3 rest api. for more information, see  in the amazon simple storage service developer guide.  you can upload files by dragging and dropping or by pointing and clicking. to upload folders, you must drag and drop them. drag and drop functionality is supported only for the chrome and firefox browsers. for information about which chrome and firefox browser versions are supported, see .  when you upload a folder, amazon s3 uploads all of the files and subfolders from the specified folder to your bucket. it then assigns an object key name that is a combination of the uploaded file name and the folder name. for example, if you upload a folder called  that contains two files,  and , amazon s3 uploads the files and then assigns the corresponding key names,  and . the key names include the folder name as a prefix. the amazon s3 console displays only the part of the key name that follows the last “/”. for example, within an images folder the  and  objects are displayed as  and a . if you upload individual files and you have a folder open in the amazon s3 console, when amazon s3 uploads the files, it includes the name of the open folder as the prefix of the key names. for example, if you have a folder named  open in the amazon s3 console and you upload a file named , the key name is . however, the object is displayed in the console as  in the  folder. if you upload individual files and you do not have a folder open in the amazon s3 console, when amazon s3 uploads the files, it assigns only the file name as the key name. for example, if you upload a file named , the key name is . for more information on key names, see  in the amazon simple storage service developer guide. if you upload an object with a key name that already exists in a versioning-enabled bucket, amazon s3 creates another version of the object instead of replacing the existing object. for more information about versioning, see . topics if you are using the chrome or firefox browsers, you can choose the folders and files to upload, and then drag and drop them into the destination bucket. dragging and dropping is the only way that you can upload folders. to upload folders and files to an s3 bucket by using drag and drop sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to upload your folders or files to. in a window other than the console window, select the files and folders that you want to upload. then drag and drop your selections into the console window that lists the objects in the destination bucket. the files you chose are listed in the upload dialog box. in the upload dialog box, do one of the following:  drag and drop more files and folders to the console window that displays the upload dialog box. to add more files, you can also choose add more files. this option works only for files, not folders. to immediately upload the listed files and folders without granting or removing permissions for specific users or setting public permissions for all of the files that you're uploading, choose upload. for information about object access permissions, see .  to set permissions or properties for the files that you are uploading, choose next. on the set permissions page, under manage users you can change the permissions for the aws account owner. the owner refers to the aws account root user, and not an aws identity and access management (iam) user. for more information about the root user, see .  choose add account to grant access to another aws account. for more information about granting permissions to another aws account, see . under manage public permissions you can grant read access to your objects to the general public (everyone in the world), for all of the files that you're uploading. granting public read access is applicable to a small subset of use cases such as when buckets are used for websites. we recommend that you do not change the default setting of do not grant public read access to this object(s). you can always make changes to object permissions after you upload the object. for information about object access permissions, see .  when you're done configuring permissions, choose next. on the set properties page, choose the storage class and encryption method to use for the files that you are uploading. you can also add or modify metadata.  choose a storage class for the files you're uploading. for more information about storage classes, see  in the amazon simple storage service developer guide. choose the type of encryption for the files that you're uploading. if you don't want to encrypt them, choose none.  to encrypt the uploaded files using keys that are managed by amazon s3, choose amazon s3 master-key. for more information, see  in the amazon simple storage service developer guide. to encrypt the uploaded files using the aws key management service (aws kms), choose aws kms master-key. then choose a customer master key (cmk) from the list of aws kms cmks. noteto encrypt objects in a bucket, you can use only cmks that are available in the same aws region as the bucket.  you can give an external account the ability to use an object that is protected by an aws kms cmk. to do this, select custom kms arn from the list and enter the amazon resource name (arn) for the external account. administrators of an external account that have usage permissions to an object protected by your aws kms cmk can further restrict access by creating a resource-level iam policy.  for more information about creating an aws kms cmk, see  in the aws key management service developer guide. for more information about protecting data with aws kms, see  in the amazon simple storage service developer guide. metadata for amazon s3 objects is represented by a name-value (key-value) pair. there are two kinds of metadata: system-defined metadata and user-defined metadata. if you want to add amazon s3 system-defined metadata to all of the objects you are uploading, for header, select a header. you can select common http headers, such as content-type and content-disposition. type a value for the header, and then choose save. for a list of system-defined metadata and information about whether you can add the value, see  in the amazon simple storage service developer guide. any metadata starting with prefix  is treated as user-defined metadata. user-defined metadata is stored with the object, and is returned when you download the object.  to add user-defined metadata to all of the objects that you are uploading, type  plus a custom metadata name in the header field. type a value for the header, and then choose save. both the keys and their values must conform to us-ascii standards. user-defined metadata can be as large as 2 kb. for more information about user-defined metadata, see  in the amazon simple storage service developer guide. object tagging gives you a way to categorize storage. each tag is a key-value pair. key and tag values are case sensitive. you can have up to 10 tags per object.  to add tags to all of the objects that you are uploading, type a tag name in the key field. type a value for the tag, and then choose save. a tag key can be up to 128 unicode characters in length and tag values can be up to 255 unicode characters in length. for more information about object tags, see  in the amazon simple storage service developer guide. choose next. on the upload review page, verify that your settings are correct, and then choose upload. to make changes, choose previous. to see the progress of the upload, choose in progress at the bottom of the browser window. to see a history of your uploads and other operations, choose success. this procedure explains how to upload files into an s3 bucket by choosing upload. to upload files to an s3 bucket by pointing and clicking sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to upload your files to. choose upload. in the upload dialog box, choose add files. choose one or more files to upload, and then choose open. after you see the files that you chose listed in the upload dialog box, do one of the following:  to add more files, choose add more files. to immediately upload the listed files, choose upload.  to set permissions or properties for the files that you are uploading, choose next. to set permissions and properties, start with step 5 of . .
you can delete an empty bucket, and when you're using the aws management console, you can delete a bucket that contains objects. if you delete a bucket that contains objects, all the objects in the bucket are permanently deleted.  when you delete a bucket with versioning enabled, all versions of all the objects in the bucket are permanently deleted. for more information about versioning, see  in the amazon simple storage service developer guide. before deleting a bucket, consider the following: bucket names are unique. if you delete a bucket, another aws user can use the name. when you delete a bucket that contains objects, all the objects in the bucket are permanently deleted, including objects that transitioned to the  storage class.if the bucket hosts a static website, and you created and configured an amazon route 53 hosted zone as described in : you must clean up the route 53 hosted zone settings that are related to the bucket as described in .if the bucket receives log data from elastic load balancing (elb): we recommend that you stop the delivery of elb logs to the bucket before deleting it. after you delete the bucket, if another user creates a bucket using the same name, your log data could potentially be delivered to that bucket. for information about elb access logs, see  in the user guide for classic load balancers and  in the user guide for application load balancers.importantif you want to continue to use the same bucket name, don't delete the bucket. we recommend that you empty the bucket and keep it. after a bucket is deleted, the name becomes available to reuse, but the name might not be available for you to reuse for various reasons. for example, it might take some time before the name can be reused, and some other account could create a bucket with that name before you do. to delete an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the buckets list, select the option next to the name of the bucket that you want to delete and then choose delete at the top of the page. on the delete bucket page, confirm that you want to delete the bucket by entering the bucket name into the text field, and then choose delete bucket. noteif the bucket contains any objects, empty the bucket before deleting it by selecting the empty bucket configuration link in the this bucket is not empty error alert and following the instructions on the empty bucket page. then return to the delete bucket page and delete the bucket. 
replication is the automatic, asynchronous copying of objects across buckets in the same or different aws regions. replication copies newly created objects and object updates from a source bucket to a destination bucket. for more information about replication concepts and how to use replication with the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide.  replication requires versioning to be enabled on both the source and destination buckets. to review the full list of requirements, see  in the amazon simple storage service developer guide. for more information about versioning, see  the object replicas in the destination bucket are exact replicas of the objects in the source bucket. they have the same key names and the same metadata—for example, creation time, owner, user-defined metadata, version id, access control list (acl), and storage class. optionally, you can explicitly specify a different storage class for object replicas. and regardless of who owns the source bucket or the source object, you can choose to change replica ownership to the aws account that owns the destination bucket. for more information, see  in the amazon simple storage service developer guide.  you can use s3 replication time control (s3 rtc) to replicate your data in the same aws region or across different aws regions in a predictable timeframe. s3 rtc replicates 99.99 percent of new objects stored in amazon s3 within 15 minutes and most objects within seconds. for more information, see  in the amazon simple storage service developer guide. note about replication and lifecycle rulesmetadata for an object remains identical between original objects and replica objects. lifecycle rules abide by the creation time of the original object, and not by when the replicated object becomes available in the destination bucket. however, lifecycle does not act on objects that are pending replication until replication is complete. you use the amazon s3 console to add replication rules to the source bucket. replication rules define which source bucket objects to replicate and the destination bucket where the replicated objects are stored. you can create a rule to replicate all the objects in a bucket or a subset of objects with a specific key name prefix, one or more object tags, or both. a destination bucket can be in the same aws account as the source bucket, or it can be in a different account. if you specify an object version id to delete, amazon s3 deletes that object version in the source bucket. but it doesn't replicate the deletion in the destination bucket. in other words, it doesn't delete the same object version from the destination bucket. this protects data from malicious deletions. if the destination bucket is in a different account from the source bucket, you must add a bucket policy to the destination bucket to grant the owner of the source bucket account permission to replicate objects in the destination bucket. the amazon s3 console builds this required bucket policy for you to copy and add to the destination bucket in the other account.  when you add a replication rule to a bucket, the rule is enabled by default, so it starts working as soon as you save it.  topics follow these steps to configure a replication rule when the destination bucket is in the same aws account as the source bucket. sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose management, choose replication, and then choose add rule. in the replication rule wizard, under set source, you have the following options for setting the replication source: to replicate the whole bucket, choose entire bucket bucket-name. to replicate all objects that have the same prefix (for example, all objects that have names that begin with the string ), choose prefix or tags. enter a prefix in the box, choose the prefix from the drop-down list, and then press enter. if you enter a prefix that is the name of a folder, you must use / (forward slash) as the last character (for example, ). for more information about prefixes, see  in the amazon simple storage service developer guide.to replicate all objects with one or more object tags, enter a tag in the box, choose the tag from the drop-down list, and then press enter. enter a tag value and then press enter. repeat the procedure to add another tag. you can combine a prefix and tags. for more information about object tags, see  in the amazon simple storage service developer guide.the new schema supports prefix and tag filtering and the prioritization of rules. for more information about the new schema, see  in the amazon simple storage service developer guide. the developer guide describes the xml used with the amazon s3 api that works behind the user interface. in the developer guide, the new schema is described as replication configuration xml v2. to replicate objects in the source bucket that are encrypted with aws key management service (aws kms), under replication criteria, select replicate objects encrypted with aws kms. under choose one or more keys for decrypting source objects are the source aws kms customer master keys (cmks) that you allow replication to use. all source cmks are included by default. you can choose to narrow the cmk selection.  objects encrypted by aws kms cmks that you do not select are not replicated. a cmk or a group of cmks is chosen for you, but you can choose the cmks if you want. for information about using aws kms with replication, see  in the amazon simple storage service developer guide. importantwhen you replicate objects that are encrypted with aws kms, the aws kms request rate doubles in the source region and increases in the destination region by the same amount. these increased call rates to aws kms are due to the way that data is re-encrypted using the customer master key (cmk) that you define for the replication destination region. aws kms has a request rate limit that is per calling account per region. for information about the limit defaults, see  in the aws key management service developer guide.if your current amazon s3 put object request rate during replication is more than half the default aws kms rate limit for your account, we recommend that you request an increase to your aws kms request rate limit. to request an increase, create a case in the aws support center at . for example, suppose that your current put object request rate is 1,000 requests per second and you use aws kms to encrypt your objects. in this case, we recommend that you ask aws support to increase your aws kms rate limit to 2,500 requests per second, in both your source and destination regions (if different), to ensure that there is no throttling by aws kms.to see your put object request rate in the source bucket, view  in the amazon cloudwatch request metrics for amazon s3. for information about viewing cloudwatch metrics, see  choose next. to choose a destination bucket from the account that you're currently using, on the set destination page, under destination bucket, choose buckets in this account. enter the name of the destination bucket for the replication, or choose a name in the drop-down list. if you want to choose a destination bucket from a different aws account, see . if versioning is not enabled on the destination bucket, you get a warning message that contains an enable versioning button. choose this button to enable versioning on the bucket. if you chose to replicate objects encrypted with aws kms, under destination encryption settings, enter the amazon resource name (arn) of the aws kms cmk to use to encrypt the replicas in the destination bucket. you can find the arn for your aws kms cmk in the iam console, under encryption keys.  or, you can choose a cmk name from the drop-down list. for more information about creating an aws kms cmk, see  in the aws key management service developer guide. importantthe amazon s3 console lists only 100 aws kms cmks per aws region. if you have more than 100 cmks in the same region, you can see only the first 100 cmks in the s3 console. to use a kms cmk that is not listed in the console, choose custom kms arn, and enter the kms cmk arn. if you want to replicate your data into a specific storage class in the destination bucket, on the set destination page, under destination options, select change the storage class for the replicated object(s). then choose the storage class that you want to use for the replicated objects in the destination bucket. if you don't select this option, the storage class for replicated objects is the same class as the original objects.  similarly, if you want to change object ownership in the destination bucket, choose change object ownership to the destination bucket owner. for more information about this option, see .  if you want to enable s3 replication time control (s3 rtc) in your replication configuration, select replication time control. notewhen you use s3 rtc, additional per-gb data transfer fees and cloudwatch metrics fees apply. choose next. set up an aws identity and access management (iam) role that amazon s3 can assume to replicate objects on your behalf. to set up an iam role, on the configure options page, under select role, do one of the following: we highly recommend that you choose create new role to have amazon s3 create a new iam role for you. when you save the rule, a new policy is generated for the iam role that matches the source and destination buckets that you choose. the name of the generated role is based on the bucket names and uses the following naming convention: replication_role_for_source-bucket_to_destination-bucket.you can choose to use an existing iam role. if you do, you must choose a role that grants amazon s3 the necessary permissions for replication. replication fails if this role does not grant amazon s3 sufficient permissions to follow your replication rule.  importantwhen you add a replication rule to a bucket, you must have the  permission to be able to pass the iam role that grants amazon s3 replication permissions. for more information, see  in the iam user guide.under rule name, enter a name for your rule to help identify the rule later. the name is required and must be unique within the bucket. if the bucket has existing replication rules, you are instructed to set a priority for the rule. you must set a priority for the rule to avoid conflicts caused by objects that are included in the scope of more than one rule. in the case of overlapping rules, amazon s3 uses the rule priority to determine which rule to apply. the higher the number, the higher the priority. for more information about rule priority, see  in the amazon simple storage service developer guide. under status, enabled is selected by default. an enabled rule starts to work as soon as you save it. if you want to enable the rule later, select disabled. choose next. on the review page, review your replication rule. if it looks correct, choose save. otherwise, choose previous to edit the rule before saving it.  after you save your rule, you can edit, enable, disable, or delete your rule on the replication page. follow these steps to configure a replication rule when the destination bucket is in a different aws account than the source bucket. sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose management, choose replication, and then choose add rule. if you have never created a replication rule before, start with . on the replication rule wizard set destination page, under destination bucket, choose buckets in another account. then enter the name of the destination bucket and the account id from a different aws account. choose save. after you save the destination bucket name and account id, you might get a warning message telling you to add a bucket policy to the destination bucket so that amazon s3 can verify whether versioning is enabled on the bucket. you are presented with a bucket policy in a few steps that you can copy and add to the destination bucket in the other account. for information about adding a bucket policy to an s3 bucket and versioning, see  and  if you chose to replicate objects encrypted with aws kms, under destination encryption settings, enter the amazon resource name (arn) aws kms cmk to use to encrypt the replicas in the destination bucket.  for more information about creating an aws kms cmk, see  in the aws key management service developer guide. on the set destination page, under destination options: to replicate your data into a specific storage class in the destination bucket, select change the storage class for the replicated object(s). then choose the storage class that you want to use for the replicated objects in the destination bucket. if you don't select this option, the storage class for replicated objects is the same class as the original objects.to change the object ownership of the replica objects to the destination bucket owner, select change object ownership to destination owner. this option enables you to separate object ownership of the replicated data from the source. if asked, type the account id of the destination bucket. when you select this option, regardless of who owns the source bucket or the source object, the aws account that owns the destination bucket is granted full permission to replica objects. for more information, see  in the amazon simple storage service developer guide. if you want to add s3 replication time control (s3 rtc) to your replication configuration, select replication time control. notewhen you use s3 rtc, additional per-gb data transfer fees and cloudwatch metrics fees apply.choose next. set up an aws identity and access management (iam) role that amazon s3 can assume to perform replication of objects on your behalf. to set up an iam role, on the configure options page, under select role, do one of the following: we highly recommend that you choose create new role to have amazon s3 create a new iam role for you. when you save the rule, a new policy is generated for the iam role that matches the source and destination buckets that you choose. the name of the generated role is based on the bucket names and uses the following naming convention: replication_role_for_source-bucket_to_destination-bucket.you can choose to use an existing iam role. if you do, you must choose a role that allows amazon s3 to replicate objects from the source bucket to the destination bucket on your behalf.a bucket policy is provided on the configure options page that you can copy and add to the destination bucket in the other account. for information about adding a bucket policy to an s3 bucket, see  if you chose to replicate objects encrypted with aws kms, an aws kms key policy is provided on the configure options page. you can copy this policy to add to the key policy for the aws kms cmk that you are using. the key policy grants the source bucket owner permission to use the cmk. for information about updating the key policy, see . on the review page, review your replication rule. if it looks correct, choose save. otherwise, choose previous to edit the rule before saving it.  after you save your rule, you can edit, enable, disable, or delete your rule on the replication page. follow the instructions given on the replication page under the warning message, the replication rule is saved, but additional settings are required in the destination account. sign out of the aws account that you are currently in, and then sign in to the destination account.  importantreplication fails until you sign in to the destination account and complete the following steps. after you sign in to the destination account, choose the management tab, choose replication, and then choose receive objects on the actions menu.  on the receive objects page, you can do the following: enable versioning on the destination bucket.apply the bucket policy provided by amazon s3 to the destination bucket.copy the aws kms key policy that you need to update the aws kms cmk that is being used to encrypt the replica objects in the destination bucket. for information about updating the key policy, see .you must grant permissions to the account of the source bucket owner to encrypt using your aws kms cmk with a key policy. the following procedure describes how to use the aws identity and access management (iam) console to modify the key policy for the aws kms cmk that is being used to encrypt the replica objects in the destination bucket. to grant permissions to encrypt using your aws kms cmk sign in to the aws management console using the aws account that owns the aws kms cmk. open the aws kms console at . choose the alias of the cmk that you want to encrypt with. in the key policy section of the page, choose switch to policy view. choose edit to edit key policy. using the key policy editor, insert the key policy provided by amazon s3 into the existing key policy, and then choose save changes. you might want to add the policy to the end of the existing policy.  for more information about creating and editing aws kms cmks, see  in the aws key management service developer guide.   in the amazon simple storage service developer guide
this section describes how to create a s3 batch operations job. for information about performing batch operations using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. to create a batch job sign in to the aws management console and open the amazon s3 console at . choose batch operations on the navigation pane of the amazon s3 console. choose create job. choose the region where you want to create your job. under manifest format, choose the type of manifest object to use. if you choose s3 inventory report, enter the path to the manifest.json object that amazon s3 generated as part of the csv-formatted inventory report, and optionally the version id for the manifest object if you want to use a version other than the most recent.if you choose csv, enter the path to a csv-formatted manifest object. the manifest object must follow the format described in the console. you can optionally include the version id for the manifest object if you want to use a version other than the most recent.choose next under operation, choose the operation that you want to perform on all objects listed in the manifest. fill out the information for the operation you chose and then choose next. fill out the information for configure additional options and then choose next. for review, verify the settings. if you need to make changes, choose previous. otherwise, choose create job.  in the amazon simple storage service developer guide in the amazon simple storage service developer guide in the amazon simple storage service developer guide
*copyright &copy; 2020 amazon web services, inc. and/or its affiliates. all rights reserved.* amazon's trademarks and trade dress may not be used in       connection with any product or service that is not amazon's,       in any manner that is likely to cause confusion among customers,       or in any manner that disparages or discredits amazon. all other       trademarks not owned by amazon are the property of their respective      owners, who may or may not be affiliated with, connected to, or       sponsored by amazon. 
versioning enables you to keep multiple versions of an object in one bucket. this section describes how to enable object versioning on a bucket. for more information about versioning support in amazon s3, see  and  in the amazon simple storage service developer guide. to enable or disable versioning on an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to enable versioning for. choose properties. choose versioning. choose enable versioning or suspend versioning, and then choose save. noteyou can use aws multi-factor authentication (mfa) with versioning. when you use mfa with versioning, you must provide your aws account’s access keys and a valid code from the account’s mfa device in order to permanently delete an object version or suspend or reactivate versioning. to use mfa with versioning, you enable . however, you cannot enable  using the aws management console. you must use the aws cli or api. for more information, see . 
this section explains how to use the amazon s3 console to recover (undelete) deleted objects. to be able to undelete a deleted object, you must have had versioning enabled on the bucket that contains the object before the object was deleted. for information about enabling versioning, see . when you delete an object in a versioning-enabled bucket, all versions remain in the bucket and amazon s3 creates a delete marker for the object. to undelete the object, you must delete this delete marker. for more information about versioning and delete markers, see  in the amazon simple storage service developer guide. to recover deleted objects from an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. to see a list of the versions of the objects in the bucket, select show. you'll be able to see the delete markers for deleted objects.  to undelete an object, you must delete the delete marker. select the check box next to the delete marker of the object to recover, and then choose delete from the actions menu. then, choose hide and you'll see the undeleted object listed.  in the amazon simple storage service developer guide
this section explains how to use the amazon s3 console to restore an object that has been archived to the s3 glacier or s3 glacier deep archive storage classes. objects stored in the s3 glacier or s3 glacier deep archive are not immediately accessible. to access an object in this class, you must restore a temporary copy of it to its s3 bucket for the duration (number of days) that you specify. for information about the s3 glacier or s3 glacier deep archive storage classes, see  in the amazon simple storage service developer guide.  when you restore an archive, you pay for both the archive and the restored copy. because there is a storage cost for the copy, restore objects only for the duration you need them. if you want a permanent copy of the object, create a copy of it in your s3 bucket. for information about amazon s3 features and pricing, see . after restoring an object, you can download it from the overview page. for more information, see . topics the following are the available retrieval options when restoring an archived object:   - expedited retrievals allow you to quickly access your data stored in the s3 glacier storage class when occasional urgent requests for a subset of archives are required. for all but the largest archived objects (250 mb+), data accessed using expedited retrievals is typically made available within 1–5 minutes. provisioned capacity ensures that retrieval capacity for expedited retrievals is available when you need it. for more information, see . expedited retrievals and provisioned capacity are not available for objects stored in the s3 glacier deep archive storage class. - standard retrievals allow you to access any of your archived objects within several hours. this is the default option for the s3 glacier and s3 glacier deep archive retrieval requests that do not specify the retrieval option. standard retrievals typically finish within 3–5 hours for objects stored in the s3 glacier storage class. they typically finish within 12 hours for objects stored in the s3 glacier deep archive storage class.  - bulk retrievals are the lowest-cost retrieval option in amazon s3 glacier, enabling you to retrieve large amounts, even petabytes, of data inexpensively. bulk retrievals typically finish within 5–12 hours for objects stored in the s3 glacier storage class. they typically finish within 48 hours for objects stored in the s3 glacier deep archive storage class.for more information about retrieval options, see  in the amazon simple storage service developer guide. this topic explains how to use the amazon s3 console to restore an object that has been archived to the s3 glacier or s3 glacier deep archive storage classes. (the console uses the names glacier and glacier deep archive for these storage classes.) to restore archived s3 objects sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the objects that you want to restore. in the name list, select the object or objects that you want to restore, choose actions, and then choose restore. in the initiate restore dialog box, type the number of days that you want your archived data to be accessible.  choose one of the following retrieval options from the retrieval options menu. choose bulk retrieval or standard retrieval, and then choose restore. choose expedited retrieval (available only for the glacier storage class).provisioned capacity is only available for the glacier storage class. if you have provisioned capacity, choose restore to start a provisioned retrieval. if you have provisioned capacity, all of your expedited retrievals are served by your provisioned capacity. for more information about provisioned capacity, see .  if you don't have provisioned capacity and you don't want to buy it, choose restore. if you don't have provisioned capacity, but you want to buy it, choose add capacity unit, and then choose buy. when you get the purchase succeeded message, choose restore to start provisioned retrieval.you can upgrade the speed of your restoration while it is in progress. to upgrade an in-progress restore to a faster tier in the name list, select one or more of the objects that you are restoring, choose actions, and then choose restore from glacier. for information about checking the restoration status of an object, see . choose the tier that you want to upgrade to and then choose restore. for more information about upgrading to a faster restore tier, see see  in the amazon simple storage service developer guide.  to check the progress of the restoration, see the object overview panel. for information about the overview panel, see .  the overview section shows that restoration is in progress.  when the temporary copy of the object is available, the object's overview section shows the restoration expiry date. this is when amazon s3 will remove the restored copy of your archive.   restored objects are stored only for the number of days that you specify. if you want a permanent copy of the object, create a copy of it in your amazon s3 bucket.  amazon s3 calculates the expiry date by adding the number of days that you specify to the time you request to restore the object, and then rounding to the next day at midnight utc. this calculation applies to the initial restoration of the object and to any extensions to availability that you request. for example, if an object was restored on 10/15/2012 10:30 am utc and the number of days that you specified is 3, then the object is available until 10/19/2012 00:00 utc. if, on 10/16/2012 11:00 am utc you change the number of days that you want it to be accessible to 1, then amazon s3 makes the restored object available until 10/18/2012 00:00 utc. after restoring an object, you can download it from the overview page. for more information, see .   in the amazon s3 developer guide. in the aws cli command reference.
object tagging gives you a way to categorize storage. this topic explains how to use the console to add tags to an s3 object after the object has been uploaded. for information about adding tags to an object when the object is being uploaded, see .  each tag is a key-value pair that adheres to the following rules: you can associate up to 10 tags with an object. tags associated with an object must have unique tag keys.a tag key can be up to 128 unicode characters in length and tag values can be up to 255 unicode characters in length.key and tag values are case sensitive.  for more information about object tags, see  in the amazon simple storage service developer guide. to add tags to an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object you want to add tags to. choose properties. choose tags and then choose add tag. each tag is a key-value pair. type a key and a value. then choose add tag to add another tag or choose save.  you can enter up to 10 tags for an object. 
this section explains how to use the amazon simple storage service (amazon s3) console to grant access permissions to your buckets and objects. it also explains how to use amazon s3 block public access to prevent the application of any settings that allow public access to data within s3 buckets.   buckets and objects are amazon s3 resources.  you grant access permissions to your buckets and objects by using resource-based access policies. you can associate an access policy with a resource. an access policy describes who has access to resources. the resource owner is the aws account that creates the resource. for more information about resource ownership and access policies, see  in the amazon simple storage service developer guide.  bucket access permissions specify which users are allowed access to the objects in a bucket and which types of access they have. object access permissions specify which users are allowed access to the object and which types of access they have. for example, one user might have only read permission, while another might have read and write permissions. bucket and object permissions are independent of each other. an object does not inherit the permissions from its bucket. for example, if you create a bucket and grant write access to a user, you can't access that user’s objects unless the user explicitly grants you access.  to grant access to your buckets and objects to other aws accounts and to the general public, you use resource-based access policies known as access control lists (acls).  a bucket policy is a resource-based aws identity and access management (iam) policy that grants other aws accounts or iam users access to an s3 bucket. bucket policies supplement, and in many cases, replace acl-based access policies.  for more information about using iam with amazon s3, see  in the amazon simple storage service developer guide.  for more in-depth information about managing access permissions, see  in the amazon simple storage service developer guide. this section also explains how to use the amazon s3 console to add a cross-origin resource sharing (cors) configuration to an s3 bucket. cors allows client web applications that are loaded in one domain to interact with resources in another domain. topics 
this section explains how to use the amazon simple storage service (amazon s3) console to manage access permissions for s3 buckets by using access control lists (acls). acls are resource-based access policies that grant access permissions to buckets and objects. for more information about managing access permissions with resource-based policies, see  in the amazon simple storage service developer guide. you can grant permissions to other aws account users or to predefined groups. the user or group that you are granting permissions to is called the grantee. by default, the owner, which is the aws account that created the bucket, has full permissions. each permission you grant for a user or group adds an entry in the acl that is associated with the bucket. the acl lists grants, which identify the grantee and the permission granted. for more information about acls, see  in the amazon simple storage service developer guide. to set acl access permissions for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to set permissions for. choose permissions, and then choose access control list. you can manage bucket access permissions for the following:  1.  access for your aws account root user  access for other aws accounts to grant permissions to an aws user from a different aws account, under access for other aws accounts, choose add account. in the enter an id field, enter the canonical id of the aws user that you want to grant bucket permissions to. for information about finding a canonical id, see  in the amazon web services general reference. you can add as many as 99 users. select the check boxes next to the permissions that you want to grant to the user, and then choose save. to display information about the permissions, choose the help icons. warningwhen you grant other aws accounts access to your resources, be aware that the aws accounts can delegate their permissions to users under their accounts. this is known as cross-account access. for information about using cross-account access, see  in the iam user guide.  public access  warninguse caution when granting the everyone group public access to your s3 bucket. when you grant access to this group, anyone in the world can access your bucket. we highly recommend that you never grant any kind of public write access to your s3 bucket. s3 log delivery group to grant access to amazon s3 to write server access logs to the bucket, under s3 log delivery group, choose log delivery. if a bucket is set up as the target bucket to receive access logs, the bucket permissions must allow the log delivery group write access to the bucket. when you enable server access logging on a bucket, the amazon s3 console grants write access to the log delivery group for the target bucket that you choose to receive the logs. for more information about server access logging, see . you can also set bucket permissions when you are creating a bucket. for more information about setting permissions when creating a bucket, see .  
amazon s3 default encryption provides a way to set the default encryption behavior for an amazon s3 bucket. you can set default encryption on a bucket so that all objects are encrypted when they are stored in the bucket. the objects are encrypted using server-side encryption with either amazon s3-managed keys (sse-s3) or aws key management service (aws kms) customer master keys (cmks).  when you use server-side encryption, amazon s3 encrypts an object before saving it to disk in its data centers and decrypts it when you download the objects. for more information about protecting data using server-side encryption and encryption key management, see  in the amazon simple storage service developer guide. default encryption works with all existing and new amazon s3 buckets. without default encryption, to encrypt all objects stored in a bucket, you must include encryption information with every object storage request. you must also set up an amazon s3 bucket policy to reject storage requests that don't include encryption information.  there are no new charges for using default encryption for s3 buckets. requests to configure the default encryption feature incur standard amazon s3 request charges. for information about pricing, see . for sse-kms cmk storage, aws kms charges apply and are listed at .  to enable default encryption on an amazon s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose properties. choose default encryption. if you want to use keys that are managed by amazon s3 for default encryption, choose aes-256, and choose save.  for more information about using amazon s3 server-side encryption to encrypt your data, see  in the amazon simple storage service developer guide. importantyou might need to update your bucket policy when enabling default encryption. for more information, see  in the amazon simple storage service developer guide. if you want to use cmks that are stored in aws kms for default encryption, follow these steps: choose aws-kms. to choose a customer-managed aws kms cmk that you have created, use one of these methods: in the list that appears, choose the aws kms cmk.in the list that appears, choose custom kms arn, and then enter the amazon resource name of the aws kms cmk. importantyou can use only kms cmks that are enabled in the same aws region as the bucket. the s3 console only lists 100 kms cmks per region. if you have more than 100 cmks in the same region, you can only see the first 100 cmks in the s3 console. to use a kms cmk that is not listed in the console, choose custom kms arn, and enter the kms cmk arn.when you use an aws kms cmk for server-side encryption in amazon s3, you must choose a symmetric cmk. amazon s3 only supports symmetric cmks and not asymmetric cmks. for more information, see  in the aws key management service developer guide. importantif you use the aws kms option for your default encryption configuration, you are subject to the rps (requests per second) limits of aws kms. for more information about aws kms limits and how to request a limit increase, see . for more information about creating an aws kms cmk, see  in the aws key management service developer guide. for more information about using aws kms with amazon s3, see  in the amazon simple storage service developer guide. choose save.  in the amazon simple storage service developer guide
this section explains how to use the amazon s3 console to see the different versions of an object. a versioning-enabled bucket can have many versions of the same object:, one current (latest) version and zero or more noncurrent (previous) versions. amazon s3 assigns each object a unique version id. for information about enabling versioning, see .  if a bucket is versioning-enabled, amazon s3 creates another version of an object under the following conditions:  if you upload an object that has the same name as an object that already exists in the bucket, amazon s3 creates another version of the object instead of replacing the existing object. if you update any object properties after you upload the object to the bucket, such as changing the storage details or other metadata , amazon s3 creates a new object version in the bucket. for more information about versioning support in amazon s3, see  and  in the amazon simple storage service developer guide. to see multiple versions of an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. to see a list of the versions of the objects in the bucket, choose show. for each object version, the console shows a unique version id, the date and time the object version was created, and other properties. (objects stored in your bucket before you set the versioning state have a version id of null.)  to list the objects without the versions, choose hide. you also can view, download, and delete object versions in the object overview panel. for more information, see . importantyou can undelete an object only if it was deleted as the latest (current) version. you can't undelete a previous version of an object that was deleted. for more information, see  and  in the amazon simple storage service developer guide. 
this section explains how to use the amazon s3 console to add a cross-origin resource sharing (cors) configuration to an s3 bucket. cors allows client web applications that are loaded in one domain to interact with resources in another domain.  to configure your bucket to allow cross-origin requests, you add cors configuration to the bucket. a cors configuration is an xml document that defines rules that identify the origins that you will allow to access your bucket, the operations (http methods) supported for each origin, and other operation-specific information. for more information about cors and examples, see  in the amazon simple storage service developer guide. when you enable cors on the bucket, the access control lists (acls) and other access permission policies continue to apply. to add a cors configuration to an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to create a bucket policy for. choose permissions, and then choose cors configuration. in the cors configuration editor text box, type or copy and paste a new cors configuration, or edit an existing configuration. the cors configuration is an xml file. the text that you type in the editor must be valid xml. for more information, see  choose save. noteamazon s3 displays the amazon resource name (arn) for the bucket next to the cors configuration editor title. for more information about arns, see  in the amazon web services general reference. 
you can use lifecycle policies to define actions that you want amazon s3 to take during an object's lifetime (for example, transition objects to another storage class, archive them, or delete them after a specified period of time). you can define a lifecycle policy for all objects or a subset of objects in the bucket by using a shared prefix (objects names that begin with a common string) or a tag.  using a lifecycle policy, you can define actions specific to current and noncurrent object versions. for more information, see  and  and  in the amazon simple storage service developer guide. to create a lifecycle policy sign in to the aws management console and open the amazon s3 console at . in the buckets list, choose the name of the bucket that you want to create a lifecycle policy for. choose the management tab, and choose add lifecycle rule. in the lifecycle rule dialog box, type a name for your rule.  the name must be unique within the bucket.  choose the scope of the lifecycle rule: all objects with a specific prefix or tag or all the objects in the bucket. to apply this lifecycle rule to all objects with a specific prefix or tag, choose limit the scope to specific prefixes or tags. in the add prefix or tag filter box, type the prefix or tag name, and press enter.  for more information about object name prefixes, see  in the amazon simple storage service developer guide.for more information about object tags, see  in the amazon simple storage service developer guide.to apply this lifecycle rule to all objects in the bucket, choose apply to all objects in the bucket.choose next. the storage class transition page opens. when you configure your storage class transitions, you define the rules to transition objects to the standard-ia, one zone-ia, glacier, and deep archive storage classes. for more information, see  in the amazon simple storage service developer guide. you can define transitions for current or previous object versions or for both current and previous versions. versioning enables you to keep multiple versions of an object in one bucket. for more information about versioning, see . choose the versions for which you want to define transitions, current or noncurrent: to define transitions that are applied to the current verion of the object, choose current version. to define transitions that are applied to all previous versions of the object, choose previous versions. in the screenshot below, current version is chosen. to add a transition:  for a current version, under for current object versions, choose add transition. for a non-current version, under for non-current object versions, choose add transition. for each transition that you add, choose one of the following: transition to standard-ia after.transition to intelligent-tiering after. transition to one zone-ia after. transition to glacier after.transition to glacier deep archive after. importantwhen you choose the glacier or glacier deep archive storage class, your objects remain in amazon s3. you cannot access them directly through the separate amazon s3 glacier service. for more information, see . in the days after creation box, enter the number of days after the creation of the object that you want the transition to be applied (for example, 30 or 100 days). when you are done configuring transitions, choose next. under configure expiration, for this example, choose both current version and previous versions.  importantin a non-versioned bucket the expiration action results in amazon s3 permanently removing the object. for more information about lifecycle actions, see  in the amazon simple storage service developer guide. choose expire current version of object, and then enter the number of days after object creation to delete the object (for example, 395 days). if you choose this expire option, you cannot choose the option to clean up expired delete markers.  choose permanently delete previous versions, and then enter the number of days after an object becomes a previous version to permanently delete the object (for example, 465 days). to clean up incomplete multipart uploads, we recommend that you choose clean up incomplete multipart uploads and enter the number of days after the multipart upload initiation that you want to end and clean up incomplete multipart uploads(for example, 7 days).  for more information about multipart uploads, see  in the amazon simple storage service developer guide. choose next. for review, verify the settings for your rule. if you need to make changes, choose previous. otherwise, choose save.  if the rule does not contain any errors, it is enabled and you can see it on the lifecycle page. 
with s3 object lock, you can store objects in amazon s3 using a write-once-read-many (worm) model. you can use s3 object lock to prevent an object from being deleted or overwritten for a fixed amount of time or indefinitely. for information about object locking using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. before you lock any objects, you have to enable a bucket to use s3 object lock. you enable object lock when you create a bucket. after you enable object lock on a bucket, you can lock objects in that bucket. when you create a bucket with object lock enabled, you can't disable object lock or suspend versioning for that bucket.  for information about creating a bucket with s3 object lock enabled, see . to lock an amazon s3 object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. in the name list, choose the name of the object that you want to lock. choose properties. choose object lock. choose a retention mode. you can change the retain until date. you can also choose to enable a legal hold. for more information, see  in the amazon simple storage service developer guide. choose save. 
welcome to the amazon simple storage service console user guide for the amazon simple storage service (amazon s3) console.  amazon s3 provides virtually limitless storage on the internet. this guide explains how you can manage buckets, objects, and folders in amazon s3 by using the aws management console, a browser-based graphical user interface for interacting with aws services.  for detailed conceptual information about how amazon s3 works, see  in the amazon simple storage service developer guide. the developer guide also has detailed information about amazon s3 features and code examples to support those features. topics 
this topic describes how to enable server access logging for an amazon s3 bucket using the aws management console. for information about how to enable logging programmatically and details about how logs are delivered, see  in the amazon simple storage service developer guide. by default, amazon simple storage service (amazon s3) doesn't collect server access logs. when you enable logging, amazon s3 delivers access logs for a source bucket to a target bucket that you choose. the target bucket must be in the same aws region as the source bucket and must not have a default retention period configuration.  server access logging provides detailed records for the requests that are made to an s3 bucket. server access logs are useful for many applications. for example, access log information can be useful in security and access audits. it can also help you learn about your customer base and understand your amazon s3 bill.  an access log record contains details about the requests that are made to a bucket. this information can include the request type, the resources that are specified in the request, and the time and date that the request was processed. for more information, see  in the amazon simple storage service developer guide. importantthere is no extra charge for enabling server access logging on an amazon s3 bucket. however, any log files that the system delivers to you will accrue the usual charges for storage. (you can delete the log files at any time.) we do not assess data transfer charges for log file delivery, but we do charge the normal data transfer rate for accessing the log files. to enable server access logging for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to enable server access logging for. choose properties. choose server access logging. choose enable logging. for target, choose the name of the bucket that you want to receive the log record objects.  the target bucket must be in the same region as the source bucket and must not have a default retention period configuration. (optional) for target prefix, type a key name prefix for log objects, so that all of the log object names begin with the same string. choose save. you can view the logs in the target bucket. if you specified a prefix, the prefix shows as a folder in the target bucket in the console. after you enable server access logging, it might take a few hours before the logs are delivered to the target bucket. for more information about how and when logs are delivered, see  in the amazon simple storage service developer guide. more info  
this topic describes how to set or change the type of encryption an object using the amazon s3 console. noteif you change an object's encryption, a new object is created to replace the old one. if s3 versioning is enabled, a new version of the object is created, and the existing object becomes an older version. the role that changes the property also becomes the owner of the new object or (object version).  to add or change encryption for an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object that you want to add or change encryption for. choose properties, and then choose encryption. the encryption dialog box opens, giving you three choices for object encryption: none‐ no object encryption.aes-256 ‐ server-side encryption with amazon s3 managed keys (sse-s3).aws‐kms ‐ server-side encryption with aws key management service (aws kms) customer master keys (sse-kms).if you want to remove encryption from an object that already has encryption settings, choose none and then choose save. if you want to encrypt your object using keys that are managed by amazon s3, follow these steps: chooseaes-256.  for more information about using amazon s3 server-side encryption to encrypt your data, see  in the amazon simple storage service developer guide. choose save. if you want to encrypt your object using aws kms, follow these steps: choose aws-kms. choose an aws kms customer master key (cmk). the list shows  that you have created and your aws managed cmk for amazon s3. for more information about creating a customer managed aws kms cmk, see  in the aws key management service developer guide.  importantthe amazon s3 console lists only 100 aws kms cmks per aws region. if you have more than 100 cmks in the same region, you can see only the first 100 cmks in the s3 console. to use a kms cmk that is not listed in the console, choose custom kms arn, and enter the kms cmk arn. choose save. importantto encrypt objects in the bucket, you can use only cmks that are enabled in the same aws region as the bucket. amazon s3 only supports symmetric cmks. amazon s3 does not support asymmetric cmks. for more information, see . to give an external account the ability to use an object that is protected by an aws kms cmk, follow these steps:  choose aws-kms. enter the amazon resource name (arn) for the external account. choose save. administrators of an external account that have usage permissions to an object protected by your aws kms cmk can further restrict access by creating a resource-level aws identity and access management (iam) policy.  in the amazon simple storage service developer guide
this section explains how to manage and use your amazon s3 access points using the aws management console. each access point is associated with a single amazon s3 bucket. before you begin, navigate to the list of your access points for a bucket as described in the following procedure. to find a list of access points for a bucket sign in to the aws management console and open the amazon s3 console at . in the s3 buckets section, select the bucket whose access points you want to manage. on the bucket detail page, choose the access points tab. from the access points tab, you can view an access point's configuration details, edit an access point's policy, use an access point to access your bucket, or delete an access point. the following procedures explain how to perform each of these tasks. to view access point configuration details navigate to the access points tab for your bucket. locate the access point whose configuration you want to view. you can browse for the access point in the access point list, or you can search for a specific access point using the search by name field. choose the name of the access point whose configuration you want to view. noteto view an access point's configuration, choose (click on) the name of the access point, not the option button next to the access point name. to edit an access point policy navigate to the access points tab for your bucket. select the option button next to the name of the access point whose policy you want to edit. choose edit access point policy. enter the policy in the text field. the console automatically displays the amazon resource name (arn) for the access point, which you can use in the policy. you can also choose policy generator to use the aws policy generator to help construct the policy. choose save. to use an access point to access your bucket navigate to the access points tab for your bucket. select the option button next to the name of the access point you want to use. choose use this access point. the console displays a label above the name of your bucket that shows the access point that you're currently using. while you're using the access point, you can only perform the object operations that are allowed by the access point permissions. notethe console view always shows all objects in the bucket. using an access point as described in this procedure restricts the operations you can perform on those objects, but not whether you can see that they exist in the bucket. to exit the access point view of your bucket, choose exit access point. notethe s3 management console doesn't support using virtual private cloud (vpc) access points to access bucket resources. to access bucket resources from a vpc access point, use the aws cli, aws sdks, or amazon s3 rest apis. to delete an access point navigate to the access points tab for your bucket. select the option button next to the name of the access point that you want to delete. choose delete. confirm that you want to delete your access point by entering its name in the text field that appears, and choose confirm. 
this section explains how to use the amazon s3 console to view the object overview panel. this panel provides an overview of all the essential information for an object in one place. to see the overview panel for an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, select the check box next to the name of the object for which you want an overview. to download the object, choose download in the object overview panel. to copy the path of the object to the clipboard, choose copy path. if versioning is enabled on the bucket, choose latest versions to list the versions of the object. you can then choose the download icon to download an object version, or choose the trash can icon to delete an object version. importantyou can undelete an object only if it was deleted as the latest (current) version. you can't undelete a previous version of an object that was deleted. for more information, see  and  in the amazon simple storage service developer guide. 
this section explains how to use the amazon s3 console to download objects from an s3 bucket. data transfer fees apply when you download objects. for information about amazon s3 features, and pricing, see . importantif an object key name consists of a single period (.), or two periods (..), you can’t download the object using the amazon s3 console. to download an object with a key name of “.” or “..”, you must use the aws cli, aws sdks, or rest api. for more information about naming objects, see  in the amazon simple storage service developer guide. you can download a single object per request using the amazon s3 console. to . to download an object from an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to download an object from. you can download an object from an s3 bucket in any of the following ways: in the name list, select the check box next to the object you want to download, and then choose download on the object description page that appears.choose the name of the object that you want to download. on the overview page, choose download. choose the name of the object that you want to download and then choose download as on the overview page.choose the name of the object that you want to download. choose latest version and then choose the download icon.
each object in amazon simple storage service (amazon s3) has a set of name-value pairs that provides metadata about the object. metadata is additional information about the object. some metadata is set by amazon s3 when you upload the object, for example, and . you can also set some metadata when you upload the object, or you can add it later. this section explains how to use the amazon s3 console to add metadata to an s3 object. object metadata is a set of name-value (key-value) pairs. for example, the metadata for content length, , is the name (key) and the size of the object in bytes (value). for more information about object metadata, see  in the amazon simple storage service developer guide. there are two kinds of metadata for an s3 object, amazon s3 system metadata and user-defined metadata: system metadata–there are two categories of system metadata. metadata such as the  date is controlled by the system. only amazon s3 can modify the value. there is also system metadata that you control, for example, the storage class configured for the object. user-defined metadata–you can define your own custom metadata, called user-defined metadata. you can assign user-defined metadata to an object when you upload the object or after the object has been uploaded. user-defined metadata is stored with the object and is returned when you download the object. amazon s3 does not process user-defined metadata. the following topics describe how to add metadata to an object using the amazon s3 console. noteif you change an object's metadata, a new object will be created and will replace the old one. if s3 versioning is enabled, a new version of the object is created, and the existing object becomes an older version. the role that changes the property also becomes the owner of the new object or (object version). topics you can configure some system metadata for an s3 object. for a list of system-defined metadata and whether you can modify their values, see  in the amazon simple storage service developer guide. to add system metadata to an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object that you want to add metadata to. choose properties, and then choose metadata. choose add metadata, and then choose a key from the select a key menu. depending on which key you chose, choose a value from the select a value menu or type a value. choose save. you can assign user-defined metadata to an object. user-defined metadata must begin with the prefix "", otherwise amazon s3 will not set the key value pair as you define it. you define custom metadata by adding a name that you choose to the  key. this creates a custom key. for example, if you add the custom name , the metadata key would be .  user-defined metadata can be as large as 2 kb. both keys and their values must conform to us-ascii standards. for more information, see  in the amazon simple storage service developer guide. to add user-defined metadata to an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object that you want to add metadata to. choose properties, and then choose metadata. choose add metadata, and then choose the  key from the select a key menu. any metadata starting with the prefix  is user-defined metadata. type a custom name following the  key. for example, for the custom name , the metadata key would be . enter a value for the custom key, and then choose save. 
amazon s3 inventory provides a flat file list of your objects and metadata, which is a scheduled alternative to the amazon s3 synchronous  api operation. amazon s3 inventory provides comma-separated values (csv) or  or  output files that list your objects and their corresponding metadata on a daily or weekly basis for an s3 bucket or for objects that share a prefix (objects that have names that begin with the same string). for more information, see  in the amazon simple storage service developer guide. to configure inventory noteit may take up to 48 hours to deliver the first report. sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket for which you want to configure amazon s3 inventory. choose the management tab, and then choose inventory. choose add new. type a name for the inventory and set it up as follows: optionally, add a prefix for your filter to inventory only objects whose names begin with the same string.choose the destination bucket where you want reports to be saved. the destination bucket must be in the same aws region as the bucket for which you are setting up the inventory. the destination bucket can be in a different aws account. optionally, choose a prefix for the destination bucket.choose how frequently to generate the inventory.under advanced settings, you can set the following: choose either the csv, orc, or parquet output file format for your inventory. for more information about these formats, see  in the amazon simple storage service developer guide. to include all versions of the objects in the inventory, choose include all versions in the object versions list. by default, the inventory includes only the current versions of the objects. for optional fields, select one or more of the following to add to the inventory report: size – object size in bytes.last modified date – the object creation date or the last modified date, whichever is the latest.storage class – the storage class used for storing the object. etag – the entity tag is a hash of the object. the etag reflects changes only to the contents of an object, and not its metadata. the etag may or may not be an md5 digest of the object data. whether it is depends on how the object was created and how it is encrypted.multipart upload – specifies that the object was uploaded as a multipart upload. for more information, see  in the amazon simple storage service developer guide.replication status – the replication status of the object. for more information, see .encryption status – the server-side encryption used to encrypt the object. for more information, see  in the amazon simple storage service developer guide.s3 object lock configurations – the object lock status of the object, including the following settings:  retention mode – the level of protection applied to the object, either governance or compliance.retain until date – the date until which the locked object cannot be deleted.legal hold status – the legal hold status of the locked object. for information about s3 object lock, see  in the amazon simple storage service developer guide. for more information about the contents of an inventory report, see  in the amazon simple storage service developer guide. for encryption, choose a server-side encryption option to encrypt the inventory report, or choose none: none – do not encrypt the inventory report.aes-256 – encrypt the inventory report using server-side encryption with amazon s3-managed keys (sse-s3). amazon s3 server-side encryption uses 256-bit advanced encryption standard (aes-256). for more information, see  in the amazon simple storage service developer guide. aws-kms – encrypt the report using server-side encryption with aws key management service (aws kms) customer master keys (cmks). for more information, see  in the amazon simple storage service developer guide.  noteto encrypt the inventory list file with sse-kms, you must grant amazon s3 permission to use the aws kms cmk. for instructions, see  .choose save. amazon s3 creates a bucket policy on the destination bucket that grants amazon s3 write permission. this allows amazon s3 to write data for the inventory reports to the bucket.  if an error occurs when you try to create the bucket policy, you are given instructions on how to fix it. for example, if you choose a destination bucket in another aws account and don't have permissions to read and write to the bucket policy, you see the following message.   in this case, the destination bucket owner must add the displayed bucket policy to the destination bucket. if the policy is not added to the destination bucket, you won't get an inventory report because amazon s3 doesn’t have permission to write to the destination bucket. if the source bucket is owned by a different account than that of the current user, the correct account id of the source bucket must be substituted in the policy. for more information, see  in the amazon simple storage service developer guide. to grant amazon s3 permission to encrypt using a customer managed aws key management service (aws kms) customer master key (cmk), you must use a key policy. to update your key policy so that you can use an aws kms customer managed cmk to encrypt the inventory file, follow the steps below. to grant permissions to encrypt using your aws kms cmk using the aws account that owns the customer managed cmk, sign into the aws management console. open the aws kms console at . to change the aws region, use the region selector in the upper-right corner of the page. in the left navigation pane, choose customer managed keys. under customer managed keys, choose the customer managed cmk that you want to use to encrypt the inventory file. under key policy, choose switch to policy view. to update the key policy, choose edit. under edit key policy, add the following key policy to the existing key policy. choose save changes. for more information about creating customer managed cmks aws kms and using key policies, see the following links in the aws key management service developer guide: more info  
for more detailed information about configuring a redirect in amazon s3, see  in the amazon simple storage service developer guide. you can redirect all requests for a website endpoint for a bucket to another host. if you redirect all requests, any request made to the website endpoint is redirected to the specified host name.  for example, if your root domain is , and you want to serve requests for both  and , you can create two buckets named  and . then, maintain the content in the  bucket, and configure the other  bucket to redirect all requests to the  bucket. for more information, see . to redirect requests for a bucket website endpoint open the amazon s3 console at . choose the name of the bucket that you have configured as a static website (for example, ). choose properties. choose static website hosting. choose redirect requests.  in the target bucket or domain box, enter the bucket or domain that you want to redirect to. for example, if you are redirecting to a root domain address, you would enter example.com. in the protocol box, enter the protocol for the redirected requests (http or https). if you do not specify a protocol, the protocol of the original request is used. choose save. 
in the amazon s3 console, you can move an object to a bucket or a folder.  to move an object sign in to the aws management console and open the amazon s3 console at . in the buckets list, choose your bucket name. select the check box beside the object that you want to move. choose actions, and choose move. choose the move destination: if you want to move your object to a bucket, select the bucket.if you want to move your object to a folder in a bucket, choose the bucket name, and then choose the folder.under review, confirm the move details, and choose move. amazon s3 moves your object to the destination. 
this section explains how to use the amazon simple storage service (amazon s3) console to manage access permissions for an amazon s3 object by using access control lists (acls). acls are resource-based access policies that grant access permissions to buckets and objects. for more information about managing access permissions with resource-based policies, see  in the amazon simple storage service developer guide. bucket and object permissions are independent of each other. an object does not inherit the permissions from its bucket. for example, if you create a bucket and grant write access to a user, you can't access that user’s objects unless the user explicitly grants you access. you can grant permissions to other aws accounts or predefined groups. the user or group that you grant permissions to is called the grantee. by default, the owner, which is the aws account that created the bucket, has full permissions. each permission you grant for a user or a group adds an entry in the acl that is associated with the object. the acl lists grants, which identify the grantee and the permission granted. for more information about acls, see  in the amazon simple storage service developer guide. to set permissions for an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object for which you want to set permissions. choose permissions. you can manage object access permissions for the following:  1.  access for object owner  access for other aws accounts to grant permissions to an aws user from a different aws account, under access for other aws accounts, choose add account. in the enter an id field, enter the canonical id of the aws user that you want to grant object permissions to. for information about finding a canonical id, see  in the amazon web services general reference. you can add as many as 99 users. select the check boxes for the permissions that you want to grant to the user, and then choose save. to display information about the permissions, choose the help icons. public access  warninguse caution when granting the everyone group anonymous access to your amazon s3 objects. when you grant access to this group, anyone in the world can access your object. if you need to grant access to everyone, we highly recommend that you only grant permissions to read objects.we highly recommend that you do not grant the everyone group write object permissions. doing so allows anyone to overwrite the acl permissions for the object. you can also set object permissions when you upload objects. for more information about setting permissions when uploading objects, see .  
this section describes how to use the amazon s3 console to create a folder. to create a folder sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to create a folder in. choose create folder. enter a name for the folder (for example, favorite-pics). choose the encryption setting for the folder object, and then choose save. if you don't want to use encryption with your folder, choose none.  if you want to encrypt your object using keys that are managed by amazon s3, follow these steps: choose aes-256.  for more information about using amazon s3 server-side encryption to encrypt your data, see  in the amazon simple storage service developer guide. if you want to encrypt your object using aws kms, follow these steps: choose aws-kms. choose an aws kms customer master key (cmk). the list shows  that you have created and your aws managed cmk for amazon s3. for more information about creating a customer managed aws kms cmk, see  in the aws key management service developer guide.  importantto encrypt objects in the bucket, you can use only the cmks that are enabled in the same aws region as the bucket. amazon s3 only supports symmetric cmks and does not support asymmetric cmks. for more information, see .the amazon s3 console only lists 100 kms cmks per region. if you have more than 100 cmks in the same region, you can see only the first 100 cmks in the s3 console. to use anaws kms kms cmk that is not listed in the console, choose custom kms arn, and enter the kms cmk arn. choose save. 
to upload your data (photos, videos, documents etc.) to amazon s3, you must first create an s3 bucket in one of the aws regions. you can then upload an unlimited number of data objects to the bucket. the data that you store in amazon s3 consists of objects. every object resides within a bucket that you create in a specific aws region. every object that you store in amazon s3 resides in a bucket.  objects stored in a region never leave the region unless you explicitly transfer them to another region. for example, objects stored in the europe (ireland) region never leave it. the objects stored in an aws region physically remain in that region. amazon s3 does not keep copies of objects or move them to any other region. however, you can access the objects from anywhere, as long as you have necessary permissions to do so. before you can upload an object into amazon s3, you must have write permissions to a bucket. objects can be any file type: images, backups, data, movies, etc. you can have an unlimited number of objects in a bucket. the maximum size of file you can upload by using the amazon s3 console is 160 gb. to upload a file larger than 160 gb, use the aws cli, aws sdk, or amazon s3 rest api. for more information, see  in the amazon simple storage service developer guide. the following topics explain how to use the amazon s3 console to upload, delete, and manage objects. noteif you rename an object, or change any of the properties; storage class,  enryption, metadata, a new object is created to replace the old one. if s3 versioning is enabled, a new version of the object is created, and the existing object becomes an older version. the role that changes the property also becomes the owner of the new object or (object version). topics 
this section explains how to use the console to view the properties of an object. to view the properties of an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object you want to view the properties for. choose properties. on the properties page, you can configure the following properties for the object. noteif you change the properties; storage class,  enryption, metadata, a new object is created to replace the old one. if s3 versioning is enabled, a new version of the object is created, and the existing object becomes an older version. the role that changes the property also becomes the owner of the new object or (object version). storage class – each object in amazon s3 has a storage class associated with it. the storage class that you choose to use depends on how frequently you access the object. the default storage class for s3 objects is standard. you choose which storage class to use when you upload an object. for more information about storage classes, see  in the amazon simple storage service developer guide. to change the storage class after you upload an object, choose storage class. choose the storage class that you want, and then choose save. encryption – you can encrypt your s3 objects. for more information, see .  metadata – each object in amazon s3 has a set of name-value pairs that represents its metadata. for information on adding metadata to an s3 object, see . tags – you can add tags to an s3 object. for more information, see . 
amazon s3 block public access prevents the application of any settings that allow public access to data within s3 buckets. this section describes how to edit block public access settings for all the s3 buckets in your aws account. for information about blocking public using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. to edit block public access settings for all the s3 buckets in an aws account sign in to the aws management console and open the amazon s3 console at . choose block public access (account settings). choose edit to change the block public access settings for all the buckets in your aws account. choose the settings that you want to change, and then choose save. when you're asked for confirmation, enter confirm. then choose confirm to save your changes. 
 you can use amazon s3 access points to manage access to your s3 objects. amazon s3 access points are named network endpoints that are attached to buckets that you can use to perform s3 object operations, such as uploading and retrieving objects. a bucket can have up to 1,000 access points attached, and each access point enforces distinct permissions and network controls to give you fine-grained control over access to your s3 objects.  for more information about amazon s3 access points, see  in the amazon simple storage service developer guide. the following topics explain how to use the s3 management console to create, manage, and use amazon s3 access points. topics 
you can host a static website on amazon s3. on a static website, individual webpages include static content. a static website might also contain client-side scripts. by contrast, a dynamic website relies on server-side processing, including server-side scripts such as php, jsp, or asp.net. amazon s3 does not support server-side scripting. you can use the following quick procedures to configure an s3 bucket for static website hosting in the amazon s3 console. for more information and detailed walkthroughs, see  in the amazon simple storage service developer guide. topics sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to enable static website hosting for. choose properties. choose static website hosting. choose use this bucket to host a website. in index document, enter the name of the index document, typically .  when you configure a bucket for website hosting, you must specify an index document. amazon s3 returns this index document when requests are made to the root domain or any of the subfolders. for more information, see  in the amazon simple storage service developer guide. (optional) if you want to provide your own custom error document for 4xx class errors, in error document, enter the custom error document filename.  if you do not specify a custom error document and an error occurs, amazon s3 returns a default html error document. for more information, see  in the amazon simple storage service developer guide. (optional) if you want to specify advanced redirection rules, in the edit redirection rules box, enter xml to describe the rules. for example, you can conditionally route requests according to specific object key names or prefixes in the request. for more information, see  in the amazon simple storage service developer guide. choose save. upload the index document to your bucket. for step-by-step instructions on uploading an object to an s3 bucket, see .  upload other files for your website, including optional custom error documents. in the next section, you set the permissions required to access your bucket as a static website.. by default, amazon s3 blocks public access to your account and buckets. if you want to use a bucket to host a static website, you can use these steps to edit your block public access settings.  warningbefore you complete this step, review  to ensure that you understand and accept the risks involved with allowing public access. when you turn off block public access settings to make your bucket public, anyone on the internet can access your bucket. we recommend that you block all public access to your buckets. open the amazon s3 console at . choose the name of the bucket that you have configured as a static website. choose permissions. choose edit. clear block all public access, and choose save. warningbefore you complete this step, review  to ensure you understand and accept the risks involved with allowing public access. when you turn off block public access settings to make your bucket public, anyone on the internet can access your bucket. we recommend that you block all public access to your buckets. in the confirmation box, enter confirm, and then choose confirm. under s3 buckets, the access for your bucket updates to objects can be public. you can now add a bucket policy to make the objects in the bucket publicly readable. if the access still displays as bucket and objects not public, you might have to  for your account before adding a bucket policy. after you edit s3 block public access settings, you can add a bucket policy to grant public read access to your bucket. when you grant public read access, anyone on the internet can access your bucket. importantthe following policy is an example only and allows full access to the contents of your bucket. before you proceed with this step, review  to ensure that you understand the best practices for securing the files in your s3 bucket and risks involved in granting public access. under buckets, choose the name of your bucket. choose permissions. choose bucket policy. to grant public read access for your website, copy the following bucket policy, and paste it in the bucket policy editor. update the  to include your bucket name. in the preceding example bucket policy, example.com is the bucket name. to use this bucket policy with your own bucket, you must update this name to match your bucket name. choose save. a warning appears indicating that the bucket has public access. in bucket policy, a public label appears. if you see an error that says , confirm that the bucket name in the bucket policy matches your bucket name. for information about adding a bucket policy, see  if you get an error - access denied warning and the bucket policy editor does not allow you to save the bucket policy, check your account-level and bucket-level block public access settings to confirm that you allow public access to the bucket. once you configure your bucket as a static website and set permissions, you can access your website through an amazon s3 website endpoint. for more information, see  in the amazon simple storage service developer guide. for a complete list of amazon s3 website endpoints, see  in the amazon web services general reference. 
this section describes how to configure advanced s3 bucket property settings for object replication, event notification, and transfer acceleration. topics 
there are three types of amazon cloudwatch metrics for amazon s3: storage metrics are reported once per day and are provided to all customers at no additional cost.replication metrics are available 15 minutes after enabling a replication rule with s3 replication time control (s3 rtc). for more information, see request metrics are available at 1-minute intervals after some latency to process, and the metrics are billed at the standard cloudwatch rate. for more information about cloudwatch metrics for amazon s3, see  in the amazon simple storage service developer guide. to get request metrics, you must opt into them by configuring them on the aws management console or using the amazon s3 api. to configure request metrics on a bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the objects you want request metrics for. choose the management tab, and then choose metrics. choose requests. in the left pane, choose the edit icon next to the name of the bucket. select the request metrics check box. this also enables data transfer metrics. choose save. you have now created a metrics configuration for all the objects in an amazon s3 bucket. about 15 minutes after cloudwatch begins tracking these request metrics, you can see graphs for the metrics on the amazon s3 or cloudwatch console.  you can also define a filter so that the metrics are only collected and reported on a subset of objects in the bucket. for more information, see  
to upload your data (photos, videos, documents etc.) to amazon s3, you must first create an s3 bucket in one of the aws regions. you can then upload your data objects to the bucket. every object you store in amazon s3 resides in a bucket. you can use buckets to group related objects in the same way that you use a directory to group files in a file system.  amazon s3 creates buckets in the aws region that you specify. you can choose any aws region that is geographically close to you to optimize latency, minimize costs, or address regulatory requirements. for example, if you reside in europe, you might find it advantageous to create buckets in the europe (ireland) or europe (frankfurt) regions. for a list of amazon s3 aws regions, see  in the amazon web services general reference. you are not charged for creating a bucket. you are only charged for storing objects in the bucket and for transferring objects out of the bucket. for more information about pricing, see . amazon s3 bucket names are globally unique, regardless of the aws region in which you create the bucket. you specify the name at the time you create the bucket. for bucket naming guidelines, see  in the amazon simple storage service developer guide. the following topics explain how to use the amazon s3 console to create, delete, and manage buckets. topics 
amazon s3 provides a set of tools to help you manage your s3 batch operations jobs after you create them. for more information about managing s3 batch operations, see  in the amazon simple storage service developer guide.    in the amazon simple storage service developer guide in the amazon simple storage service developer guide in the amazon simple storage service developer guide
you can change the display language of the aws management console. several languages are supported. to change the console language sign in to the aws management console and open the amazon s3 console at . scroll down until you see the bar at the bottom of the window, and then choose the language on the left side of the bar. choose the language that you want from the menu. this will change the langauge for the entire aws management console. 
amazon simple storage service (amazon s3) transfer acceleration enables fast, easy, and secure file transfers between your client and an s3 bucket over long distances. this topic describes how to enable amazon s3 transfer acceleration for a bucket. for more information, see  in the amazon simple storage service developer guide. to enable transfer acceleration for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the buckets list, choose the name of the bucket that you want to enable transfer acceleration for. choose properties. under advanced settings, choose transfer acceleration. (optional) if you want to compare accelerated and non-accelerated upload speeds, to open the , choose the want to compare your data transfer speed by region?. the speed comparison tool uses multipart upload to transfer a file from your browser to various aws regions with and without amazon s3 transfer acceleration. you can compare the upload speed for direct uploads and transfer accelerated uploads by region.  choose enabled, and choose save. endpoint displays the transfer acceleration endpoint for your bucket. you use this endpoint to access accelerated data transfers to and from your bucket. if you suspend transfer acceleration, the accelerate endpoint no longer works. 
this section describes how to enable an aws cloudtrail trail to log data events for objects in an s3 bucket by using the amazon s3 console. cloudtrail supports logging amazon s3 object-level api operations such as , , and . these events are called data events. by default, cloudtrail trails don't log data events, but you can configure trails to log data events for s3 buckets that you specify, or to log data events for all the amazon s3 buckets in your aws account. for more information, see . cloudtrail does not populate data events in the cloudtrail event history. additionally, not all bucket-level actions are populated in the cloudtrail event history. for more information, see  to configure a trail to log data events for an s3 bucket, you can use either the aws cloudtrail console or the amazon s3 console. if you are configuring a trail to log data events for all the amazon s3 buckets in your aws account, it's easier to use the cloudtrail console. for information about using the cloudtrail console to configure a trail to log s3 data events, see  in the aws cloudtrail user guide.  importantadditional charges apply for data events. for more information, see .  the following procedure shows how to use the amazon s3 console to enable a cloudtrail trail to log data events for an s3 bucket. to enable cloudtrail data events logging for objects in an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket. choose properties. choose object-level logging. choose an existing cloudtrail trail in the drop-down menu.  the trail you select must be in the same aws region as your bucket, so the drop-down list contains only trails that are in the same region as the bucket or trails that were created for all regions.  if you need to create a trail, choose the cloudtrail console link to go to the cloudtrail console. for information about how to create trails in the cloudtrail console, see  in the aws cloudtrail user guide. under events, choose one of the following: read to specify that you want cloudtrail to log amazon s3 read apis such as . write to log amazon s3 write apis such as . read and write to log both read and write object apis.for a list of supported data events that cloudtrail logs for amazon s3 objects, see  in the amazon simple storage service developer guide. choose create to enable object-level logging for the bucket. to disable object-level logging for the bucket, you must go to the cloudtrail console and remove the bucket name from the trail's data events. noteif you use the cloudtrail console or the amazon s3 console to configure a trail to log data events for an s3 bucket, the amazon s3 console shows that object-level logging is enabled for the bucket.  for information about enabling object-level logging when you create an s3 bucket, see .  in the amazon simple storage service developer guide  in the aws cloudtrail user guide
this topic explains how to view the properties for an s3 bucket. to view the properties for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to view the properties for. choose properties. on the properties page, you can configure the following properties for the bucket. versioning – versioning enables you to keep multiple versions of an object in one bucket. by default, versioning is disabled for a new bucket. for information about enabling versioning, see . server access logging – server access logging provides detailed records for the requests that are made to your bucket. by default, amazon s3 does not collect server access logs. for information about enabling server access logging, see . static website hosting – you can host a static website on amazon s3. to enable static website hosting, choose static website hosting and then specify the settings you want to use. for more information, see . object-level logging – object-level logging records object-level api activity by using cloudtrail data events. for information about enabling object-level logging, see . tags – with aws cost allocation, you can use bucket tags to annotate billing for your use of a bucket. a tag is a key-value pair that represents a label that you assign to a bucket. to add tags, choose tags, and then choose add tag. for more information, see  in the amazon simple storage service developer guide.  transfer acceleration – amazon s3 transfer acceleration enables fast, easy, and secure transfers of files over long distances between your client and an s3 bucket. for information about enabling transfer acceleration, see . events – you can enable certain amazon s3 bucket events to send a notification message to a destination whenever the events occur. to enable events, choose events and then specify the settings you want to use. for more information, see .  requester pays – you can enable requester pays so that the requester (instead of the bucket owner) pays for requests and data transfers. for more information, see  in the amazon simple storage service developer guide.  
in the amazon s3 console, you can copy an object to a bucket or a folder. for more information, see  in the amazon simple storage service developer guide. to copy an object sign in to the aws management console and open the amazon s3 console at . in the buckets list, choose your bucket name. on the overview tab, select the check box beside the object that you want to copy. choose actions, and choose copy. choose the copy destination: if you want to copy your object to a bucket, select the bucket.if you want to copy your object to a folder in a bucket, choose the bucket name, and then choose the folder.under review, confirm the copy details, and choose copy. amazon s3 copies your object to the destination. 
amazon s3 block public access prevents the application of any settings that allow public access to data within s3 buckets. this section describes how to edit block public access settings for one or more s3 buckets. for information about blocking public access using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. topics follow these steps if you need to change the public access settings for a single s3 bucket. to edit the amazon s3 block public access settings for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose permissions. choose edit to change the public access settings for the bucket. for more information about the four amazon s3 block public access settings, see  in the amazon simple storage service developer guide. choose the setting that you want to change, and then choose save. when you're asked for confirmation, enter confirm. then choose confirm to save your changes. follow these steps if you need to change the public access settings for more than one s3 bucket. to edit the amazon s3 block public access settings for multiple s3 buckets sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the buckets that you want, and then choose edit public access settings. choose the setting that you want to change, and then choose save. when you're asked for confirmation, enter confirm. then choose confirm to save your changes. you can change amazon s3 block public access settings when you create a bucket. for more information, see .  
s3 batch operations performs large-scale batch operations on amazon s3 objects. you can use s3 batch operations to copy objects, set object tags or access control lists (acls), initiate object restores from amazon s3 glacier, or invoke an aws lambda function to perform custom actions using your objects. you can perform these operations on a custom list of objects, or you can use an amazon s3 inventory report to make generating even the largest lists of objects easy. s3 batch operations use the same amazon s3 apis that you already use, so you'll find the interface familiar. for information about performing s3 batch operations using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide.  the following topics explain how to use the amazon s3 console to configure and run batch operations. topics 
this section explains how to configure amazon s3 storage management tools. topics 
before you can enable event notifications for your bucket you must set up one of the following destination types: an amazon sns topicamazon simple notification service (amazon sns) is a web service that coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. you can use the amazon sns console to create an amazon sns topic that your notifications can be sent to. the amazon sns topic must be in the same region as your amazon s3 bucket. for information about creating an amazon sns topic, see  in the amazon simple notification service developer guide.before you can use the amazon sns topic that you create as an event notification destination, you need the following:   the amazon resource name (arn) for the amazon sns topica valid amazon sns topic subscription (the topic subscribers are notified when a message is published to your amazon sns topic)a permissions policy that you set up in the amazon sns console (as shown in the following example) an amazon sqs queueyou can use the amazon sqs console to create an amazon sqs queue that your notifications can be sent to. the amazon sqs queue must be in the same region as your amazon s3 bucket. for information about creating an amazon sqs queue, see  in the amazon simple queue service developer guide.before you can use the amazon sqs queue as an event notification destination, you need the following:   the amazon resource name (arn) for the amazon sqs topica permissions policy that you set up in the amazon sqs console (as shown in the following example) a lambda functionyou can use the aws lambda console to create a lambda function. the lambda function must be in the same region as your s3 bucket. for information about creating a lambda function, see the .before you can use the lambda function as an event notification destination, you must have the name or the arn of a lambda function to set up the lambda function as a event notification destination.  warningif your notification ends up writing to the bucket that triggers the notification, this could cause an execution loop. for example, if the bucket triggers a lambda function each time an object is uploaded, and the function uploads an object to the bucket, then the function indirectly triggers itself. to avoid this, use two buckets, or configure the trigger to only apply to a prefix used for incoming objects.for more information and an example of using amazon s3 notifications with aws lambda, see  in the aws lambda developer guide.  
replication is the automatic, asynchronous copying of objects across buckets in the same or different aws regions. it replicates newly created objects and object updates from a source bucket to a specified destination bucket.  you use the amazon s3 console to add replication rules to the source bucket. replication rules define the source bucket objects to replicate and the destination bucket where the replicated objects are stored. for more information about replication, see  in the amazon simple storage service developer guide. you can manage replication rules on the replication page. you can add, view, enable, disable, delete, and change the priority of the replication rules. for information about adding replication rules to a bucket, see . to manage the replication rules for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose management, and then choose replication. you change the replication rules in the following ways. to change settings that affect all the replication rules in the bucket, choose edit global settings. you can change the destination bucket, and the iam role. if needed, you can copy the required bucket policy for cross-account destination buckets. to change a replication rule, select the rule and choose edit, which starts the replication wizard to help you make the change. for information about using the wizard, see .to enable or disable a replication rule, select the rule, choose more, and in the drop-down list, choose enable rule or disable rule. you can also disable, enable, or delete all the rules in the bucket from the more drop-down list.to change the rule priorities, choose edit priorities. you can then change the priority for each rule under the priority column heading. choose save to save your changes. you set rule priorities to avoid conflicts caused by objects that are included in the scope of more than one rule. in the case of overlapping rules, amazon s3 uses the rule priority to determine which rule to apply. the higher the number, the higher the priority. for more information about rule priority, see  in the amazon simple storage service developer guide.  in the amazon simple storage service developer guide
for the latest aws terminology, see the  in the aws general reference. 
this section explains how to use the amazon simple storage service (amazon s3) console to add a new bucket policy or edit an existing bucket policy. a bucket policy is a resource-based aws identity and access management (iam) policy. you add a bucket policy to a bucket to grant other aws accounts or iam users access permissions for the bucket and the objects in it. object permissions apply only to the objects that the bucket owner creates. for more information about bucket policies, see  in the amazon simple storage service developer guide.  for examples of amazon s3 bucket policies, see  in the amazon simple storage service developer guide.  to create or edit a bucket policy sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to create a bucket policy for or whose bucket policy you want to edit. choose permissions, and then choose bucket policy. in the bucket policy editor text box, type or copy and paste a new bucket policy, or edit an existing policy. the bucket policy is a json file. the text you type in the editor must be valid json. choose save. noteamazon s3 displays the amazon resource name (arn) for the bucket next to the bucket policy editor title. for more information about arns, see  in the amazon web services general reference.directly below the bucket policy editor text box is a link to the policy generator, which you can use to create a bucket policy. 
before you can upload data to amazon s3, you must create a bucket in one of the aws regions to store your data. after you create a bucket, you can upload an unlimited number of data objects to the bucket.  the aws account that creates the bucket owns it. by default, you can create up to 100 buckets in each of your aws accounts. if you need additional buckets, you can increase your account bucket quota to a maximum of 1,000 buckets by submitting a service quota increase. for information about how to increase your bucket quota, see  in the aws general reference.  buckets have configuration properties, including geographical region, access settings for the objects in the bucket, and other metadata.  to create a bucket sign in to the aws management console and open the amazon s3 console at . choose create bucket. the create bucket wizard opens. in bucket name, enter a dns-compliant name for your bucket. the bucket name must: be unique across all of amazon s3.be between 3 and 63 characters long.not contain uppercase characters.start with a lowercase letter or number.after you create the bucket, you can't change its name. for information about naming buckets, see  in the amazon simple storage service developer guide. importantavoid including sensitive information, such as account numbers, in the bucket name. the bucket name is visible in the urls that point to the objects in the bucket. in region, choose the aws region where you want the bucket to reside.  choose a region close to you to minimize latency and costs and address regulatory requirements. objects stored in a region never leave that region unless you explicitly transfer them to another region. for a list of amazon s3 aws regions, see  in the amazon web services general reference. in bucket settings for block public access, choose the block public access settings that you want to apply to the bucket.  we recommend that you leave all settings enabled unless you know you need to turn one or more of them off for your use case, such as to host a public website. block public access settings that you enable for the bucket will also be enabled for all access points that you create on the bucket. for more information about blocking public access, see  in the amazon simple storage service developer guide. (optional) if you want to enable s3 object lock: choose advanced settings, and read the message that appears. importantyou can only enable s3 object lock for a bucket when you create it. if you enable object lock for the bucket, you can't disable it later. enabling object lock also enables versioning for the bucket. after you enable object lock for the bucket, you must configure the object lock settings before any objects in the bucket will be protected. for more information about configuring protection for objects, see . if you want to enable object lock, enter enable in the text box and choose confirm. for more information about the s3 object lock feature, see  in the amazon simple storage service developer guide. choose create bucket. 
this section explains how to use the amazon s3 console to delete folders from an s3 bucket.  for information about amazon s3 features and pricing, see . to delete folders from an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to delete folders from. in the name list, select the check box next to the folders and objects that you want to delete, choose more, and then choose delete. in the delete objects dialog box, verify that the names of the folders you selected for deletion are listed and then choose delete. 
latest documentation update: march 27, 2019 the following table describes the important changes in each release of the amazon simple storage service console user guide from june 19, 2018, onward. for notification about updates to this documentation, you can subscribe to an rss feed. the following table describes the important changes in each release of the amazon simple storage service console user guide before june 19, 2018.  
amazon s3 block public access prevents the application of any settings that allow public access to data within s3 buckets. you can configure block public access settings for an individual s3 bucket or for all the buckets in your account. for information about blocking public access using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. the following topics explain how to use the amazon s3 console to configure block public access settings:  the following sections explain viewing bucket access status and searching by access types. the list buckets view shows whether your bucket is publicly accessible. amazon s3 labels the permissions for a bucket as follows: public – everyone has access to one or more of the following: list objects, write objects, read and write permissions. objects can be public – the bucket is not public, but anyone with the appropriate permissions can grant public access to objects. buckets and objects not public – the bucket and objects do not have any public access.only authorized users of this account – access is isolated to iam users and roles in this account and aws service principals because there is a policy that grants public access.the access column shows the access status of the listed buckets.  you can also filter bucket searches by access type. choose an access type from the drop-down list that is next to the search for buckets bar.   
this section explains how to create an amazon s3 access point using the aws management console. for information about creating access points using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. an access point is associated with exactly one amazon s3 bucket. before you begin, make sure that you have created a bucket that you want to use with this access point. for more information about creating buckets, see . to create an access point sign in to the aws management console and open the amazon s3 console at . in the s3 buckets section, select the bucket that you want to attach this access point to. on the bucket detail page, choose the access points tab. choose create access point. enter your desired name for the access point in the access point name field. choose a network access type. if you choose virtual private cloud (vpc), enter the vpc id that you want to use with the access point. for more information about network access type for access points, see  in the amazon simple storage service developer guide. select the block public access settings that you want to apply to the access point. all block public access settings are enabled by default for new access points, and we recommend that you leave all settings enabled unless you know you have a specific need to disable any of them. amazon s3 currently doesn't support changing an access point's block public access settings after the access point has been created. for more information about using amazon s3 block public access with access points, see  in the amazon simple storage service developer guide. (optional) specify the access point policy. the console automatically displays the amazon resource name (arn) for the access point, which you can use in the policy. choose create access point. 
this section explains how to use the amazon s3 console to delete objects. because all objects in your s3 bucket incur storage costs, you should delete objects that you no longer need. if you are collecting log files, for example, it's a good idea to delete them when they're no longer needed. you can set up a lifecycle rule to automatically delete objects such as log files. for information about amazon s3 features and pricing, see . to delete objects from an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to delete an object from. you can delete objects from an s3 bucket in any of the following ways: in the name list, select the check box next to the objects and folders that you want to delete, choose actions, and then choose delete from the drop-down menu. in the delete objects dialog box, verify that the name(s) of the object(s) and/or folder(s) you selected for deletion are listed and then choose delete. or, choose the name of the object that you want to delete, choose latest version, and then choose the trash can icon.
by using the amazon s3 analytics storage class analysis tool, you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. storage class analysis observes data access patterns to help you determine when to transition less frequently accessed standard storage to the standard_ia (ia, for infrequent access) storage class. for more information about standard_ia, see the  and  in the amazon simple storage service developer guide. importantstorage class analysis does not give recommendations for transitions to the onezone_ia or s3 glacier storage classes. for more information about analytics, see  in the amazon simple storage service developer guide. to configure storage class analysis sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket for which you want to configure storage class analysis. choose the management tab, and then choose analytics. choose add. type a name for the filter. if you want to analyze the whole bucket, leave the prefix / tags field empty. in the prefix / tags  field, type text for the prefix or tag for the objects that you want to analyze, or choose from the dropdown list that appears when you start typing. if you chose tag, enter a value for the tag. you can enter one prefix and multiple tags. optionally, you can choose export data to export analysis reports to a comma-separated values (.csv) flat file. choose a destination bucket where the file can be stored. you can type a prefix for the destination bucket. the destination bucket must be in the same aws region as the bucket for which you are setting up the analysis. the destination bucket can be in a different aws account. choose save. amazon s3 creates a bucket policy on the destination bucket that grants amazon s3 write permission. this allow it to write the export data to the bucket.  if an error occurs when you try to create the bucket policy, you'll be given instructions on how to fix it. for example, if you chose a destination bucket in another aws account and do not have permissions to read and write to the bucket policy, you'll see the following message. you must have the destination bucket owner add the displayed bucket policy to the destination bucket. if the policy is not added to the destination bucket you won’t get the export data because amazon s3 doesn’t have permission to write to the destination bucket. if the source bucket is owned by a different account than that of the current user, then the correct account id of the source bucket must be substituted in the policy.  for information about the exported data and how the filter works, see  in the amazon simple storage service developer guide. more info  
there are three types of amazon cloudwatch metrics for amazon s3: storage metrics, request metrics, and replication metrics. replication metrics are available 15 minutes after a replication rule with s3 replication time control (s3 rtc) has been enabled. replication metrics are billed at the standard amazon cloudwatch rate. they are turned on automatically when you enable replication with s3 rtc using the aws management console or the amazon s3 api. replication metrics track the rule ids of the replication configuration. a replication rule id can be specific to a prefix, a tag, or a combination of both. for more information about s3 replication time control (s3 rtc), see  in the amazon simple storage service developer guide. for more information about cloudwatch metrics for amazon s3, see  in the amazon simple storage service developer guide. to view replication metrics sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the objects you want replication metrics for. choose the management tab, and then choose metrics. choose replication.   in the rule ids list in the left pane, select the rule ids that you want. if you have several rule ids to choose from, you can search for the ids that you want. after choosing the rule ids that you want, choose display graphs below the rule ids selection box. you can then view the replication metrics replication latency (in seconds), operations pending replication, and bytes pending replication for the rules that you selected. amazon cloudwatch begins reporting replication metrics 15 minutes after you enable s3 rtc on the respective replication rule. you can view replication metrics on the amazon s3 or cloudwatch console. for information, see  in the amazon simple storage service developer guide. 
amazon s3 has a flat structure instead of a hierarchy like you would typically see in a file system. however, for the sake of organizational simplicity, the amazon s3 console supports a folder concept as a way to group objects. in amazon s3, the folder is a naming prefix for an object or group of objects. for more information, see  we recommend blocking all public access to your amazon s3 folders and buckets unless you specifically require a public folder or bucket. when you make a folder public, anyone on the internet can view all the objects that are grouped in that folder. in the amazon s3 console, you can make a folder public. you can also make a folder public by creating a bucket policy that limits access by prefix. for more information, see .  warningafter you make a folder public in the amazon s3 console, you can't make it private again. instead, you must set permissions on each individual object in the public folder so that the objects have no public access. for more information, see  
access analyzer for s3 alerts you to s3 buckets that are configured to allow access to anyone on the internet or other aws accounts, including aws accounts outside of your organization. for each public or shared bucket, you receive findings into the source and level of public or shared access. for example, access analyzer for s3 might show that a bucket has read or write access provided through a bucket access control list (acl), a bucket policy, or an access point policy. armed with this knowledge, you can take immediate and precise corrective action to restore your bucket access to what you intended.  when reviewing an at-risk bucket in access analyzer for s3, you can block all public access to the bucket with a single click. we recommend that you block all access to your buckets unless you require public access to support a specific use case. before you block all public access, ensure that your applications will continue to work correctly without public access. for more information, see  in the amazon simple storage service developer guide. you can also drill down into bucket-level permission settings to configure granular levels of access. for specific and verified use cases that require public access, such as static website hosting, public downloads, or cross-account sharing, you can acknowledge and record your intent for the bucket to remain public or shared by archiving the findings for the bucket. you can revisit and modify these bucket configurations at any time. you can also download your findings as a csv report for auditing purposes. access analyzer for s3 is available at no extra cost on the amazon s3 console. access analyzer for s3 is powered by aws identity and access management (iam) access analyzer. to use access analyzer for s3 in the amazon s3 console, you must visit the iam console and enable iam access analyzer on a per-region basis.  for more information about iam access analyzer, see  in the iam user guide. for more information about access analyzer for s3, review the following sections. importantaccess analyzer for s3 requires an account-level analyzer. to use access analyzer for s3, you must visit iam access analyzer and create an analyzer that has an account as the zone of trust. for more information, see  in iam user guide. when a bucket policy or bucket acl is added or modified, access analyzer generates and updates findings based on the change within 30 minutes. findings related to account level block public access settings may not be generated or updated for up to 6 hours after you change the settings. topics access analyzer for s3 provides findings for buckets that can be accessed outside your aws account. buckets that are listed under buckets with public access can be accessed by anyone on the internet. if access analyzer for s3 identifies public buckets, you also see a warning at the top of the page that shows you the number of public buckets in your region. buckets listed under buckets with access from other aws accounts — including third-party aws accounts are shared conditionally with other aws accounts, including accounts outside of your organization.  for each bucket, access analyzer for s3 provides the following information: bucket namediscovered by access analyzer ‐ when access analyzer for s3 discovered the public or shared bucket access.shared through ‐ how the bucket is shared—through a bucket policy, a bucket acl, or an access point policy. a bucket can be shared through both policies and acls. if you want to find and review the source for your bucket access, you can use the information in this column as a starting point for taking immediate and precise corrective action. status ‐ the status of the bucket finding. access analyzer for s3 displays findings for all public and shared buckets. active ‐ finding has not been reviewed. archived ‐ finding has been reviewed and confirmed as intended. all ‐ all findings for buckets that are public or shared with other aws accounts, including aws accounts outside of your organization.access level ‐ access permissions granted for the bucket:list ‐ list resources.read ‐ read but not edit resource contents and attributes.write ‐ create, delete, or modify resources.permissions ‐ grant or modify resource permissions.tagging ‐ update tags associated with the resource.to use access analyzer for s3, you must complete the following prerequisite steps. grant the required permissions. for more information, see  in the iam user guide. visit iam to create an account-level analyzer for each region where you want to use access analyzer. access analyzer for s3 requires an account-level analyzer. to use access analyzer for s3, you must create an analyzer that has an account as the zone of trust. for more information, see  in iam user guide. if you want to block all access to a bucket in a single click, you can use the block all public access button in access analyzer for s3. when you block all public access to a bucket, no public access is granted. we recommend that you block all public access to your buckets unless you require public access to support a specific and verified use case. before you block all public access, ensure that your applications will continue to work correctly without public access. if you don't want to block all public access to your bucket, you can edit your block public access settings on the amazon s3 console to configure granular levels of access to your buckets. for more information, see  in the amazon simple storage service developer guide. in rare events, access analyzer for s3 might report no findings for a bucket that an amazon s3 block public access evaluation reports as public. this happens because amazon s3 block public access reviews policies for current actions and any potential actions that might be added in the future, leading to a bucket becoming public. on the other hand, access analyzer for s3 only analyzes the current actions specified for the amazon s3 service in the evaluation of access status. to block all public access to a bucket using access analyzer for s3 sign in to the aws management console and open the amazon s3 console at . in the navigation pane on the left, under dashboards, choose access analyzer for s3. in access analyzer for s3, choose a bucket. choose block all public access. to confirm your intent to block all public access to the bucket, in block all public access (bucket settings), enter confirm. amazon s3 blocks all public access to your bucket. the status of the bucket finding updates to resolved, and the bucket disappears from the access analyzer for s3 listing. if you want to review resolved buckets, open iam access analyzer on the iam console. if you did not intend to grant access to the public or other aws accounts, including accounts outside of your organization, you can modify the bucket acl, bucket policy, or access point policy to remove the access to the bucket. the shared through column shows all sources of bucket access: bucket policy, bucket acl, and/or access point policy. to review and change a bucket policy, a bucket acl, or an access point policy open the amazon s3 console at . in the navigation pane, choose access analyzer for s3. to see whether public access or shared access is granted through a bucket policy, a bucket acl, or an access point policy, look in the shared through column. under bucket name, choose the name of the bucket with the bucket policy, bucket acl, or access point policy that you want to change or review. if you want to change or view a bucket acl: choose permissions. choose access control list. review your bucket acl, and make changes as required. for more information, see  if you want to change or review a bucket policy: choose permissions. choose bucket policy. review or change your bucket policy as required. for more information, see  if you want to review or change an access point policy: choose access points. choose the access point name. review or change access as required.  for more information, see . if you edit or remove a bucket acl, a bucket policy, or an access point policy to remove public or shared access, the status for the bucket findings updates to resolved. the resolved bucket findings disappear from the access analyzer for s3 listing, but you can view them in iam access analyzer. if a bucket grants access to the public or other aws accounts, including accounts outside of your organization, to support a specific use case (for example, a static website, public downloads, or cross-account sharing), you can archive the finding for the bucket. when you archive bucket findings, you acknowledge and record your intent for the bucket to remain public or shared. archived bucket findings remain in your access analyzer for s3 listing so that you always know which buckets are public or shared. to archive bucket findings in access analyzer for s3 open the amazon s3 console at . in the navigation pane, choose access analyzer for s3. in access analyzer for s3, choose an active bucket. to acknowledge your intent for this bucket to be accessed by the public or other aws accounts, including accounts outside of your organization, choose archive. enter confirm, and choose archive. after you archive findings, you can always revisit them and change their status back to active, indicating that the bucket requires another review.  to activate an archived bucket finding in access analyzer for s3 open the amazon s3 console at . in the navigation pane, choose access analyzer for s3. choose the archived bucket findings. choose mark as active. if you need to see more information about a bucket, you can open the bucket finding details in iam access analyzer on the iam console. to view finding details in access analyzer for s3 open the amazon s3 console at . in the navigation pane, choose access analyzer for s3. in access analyzer for s3, choose a bucket. choose view details. the finding details open in iam access analyzer on the iam console.  you can download your bucket findings as a csv report that you can use for auditing purposes. the report includes the same information that you see in access analyzer for s3 on the amazon s3 console. to download a report open the amazon s3 console at . in the navigation pane on the left, choose access analyzer for s3. in the region filter, choose the region. access analyzer for s3 updates to shows buckets for the chosen region. choose download report. a csv report is generated and saved to your computer. 
there are three types of amazon cloudwatch metrics for amazon s3: storage metrics, request metrics and replication. storage metrics are reported once per day and are provided to all customers at no additional cost. request metrics are available at 1 minute intervals after some latency to process, and metrics are billed at the standard cloudwatch rate. to get request metrics, you must opt into them by configuring them in the console or with the amazon s3 api. for more conceptual information about cloudwatch metrics for amazon s3, see  in the amazon simple storage service developer guide. to filter request metrics on a subset of objects in a bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that has the objects you want to get request metrics for. choose the management tab. and then choose metrics. choose requests. from filters in the left-side pane, choose add. provide a name for this metrics configuration. provide one or more prefixes or tags, separated by commas, in prefix /tags that you want to monitor. from the drop down, select whether the value you provided is a tag or a prefix. choose save. you have now created a metrics configuration for request metrics on a subset of the objects in an amazon s3 bucket. about 15 minutes after cloudwatch begins tracking these request metrics, you can see graphs for the metrics in both the amazon s3 or cloudwatch consoles. you can also request metrics at the bucket level. for information, see  
in amazon s3, buckets and objects are the primary resources, and objects are stored in buckets. amazon s3 has a flat structure instead of a hierarchy like you would see in a file system. however, for the sake of organizational simplicity, the amazon s3 console supports the folder concept as a means of grouping objects. amazon s3 does this by using a shared name prefix for objects (that is, objects have names that begin with a common string). object names are also referred to as key names. for example, you can create a folder on the console named  and store an object named  in it. the object is then stored with the key name , where  is the prefix. here are two more examples:  if you have three objects in your bucket—, , and —the console will show a folder named . if you open the folder in the console, you will see three objects: , , and .if you have an object named , the console will show you a folder named  containing the folder  and the object .topics you can have folders within folders, but not buckets within buckets. you can upload and copy objects directly into a folder. folders can be created, deleted, and made public, but they cannot be renamed. objects can be copied from one folder to another.  importantthe amazon s3 console treats all objects that have a forward slash ("/") character as the last (trailing) character in the key name as a folder, for example . you can't upload an object that has a key name with a trailing "/" character using the amazon s3 console. however, you can upload objects that are named with a trailing "/" with the amazon s3 api by using the aws cli, aws sdks, or rest api.an object that is named with a trailing "/" appears as a folder in the amazon s3 console. the amazon s3 console does not display the content and metadata for such an object. when you use the console to copy an object named with a trailing "/", a new folder is created in the destination location, but the object's data and metadata are not copied.  
you can enable certain amazon s3 bucket events to send a notification message to a destination whenever the events occur. this section explains how to use the amazon s3 console to enable event notifications. for information about using event notifications with the aws sdks and the amazon s3 rest apis, see  in the amazon simple storage service developer guide.  topics when you configure event notifications for a bucket, you must specify the type of events for which you want to receive notifications. for a complete list of event types, see  section in the amazon simple storage service developer guide.  in the amazon s3 console, you have the following options for configuring event notifications. you can choose a single option or multiple options. object creationobjectcreated (all) – receive a notification when an object is created in your bucket for any of the following object creation actions: put, post, copy, and multipart upload completed.put, post, copy, and multipart upload completed – receive a notification for one of these specific object creation actions.object deletion objectdelete (all) – receive a notification any time an object in your bucket is deleted.delete marker created – receive a notification when a delete marker is created for a versioned object.  for information about deleting versioned objects, see . for information about object versioning, see  and . object restoration from the s3 glacier storage class restore initiated – receive a notification for initiation of object restoration.restore completed – receive a notification for completion of object restoration.reduced redundancy storage (rrs) object lost eventsobject in rss lost – receive a notification that an object of the rrs storage class has been lostobjects eligible for replication using amazon s3 replication time controlreplication time missed threshold – receive a notification that an object failed to replicate.replication time completed after threshold – receive a notification that an object exceeded the 15-minute threshold for replication.replication time not tracked – receive a notification that an object replicated after the 15-minute threshold.replication time failed – receive a notification that an object that was eligible for replication is no longer being tracked by replication metrics.notewhen you delete the last object from a folder, amazon s3 can generate an object creation event. when there are multiple objects with the same prefix with a trailing slash (/) as part of their names, those objects are shown as being part of a folder in the amazon s3 console. the name of the folder is formed from the characters preceding the trailing slash (/).when you delete all the objects listed under that folder, no actual object is available to represent the empty folder. under such circumstances, the amazon s3 console creates a zero-byte object to represent that folder. if you enabled event notification for the creation of objects, the zero-byte object creation action that is taken by the console triggers an object creation event.the amazon s3 console displays a folder under the following circumstances:when a zero-byte object has a trailing slash (/) in its name. in this case, there is an actual amazon s3 object of 0 bytes that represents a folder. if the object has a slash (/) within its name. in this case, there isn't an actual object representing the folder. when you configure event notifications for a bucket, you also choose the notification destination. event notification messages can be sent to the following destinations: amazon simple notification service (amazon sns) topic – coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. for information about the amazon sns topic format, see .amazon simple queue service (amazon sqs) queue – offers reliable and scalable hosted queues for storing messages as they travel between computers. for information about amazon sqs, see  in the amazon simple queue service developer guide.aws lambda  – invoke a lambda function and provide the event message as an argument. when you create a lambda function, you package up and upload your custom code to aws lambda. aws lambda uses the aws infrastructure to run the code on your behalf. for information about using lambda with amazon s3, see  in the aws lambda developer guide.for more information about granting the amazon s3 service principal the permissions required to publish event notifications to a destination, see  in the amazon s3 developer guide. warningif your notification ends up writing to the bucket that triggers the notification, this could cause an execution loop. for example, if the bucket triggers a lambda function each time an object is uploaded, and the function uploads an object to the bucket, then the function indirectly triggers itself. to avoid this, use two buckets, or configure the trigger to only apply to a prefix used for incoming objects.for more information and an example of using amazon s3 notifications with aws lambda, see  in the aws lambda developer guide.  before you can enable event notifications for your bucket, you must set up one of the destination types. for more information, see  to enable and configure event notifications for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to enable events for. choose properties. under advanced settings, choose events. choose add notification. in name, enter a descriptive name for your event notification.  if you don't enter a name, a guid is generated and used for the name. under events, select one or more events.  for a listing of the event types, see . to filter event notifications by a prefix or suffix, enter a prefix or a suffix.  for example, you can set up a prefix filter so that you receive notifications only when files are added to a specific folder (for example, ). for more information, see . choose the event notification destination: sns topic, sqs queue, or lambda function.  for a description of the destinations, see . when you choose your send to destination, a box appears for you to enter your specific sns, sqs, or lambda function destination. in the example image below, the send to location is sns topic, and you can see a sns box for the sns topic name. in the box that appears, choose or enter the destination sns, sqs, or lambda function.  you can choose or enter the sns, sqs, or lambda function name, or you can choose to the destination amazon resource name (arn). the example screenshot below shows the add sns topic arn option. if you chose add arn, enter the sns topic, sqs queue, or lambda function arn. choose save.  amazon s3 sends a test message to the event notification destination. 
you can empty a bucket, which deletes all of the objects in the bucket without deleting the bucket. when you empty a bucket with versioning enabled, all versions of all the objects in the bucket are deleted. for more information, see  and  in the amazon simple storage service developer guide. to empty an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, select the option next to the name of the bucket that you want to empty and then choose empty. on the empty bucket page, confirm that you want to empty the bucket by entering the bucket name into the text field, and then choose empty. (optional) monitor the progress of the bucket emptying process on the empty bucket: status page. 
this topic explains how to use the aws management console to upload one or more files or entire folders to an amazon s3 bucket. before you can upload files and folders to an amazon s3 bucket, you need write permissions for the bucket. for more information about access permissions, see . for information about uploading files programmatically, see  in the amazon simple storage service developer guide.  when you upload a file to amazon s3, it is stored as an s3 object. objects consist of the file data and metadata that describes the object. you can have an unlimited number of objects in a bucket. you can upload any file type—images, backups, data, movies, etc.—into an s3 bucket. the maximum size of a file that you can upload by using the amazon s3 console is 160 gb. to upload a file larger than 160 gb, use the aws cli, aws sdk, or amazon s3 rest api. for more information, see  in the amazon simple storage service developer guide.  you can upload files by dragging and dropping or by pointing and clicking. to upload folders, you must drag and drop them. drag and drop functionality is supported only for the chrome and firefox browsers. for information about which chrome and firefox browser versions are supported, see .  when you upload a folder, amazon s3 uploads all of the files and subfolders from the specified folder to your bucket. it then assigns an object key name that is a combination of the uploaded file name and the folder name. for example, if you upload a folder called  that contains two files,  and , amazon s3 uploads the files and then assigns the corresponding key names,  and . the key names include the folder name as a prefix. the amazon s3 console displays only the part of the key name that follows the last “/”. for example, within an images folder the  and  objects are displayed as  and a . if you upload individual files and you have a folder open in the amazon s3 console, when amazon s3 uploads the files, it includes the name of the open folder as the prefix of the key names. for example, if you have a folder named  open in the amazon s3 console and you upload a file named , the key name is . however, the object is displayed in the console as  in the  folder. if you upload individual files and you do not have a folder open in the amazon s3 console, when amazon s3 uploads the files, it assigns only the file name as the key name. for example, if you upload a file named , the key name is . for more information on key names, see  in the amazon simple storage service developer guide. if you upload an object with a key name that already exists in a versioning-enabled bucket, amazon s3 creates another version of the object instead of replacing the existing object. for more information about versioning, see . topics if you are using the chrome or firefox browsers, you can choose the folders and files to upload, and then drag and drop them into the destination bucket. dragging and dropping is the only way that you can upload folders. to upload folders and files to an s3 bucket by using drag and drop sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to upload your folders or files to. in a window other than the console window, select the files and folders that you want to upload. then drag and drop your selections into the console window that lists the objects in the destination bucket. the files you chose are listed in the upload dialog box. in the upload dialog box, do one of the following:  drag and drop more files and folders to the console window that displays the upload dialog box. to add more files, you can also choose add more files. this option works only for files, not folders. to immediately upload the listed files and folders without granting or removing permissions for specific users or setting public permissions for all of the files that you're uploading, choose upload. for information about object access permissions, see .  to set permissions or properties for the files that you are uploading, choose next. on the set permissions page, under manage users you can change the permissions for the aws account owner. the owner refers to the aws account root user, and not an aws identity and access management (iam) user. for more information about the root user, see .  choose add account to grant access to another aws account. for more information about granting permissions to another aws account, see . under manage public permissions you can grant read access to your objects to the general public (everyone in the world), for all of the files that you're uploading. granting public read access is applicable to a small subset of use cases such as when buckets are used for websites. we recommend that you do not change the default setting of do not grant public read access to this object(s). you can always make changes to object permissions after you upload the object. for information about object access permissions, see .  when you're done configuring permissions, choose next. on the set properties page, choose the storage class and encryption method to use for the files that you are uploading. you can also add or modify metadata.  choose a storage class for the files you're uploading. for more information about storage classes, see  in the amazon simple storage service developer guide. choose the type of encryption for the files that you're uploading. if you don't want to encrypt them, choose none.  to encrypt the uploaded files using keys that are managed by amazon s3, choose amazon s3 master-key. for more information, see  in the amazon simple storage service developer guide. to encrypt the uploaded files using the aws key management service (aws kms), choose aws kms master-key. then choose a customer master key (cmk) from the list of aws kms cmks. noteto encrypt objects in a bucket, you can use only cmks that are available in the same aws region as the bucket.  you can give an external account the ability to use an object that is protected by an aws kms cmk. to do this, select custom kms arn from the list and enter the amazon resource name (arn) for the external account. administrators of an external account that have usage permissions to an object protected by your aws kms cmk can further restrict access by creating a resource-level iam policy.  for more information about creating an aws kms cmk, see  in the aws key management service developer guide. for more information about protecting data with aws kms, see  in the amazon simple storage service developer guide. metadata for amazon s3 objects is represented by a name-value (key-value) pair. there are two kinds of metadata: system-defined metadata and user-defined metadata. if you want to add amazon s3 system-defined metadata to all of the objects you are uploading, for header, select a header. you can select common http headers, such as content-type and content-disposition. type a value for the header, and then choose save. for a list of system-defined metadata and information about whether you can add the value, see  in the amazon simple storage service developer guide. any metadata starting with prefix  is treated as user-defined metadata. user-defined metadata is stored with the object, and is returned when you download the object.  to add user-defined metadata to all of the objects that you are uploading, type  plus a custom metadata name in the header field. type a value for the header, and then choose save. both the keys and their values must conform to us-ascii standards. user-defined metadata can be as large as 2 kb. for more information about user-defined metadata, see  in the amazon simple storage service developer guide. object tagging gives you a way to categorize storage. each tag is a key-value pair. key and tag values are case sensitive. you can have up to 10 tags per object.  to add tags to all of the objects that you are uploading, type a tag name in the key field. type a value for the tag, and then choose save. a tag key can be up to 128 unicode characters in length and tag values can be up to 255 unicode characters in length. for more information about object tags, see  in the amazon simple storage service developer guide. choose next. on the upload review page, verify that your settings are correct, and then choose upload. to make changes, choose previous. to see the progress of the upload, choose in progress at the bottom of the browser window. to see a history of your uploads and other operations, choose success. this procedure explains how to upload files into an s3 bucket by choosing upload. to upload files to an s3 bucket by pointing and clicking sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to upload your files to. choose upload. in the upload dialog box, choose add files. choose one or more files to upload, and then choose open. after you see the files that you chose listed in the upload dialog box, do one of the following:  to add more files, choose add more files. to immediately upload the listed files, choose upload.  to set permissions or properties for the files that you are uploading, choose next. to set permissions and properties, start with step 5 of . .
you can delete an empty bucket, and when you're using the aws management console, you can delete a bucket that contains objects. if you delete a bucket that contains objects, all the objects in the bucket are permanently deleted.  when you delete a bucket with versioning enabled, all versions of all the objects in the bucket are permanently deleted. for more information about versioning, see  in the amazon simple storage service developer guide. before deleting a bucket, consider the following: bucket names are unique. if you delete a bucket, another aws user can use the name. when you delete a bucket that contains objects, all the objects in the bucket are permanently deleted, including objects that transitioned to the  storage class.if the bucket hosts a static website, and you created and configured an amazon route 53 hosted zone as described in : you must clean up the route 53 hosted zone settings that are related to the bucket as described in .if the bucket receives log data from elastic load balancing (elb): we recommend that you stop the delivery of elb logs to the bucket before deleting it. after you delete the bucket, if another user creates a bucket using the same name, your log data could potentially be delivered to that bucket. for information about elb access logs, see  in the user guide for classic load balancers and  in the user guide for application load balancers.importantif you want to continue to use the same bucket name, don't delete the bucket. we recommend that you empty the bucket and keep it. after a bucket is deleted, the name becomes available to reuse, but the name might not be available for you to reuse for various reasons. for example, it might take some time before the name can be reused, and some other account could create a bucket with that name before you do. to delete an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the buckets list, select the option next to the name of the bucket that you want to delete and then choose delete at the top of the page. on the delete bucket page, confirm that you want to delete the bucket by entering the bucket name into the text field, and then choose delete bucket. noteif the bucket contains any objects, empty the bucket before deleting it by selecting the empty bucket configuration link in the this bucket is not empty error alert and following the instructions on the empty bucket page. then return to the delete bucket page and delete the bucket. 
replication is the automatic, asynchronous copying of objects across buckets in the same or different aws regions. replication copies newly created objects and object updates from a source bucket to a destination bucket. for more information about replication concepts and how to use replication with the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide.  replication requires versioning to be enabled on both the source and destination buckets. to review the full list of requirements, see  in the amazon simple storage service developer guide. for more information about versioning, see  the object replicas in the destination bucket are exact replicas of the objects in the source bucket. they have the same key names and the same metadata—for example, creation time, owner, user-defined metadata, version id, access control list (acl), and storage class. optionally, you can explicitly specify a different storage class for object replicas. and regardless of who owns the source bucket or the source object, you can choose to change replica ownership to the aws account that owns the destination bucket. for more information, see  in the amazon simple storage service developer guide.  you can use s3 replication time control (s3 rtc) to replicate your data in the same aws region or across different aws regions in a predictable timeframe. s3 rtc replicates 99.99 percent of new objects stored in amazon s3 within 15 minutes and most objects within seconds. for more information, see  in the amazon simple storage service developer guide. note about replication and lifecycle rulesmetadata for an object remains identical between original objects and replica objects. lifecycle rules abide by the creation time of the original object, and not by when the replicated object becomes available in the destination bucket. however, lifecycle does not act on objects that are pending replication until replication is complete. you use the amazon s3 console to add replication rules to the source bucket. replication rules define which source bucket objects to replicate and the destination bucket where the replicated objects are stored. you can create a rule to replicate all the objects in a bucket or a subset of objects with a specific key name prefix, one or more object tags, or both. a destination bucket can be in the same aws account as the source bucket, or it can be in a different account. if you specify an object version id to delete, amazon s3 deletes that object version in the source bucket. but it doesn't replicate the deletion in the destination bucket. in other words, it doesn't delete the same object version from the destination bucket. this protects data from malicious deletions. if the destination bucket is in a different account from the source bucket, you must add a bucket policy to the destination bucket to grant the owner of the source bucket account permission to replicate objects in the destination bucket. the amazon s3 console builds this required bucket policy for you to copy and add to the destination bucket in the other account.  when you add a replication rule to a bucket, the rule is enabled by default, so it starts working as soon as you save it.  topics follow these steps to configure a replication rule when the destination bucket is in the same aws account as the source bucket. sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose management, choose replication, and then choose add rule. in the replication rule wizard, under set source, you have the following options for setting the replication source: to replicate the whole bucket, choose entire bucket bucket-name. to replicate all objects that have the same prefix (for example, all objects that have names that begin with the string ), choose prefix or tags. enter a prefix in the box, choose the prefix from the drop-down list, and then press enter. if you enter a prefix that is the name of a folder, you must use / (forward slash) as the last character (for example, ). for more information about prefixes, see  in the amazon simple storage service developer guide.to replicate all objects with one or more object tags, enter a tag in the box, choose the tag from the drop-down list, and then press enter. enter a tag value and then press enter. repeat the procedure to add another tag. you can combine a prefix and tags. for more information about object tags, see  in the amazon simple storage service developer guide.the new schema supports prefix and tag filtering and the prioritization of rules. for more information about the new schema, see  in the amazon simple storage service developer guide. the developer guide describes the xml used with the amazon s3 api that works behind the user interface. in the developer guide, the new schema is described as replication configuration xml v2. to replicate objects in the source bucket that are encrypted with aws key management service (aws kms), under replication criteria, select replicate objects encrypted with aws kms. under choose one or more keys for decrypting source objects are the source aws kms customer master keys (cmks) that you allow replication to use. all source cmks are included by default. you can choose to narrow the cmk selection.  objects encrypted by aws kms cmks that you do not select are not replicated. a cmk or a group of cmks is chosen for you, but you can choose the cmks if you want. for information about using aws kms with replication, see  in the amazon simple storage service developer guide. importantwhen you replicate objects that are encrypted with aws kms, the aws kms request rate doubles in the source region and increases in the destination region by the same amount. these increased call rates to aws kms are due to the way that data is re-encrypted using the customer master key (cmk) that you define for the replication destination region. aws kms has a request rate limit that is per calling account per region. for information about the limit defaults, see  in the aws key management service developer guide.if your current amazon s3 put object request rate during replication is more than half the default aws kms rate limit for your account, we recommend that you request an increase to your aws kms request rate limit. to request an increase, create a case in the aws support center at . for example, suppose that your current put object request rate is 1,000 requests per second and you use aws kms to encrypt your objects. in this case, we recommend that you ask aws support to increase your aws kms rate limit to 2,500 requests per second, in both your source and destination regions (if different), to ensure that there is no throttling by aws kms.to see your put object request rate in the source bucket, view  in the amazon cloudwatch request metrics for amazon s3. for information about viewing cloudwatch metrics, see  choose next. to choose a destination bucket from the account that you're currently using, on the set destination page, under destination bucket, choose buckets in this account. enter the name of the destination bucket for the replication, or choose a name in the drop-down list. if you want to choose a destination bucket from a different aws account, see . if versioning is not enabled on the destination bucket, you get a warning message that contains an enable versioning button. choose this button to enable versioning on the bucket. if you chose to replicate objects encrypted with aws kms, under destination encryption settings, enter the amazon resource name (arn) of the aws kms cmk to use to encrypt the replicas in the destination bucket. you can find the arn for your aws kms cmk in the iam console, under encryption keys.  or, you can choose a cmk name from the drop-down list. for more information about creating an aws kms cmk, see  in the aws key management service developer guide. importantthe amazon s3 console lists only 100 aws kms cmks per aws region. if you have more than 100 cmks in the same region, you can see only the first 100 cmks in the s3 console. to use a kms cmk that is not listed in the console, choose custom kms arn, and enter the kms cmk arn. if you want to replicate your data into a specific storage class in the destination bucket, on the set destination page, under destination options, select change the storage class for the replicated object(s). then choose the storage class that you want to use for the replicated objects in the destination bucket. if you don't select this option, the storage class for replicated objects is the same class as the original objects.  similarly, if you want to change object ownership in the destination bucket, choose change object ownership to the destination bucket owner. for more information about this option, see .  if you want to enable s3 replication time control (s3 rtc) in your replication configuration, select replication time control. notewhen you use s3 rtc, additional per-gb data transfer fees and cloudwatch metrics fees apply. choose next. set up an aws identity and access management (iam) role that amazon s3 can assume to replicate objects on your behalf. to set up an iam role, on the configure options page, under select role, do one of the following: we highly recommend that you choose create new role to have amazon s3 create a new iam role for you. when you save the rule, a new policy is generated for the iam role that matches the source and destination buckets that you choose. the name of the generated role is based on the bucket names and uses the following naming convention: replication_role_for_source-bucket_to_destination-bucket.you can choose to use an existing iam role. if you do, you must choose a role that grants amazon s3 the necessary permissions for replication. replication fails if this role does not grant amazon s3 sufficient permissions to follow your replication rule.  importantwhen you add a replication rule to a bucket, you must have the  permission to be able to pass the iam role that grants amazon s3 replication permissions. for more information, see  in the iam user guide.under rule name, enter a name for your rule to help identify the rule later. the name is required and must be unique within the bucket. if the bucket has existing replication rules, you are instructed to set a priority for the rule. you must set a priority for the rule to avoid conflicts caused by objects that are included in the scope of more than one rule. in the case of overlapping rules, amazon s3 uses the rule priority to determine which rule to apply. the higher the number, the higher the priority. for more information about rule priority, see  in the amazon simple storage service developer guide. under status, enabled is selected by default. an enabled rule starts to work as soon as you save it. if you want to enable the rule later, select disabled. choose next. on the review page, review your replication rule. if it looks correct, choose save. otherwise, choose previous to edit the rule before saving it.  after you save your rule, you can edit, enable, disable, or delete your rule on the replication page. follow these steps to configure a replication rule when the destination bucket is in a different aws account than the source bucket. sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose management, choose replication, and then choose add rule. if you have never created a replication rule before, start with . on the replication rule wizard set destination page, under destination bucket, choose buckets in another account. then enter the name of the destination bucket and the account id from a different aws account. choose save. after you save the destination bucket name and account id, you might get a warning message telling you to add a bucket policy to the destination bucket so that amazon s3 can verify whether versioning is enabled on the bucket. you are presented with a bucket policy in a few steps that you can copy and add to the destination bucket in the other account. for information about adding a bucket policy to an s3 bucket and versioning, see  and  if you chose to replicate objects encrypted with aws kms, under destination encryption settings, enter the amazon resource name (arn) aws kms cmk to use to encrypt the replicas in the destination bucket.  for more information about creating an aws kms cmk, see  in the aws key management service developer guide. on the set destination page, under destination options: to replicate your data into a specific storage class in the destination bucket, select change the storage class for the replicated object(s). then choose the storage class that you want to use for the replicated objects in the destination bucket. if you don't select this option, the storage class for replicated objects is the same class as the original objects.to change the object ownership of the replica objects to the destination bucket owner, select change object ownership to destination owner. this option enables you to separate object ownership of the replicated data from the source. if asked, type the account id of the destination bucket. when you select this option, regardless of who owns the source bucket or the source object, the aws account that owns the destination bucket is granted full permission to replica objects. for more information, see  in the amazon simple storage service developer guide. if you want to add s3 replication time control (s3 rtc) to your replication configuration, select replication time control. notewhen you use s3 rtc, additional per-gb data transfer fees and cloudwatch metrics fees apply.choose next. set up an aws identity and access management (iam) role that amazon s3 can assume to perform replication of objects on your behalf. to set up an iam role, on the configure options page, under select role, do one of the following: we highly recommend that you choose create new role to have amazon s3 create a new iam role for you. when you save the rule, a new policy is generated for the iam role that matches the source and destination buckets that you choose. the name of the generated role is based on the bucket names and uses the following naming convention: replication_role_for_source-bucket_to_destination-bucket.you can choose to use an existing iam role. if you do, you must choose a role that allows amazon s3 to replicate objects from the source bucket to the destination bucket on your behalf.a bucket policy is provided on the configure options page that you can copy and add to the destination bucket in the other account. for information about adding a bucket policy to an s3 bucket, see  if you chose to replicate objects encrypted with aws kms, an aws kms key policy is provided on the configure options page. you can copy this policy to add to the key policy for the aws kms cmk that you are using. the key policy grants the source bucket owner permission to use the cmk. for information about updating the key policy, see . on the review page, review your replication rule. if it looks correct, choose save. otherwise, choose previous to edit the rule before saving it.  after you save your rule, you can edit, enable, disable, or delete your rule on the replication page. follow the instructions given on the replication page under the warning message, the replication rule is saved, but additional settings are required in the destination account. sign out of the aws account that you are currently in, and then sign in to the destination account.  importantreplication fails until you sign in to the destination account and complete the following steps. after you sign in to the destination account, choose the management tab, choose replication, and then choose receive objects on the actions menu.  on the receive objects page, you can do the following: enable versioning on the destination bucket.apply the bucket policy provided by amazon s3 to the destination bucket.copy the aws kms key policy that you need to update the aws kms cmk that is being used to encrypt the replica objects in the destination bucket. for information about updating the key policy, see .you must grant permissions to the account of the source bucket owner to encrypt using your aws kms cmk with a key policy. the following procedure describes how to use the aws identity and access management (iam) console to modify the key policy for the aws kms cmk that is being used to encrypt the replica objects in the destination bucket. to grant permissions to encrypt using your aws kms cmk sign in to the aws management console using the aws account that owns the aws kms cmk. open the aws kms console at . choose the alias of the cmk that you want to encrypt with. in the key policy section of the page, choose switch to policy view. choose edit to edit key policy. using the key policy editor, insert the key policy provided by amazon s3 into the existing key policy, and then choose save changes. you might want to add the policy to the end of the existing policy.  for more information about creating and editing aws kms cmks, see  in the aws key management service developer guide.   in the amazon simple storage service developer guide
this section describes how to create a s3 batch operations job. for information about performing batch operations using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. to create a batch job sign in to the aws management console and open the amazon s3 console at . choose batch operations on the navigation pane of the amazon s3 console. choose create job. choose the region where you want to create your job. under manifest format, choose the type of manifest object to use. if you choose s3 inventory report, enter the path to the manifest.json object that amazon s3 generated as part of the csv-formatted inventory report, and optionally the version id for the manifest object if you want to use a version other than the most recent.if you choose csv, enter the path to a csv-formatted manifest object. the manifest object must follow the format described in the console. you can optionally include the version id for the manifest object if you want to use a version other than the most recent.choose next under operation, choose the operation that you want to perform on all objects listed in the manifest. fill out the information for the operation you chose and then choose next. fill out the information for configure additional options and then choose next. for review, verify the settings. if you need to make changes, choose previous. otherwise, choose create job.  in the amazon simple storage service developer guide in the amazon simple storage service developer guide in the amazon simple storage service developer guide
*copyright &copy; 2020 amazon web services, inc. and/or its affiliates. all rights reserved.* amazon's trademarks and trade dress may not be used in       connection with any product or service that is not amazon's,       in any manner that is likely to cause confusion among customers,       or in any manner that disparages or discredits amazon. all other       trademarks not owned by amazon are the property of their respective      owners, who may or may not be affiliated with, connected to, or       sponsored by amazon. 
versioning enables you to keep multiple versions of an object in one bucket. this section describes how to enable object versioning on a bucket. for more information about versioning support in amazon s3, see  and  in the amazon simple storage service developer guide. to enable or disable versioning on an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to enable versioning for. choose properties. choose versioning. choose enable versioning or suspend versioning, and then choose save. noteyou can use aws multi-factor authentication (mfa) with versioning. when you use mfa with versioning, you must provide your aws account’s access keys and a valid code from the account’s mfa device in order to permanently delete an object version or suspend or reactivate versioning. to use mfa with versioning, you enable . however, you cannot enable  using the aws management console. you must use the aws cli or api. for more information, see . 
this section explains how to use the amazon s3 console to recover (undelete) deleted objects. to be able to undelete a deleted object, you must have had versioning enabled on the bucket that contains the object before the object was deleted. for information about enabling versioning, see . when you delete an object in a versioning-enabled bucket, all versions remain in the bucket and amazon s3 creates a delete marker for the object. to undelete the object, you must delete this delete marker. for more information about versioning and delete markers, see  in the amazon simple storage service developer guide. to recover deleted objects from an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. to see a list of the versions of the objects in the bucket, select show. you'll be able to see the delete markers for deleted objects.  to undelete an object, you must delete the delete marker. select the check box next to the delete marker of the object to recover, and then choose delete from the actions menu. then, choose hide and you'll see the undeleted object listed.  in the amazon simple storage service developer guide
this section explains how to use the amazon s3 console to restore an object that has been archived to the s3 glacier or s3 glacier deep archive storage classes. objects stored in the s3 glacier or s3 glacier deep archive are not immediately accessible. to access an object in this class, you must restore a temporary copy of it to its s3 bucket for the duration (number of days) that you specify. for information about the s3 glacier or s3 glacier deep archive storage classes, see  in the amazon simple storage service developer guide.  when you restore an archive, you pay for both the archive and the restored copy. because there is a storage cost for the copy, restore objects only for the duration you need them. if you want a permanent copy of the object, create a copy of it in your s3 bucket. for information about amazon s3 features and pricing, see . after restoring an object, you can download it from the overview page. for more information, see . topics the following are the available retrieval options when restoring an archived object:   - expedited retrievals allow you to quickly access your data stored in the s3 glacier storage class when occasional urgent requests for a subset of archives are required. for all but the largest archived objects (250 mb+), data accessed using expedited retrievals is typically made available within 1–5 minutes. provisioned capacity ensures that retrieval capacity for expedited retrievals is available when you need it. for more information, see . expedited retrievals and provisioned capacity are not available for objects stored in the s3 glacier deep archive storage class. - standard retrievals allow you to access any of your archived objects within several hours. this is the default option for the s3 glacier and s3 glacier deep archive retrieval requests that do not specify the retrieval option. standard retrievals typically finish within 3–5 hours for objects stored in the s3 glacier storage class. they typically finish within 12 hours for objects stored in the s3 glacier deep archive storage class.  - bulk retrievals are the lowest-cost retrieval option in amazon s3 glacier, enabling you to retrieve large amounts, even petabytes, of data inexpensively. bulk retrievals typically finish within 5–12 hours for objects stored in the s3 glacier storage class. they typically finish within 48 hours for objects stored in the s3 glacier deep archive storage class.for more information about retrieval options, see  in the amazon simple storage service developer guide. this topic explains how to use the amazon s3 console to restore an object that has been archived to the s3 glacier or s3 glacier deep archive storage classes. (the console uses the names glacier and glacier deep archive for these storage classes.) to restore archived s3 objects sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the objects that you want to restore. in the name list, select the object or objects that you want to restore, choose actions, and then choose restore. in the initiate restore dialog box, type the number of days that you want your archived data to be accessible.  choose one of the following retrieval options from the retrieval options menu. choose bulk retrieval or standard retrieval, and then choose restore. choose expedited retrieval (available only for the glacier storage class).provisioned capacity is only available for the glacier storage class. if you have provisioned capacity, choose restore to start a provisioned retrieval. if you have provisioned capacity, all of your expedited retrievals are served by your provisioned capacity. for more information about provisioned capacity, see .  if you don't have provisioned capacity and you don't want to buy it, choose restore. if you don't have provisioned capacity, but you want to buy it, choose add capacity unit, and then choose buy. when you get the purchase succeeded message, choose restore to start provisioned retrieval.you can upgrade the speed of your restoration while it is in progress. to upgrade an in-progress restore to a faster tier in the name list, select one or more of the objects that you are restoring, choose actions, and then choose restore from glacier. for information about checking the restoration status of an object, see . choose the tier that you want to upgrade to and then choose restore. for more information about upgrading to a faster restore tier, see see  in the amazon simple storage service developer guide.  to check the progress of the restoration, see the object overview panel. for information about the overview panel, see .  the overview section shows that restoration is in progress.  when the temporary copy of the object is available, the object's overview section shows the restoration expiry date. this is when amazon s3 will remove the restored copy of your archive.   restored objects are stored only for the number of days that you specify. if you want a permanent copy of the object, create a copy of it in your amazon s3 bucket.  amazon s3 calculates the expiry date by adding the number of days that you specify to the time you request to restore the object, and then rounding to the next day at midnight utc. this calculation applies to the initial restoration of the object and to any extensions to availability that you request. for example, if an object was restored on 10/15/2012 10:30 am utc and the number of days that you specified is 3, then the object is available until 10/19/2012 00:00 utc. if, on 10/16/2012 11:00 am utc you change the number of days that you want it to be accessible to 1, then amazon s3 makes the restored object available until 10/18/2012 00:00 utc. after restoring an object, you can download it from the overview page. for more information, see .   in the amazon s3 developer guide. in the aws cli command reference.
object tagging gives you a way to categorize storage. this topic explains how to use the console to add tags to an s3 object after the object has been uploaded. for information about adding tags to an object when the object is being uploaded, see .  each tag is a key-value pair that adheres to the following rules: you can associate up to 10 tags with an object. tags associated with an object must have unique tag keys.a tag key can be up to 128 unicode characters in length and tag values can be up to 255 unicode characters in length.key and tag values are case sensitive.  for more information about object tags, see  in the amazon simple storage service developer guide. to add tags to an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object you want to add tags to. choose properties. choose tags and then choose add tag. each tag is a key-value pair. type a key and a value. then choose add tag to add another tag or choose save.  you can enter up to 10 tags for an object. 
this section explains how to use the amazon simple storage service (amazon s3) console to grant access permissions to your buckets and objects. it also explains how to use amazon s3 block public access to prevent the application of any settings that allow public access to data within s3 buckets.   buckets and objects are amazon s3 resources.  you grant access permissions to your buckets and objects by using resource-based access policies. you can associate an access policy with a resource. an access policy describes who has access to resources. the resource owner is the aws account that creates the resource. for more information about resource ownership and access policies, see  in the amazon simple storage service developer guide.  bucket access permissions specify which users are allowed access to the objects in a bucket and which types of access they have. object access permissions specify which users are allowed access to the object and which types of access they have. for example, one user might have only read permission, while another might have read and write permissions. bucket and object permissions are independent of each other. an object does not inherit the permissions from its bucket. for example, if you create a bucket and grant write access to a user, you can't access that user’s objects unless the user explicitly grants you access.  to grant access to your buckets and objects to other aws accounts and to the general public, you use resource-based access policies known as access control lists (acls).  a bucket policy is a resource-based aws identity and access management (iam) policy that grants other aws accounts or iam users access to an s3 bucket. bucket policies supplement, and in many cases, replace acl-based access policies.  for more information about using iam with amazon s3, see  in the amazon simple storage service developer guide.  for more in-depth information about managing access permissions, see  in the amazon simple storage service developer guide. this section also explains how to use the amazon s3 console to add a cross-origin resource sharing (cors) configuration to an s3 bucket. cors allows client web applications that are loaded in one domain to interact with resources in another domain. topics 
this section explains how to use the amazon simple storage service (amazon s3) console to manage access permissions for s3 buckets by using access control lists (acls). acls are resource-based access policies that grant access permissions to buckets and objects. for more information about managing access permissions with resource-based policies, see  in the amazon simple storage service developer guide. you can grant permissions to other aws account users or to predefined groups. the user or group that you are granting permissions to is called the grantee. by default, the owner, which is the aws account that created the bucket, has full permissions. each permission you grant for a user or group adds an entry in the acl that is associated with the bucket. the acl lists grants, which identify the grantee and the permission granted. for more information about acls, see  in the amazon simple storage service developer guide. to set acl access permissions for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to set permissions for. choose permissions, and then choose access control list. you can manage bucket access permissions for the following:  1.  access for your aws account root user  access for other aws accounts to grant permissions to an aws user from a different aws account, under access for other aws accounts, choose add account. in the enter an id field, enter the canonical id of the aws user that you want to grant bucket permissions to. for information about finding a canonical id, see  in the amazon web services general reference. you can add as many as 99 users. select the check boxes next to the permissions that you want to grant to the user, and then choose save. to display information about the permissions, choose the help icons. warningwhen you grant other aws accounts access to your resources, be aware that the aws accounts can delegate their permissions to users under their accounts. this is known as cross-account access. for information about using cross-account access, see  in the iam user guide.  public access  warninguse caution when granting the everyone group public access to your s3 bucket. when you grant access to this group, anyone in the world can access your bucket. we highly recommend that you never grant any kind of public write access to your s3 bucket. s3 log delivery group to grant access to amazon s3 to write server access logs to the bucket, under s3 log delivery group, choose log delivery. if a bucket is set up as the target bucket to receive access logs, the bucket permissions must allow the log delivery group write access to the bucket. when you enable server access logging on a bucket, the amazon s3 console grants write access to the log delivery group for the target bucket that you choose to receive the logs. for more information about server access logging, see . you can also set bucket permissions when you are creating a bucket. for more information about setting permissions when creating a bucket, see .  
amazon s3 default encryption provides a way to set the default encryption behavior for an amazon s3 bucket. you can set default encryption on a bucket so that all objects are encrypted when they are stored in the bucket. the objects are encrypted using server-side encryption with either amazon s3-managed keys (sse-s3) or aws key management service (aws kms) customer master keys (cmks).  when you use server-side encryption, amazon s3 encrypts an object before saving it to disk in its data centers and decrypts it when you download the objects. for more information about protecting data using server-side encryption and encryption key management, see  in the amazon simple storage service developer guide. default encryption works with all existing and new amazon s3 buckets. without default encryption, to encrypt all objects stored in a bucket, you must include encryption information with every object storage request. you must also set up an amazon s3 bucket policy to reject storage requests that don't include encryption information.  there are no new charges for using default encryption for s3 buckets. requests to configure the default encryption feature incur standard amazon s3 request charges. for information about pricing, see . for sse-kms cmk storage, aws kms charges apply and are listed at .  to enable default encryption on an amazon s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. choose properties. choose default encryption. if you want to use keys that are managed by amazon s3 for default encryption, choose aes-256, and choose save.  for more information about using amazon s3 server-side encryption to encrypt your data, see  in the amazon simple storage service developer guide. importantyou might need to update your bucket policy when enabling default encryption. for more information, see  in the amazon simple storage service developer guide. if you want to use cmks that are stored in aws kms for default encryption, follow these steps: choose aws-kms. to choose a customer-managed aws kms cmk that you have created, use one of these methods: in the list that appears, choose the aws kms cmk.in the list that appears, choose custom kms arn, and then enter the amazon resource name of the aws kms cmk. importantyou can use only kms cmks that are enabled in the same aws region as the bucket. the s3 console only lists 100 kms cmks per region. if you have more than 100 cmks in the same region, you can only see the first 100 cmks in the s3 console. to use a kms cmk that is not listed in the console, choose custom kms arn, and enter the kms cmk arn.when you use an aws kms cmk for server-side encryption in amazon s3, you must choose a symmetric cmk. amazon s3 only supports symmetric cmks and not asymmetric cmks. for more information, see  in the aws key management service developer guide. importantif you use the aws kms option for your default encryption configuration, you are subject to the rps (requests per second) limits of aws kms. for more information about aws kms limits and how to request a limit increase, see . for more information about creating an aws kms cmk, see  in the aws key management service developer guide. for more information about using aws kms with amazon s3, see  in the amazon simple storage service developer guide. choose save.  in the amazon simple storage service developer guide
this section explains how to use the amazon s3 console to see the different versions of an object. a versioning-enabled bucket can have many versions of the same object:, one current (latest) version and zero or more noncurrent (previous) versions. amazon s3 assigns each object a unique version id. for information about enabling versioning, see .  if a bucket is versioning-enabled, amazon s3 creates another version of an object under the following conditions:  if you upload an object that has the same name as an object that already exists in the bucket, amazon s3 creates another version of the object instead of replacing the existing object. if you update any object properties after you upload the object to the bucket, such as changing the storage details or other metadata , amazon s3 creates a new object version in the bucket. for more information about versioning support in amazon s3, see  and  in the amazon simple storage service developer guide. to see multiple versions of an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. to see a list of the versions of the objects in the bucket, choose show. for each object version, the console shows a unique version id, the date and time the object version was created, and other properties. (objects stored in your bucket before you set the versioning state have a version id of null.)  to list the objects without the versions, choose hide. you also can view, download, and delete object versions in the object overview panel. for more information, see . importantyou can undelete an object only if it was deleted as the latest (current) version. you can't undelete a previous version of an object that was deleted. for more information, see  and  in the amazon simple storage service developer guide. 
this section explains how to use the amazon s3 console to add a cross-origin resource sharing (cors) configuration to an s3 bucket. cors allows client web applications that are loaded in one domain to interact with resources in another domain.  to configure your bucket to allow cross-origin requests, you add cors configuration to the bucket. a cors configuration is an xml document that defines rules that identify the origins that you will allow to access your bucket, the operations (http methods) supported for each origin, and other operation-specific information. for more information about cors and examples, see  in the amazon simple storage service developer guide. when you enable cors on the bucket, the access control lists (acls) and other access permission policies continue to apply. to add a cors configuration to an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to create a bucket policy for. choose permissions, and then choose cors configuration. in the cors configuration editor text box, type or copy and paste a new cors configuration, or edit an existing configuration. the cors configuration is an xml file. the text that you type in the editor must be valid xml. for more information, see  choose save. noteamazon s3 displays the amazon resource name (arn) for the bucket next to the cors configuration editor title. for more information about arns, see  in the amazon web services general reference. 
you can use lifecycle policies to define actions that you want amazon s3 to take during an object's lifetime (for example, transition objects to another storage class, archive them, or delete them after a specified period of time). you can define a lifecycle policy for all objects or a subset of objects in the bucket by using a shared prefix (objects names that begin with a common string) or a tag.  using a lifecycle policy, you can define actions specific to current and noncurrent object versions. for more information, see  and  and  in the amazon simple storage service developer guide. to create a lifecycle policy sign in to the aws management console and open the amazon s3 console at . in the buckets list, choose the name of the bucket that you want to create a lifecycle policy for. choose the management tab, and choose add lifecycle rule. in the lifecycle rule dialog box, type a name for your rule.  the name must be unique within the bucket.  choose the scope of the lifecycle rule: all objects with a specific prefix or tag or all the objects in the bucket. to apply this lifecycle rule to all objects with a specific prefix or tag, choose limit the scope to specific prefixes or tags. in the add prefix or tag filter box, type the prefix or tag name, and press enter.  for more information about object name prefixes, see  in the amazon simple storage service developer guide.for more information about object tags, see  in the amazon simple storage service developer guide.to apply this lifecycle rule to all objects in the bucket, choose apply to all objects in the bucket.choose next. the storage class transition page opens. when you configure your storage class transitions, you define the rules to transition objects to the standard-ia, one zone-ia, glacier, and deep archive storage classes. for more information, see  in the amazon simple storage service developer guide. you can define transitions for current or previous object versions or for both current and previous versions. versioning enables you to keep multiple versions of an object in one bucket. for more information about versioning, see . choose the versions for which you want to define transitions, current or noncurrent: to define transitions that are applied to the current verion of the object, choose current version. to define transitions that are applied to all previous versions of the object, choose previous versions. in the screenshot below, current version is chosen. to add a transition:  for a current version, under for current object versions, choose add transition. for a non-current version, under for non-current object versions, choose add transition. for each transition that you add, choose one of the following: transition to standard-ia after.transition to intelligent-tiering after. transition to one zone-ia after. transition to glacier after.transition to glacier deep archive after. importantwhen you choose the glacier or glacier deep archive storage class, your objects remain in amazon s3. you cannot access them directly through the separate amazon s3 glacier service. for more information, see . in the days after creation box, enter the number of days after the creation of the object that you want the transition to be applied (for example, 30 or 100 days). when you are done configuring transitions, choose next. under configure expiration, for this example, choose both current version and previous versions.  importantin a non-versioned bucket the expiration action results in amazon s3 permanently removing the object. for more information about lifecycle actions, see  in the amazon simple storage service developer guide. choose expire current version of object, and then enter the number of days after object creation to delete the object (for example, 395 days). if you choose this expire option, you cannot choose the option to clean up expired delete markers.  choose permanently delete previous versions, and then enter the number of days after an object becomes a previous version to permanently delete the object (for example, 465 days). to clean up incomplete multipart uploads, we recommend that you choose clean up incomplete multipart uploads and enter the number of days after the multipart upload initiation that you want to end and clean up incomplete multipart uploads(for example, 7 days).  for more information about multipart uploads, see  in the amazon simple storage service developer guide. choose next. for review, verify the settings for your rule. if you need to make changes, choose previous. otherwise, choose save.  if the rule does not contain any errors, it is enabled and you can see it on the lifecycle page. 
with s3 object lock, you can store objects in amazon s3 using a write-once-read-many (worm) model. you can use s3 object lock to prevent an object from being deleted or overwritten for a fixed amount of time or indefinitely. for information about object locking using the aws cli, aws sdks, and the amazon s3 rest apis, see  in the amazon simple storage service developer guide. before you lock any objects, you have to enable a bucket to use s3 object lock. you enable object lock when you create a bucket. after you enable object lock on a bucket, you can lock objects in that bucket. when you create a bucket with object lock enabled, you can't disable object lock or suspend versioning for that bucket.  for information about creating a bucket with s3 object lock enabled, see . to lock an amazon s3 object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want. in the name list, choose the name of the object that you want to lock. choose properties. choose object lock. choose a retention mode. you can change the retain until date. you can also choose to enable a legal hold. for more information, see  in the amazon simple storage service developer guide. choose save. 
welcome to the amazon simple storage service console user guide for the amazon simple storage service (amazon s3) console.  amazon s3 provides virtually limitless storage on the internet. this guide explains how you can manage buckets, objects, and folders in amazon s3 by using the aws management console, a browser-based graphical user interface for interacting with aws services.  for detailed conceptual information about how amazon s3 works, see  in the amazon simple storage service developer guide. the developer guide also has detailed information about amazon s3 features and code examples to support those features. topics 
this topic describes how to enable server access logging for an amazon s3 bucket using the aws management console. for information about how to enable logging programmatically and details about how logs are delivered, see  in the amazon simple storage service developer guide. by default, amazon simple storage service (amazon s3) doesn't collect server access logs. when you enable logging, amazon s3 delivers access logs for a source bucket to a target bucket that you choose. the target bucket must be in the same aws region as the source bucket and must not have a default retention period configuration.  server access logging provides detailed records for the requests that are made to an s3 bucket. server access logs are useful for many applications. for example, access log information can be useful in security and access audits. it can also help you learn about your customer base and understand your amazon s3 bill.  an access log record contains details about the requests that are made to a bucket. this information can include the request type, the resources that are specified in the request, and the time and date that the request was processed. for more information, see  in the amazon simple storage service developer guide. importantthere is no extra charge for enabling server access logging on an amazon s3 bucket. however, any log files that the system delivers to you will accrue the usual charges for storage. (you can delete the log files at any time.) we do not assess data transfer charges for log file delivery, but we do charge the normal data transfer rate for accessing the log files. to enable server access logging for an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to enable server access logging for. choose properties. choose server access logging. choose enable logging. for target, choose the name of the bucket that you want to receive the log record objects.  the target bucket must be in the same region as the source bucket and must not have a default retention period configuration. (optional) for target prefix, type a key name prefix for log objects, so that all of the log object names begin with the same string. choose save. you can view the logs in the target bucket. if you specified a prefix, the prefix shows as a folder in the target bucket in the console. after you enable server access logging, it might take a few hours before the logs are delivered to the target bucket. for more information about how and when logs are delivered, see  in the amazon simple storage service developer guide. more info  
this topic describes how to set or change the type of encryption an object using the amazon s3 console. noteif you change an object's encryption, a new object is created to replace the old one. if s3 versioning is enabled, a new version of the object is created, and the existing object becomes an older version. the role that changes the property also becomes the owner of the new object or (object version).  to add or change encryption for an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object that you want to add or change encryption for. choose properties, and then choose encryption. the encryption dialog box opens, giving you three choices for object encryption: none‐ no object encryption.aes-256 ‐ server-side encryption with amazon s3 managed keys (sse-s3).aws‐kms ‐ server-side encryption with aws key management service (aws kms) customer master keys (sse-kms).if you want to remove encryption from an object that already has encryption settings, choose none and then choose save. if you want to encrypt your object using keys that are managed by amazon s3, follow these steps: chooseaes-256.  for more information about using amazon s3 server-side encryption to encrypt your data, see  in the amazon simple storage service developer guide. choose save. if you want to encrypt your object using aws kms, follow these steps: choose aws-kms. choose an aws kms customer master key (cmk). the list shows  that you have created and your aws managed cmk for amazon s3. for more information about creating a customer managed aws kms cmk, see  in the aws key management service developer guide.  importantthe amazon s3 console lists only 100 aws kms cmks per aws region. if you have more than 100 cmks in the same region, you can see only the first 100 cmks in the s3 console. to use a kms cmk that is not listed in the console, choose custom kms arn, and enter the kms cmk arn. choose save. importantto encrypt objects in the bucket, you can use only cmks that are enabled in the same aws region as the bucket. amazon s3 only supports symmetric cmks. amazon s3 does not support asymmetric cmks. for more information, see . to give an external account the ability to use an object that is protected by an aws kms cmk, follow these steps:  choose aws-kms. enter the amazon resource name (arn) for the external account. choose save. administrators of an external account that have usage permissions to an object protected by your aws kms cmk can further restrict access by creating a resource-level aws identity and access management (iam) policy.  in the amazon simple storage service developer guide
this section explains how to manage and use your amazon s3 access points using the aws management console. each access point is associated with a single amazon s3 bucket. before you begin, navigate to the list of your access points for a bucket as described in the following procedure. to find a list of access points for a bucket sign in to the aws management console and open the amazon s3 console at . in the s3 buckets section, select the bucket whose access points you want to manage. on the bucket detail page, choose the access points tab. from the access points tab, you can view an access point's configuration details, edit an access point's policy, use an access point to access your bucket, or delete an access point. the following procedures explain how to perform each of these tasks. to view access point configuration details navigate to the access points tab for your bucket. locate the access point whose configuration you want to view. you can browse for the access point in the access point list, or you can search for a specific access point using the search by name field. choose the name of the access point whose configuration you want to view. noteto view an access point's configuration, choose (click on) the name of the access point, not the option button next to the access point name. to edit an access point policy navigate to the access points tab for your bucket. select the option button next to the name of the access point whose policy you want to edit. choose edit access point policy. enter the policy in the text field. the console automatically displays the amazon resource name (arn) for the access point, which you can use in the policy. you can also choose policy generator to use the aws policy generator to help construct the policy. choose save. to use an access point to access your bucket navigate to the access points tab for your bucket. select the option button next to the name of the access point you want to use. choose use this access point. the console displays a label above the name of your bucket that shows the access point that you're currently using. while you're using the access point, you can only perform the object operations that are allowed by the access point permissions. notethe console view always shows all objects in the bucket. using an access point as described in this procedure restricts the operations you can perform on those objects, but not whether you can see that they exist in the bucket. to exit the access point view of your bucket, choose exit access point. notethe s3 management console doesn't support using virtual private cloud (vpc) access points to access bucket resources. to access bucket resources from a vpc access point, use the aws cli, aws sdks, or amazon s3 rest apis. to delete an access point navigate to the access points tab for your bucket. select the option button next to the name of the access point that you want to delete. choose delete. confirm that you want to delete your access point by entering its name in the text field that appears, and choose confirm. 
this section explains how to use the amazon s3 console to view the object overview panel. this panel provides an overview of all the essential information for an object in one place. to see the overview panel for an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, select the check box next to the name of the object for which you want an overview. to download the object, choose download in the object overview panel. to copy the path of the object to the clipboard, choose copy path. if versioning is enabled on the bucket, choose latest versions to list the versions of the object. you can then choose the download icon to download an object version, or choose the trash can icon to delete an object version. importantyou can undelete an object only if it was deleted as the latest (current) version. you can't undelete a previous version of an object that was deleted. for more information, see  and  in the amazon simple storage service developer guide. 
this section explains how to use the amazon s3 console to download objects from an s3 bucket. data transfer fees apply when you download objects. for information about amazon s3 features, and pricing, see . importantif an object key name consists of a single period (.), or two periods (..), you can’t download the object using the amazon s3 console. to download an object with a key name of “.” or “..”, you must use the aws cli, aws sdks, or rest api. for more information about naming objects, see  in the amazon simple storage service developer guide. you can download a single object per request using the amazon s3 console. to . to download an object from an s3 bucket sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that you want to download an object from. you can download an object from an s3 bucket in any of the following ways: in the name list, select the check box next to the object you want to download, and then choose download on the object description page that appears.choose the name of the object that you want to download. on the overview page, choose download. choose the name of the object that you want to download and then choose download as on the overview page.choose the name of the object that you want to download. choose latest version and then choose the download icon.
each object in amazon simple storage service (amazon s3) has a set of name-value pairs that provides metadata about the object. metadata is additional information about the object. some metadata is set by amazon s3 when you upload the object, for example, and . you can also set some metadata when you upload the object, or you can add it later. this section explains how to use the amazon s3 console to add metadata to an s3 object. object metadata is a set of name-value (key-value) pairs. for example, the metadata for content length, , is the name (key) and the size of the object in bytes (value). for more information about object metadata, see  in the amazon simple storage service developer guide. there are two kinds of metadata for an s3 object, amazon s3 system metadata and user-defined metadata: system metadata–there are two categories of system metadata. metadata such as the  date is controlled by the system. only amazon s3 can modify the value. there is also system metadata that you control, for example, the storage class configured for the object. user-defined metadata–you can define your own custom metadata, called user-defined metadata. you can assign user-defined metadata to an object when you upload the object or after the object has been uploaded. user-defined metadata is stored with the object and is returned when you download the object. amazon s3 does not process user-defined metadata. the following topics describe how to add metadata to an object using the amazon s3 console. noteif you change an object's metadata, a new object will be created and will replace the old one. if s3 versioning is enabled, a new version of the object is created, and the existing object becomes an older version. the role that changes the property also becomes the owner of the new object or (object version). topics you can configure some system metadata for an s3 object. for a list of system-defined metadata and whether you can modify their values, see  in the amazon simple storage service developer guide. to add system metadata to an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object that you want to add metadata to. choose properties, and then choose metadata. choose add metadata, and then choose a key from the select a key menu. depending on which key you chose, choose a value from the select a value menu or type a value. choose save. you can assign user-defined metadata to an object. user-defined metadata must begin with the prefix "", otherwise amazon s3 will not set the key value pair as you define it. you define custom metadata by adding a name that you choose to the  key. this creates a custom key. for example, if you add the custom name , the metadata key would be .  user-defined metadata can be as large as 2 kb. both keys and their values must conform to us-ascii standards. for more information, see  in the amazon simple storage service developer guide. to add user-defined metadata to an object sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket that contains the object. in the name list, choose the name of the object that you want to add metadata to. choose properties, and then choose metadata. choose add metadata, and then choose the  key from the select a key menu. any metadata starting with the prefix  is user-defined metadata. type a custom name following the  key. for example, for the custom name , the metadata key would be . enter a value for the custom key, and then choose save. 
amazon s3 inventory provides a flat file list of your objects and metadata, which is a scheduled alternative to the amazon s3 synchronous  api operation. amazon s3 inventory provides comma-separated values (csv) or  or  output files that list your objects and their corresponding metadata on a daily or weekly basis for an s3 bucket or for objects that share a prefix (objects that have names that begin with the same string). for more information, see  in the amazon simple storage service developer guide. to configure inventory noteit may take up to 48 hours to deliver the first report. sign in to the aws management console and open the amazon s3 console at . in the bucket name list, choose the name of the bucket for which you want to configure amazon s3 inventory. choose the management tab, and then choose inventory. choose add new. type a name for the inventory and set it up as follows: optionally, add a prefix for your filter to inventory only objects whose names begin with the same string.choose the destination bucket where you want reports to be saved. the destination bucket must be in the same aws region as the bucket for which you are setting up the inventory. the destination bucket can be in a different aws account. optionally, choose a prefix for the destination bucket.choose how frequently to generate the inventory.under advanced settings, you can set the following: choose either the csv, orc, or parquet output file format for your inventory. for more information about these formats, see  in the amazon simple storage service developer guide. to include all versions of the objects in the inventory, choose include all versions in the object versions list. by default, the inventory includes only the current versions of the objects. for optional fields, select one or more of the following to add to the inventory report: size – object size in bytes.last modified date – the object creation date or the last modified date, whichever is the latest.storage class – the storage class used for storing the object. etag – the entity tag is a hash of the object. the etag reflects changes only to the contents of an object, and not its metadata. the etag may or may not be an md5 digest of the object data. whether it is depends on how the object was created and how it is encrypted.multipart upload – specifies that the object was uploaded as a multipart upload. for more information, see  in the amazon simple storage service developer guide.replication status – the replication status of the object. for more information, see .encryption status – the server-side encryption used to encrypt the object. for more information, see  in the amazon simple storage service developer guide.s3 object lock configurations – the object lock status of the object, including the following settings:  retention mode – the level of protection applied to the object, either governance or compliance.retain until date – the date until which the locked object cannot be deleted.legal hold status – the legal hold status of the locked object. for information about s3 object lock, see  in the amazon simple storage service developer guide. for more information about the contents of an inventory report, see  in the amazon simple storage service developer guide. for encryption, choose a server-side encryption option to encrypt the inventory report, or choose none: none – do not encrypt the inventory report.aes-256 – encrypt the inventory report using server-side encryption with amazon s3-managed keys (sse-s3). amazon s3 server-side encryption uses 256-bit advanced encryption standard (aes-256). for more information, see  in the amazon simple storage service developer guide. aws-kms – encrypt the report using server-side encryption with aws key management service (aws kms) customer master keys (cmks). for more information, see  in the amazon simple storage service developer guide.  noteto encrypt the inventory list file with sse-kms, you must grant amazon s3 permission to use the aws kms cmk. for instructions, see  .choose save. amazon s3 creates a bucket policy on the destination bucket that grants amazon s3 write permission. this allows amazon s3 to write data for the inventory reports to the bucket.  if an error occurs when you try to create the bucket policy, you are given instructions on how to fix it. for example, if you choose a destination bucket in another aws account and don't have permissions to read and write to the bucket policy, you see the following message.   in this case, the destination bucket owner must add the displayed bucket policy to the destination bucket. if the policy is not added to the destination bucket, you won't get an inventory report because amazon s3 doesn’t have permission to write to the destination bucket. if the source bucket is owned by a different account than that of the current user, the correct account id of the source bucket must be substituted in the policy. for more information, see  in the amazon simple storage service developer guide. to grant amazon s3 permission to encrypt using a customer managed aws key management service (aws kms) customer master key (cmk), you must use a key policy. to update your key policy so that you can use an aws kms customer managed cmk to encrypt the inventory file, follow the steps below. to grant permissions to encrypt using your aws kms cmk using the aws account that owns the customer managed cmk, sign into the aws management console. open the aws kms console at . to change the aws region, use the region selector in the upper-right corner of the page. in the left navigation pane, choose customer managed keys. under customer managed keys, choose the customer managed cmk that you want to use to encrypt the inventory file. under key policy, choose switch to policy view. to update the key policy, choose edit. under edit key policy, add the following key policy to the existing key policy. choose save changes. for more information about creating customer managed cmks aws kms and using key policies, see the following links in the aws key management service developer guide: more info  
for more detailed information about configuring a redirect in amazon s3, see  in the amazon simple storage service developer guide. you can redirect all requests for a website endpoint for a bucket to another host. if you redirect all requests, any request made to the website endpoint is redirected to the specified host name.  for example, if your root domain is , and you want to serve requests for both  and , you can create two buckets named  and . then, maintain the content in the  bucket, and configure the other  bucket to redirect all requests to the  bucket. for more information, see . to redirect requests for a bucket website endpoint open the amazon s3 console at . choose the name of the bucket that you have configured as a static website (for example, ). choose properties. choose static website hosting. choose redirect requests.  in the target bucket or domain box, enter the bucket or domain that you want to redirect to. for example, if you are redirecting to a root domain address, you would enter example.com. in the protocol box, enter the protocol for the redirected requests (http or https). if you do not specify a protocol, the protocol of the original request is used. choose save. 
in the amazon s3 console, you can move an object to a bucket or a folder.  to move an object sign in to the aws management console and open the amazon s3 console at . in the buckets list, choose your bucket name. select the check box beside the object that you want to move. choose actions, and choose move. choose the move destination: if you want to move your object to a bucket, select the bucket.if you want to move your object to a folder in a bucket, choose the bucket name, and then choose the folder.under review, confirm the move details, and choose move. amazon s3 moves your object to the destination. 
hyperparameter tuning finds the best hyperparameter values for your model by searching over ranges of hyperparameters. you specify the hyperparameters and range of values over which to search by defining hyperparameter ranges for your tuning job. choosing hyperparameters and ranges significantly affects the performance of your tuning job. for guidance on choosing hyperparameters and ranges, see . to define hyperparameter ranges by using the low-level api, you specify the names of hyperparameters and ranges of values in the  field of the  parameter that you pass to the  operation. the  field has three subfields, one for each of the categorical, integer, and continuous hyperparameter ranges. you can define up to 20 hyperparameters to search over. each value of a categorical hyperparameter range counts as a hyperparameter against the limit. hyperparameter ranges have the following structure: for integer and continuous hyperparameter ranges, you can choose the scale you want hyperparameter tuning to use to search the range of values by specifying a value for the  field of the hyperparameter range. you can choose from the following scaling types: autoamazon sagemaker hyperparameter tuning chooses the best scale for the hyperparameter. linearhyperparameter tuning searches the values in the hyperparameter range by using a linear scale. typically, you choose this if the range of all values from the lowest to the highest is relatively small (within one order of magnitude), because uniformly searching values from the range will give you a reasonable exploration of the entire range. logarithmichyperparameter tuning searches the values in the hyperparameter range by using a logarithmic scale.logarithmic scaling works only for ranges that have only values greater than 0.choose logarithmic scaling when you are searching a range that spans several orders of magnitude. for example, if you are tuning a  model, and you specify a range of values between .0001 and 1.0 for the  hyperparameter, searching uniformly on a logarithmic scale gives you a better sample of the entire range than searching on a linear scale would, because searching on a linear scale would, on average, devote 90 percent of your training budget to only the values between .1 and 1.0, leaving only 10 percent of your training budget for the values between .0001 and .1. reverselogarithmichyperparameter tuning searches the values in the hyperparameter range by using a reverse logarithmic scale. reverse logarithmic scaling is supported only for continuous hyperparameter ranges. it is not supported for integer hyperparameter ranges.reverse logarithmic scaling works only for ranges that are entirely within the range 0<=x<1.0.choose reverse logarithmic scaling when you are searching a range that is highly sensitive to small changes that are very close to 1. for an example notebook that uses hyperparameter scaling, see . 
an amazon sagemaker notebook instance is a fully managed machine learning (ml) amazon elastic compute cloud (amazon ec2) compute instance that runs the jupyter notebook app. you use the notebook instance to create and manage jupyter notebooks that you can use to prepare and process data and to train and deploy machine learning models. for more information, see .  noteif necessary, you can change the notebook instance settings, including the ml compute instance type, later. to create an amazon sagemaker notebook instance open the amazon sagemaker console at .  choose notebook instances, then choose create notebook instance. on the create notebook instance page, provide the following information (if a field is not mentioned, leave the default values): for notebook instance name, type a name for your notebook instance. for instance type, choose . this is the least expensive instance type that notebook instances support, and it suffices for this exercise. for iam role, choose create a new role, then choose create role. choose create notebook instance.  in a few minutes, amazon sagemaker launches an ml compute instance—in this case, a notebook instance—and attaches an ml storage volume to it. the notebook instance has a preconfigured jupyter notebook server and a set of anaconda libraries.  . 
a box that accepts input data. cannot be self-closingunlike the  element in the html standard, this element cannot be self-closed by putting a slash before the ending bracket, e.g. . it must be followed with a  to close the element. the following is an example of a liquid template that uses the  element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  the following attributes are supported by this element. a regular expression that is used with the auto-validate attribute to ignore non-matching characters as the worker types. when the value is set to true, the browser places focus inside the input area after loading. this way, the worker can start typing without having to select it first. a boolean switch that, if present, turns on input validation. the behavior of the validator can be modified by the error-message and allowed-pattern attributes. a boolean switch that, if present, displays the input area as disabled. the text to be displayed below the input field, on the left side, if validation fails. a string that is displayed inside a text field. this text shrinks and rises up above a text field when the worker starts typing in the field or when the value attribute is set.  a maximum number of characters the input will accept. input beyond this limit is ignored. a minimum length for the input in the field  sets the name of the input to be used in the dom and the output of the form. a string value that is used as placeholder text, displayed until the worker starts entering data into the input, it is not used as a default value. a boolean switch that, if present, requires the worker to provide input. takes a string to set the html5  behavior for the input. examples include  and . a preset that becomes the default if the worker does not provide input. the preset appears in a text field. this element has the following parent and child elements. parent elements: child elements: noneprovides a  string as the property name, and the text that was entered in the field as its value. example : sample json outputthe values for multiple elements are output in the same object, with their  attribute value as their property name. elements with no input do not appear in the output. for example, let's use three inputs:   this is the output if only two have input:   this means any code built to parse these results should be able to handle the presence or absence of each input in the answers. for more information, see the following. 
amazon sagemaker conforms to the aws , which includes regulations and guidelines for data protection. aws is responsible for protecting the global infrastructure that runs all the aws services. aws maintains control over data hosted on this infrastructure, including the security configuration controls for handling customer content and personal data. aws customers and apn partners, acting either as data controllers or data processors, are responsible for any personal data that they put in the aws cloud.  for data protection purposes, we recommend that you protect aws account credentials and set up individual user accounts with aws identity and access management (iam), so that each user is given only the permissions necessary to fulfill their job duties. we also recommend that you secure your data in the following ways: use multi-factor authentication (mfa) with each account.use ssl/tls to communicate with aws resources.set up api and user activity logging with aws cloudtrail.use aws encryption solutions, along with all default security controls within aws services.use advanced managed security services such as amazon macie, which assists in discovering and securing personal data that is stored in amazon s3.we strongly recommend that you never put sensitive identifying information, such as your customers' account numbers, into free-form fields such as a name field. this includes when you work with amazon sagemaker or other aws services using the console, api, aws cli, or aws sdks. any metadata fields that you enter into amazon sagemaker or other services, such as bucket names, job names, tags, and similar fields might get picked up for inclusion in diagnostic logs. when you provide a url to an external server, don't include credentials information in the url to validate your request to that server. for more information about data protection, see the  blog post on the aws security blog. topics 
create a jupyter notebook that contains a preinstalled environment with the default anaconda installation and python3.  to create a jupyter notebook open the amazon sagemaker console at . open a running  notebook instance, by choosing open next to its name. the jupyter notebook server page appears: to create a notebook, choose files, new, and conda_python3. . name the notebook.  
amazon sagemaker autopilot simplifies the machine learning experience by automating machine learning processes. it helps you explore your data, engineer features, try different algorithms, and select the best model. the result is the best performing model that you can deploy at a fraction of the time normally required.  you also get full visibility into how the data was wrangled and how the model was created and selected, based on performance, from the various candidates explored. this is provided by notebooks that autopilot generates that describe the plan it followed when selecting candidate models from trials combining data processing and algorithm training and tuning. the notebooks also provide educational tools that enable you to conduct ml experiments. you can learn about the impact of various inputs and trade-offs made in experiments first by examining the data exploration and candidate generation notebooks exposed by autopilot and then by making your own modifications and rerunning the notebooks. the following graphic shows how autopilot manages the principal tasks of the machine learning process.  you can use autopilot in different ways: on autopilot (hence the name) or with various degrees of human guidance, without code through amazon sagemaker studio, or with code using one of the aws sdks. autopilot currently supports regression and binary and multiclass classification. it also only supports tabular data formatted in files with comma-separated values. topics 
a bar with a sliding knob that allows a worker to select a value from a range of values by moving the knob. the slider makes it a great choice for settings that reflect intensity levels, such as volume, brightness, or color saturation. the following is an example of a survey template that uses the  element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  the following attributes are supported by this element. a boolean switch that, if present, displays the slider as disabled. a boolean switch that, if present, displays an up/down button that can be chosen to select the value. selecting the value via the up/down button is an alternative to selecting the value by moving the knob on the slider. the knob on the slider will move synchronously with the up/down button choices. a number that specifies the maximum value on the slider. a number that specifies the minimum value on the slider. a string that is used to identify the answer submitted by the worker. this value will match a key in the json object that specifies the answer. a boolean switch that, if present, displays the current value above the knob as the knob is moved. a boolean switch that, if present, requires the worker to provide input. when used with a  css attribute, the progress bar is colored to the point represented by the . for example, if this was representing the progress on a streaming video, the  would represent where the viewer was in the video timeline. the  value would represent the point on the timeline to which the video had buffered. a number that specifies the difference between selectable values on the slider. a preset that becomes the default if the worker does not provide input. this element has the following parent and child elements. parent elements: child elements: nonefor more information, see the following. 
for an overview on training models with amazon sagemaker, see . amazon sagemaker provides features to monitor and manage the training and validation of machine learning models. for guidance on metrics available, incremental training, automatic model tuning, and the use of augmented manifest files to label training data, see the following topics. for guidance on debugging the training of machine learning models, see . for guidance on metrics used to monitor and train models, see . for guidance on incremental training in amazon sagemaker, see .for guidance on using managed spot training in amazon sagemaker, see .for guidance on using training checkpoints in amazon sagemaker, see .for guidance on automatic model tuning, also known as hyperparameter tuning, see . for guidance on using an augmented manifest file to label training data, see .topics 
this section walks you through training your first model using amazon sagemaker studio, or the amazon sagemaker console and the amazon sagemaker api. you use notebooks and training algorithms provided by amazon sagemaker. topics 
in addition to using the built-in mechanisms, you can extend the code with the preprocessing and postprocessing scripts. topics you can extend the code with the post processing script by following this contract.  specify it as a path in amazon simple storage service (amazon s3) in the  request. the amazon sagemaker model monitor container works only with tabular or flattened json structures. we provide a per-record preprocessor for some small changes required to transform the dataset. for example, if your output is an array [1.0, 2.1], you need to convert this into a flattened json, like {“prediction0”: 1.0, “prediction1” : 2.1"}. a sample implementation might look like the following. specify it as a path in amazon s3 in the  request: the structure of the inference_record is defined as follows. 
with amazon sagemaker debugger, you can go beyond just looking at scalars, like losses and accuracies, when evaluating model training. debugger gives you full visibility into a training job by using a hook to capture tensors that define the state of the training process at each point in the job's lifecycle. debugger also provides rules to inspect the captured tensors.  built-in rules monitor the training flow and alert you to problems with common conditions that are critical for the success of the training job. you can also create your own custom rules to watch for any issues specific to your model. you can monitor the results of the analysis done by rules with amazon cloudwatch events, using an amazon sagemaker notebook, or in visualization provided by amazon sagemaker studio. the following diagram shows the flow for the model training process with amazon sagemaker debugger.  to use amazon sagemaker debugger hooks and rules, you activate them by adding only a few lines of code in your estimator object. debugger and its client library  help you set up hooks and rules that give you transparency into training jobs. debugger and  support the major machine learning frameworks—tensorflow, mxnet, and pytorch—and the amazon sagemaker pre-built algorithm xgboost, while you run training jobs in the amazon sagemaker environment.  choose a framework and use amazon sagemaker pre-built training containers:  amazon sagemaker gives you options to use amazon pre-built training containers or custom containers to run training jobs. debugger features and pre-built training containers such as amazon sagemaker containers and deep learning containers help make your model debugging process budget-friendly. you can also bring your own containers if you prefer to customize debugger for your training algorithms. to learn more, go to . use a debugger hook to save tensors:  after you choose a container and a framework that fit your training script, use a debugger hook to configure which tensors to save and to which directory to save them, such as a amazon s3 bucket. a debugger hook helps you to build the configuration and to keep it in your account to use in subsequent analyses, where it is secured for use with the most privacy-sensitive applications. to learn more, go to . use debugger rules to inspect tensors in parallel with a training job:  to analyze tensors, debugger provides built-in rules for over a dozen abnormal training process behaviors. for example, a debugger rule detects issues when the training process suffers from vanishing gradients, exploding tensors, overfitting, or overtraining. if necessary, you can build customized rules to analyze saved tensors using the amazon sagemaker debugger sdk. you can then use the amazon sagemaker python sdk to configure debugger to save the required tensors and to deploy built-in or custom rules for monitoring them. to learn more, go to ,  for the full debugger rule api, and  for customizing debugger rules. in combination with debugger rules, you can also use amazon cloudwatch events to call an aws lambda function that automatically stops the training job when the debugger rules trigger  status.create trials to analyze tensors: the  trial is an object that lets you query the saved tensors from a given training job, specified by the path to which  artifacts are saved. a trial is capable of loading new tensors as they become available at a given path, enabling you to do both offline and real-time analysis. to learn more about the smdebug trial, see . use amazon sagemaker studio for visualization:  you can use debugger in amazon sagemaker studio for visulizing collected trials by debugger. amazon sagemaker studio makes inspecting training job issues easier through its visual interface for analyzing your tensor data. to learn more, go to . 
if you choose, amazon sagemaker ground truth can use active learning to automate the labeling of your input data. active learning is a machine learning technique that identifies data that should be labeled by your workers. in ground truth, this functionality is called automated data labeling. automated data labeling helps to reduce the cost and time that it takes to label your dataset compared to using only humans. when you use automated labeling, you incur amazon sagemaker training and inference costs.  we recommend using automated data labeling on large datasets because the neural networks used with active learning require a significant amount of data for every new dataset. typically, as more data is provided, the potential for high accuracy predictions goes up. data will only be auto-labeled if the neural network used in the auto-labeling model can achieve an acceptably high level of accuracy. therefore, with larger datasets, there is more potential to automatically label the data because the neural network can achieve high enough accuracy for auto-labeling. automated data labeling is most appropriate when you have thousands of data objects. the minimum number of objects allowed for automated data labeling is 1,250, but we strongly suggest providing a minimum of 5,000 objects. automated data labeling is available only for the following ground truth built-in task types:  image classification (single label)semantic segmentationbounding box text classificationyou enable automated data labeling when you create a labeling job. this is how it works: when ground truth starts an automated data labeling job, it selects a random sample of input data (objects) and sends it to human workers. when the labeled data are returned, ground truth uses this data, the validation data, to validate the models trained for automated data labeling.  ground truth runs a batch transform job, using the validated model for inference on the validation data. batch inference produces a confidence score and quality metric for each object in the validation data. the auto labeling component will use these quality metrics and confidence scores to create a confidence score threshold that ensure quality labels.  ground truth runs a batch transform job on the unlabeled data in the dataset, using the same validated model for inference. this will produce a confidence score for each object.  the ground truth auto labeling component determines if the confidence score produced in step 5 for each object meets the required threshold determined in step 4. if the confidence score meets the threshold, the expected quality of automatically labeling exceeds the requested level of accuracy and that object will be considered auto-labeled.  step 6 will produce a dataset of unlabeled data with confidence scores. ground truth will select data points with low confidence scores from this dataset and send them to human workers.  ground truth uses the existing human-labeled data and this additional labeled data from human workers to train a new model. the process is repeated until the dataset is fully labeled or until another stopping condition is met. for example, auto labeling will stop if your human annotation budget is reached. input data quotas apply for automated data labeling jobs. see  for information about dataset size, input data size and resolution limits for automated semantic segmentation, object detection, and image classification.  notebefore you use an the automated-labeling model in production, you need to fine-tune or test the it, or both . you might fine-tune the model (or create and tune another supervised model of your choice) on the dataset produced by your labeling job to optimize the model’s architecture and hyperparameters. if you decide to use the model for inference without fine-tuning it, we strongly recommend making sure that you evaluate its accuracy on a representative (for example, randomly selected) subset of the dataset labeled with ground truth and that it matches your expectations. automated data labeling is available only for ground truth built-in algorithms.  to use the amazon sagemaker console to create a labeling job that uses automated data labeling you need to know how to create a labeling job using the amazon sagemaker console. to learn how to create a labeling job in the console using ground truth, see .  to create an automated data labeling job (console) open the ground truth labeling jobs section of the amazon sagemaker console: . using  as a guide, complete the job overview and task type sections. note that auto labeling is not support for custom task types. under workers, choose your workforce type.  in the same section, choose enable automated data labeling.  using  as a guide, create worker instructions in the section task type labeling tool. for example, if you chose semantic segmentation as your labeling job type, this section will be called semantic segmentation labeling tool. to preview your worker instructions and dashboard, choose preview. choose create. this will create and start your labeling job and the auto labeling process.  you will see your labeling job appear in the labeling jobs section of the amazon sagemaker console. your output data will appear in the amazon s3 bucket that you specified when creating the labeling job. for more information about the format and file structure of your labeling job output data, see . to create an automated data labeling job using the the amazon sagemaker api, use the  parameter of the  operation.  specify the amazon resource name (arn) of the algorithm that you are using for automated data labeling in the  parameter. choose from one of the four ground truth built-in algorithms that are supported with automated labeling: image classificationsemantic segmentationobject detection (bounding box) text classificationwhen an automated data labeling job finishes, ground truth returns the arn of the model it used for the automated data labeling job. use this model as the starting model for similar auto-labeling job types by providing the arn, in string format, in the  parameter. to retrieve the model's arn, use an aws command line interface (aws cli) command similar to the following.  to encrypt data on the storage volume attached to the ml compute instance(s) that are used in automated labeling, include an aws key management service (aws kms) key in the  parameter. for information about aws kms keys see aws key management service developer guide. for an example that uses the  operation to create an automated data labeling job, see the object_detection_tutorial example in the sagemaker examples, ground truth labeling jobs section of a amazon sagemaker notebook instance. to learn how to create and open a notebook instance, see . to learn how to access amazon sagemaker example notebooks, see . the following table lists the amazon elastic compute cloud (amazon ec2) instances that you need to run automated data labeling for training and batch inference jobs. * in the asia pacific (mumbai) region (ap-south-1) use ml.p2.8xlarge instead. automated data labeling incurs two separate charges: the per-item charge (for information, see ), and the charge for the amazon ec2 instance required to run the model (see ).  ground truth manages the instances that you use for automated data labeling jobs. it creates, configures, and terminates the instances as needed to perform your job. these instances don't appear in your amazon ec2 instance dashboard. 
use this page to become familiarize with the user interface and tools available to complete your 3d point cloud object detection task. topics when you work on a 3d point cloud object detection task, you need to select a category from the annotations menu on the right side of your worker portal using the label categories menu. after you've chosen a category, use the add cuboid and fit cuboid tools to fit a cuboid around objects in the 3d point cloud that this category applies to. after you place a cuboid, you can modify its dimensions, location, and orientation directly in the point cloud, and the three panels shown on the right.  if you see one or more images in your worker portal, you can also modify cuboids in the images or in the 3d point cloud and the edits will show up in the other medium.  if you see cuboids have already been added to the 3d point cloud when you open your task, adjust those cuboids and add additional cuboids as needed.  to edit a cuboid, including moving, re-orienting, and changing cuboid dimensions, you must use shortcut keys. you can see a full list of shortcut keys in the shortcuts menu in your ui. the following are important key-combinations that you should become familiar with before starting your labeling task.  to edit a cuboid, including moving, re-orienting, and changing cuboid dimensions, you must use shortcut keys. you can see a full list of shortcut keys in the shortcuts menu in your ui. the following are important key-combinations that you should become familiar with before starting your labeling task.  you can navigate in the 3d scene using your keyboard and mouse. you can: double click on specific objects in the point cloud to zoom into them.use a mouse-scroller or trackpad to zoom in and out of the point cloud.use both keyboard arrow keys and q, e, a, and d keys to move up, down, left, right. use keyboard keys w and s to zoom in and out. once you place a cuboids in the 3d scene, a side-view will appear with three projected views: top, side, and back. these side-views show points in and around the placed cuboid and help workers refine cuboid boundaries in that area. workers can zoom in and out of each of those side-views using their mouse.  the following video demonstrates movements around the 3d point cloud and in the side-view.   when you are in the worker ui, you see the following menus: instructions – review these instructions before starting your task.shortcuts – use this menu to view keyboard shortcuts that you can use to navigate the point cloud and use the annotation tools provided. label – use this menu to modify a cuboid. first, select a cuboid, and then choose an option from this menu. this menu includes assistive labeling tools like setting a cuboid to the ground and automatically fitting the cuboid to the object's boundaries. view – use this menu to toggle different view options on and off. for example, you can use this menu to add a ground mesh to the point cloud, and to choose the projection of the point cloud. 3d point cloud – use this menu to add additional attributes to the points in the point cloud, such as color, and pixel intensity. note that these options may not be available.when you open a task, the move scene icon is on, and you can move around the point cloud using your mouse and the navigation buttons in the point cloud area of the screen. to return to the original view you see when you first opened the task, choose the reset scene icon. resetting the view will not modify your annotations.  after you select the add cuboid icon, you can add cuboids to the 3d point cloud visualization. once you've added a cuboid, you can adjust it in the three views (top, side, and front) and in the images (if included).   you must choose the move scene icon again to move to another area in the 3d point cloud or image.  to collapse all panels on the right and make the 3d point cloud full-screen, choose the full screen icon.  if camera images are included, you may have the following view options: c – view the camera angle on point cloud view.f – view the frustum, or field of view, of the camera used to capture that image on point cloud view. p – view the point cloud overlaid on the image.b – view cuboids in the image. the following video demonstrates how to use these view options. the f option is used to view the field of view of the camera (the gray area), the c options shows the direction the camera is facing and angle of the camera (blue lines), and the b option is used to view the cuboid.   use this table to learn about the icons you see in your worker task portal.  the shortcuts listed in the shortcuts menu can help you navigate the 3d point cloud and use tools to add and edit cuboids. before you start your task, we recommend that you review the shortcuts menu and become acquainted with these commands. you need to use some of the 3d cuboid controls to edit your cuboid.  you should periodically save your work. ground truth will automatically save your work ever 15 minutes.  when you open a task, you must complete your work on it before pressing submit. if you select stop working you will loose that task, and other workers will be able to start working on it.  
automatic model tuning, also known as hyperparameter tuning or hyperparameter optimization, finds the best version of a model by running many jobs that test a range of hyperparameters on your dataset. you choose the tunable hyperparameters, a range of values for each, and an objective metric. you choose the objective metric from the metrics that the algorithm computes. automatic model tuning searches the hyperparameters chosen to find the combination of values that result in the model that optimizes the objective metric. the amazon sagemaker rcf algorithm is an unsupervised anomaly-detection algorithm that requires a labeled test dataset for hyperparameter optimization. rcf calculates anomaly scores for test datapoints and then labels the datapoints as anomalous if their scores are beyond three standard deviations from the mean score. this is known as the three-sigma limit heuristic. the f1-score is based on the difference between calculated labels and actual labels. the hyperparameter tuning job finds the model that maximizes that score. the success of hyperparameter optimization depends on the applicability of the three-sigma limit heuristic to the test dataset. for more information about model tuning, see . the rcf algorithm computes the following metric during training. when tuning the model, choose this metric as the objective metric. you can tune a rcf model with the following hyperparameters. 
you can use scikit-learn scripts to preprocess data and evaluate your models. to see how to run scikit-learn scripts to perform these tasks see the  sample notebook. this notebook uses the scriptprocessor class from the amazon sagemaker python sdk for processing,  the following example shows how to use a  class to run a python script with your own image that runs a processing job that processes input data, and saves the processed data in amazon simple storage service (amazon s3). the notebook shows the general workflow for using a  class.  create a docker directory and add the dockerfile used to create the processing container. install pandas and scikit-learn into it. (you could also install your own dependencies with a similar  command.) build the container using the docker command, creates an amazon elastic container registry (amazon ecr) repository, and pushes the image to amazon ecr. set up the  from the amazon sagemaker python sdk to run the script. run the script. you can use the same procedure with any other library or system dependencies. you can also use existing docker images, including images that you run on other platforms, such kubernetes. 
you can install libraries to run your scripts in your own processing container or, in a more advanced scenario, you can build your own processing container that satisfies the contract to run in amazon sagemaker. for a formal specification that defines the contract for an an amazon sagemaker processing container, see .  topics 
to create a model package resource that you can use to create deployable models in amazon sagemaker and publish on aws marketplace specify the following information: the docker container that contains the inference code, or the algorithm resource that was used to train the model.the location of the model artifacts. model artifacts can either be packaged in the same docker container as the inference code or stored in amazon s3.the instance types that your model package supports for both real-time inference and batch transform jobs.validation profiles, which are batch transform jobs that amazon sagemaker runs to test your model package's inference code. before listing model packages on aws marketplace, you must validate them. this ensures that buyers and sellers can be confident that products work in amazon sagemaker. you can list products on aws marketplace only if validation succeeds.  the validation procedure uses your validation profile and sample data to run the following validations tasks: create a model in your account using the model package's inference image and the optional model artifacts that are stored in amazon s3. notea model package is specific to the region in which you create it. the s3 bucket where the model artifacts are stored must be in the same region where your created the model package. create a transform job in your account using the model to verify that your inference image works with amazon sagemaker. create a validation profile. notein your validation profile, provide only data that you want to expose publicly. validation can take up to a few hours. to see the status of the jobs in your account, in the amazon sagemaker console, see the transform jobs pages. if validation fails, you can access the scan and validation reports from the amazon sagemaker console. after fixing issues, recreate the algorithm. when the status of the algorithm is , find it in the amazon sagemaker console and start the listing process noteto publish your model package on aws marketplace, at least one validation profile is required. you can create an model package either by using the amazon sagemaker console or by using the amazon sagemaker api. topics to create a model package in the amazon sagemaker console: open the amazon sagemaker console at . choose model packages, then choose create model package. on the inference specifications page, provide the following information: for model package name, type a name for your model package. the model package name must be unique in your account and in the aws region. the name must have 1 to 64 characters. valid characters are a-z, a-z, 0-9, and - (hyphen). type a description for your model package. this description appears in the amazon sagemaker console and in the aws marketplace. for inference specification options, choose provide the location of the inference image and model artifacts to create a model package by using an inference container and model artifacts. choose provide the algorithm used for training and its model artifacts to create a model package from an algorithm resource that you created or subscribe to from aws marketplace. if you chose provide the location of the inference image and model artifacts for inference specification options, provide the following information for container definition and supported resources: for location of inference image, type the path to the image that contains your inference code. the image must be stored as a docker container in amazon ecr. for location of model data artifacts, type the location in s3 where your model artifacts are stored. for container dns host name , type the name of the dns host to use for your container. for supported instance types for real-time inference, choose the instance types that your model package supports for real-time inference from amazon sagemaker hosted endpoints. for supported instance types for batch transform jobs, choose the instance types that your model package supports for batch transform jobs. supported content types, type the content types that your model package expects for inference requests. for supported response mime types, type the mime types that your model package uses to provide inferences. if you chose provide the algorithm used for training and its model artifacts for inference specification options, provide the following information: for algorithm arn, type the amazon resource name (arn) of the algorithm resource to use to create the model package. for location of model data artifacts, type the location in s3 where your model artifacts are stored. choose next. on the validation and scanning page, provide the following information: for publish this model package on aws marketplace, choose yes to publish the model package on aws marketplace. for validate this model package, choose yes if you want amazon sagemaker to run batch transform jobs that you specify to test the inference code of your model package. noteto publish your model package on aws marketplace, your model package must be validated. for iam role, choose an iam role that has the required permissions to run batch transform jobs in amazon sagemaker, or choose create a new role to allow amazon sagemaker to create a role that has the  managed policy attached. for information, see . for validation profile, specify the following: a name for the validation profile.a transform job definition. this is a json block that describes a batch transform job. this is in the same format as the  input parameter of the  api.choose create model package. to create a model package by using the amazon sagemaker api, call the  api.  
the output from a labeling job is placed in the location that you specified in the console or in the call to the  operation. each line in the output data file is identical to the manifest file with the addition of an attribute and value for the label assigned to the input object. the attribute name for the value is defined in the console or in the call to the  operation. you can't use  in the label attribute name. if you are running an image semantic segmentation, 3d point cloud semantic segmentation, or 3d point cloud object tracking job, the label attribute must end with . for any other type of job, the attribute name can't end with . the output of the labeling job is the value of the key-value pair with the label. the label and the value overwrites any existing json data in the input file with the new value.  for example, the following is the output from an image classification labeling job where the input data files were stored in an amazon s3  and the label attribute name was defined as . in this example the json object is formatted for readability, in the actual output file the json object is on a single line. for more information about the data format, see .  the value of the label can be any valid json. in this case the label's value is the index of the class in the classification list. other job types, such as bounding box, have more complex values. any key-value pair in the input manifest file other than the label attribute is unchanged in the output file. you can use this to pass data to your application. the output from a labeling job can be used as the input to another labeling job. you can use this when you are chaining together labeling jobs. for example, you can send one labeling job to determine the sport that is being played. then you send another using the same data to determine if the sport is being played indoors or outdoors. by using the output data from the first job as the manifest for the second job, you can consolidate the results of the two jobs into one output file for easier processing by your applications.  the output data file is written to the output location periodically while the job is in progress. these intermediate files contain one line for each line in the manifest file. if an object is labeled, the label is included. if the object hasn't been labeled, it is written to the intermediate output file identically to the manifest file. ground truth creates several directories in your amazon s3 output path. these directories contain the results of your labeling job and other artifacts of the job. the top-level directory for a labeling job is given the same name as your labeling job, the output directories are placed beneath it. for example, if you named your labeling job find-people your output would be in the following directories: each directories contain the following output: the  directory is only present when you are using automated data labeling. it contains the input and output validation set for automated data labeling, and the input and output folder for automatically labeled data. the  directory contains all of the annotations made by the workforce. these are the responses from individual workers that have not been consolidated into a single label for the data object.  there are three subdirectories in the  directory.  the first, , contains the responses from individual workers. this contains a subdirectory for each iteration, which in turn contains a subdirectory for each data object in that iteration. the annotation for each data object is stored in a timestamped .json file. there may be more than one annotation for each data object in this directory, depending on how many workers you want to annotate each object. the second, , contains information required to consolidate the annotations in the current batch into labels for your data objects. the third, , contains the output manifest for the current batch with any completed labels. this file is updated as the label for each data object is completed. the  directory is only present when you are using automated data labeling. this directory contains the input and output files for the amazon sagemaker batch transform used while labeling data objects. the  directory contains the output manifest from your labeling job. there is one subdirectory in the manifest directory, . the  directory contains the output manifest file for your labeling job. the file is named . the  directory is only present when you are using automated data labeling. this directory contains the input and output files used to train the automated data labeling model. when you have more than one worker annotate a single task, your label results from annotation conslidation. ground truth calculates a confidence score for each label. a confidence score is a number between 0 and 1 that indicates how confident ground truth is in the label. you can use the confidence score to compare labeled data objects to each other, and to identify the least or most confident labels. you should not interpret the value of the confidence scores as an absolute value, or compare them across labeling jobs. for example, if all of the confidence scores are between 0.98 and 0.998, you should only compare the data objects with each other and not rely on the high confidence scores.  you should not compare the confidence scores of human-labeled data objects and auto-labeled data objects. the confidence scores for humans are calculated using the annotation consolidation function for the task, the confidence scores for automated labeling are calculated using a model that incorporates object features. the two models generally have different scales and average confidence. for a bounding box labeling job, ground truth calculates a confidence score per box. you can compare confidence scores within one image or across images for the same labeling type (human or auto). you can't compare confidence scores across labeling jobs. if a single worker annotates a task ( is set to  or, in the console you enter 1 for number of workers per dataset object), the confidence score will be set to .  the output from each job contains metadata about the label assigned to data objects. these elements are the same for all jobs with minor variations. the following example shows the metadata elements: the elements have the following meaning:  – the confidence that ground truth has that the label is correct. for more information, see . – the type of classification job. for job types, see .  – the name assigned to the job when it was created. – indicates whether the data object was labeled by a human or by automated data labeling. for more information, see . – the date and time that the label was created.the following are sample outputs (output manifest files) from an image classification job and a text classification job. they includes the label that ground truth assigned to the data object, the value for the label, and metadata that describes the label. in addition to the standard metadata elements, the metadata for a classification job includes the text value of the label's class. for more information, see . the red, italicized text in the examples below depends on labeling job specifications and output data.  the following are example output manifest files from a multi-label image classification job and a multi-label text classification job. they include the labels that ground truth assigned to the data object (for example the image or piece of text) and metadata that describes the labels the worker saw when completing the labeling task.  the label attribute name parameter (for example, ) contains an array of all of the labels that were selected by at least one of the workers who completed this task. this array contains integer keys (for example, ) that correspond to the labels found in . in the multi-label image classification example, , , and  were selected by at least one of the workers who completed the labeling task for the image, . the  shows the confidence-score that ground truth assigned to each label that was selected by a worker. to learn more about ground truth confidence scores, see . the red, italicized text in the examples below depends on labeling job specifications and output data.  the following is an example of a multi-label image classification output manifest file.  the following is an example of a multi-label text classification output manifest file. in this example, ,  and  were selected by at least one of the workers who completed the labeling task for the object  found in . the following is sample output (output manifest file) from a bounding box job. for this task, three bounding boxes are returned. the label value contains information about the size of the image, and the location of the bounding boxes. the  element is the index of the box's class in the list of available classes for the task. the  metadata element contains the text of the class. the metadata has a separate confidence score for each bounding box. the metadata also includes the  element that maps the  to the text value of the class. for more information, see . the red, italicized text in the examples below depends on labeling job specifications and output data.  the output of a bounding box adjustment job looks like the following json. note that the original json is kept intact and two new jobs are listed, each with “adjust-” prepended to the original attribute’s name.  in this output, the job's  doesn't change, but an  field is added. this field has the value of  or . if multiple workers have reviewed the object and at least one adjusted the label, the status is . the output (output manifest file) of a bounding box verification job looks much different than the output of a bounding box annotation job. that's because the workers having a different type of task. they're not labeling objects, but evaluating the accuracy of prior labeling, making a judgment, and then providing that judgment and perhaps some comments. if you are having human workers verify or adjust prior bounding box labels, the output of a verification job would look like the following json. the red, italicized text in the examples below depends on labeling job specifications and output data.  although the  on the original bounding box output was , the new  is . also note that the  array provides worker comments. if the worker doesn't provide comments, the empty fields are excluded during consolidation. the following is the output manifest file from a semantic segmentation labeling job. the value of the label for this job is a reference to a png file in an s3 bucket. in addition to the standard elements, the metadata for the label includes a color map that defines which color was used to label the image, the class name associated with the color, and the confidence score for each color. for more information, see . the red, italicized text in the examples below depends on labeling job specifications and output data.  confidence is scored on a per-image basis. confidence scores are the same across all classes within an image.  the output of a semantic segmentation adjustment job looks similar to the following json. the following is the output manifest file from a semantic segmentation labeling job.  in addition to the standard elements, the metadata for the label includes a color map that defines which color was used to label the image, the class name associated with the color, and the confidence score for each color. additionally, there is an  parameter in the metadata for audit-workflows that is set to  if the color mask was modified.  the  parameter contains the location of a compressed file with a .zlib extension. when you uncompress this file, it contains an array that indicates the indexes of each annotated point in the input point cloud and the value of the array gives the class of the point based on the semantic color map found in metadata.  the red, italicized text in the examples below depends on labeling job specifications and output data.  the following is sample output from a 3d point cloud objected detection job. for this task type, the data about 3d cuboids is returned in the  parameter, in a list named . in this list, each 3d cuboid is described using the following information. if one or more cuboids were modified, there is an  parameter in the metadata for audit workflows that is set to .  each class, or label category, that you specified in your input manifest is associated with a . use the  to identify the class associated with each class id.these classes are used to give each 3d cuboid an  in the format  where  is a unique number to identify that cuboid in the frame. , , and  are used to describe the center of the cuboid in the world coordinate system. , , and  are used to describe the dimensions of the cuboid. , , and  are used to describe the orientation (heading) of the cuboid. if you included label attributes in your input manifest file for a given class, a  parameter is included for all cuboids that workers selected label attributes for. the red, italicized text in the examples below depends on labeling job specifications and output data. the ellipses (...) denote a continuation of that list, where additional objects with the same format as the proceeding object can appear. the following is the output manifest file from a object tracking labeling job. the red, italicized text in the examples below depends on labeling job specifications and output data. the ellipses (...) denote a continuation of that list, where additional objects with the same format as the proceeding object can appear.  in addition to the standard elements, the metadata includes a class-map that lists each class that has at least one label in the sequence. if one or more cuboids were modified, there is an  parameter in the metadata for audit workflows that is set to .  in the above example, the cuboid data for each frame in  will be in  in the amazon s3 location, . the following is an example of this label-sequence file. for each frame in the sequence, you see a list of 3d cubiods that were drawn for that frame. each frame includes the following information:  an  in the format  where  identifies the label category and  is a unique id across the the dataset.when workers draw a cuboid, it is associated with a unique  which is associated with all cuboids that identify the same object across multiple frames.each class, or label category, that you specified in your input manifest is associated with a . use the  to identify the class associated with each class id., , and  are used to describe the center of the cuboid in the world coordinate system. , , and  are used to describe the dimensions of the cuboid. , , and  are used to describe the orientation (heading) of the cuboid. if you included label attributes in your input manifest file for a given class, a  parameter is included for all cuboids that workers selected label attributes for. 
automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many jobs that test a range of hyperparameters on your dataset. you choose the tunable hyperparameters, a range of values for each, and an objective metric. you choose the objective metric from the metrics that the algorithm computes. automatic model tuning searches the hyperparameters chosen to find the combination of values that result in the model that optimizes the objective metric. the amazon sagemaker k-means algorithm is an unsupervised algorithm that groups data into clusters whose members are as similar as possible. because it is unsupervised, it doesn't use a validation dataset that hyperparameters can optimize against. but it does take a test dataset and emits metrics that depend on the squared distance between the data points and the final cluster centroids at the end of each training run. to find the model that reports the tightest clusters on the test dataset, you can use a hyperparameter tuning job. the clusters optimize the similarity of their members. for more information about model tuning, see . the k-means algorithm computes the following metrics during training. when tuning a model, choose one of these metrics as the objective metric.  tune the amazon sagemaker k-means model with the following hyperparameters. the hyperparameters that have the greatest impact on k-means objective metrics are: , , and . tuning the hyperparameter  generally results in minor improvements. 
use the following values for the components of the registry url for the images that provide custom rule evaluators for amazon sagemaker debugger. for account ids, see the following table. ecr repository name: sagemaker-debugger-rule-evaluator  tag: latest  example full name registry address:   account ids for custom rules container images by aws region   
perform load tests to choose an automatic scaling configuration that works the way you want. for an example of load testing to optimize automatic scaling for a amazon sagemaker endpoint, see . the following guidelines for load testing assume you are using an automatic scaling policy that uses the predefined target metric . topics perform load testing to find the peak  that your model's production variant can handle, and the latency of requests, as concurrency increases. this value depends on the instance type chosen, payloads that clients of your model typically send, and the performance of any external dependencies your model has. to find the peak requests-per-second (rps) your model's production variant can handle and latency of requests set up an endpoint with your model using a single instance. for information about how to set up an endpoint, see . use a load testing tool to generate an increasing number of parallel requests, and monitor the rps and model latency in the out put of the load testing tool.  noteyou can also monitor requests-per-minute instead of rps. in that case don't multiply by 60 in the equation to calculate  shown below. when the model latency increases or the proportion of successful transactions decreases, this is the peak rps that your model can handle. after you find the performance characteristics of the variant, you can determine the maximum rps we should allow to be sent to an instance. the threshold used for scaling must be less than this maximum value. use the following equation in combination with load testing to determine the correct value for the  target metric in your automatic scaling configuration. where  is the maximum rps that you determined previously, and  is the safety factor that you chose to ensure that your clients don't exceed the maximum rps. multiply by 60 to convert from rps to invocations-per-minute to match the per-minute cloudwatch metric that amazon sagemaker uses to implement automatic scaling (you don't need to do this if you measured requests-per-minute instead of requests-per-second). noteamazon sagemaker recommends that you start testing with a  of 0.5. test your automatic scaling configuration to ensure it operates in the way you expect with your model for both increasing and decreasing customer traffic on your endpoint. 
linear models are supervised learning algorithms used for solving either classification or regression problems. for input, you give the model labeled examples (x, y). x is a high-dimensional vector and y is a numeric label. for binary classification problems, the label must be either 0 or 1. for multiclass classification problems, the labels must be from 0 to  - 1. for regression problems, y is a real number. the algorithm learns a linear function, or, for classification problems, a linear threshold function, and maps a vector x to an approximation of the label y.  the amazon sagemaker linear learner algorithm provides a solution for both classification and regression problems. with the amazon sagemaker algorithm, you can simultaneously explore different training objectives and choose the best solution from a validation set. you can also explore a large number of models and choose the best. the best model optimizes either of the following: continuous objectives, such as mean square error, cross entropy loss, absolute error.discrete objectives suited for classification, such as f1 measure, precision, recall, or accuracy. compared with methods that provide a solution for only continuous objectives, the amazon sagemaker linear learner algorithm provides a significant increase in speed over naive hyperparameter optimization techniques. it is also more convenient.  the linear learner algorithm requires a data matrix, with rows representing the observations, and columns representing the dimensions of the features. it also requires an additional column that contains the labels that match the data points. at a minimum, amazon sagemaker linear learner requires you to specify input and output data locations, and objective type (classification or regression) as arguments. the feature dimension is also required. for more information, see . you can specify additional parameters in the  string map of the request body. these parameters control the optimization procedure, or specifics of the objective function that you train on. for example, the number of epochs, regularization, and loss type.  topics the amazon sagemaker linear learner algorithm supports three data channels: train, validation (optional), and test (optional). if you provide validation data, the  should be . the algorithm logs validation loss at every epoch, and uses a sample of the validation data to calibrate and select the best model. if you don't provide validation data, the algorithm uses a sample of the training data to calibrate and select the model. if you provide test data, the algorithm logs include the test score for the final model. for training, the linear learner algorithm supports both  and  formats. for the  input type, only float32 tensors are supported. for the  input type, the first column is assumed to be the label, which is the target variable for prediction. you can use either file mode or pipe mode to train linear learner models on data that is formatted as  or as . for inference, the linear learner algorithm supports the , , and  formats. when you make predictions on new data, the format of the response depends on the type of model. for regression (), the  is the prediction produced by the model. for classification ( or ), the model returns a  and also a . the  is the class predicted by the model and the  measures the strength of that prediction.  for binary classification,  is  or , and  is a single floating point number that indicates how strongly the algorithm believes that the label should be 1.for multiclass classification, the  will be an integer from  to , and  will be a list of one floating point number per class. to interpret the  in classification problems, you have to consider the loss function used. if the  hyperparameter value is  for binary classification or  for multiclass classification, then the  can be interpreted as the probability of the corresponding class. these are the loss values used by the linear learner when the  value is  default value. but if the loss is set to , then the score cannot be interpreted as a probability. this is because hinge loss corresponds to a support vector classifier, which does not produce probability estimates. for more information on input and output file formats, see . for more information on inference formats, and the . you can train the linear learner algorithm on single- or multi-machine cpu and gpu instances. during testing, we have not found substantial evidence that multi-gpu computers are faster than single-gpu computers. results can vary, depending on your specific use case. for a sample notebook that uses the amazon sagemaker linear learner algorithm to analyze the images of handwritten digits from zero to nine in the mnist dataset, see . for instructions on how to create and access jupyter notebook instances that you can use to run the example in amazon sagemaker, see . after you have created a notebook instance and opened it, choose the sagemaker examples tab to see a list of all of the amazon sagemaker samples. the topic modeling example notebooks using the linear learning algorithm are located in the introduction to amazon algorithms section. to open a notebook, choose its use tab and choose create copy. 
the container can analyze the data available in the  path and write reports to the path in  the container code can write any reports that suit your needs. if you use the following structure and contract, certain output files are treated specially by amazon sagemaker in the visualization and api affordances this applies only to tabular datasets. table: output files for tabular datasets   in addition, if the  value is  container code can emit amazon cloudwatch metrics in this location: . the schema for these files is described in the following sections. topics 
amazon sagemaker debugger has apis in several locations that are used to implement its monitoring and analysis of model training. amazon sagemaker debugger provides an open source smdebug python library at  that is used to configure built-in rules or to define custom rules used to analyze the tensor data from training jobs. the  is a high-level sdk focused on machine learning experimentation. the sdk can be used to deploy built-in or custom rules defined with the smdebug python library to monitor and analyze these tensors using amazon sagemaker estimators. debugger has added operations and types to the amazon sagemaker api that enable the platform to use debugger when training a model and to manage the configuration of inputs and outputs.   and  use the following debugger apis to configure tensors and rules, hook up the smdebug library, and manage the storage of tensorboard outputs: also provides an additional type to report on the status of rule evaluations:  also has parameters for accessing these debugger configuration types, as well as the time and billable time spend in model training.debugger also makes use of the amazon sagemaker processing functionality when analyzing model training. for more information on processing, see . 
you can use the amazon sagemaker console to create a labeling job for all of the ground truth built-in task types and custom labeling workflows.  you need to provide the following to create a labeling job in the amazon sagemaker console:  an input manifest file in amazon s3. for text and image labeling jobs, ground truth can generate a manifest file using your input data in amazon s3. to learn more, see .  for point cloud labeling job modalities, you need to create a point cloud frame or sequence input manifest file. to learn more, see . an amazon s3 bucket to store your output data. an iam role with permission to access your resources in amazon s3 and an amazon sagemaker execution policy attached. for a general solution, you can attach the managed policy, amazonsagemakerfullaccess, to an iam role. for more details see .if you are using your own private workers, you must create a work team and provide your work team arn when creating the labeling job. if you are using a custom labeling workflow, you must save a worker task template in amazon s3 and provide an amazon s3 uri for that template. for more information, see .(optional) an aws kms key arn if you want amazon sagemaker to encrypt the output of your labeling job using your own aws kms encryption key instead of the default amazon s3 service key.(optional) existing labels for the dataset you use for your labeling job. use this option if you want workers to adjust, or approve and reject labels. importantyour work team, input manifest file, output bucket, and other resources in amazon s3 must be in the same aws region you use to create your labeling job.  when you create a labeling job using the amazon sagemaker console, you can add worker instructions to one of the default template provided by ground truth. to preview the worker ui that is generated from these templates, see the page for your .  to create a labeling job (console) sign in to the amazon sagemaker console at .  in the left navigation pane, choose labeling jobs.  on the labeling jobs page, choose create labeling job. for job name, enter a name for your labeling job. (optional) if you want to identify your labels with a key, select i want to specify a label attribute name different from the labeling job name. if you do not select this option, the labeling job name you specified in the previous step will be used to identify your labels in your output manifest file.  for input dataset location, provide the location in amazon s3 where your input manifest file is located. for example, if your input manifest file, manifest.json, is located in example-bucket, enter s3://example-bucket/manifest.json.  for output dataset location, provide the location in amazon s3 where you want ground truth to store the output data from your labeling job.  for iam role, choose an existing iam role or create an iam role with permission to access your resources in amazon s3, to write to the output amazon s3 bucket specified above, and with an amazon sagemaker execution policy attached.  (optional) for additional configuration, you can specify how much of your dataset you want workers to label, and if you want amazon sagemaker to encrypt the output data for your labeling job using an aws kms encryption key. to encrypt your output data, you must have the required aws kms permissions attached to the iam role you provided in the previous step. for more details, see .  in the task type section, under task category use the drop-down menu to select your task category.  in task selection, choose your task type.  (optional) provide tags for your labeling job to make it easier to find in the console later.  choose next.  in the workers section, choose the type of workforce you would like to use. for more details about your workforce options see . (optional) after you've selected your workforce, specify the task timeout. this is the maximum amount of time a worker has to work on a task. for 3d point cloud annotation tasks, the default task timeout is 3 days. the default timeouts for text and image classification, and label verification labeling jobs is 5 minutes. the default timeouts for all other labeling jobs is 60 minutes. if you set your task timeout to be greater than one hour, you must  of your execution role to be greater than or equal to the task timeout. if you choose a task time that is greater than 8 hours for 3d point cloud labeling jobs, refer to . (optional) for bounding box, semantic segmentation, and point cloud task types, you can select display existing labels if you want to display labels for your input data set for workers to verify or adjust. if you choose this option, select the label attribute name for the labels that you want to verify or adjust. this can be found in the output manifest file in your amazon s3 bucket. for more information, see . in the next section, specify your worker instructions and labels. for the point cloud labeling modality, you can also specify label attributes. you can select see preview to preview your worker instructions, labels, and interact with the worker ui. for more details about the worker ui for each task type, see the page for your .  (optional) if you are not using a point cloud task type, you can add additional instructions to help your worker complete your task. choose create. after you've successfully created your labeling job, you are redirected to the labeling jobs page. the status of the labeling job you just created will be in progress. this status progressively updates as workers complete your tasks. when all tasks are successfully completed, the status changes to completed.  if an issue occurred while creating the labeling job, its status changes to failed. to view more details about the job, choose the labeling job name.  after your labeling job status changes to completed, you can view your output data in the amazon s3 bucket that you specified while creating that labeling job. for details about the format of your output data, see . 
noteamazon sagemaker studio is available in the following aws regions:us east (ohio), us-east-2 us east (n. virginia), us-east-1 us west (n. oregon), us-west-2 china (beijing), cn-north-1 china (ningxia), cn-northwest-1 eu (ireland), eu-west-1 to change the region in the amazon sagemaker console, use the region selector at the upper-right corner of the console. to use amazon sagemaker studio and amazon sagemaker studio notebooks, you must complete the studio onboarding process using the amazon sagemaker console. when onboarding, you can choose to use either aws single sign-on (aws sso) or aws identity and access management (iam) for authentication methods. when you use iam authentication, you can choose either the quick start or the standard setup procedure. noteif you onboard using iam authentication and want to switch to aws sso authentication later, you must delete the domain created for you by amazon sagemaker studio. then, you need to manually re-import all notebooks and other user data that you created. for more information, see . the simplest way to create an amazon sagemaker studio account is to follow the quick start procedure. for more control, including the option of using aws sso authentication, use the standard setup procedures. aws sso authentication to use aws sso authentication with amazon sagemaker studio, you must onboard to an aws sso organization. notethe sso organization account must be in the same aws region as amazon sagemaker studio. aws sso authentication provides the following benefits over iam authentication: members given access to studio have a unique sign-in url that directly opens studio, and they sign in with their sso credentials. when you use iam authentication, you must sign in through the amazon sagemaker console.organizations manage their members in aws sso instead of studio. you can assign multiple members access to studio at the same time. when you use iam authentication, you must add and manage members manually one at time using the studio control panel. topics 
a widget to enable human review of a amazon textract document analysis result. the following attributes are supported by this element. this is the text that is displayed as the header. this is a link to the image to be analyzed by the worker.  this sets initial values for attributes found in the worker ui. the following is an example of an  input:  this determines the kind of analysis the workers can do. only  is currently supported.  this specifies new keys and the associated text value the worker can add. the input values for  can include the following elements:   accepts strings, and is used to specify a single key.  can be used to specify aliases that are acceptable alternatives to the keys supplied. use this element to identify alternative spellings or presentations of your keys. this parameter accepts a list of one or more strings. the following is an example of an input for . this prevents the workers from editing the keys of annotations passed through . if you want to prevent workers from editing the keys that have been detected on your documents, you should include this attribute.  this prevents workers from editing the polygons of annotations passed through . for example, this would prevent the worker from editing the bounding box around a given key. in most scenarios, you should include this attribute.   this element has the following parent and child elements.  parent elements – crowd-formchild elements – ,  the following regions are supported by this element. you can use custom html and css code within these regions to format your instructions to workers. for example, use the  section to provide good and bad examples of how to complete a task.  general instructions about how to work with the widget.  important task-specific instructions that are displayed in a prominent place. an example of a worker template using this crowd element would look like the following. the following is a sample of the output from this element. you can find a detailed explanation of this output in the amazon textract  api documentation. 
jupyter logs include important information such as events, metrics, and health information that provide actionable insights when running amazon sagemaker notebooks. by importing jupyter logs into cloudwatch logs, customers can use cloudwatch logs to detect anomalous behaviors, set alarms, and discover insights to keep the amazon sagemaker notebooks running more smoothly. you can access the logs even when the amazon ec2 instance that hosts the notebook is unresponsive, and use the logs to troubleshoot the unresponsive notebook. sensitive information such as aws account ids, secret keys, and authentication tokens in presigned urls are removed so that customers can share logs without leaking private information.  to view jupyter logs for a notebook instance: sign in to the aws management console and open the amazon sagemaker console at .  choose notebook instances. in the list of notebook instances, choose the notebook instance for which you want to view jupyter logs. under monitor on the notebook instance details page, choose view logs. in the cloudwatch console, choose the log stream for your notebook instance. its name is in the form . for more information about monitoring cloudwatch logs for amazon sagemaker, see . 
to download the mnist dataset, copy and paste the following code into the notebook and run it:. the code does the following: downloads the mnist dataset () from the mnist database website to your notebook.  unzips the file and reads the following datasets into the notebook's memory:  – you use these images of handwritten numbers to train a model. – the  uses these images to evaluate the progress of the model during training. – you use this set to get inferences to test the deployed model.next step 
the amazon mechanical turk workforce provides the most workers for your amazon augmented ai task review and amazon sagemaker ground truth labeling job. you can use the console to choose the amazon mechanical turk workforce for your amazon sagemaker ground truth labeling job or amazon augmented ai human review workflow, or you can provide the amazon resource name (arn) for the amazon mechanical turk workforce when you use the amazon a2i  operation. any amazon mechanical turk workforce billing is handled as part of your ground truth or amazon augmented ai billing. you do not need to create a separate mechanical turk account to use the amazon mechanical turk workforce.  the arn for the amazon mechanical turk workforce is:  the amazon mechanical turk workforce is a world-wide resource. workers are available 24 hours a day, 7 days a week. you typically get the fastest turn-around for your human review tasks and labeling jobs when you use the amazon mechanical turk workforce. adjust the number of workers that annotate each data object based on the complexity of the job and the quality that you need. amazon sagemaker ground truth uses annotation consolidation to improve the quality of the labels. more workers can make a difference in the quality of the labels for more complex labeling jobs, but might not make a difference for simpler jobs. for more information, see . annotation consolidation is not supported for amazon augmented ai human review workflows.  importantyou should not share confidential information, personal information or protected health information with this workforce. for avoidance of doubt, you should not use the amazon mechanical turk workforce when you use amazon a2i in conjunction with aws hipaa-eligible services, such as amazon textract and amazon rekognition for workloads containing protected health information. to choose the amazon mechanical turk workforce when you are creating a labeling job or human review workflow using the console, do the following during the select workers and configure tool step: to use the amazon mechanical turk workforce choose amazon mechanical turk from worker types. choose the dataset does not contain adult content if your dataset doesn't contain potentially offensive content. this enables workers to opt out if they don't want to work with it. acknowledge that your data will be viewed by the amazon mechanical turk workforce and that all personally identifiable information (pii) has been removed. choose additional configuration to set optional parameters. optional. enable automated data labeling to have ground truth automatically label some of your dataset. for more information, see . automated data labeling is not available for amazon augmented ai. optional. set the number of workers that should see each object in your dataset. using more workers can increase the quality of your labels but also increases the cost. your labeling job or human review task will now be sent to the amazon mechanical turk workforce. you can use the console to continue configuring your labeling job. 
to avoid incurring unnecessary charges, when you are done with the example, use the aws management console to delete the resources that you created for it.  noteif you plan to explore other examples, you might want to keep some of these resources, such as your notebook instance, s3 bucket, and iam role. open the amazon sagemaker console at  and delete the notebook instance. stop the instance before deleting it. open the amazon s3 console at  and delete the bucket that you created to store model artifacts and the training dataset.  open the iam console at  and delete the iam role. if you created permission policies, you can delete them, too. open the amazon cloudwatch console at  and delete all of the log groups that have names starting with . 
the following sections describe how to perform basic tasks in an amazon sagemaker studio notebook. for more detailed information on studio notebooks and these tasks, see . topics amazon sagemaker studio can open only notebooks listed in the studio file browser. for instructions on adding a notebook to the browser, see  or . to open a notebook in the left sidebar, choose the file browser icon ( ) to display the file browser. browse to a notebook file and double-click it to open the notebook in a new tab. the top menu of your notebook should look similar to the following:  you can create a notebook from the amazon sagemaker studio file menu or the studio launcher. to create a notebook from the file menu on the top studio menu, choose file > new > notebook. the notebook is opened in a new tab. in the select kernel dialog, choose a kernel, then choose select. to create a notebook from the launcher if the launcher is not displayed, on the top studio menu, choose file > new launcher. the launcher is opened in a new tab. to create a notebook that uses the default kernel, , under notebook, choose python 3. to change the kernel, see . to create a notebook that uses a different kernel, use the selector under choose a sagemaker image to start. next, choose python 3. the notebook is opened in a new tab. for more information, see . information about the current instance type is displayed at the top right of a notebook. in the screenshot at the top of the page, this is . noteif you change the instance type, existing setting for the notebook are lost and installed package must be re-installed. to change the instance type choose the instance type to open the select instance dialog. only fast launch instance types are initially displayed. choose one of these types or to display all instance types, toggle the fast launch only selector off. after choosing a type, choose save and continue. wait for the new instance to become enabled. for more information, see . the current kernel name is displayed at the top right of a notebook. in the screenshot at the top of the page, this is .  denotes the kernel and  denotes the sagemaker image that contains the kernel. to change the image choose the kernel name to open the select kernel dialog. use the dropdown menu to choose a new kernel and then choose select. the status of the kernel is displayed on the lower left of the screen. while waiting for the kernel to start, you can edit text cells in the notebook but you must wait until the kernel is started to run a code cell. for more information, see . you can share a notebook with a colleague by sending them a link. when your colleague opens the link, a read-only version of the notebook is opened in their amazon sagemaker studio. they can save an editable copy to continue working on the notebook. to share a notebook choose share at the top right of the notebook to display the create shareable snapshot dialog. optionally, choose any of the following items: include git repo information – includes a link to the git repository that contains the notebook. this enables you and your colleague to collaborate and contribute to the same git repository.include output – includes all notebook output that has been saved.choose create. after the snapshot is created, choose copy link and then choose close. share the link with your colleague. for more information, see . when you shut down a notebook, any unsaved information in the notebook is lost. the notebook is not deleted. to shut down a notebook to save the notebook contents, on the left of the notebook menu, choose the disk icon. on the top menu of studio, choose file > close and shutdown notebook. choose ok. for more information, see . 
the object handle for the compiled model supplies the  function, which allows you to create an endpoint to serve inference requests. the function lets you set the number and type of instances that are used for the endpoint. you must choose an instance for which you have compiled your model. for example, in the job compiled in  section, this is . the neo api uses a special runtime, the neo runtime, to run neo-optimized models. after the command is done, the name of the newly created endpoint is printed in the jupyter notebook. 
a button that can be either checked or unchecked. when radio buttons are inside a radio group, exactly one radio button in the group can be checked at any time. the following is an example of how to configure a  element inside of a  element. the following is an example of the syntax that you can use with the  element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  the previous example can be seen in a custom worker task template in this github example: . the following attributes are supported by this element. a boolean switch that, if present, displays the radio button as checked. a boolean switch that, if present, displays the button as disabled and prevents it from being checked. a string that is used to identify the answer submitted by the worker. this value will match a key in the json object that specifies the answer. noteif you use the buttons outside of a  element, but with the same  string and different  strings, the  object in the output will contain a boolean value for each  string. to ensure that only one button in a group is selected, make them children of a  element and use different name values. a property name for the element's boolean value. if not specified, it uses "on" as the default, e.g. . this element has the following parent and child elements. parent elements: child elements: noneoutputs an object with the following pattern: . if you use the buttons outside of a  element, but with the same  string and different  strings, the name object will contain a boolean value for each  string. to ensure that only one in a group of buttons is selected, make them children of a  element and use different name values. example sample output of this element   for more information, see the following. 
k-means is an unsupervised learning algorithm. it attempts to find discrete groupings within data, where members of a group are as similar as possible to one another and as different as possible from members of other groups. you define the attributes that you want the algorithm to use to determine similarity.  amazon sagemaker uses a modified version of the web-scale k-means clustering algorithm. compared with the original version of the algorithm, the version used by amazon sagemaker is more accurate. like the original algorithm, it scales to massive datasets and delivers improvements in training time. to do this, the version used by amazon sagemaker streams mini-batches (small, random subsets) of the training data. for more information about mini-batch k-means, see . the k-means algorithm expects tabular data, where rows represent the observations that you want to cluster, and the columns represent attributes of the observations. the n attributes in each row represent a point in n-dimensional space. the euclidean distance between these points represents the similarity of the corresponding observations. the algorithm groups observations with similar attribute values (the points corresponding to these observations are closer together). for more information about how k-means works in amazon sagemaker, see . topics for training, the k-means algorithm expects data to be provided in the train channel (recommended ), with an optional test channel (recommended ) to score the data on. both  and  formats are supported for training. you can use either file mode or pipe mode to train models on data that is formatted as  or as . for inference, , , and  are supported. k-means returns a  label and the  for each observation. for more information on input and output file formats, see  for inference and the . the k-means algorithm does not support multiple instance learning, in which the training set consists of labeled “bags”, each of which is a collection of unlabeled instances. we recommend training k-means on cpu instances. you can train on gpu instances, but should limit gpu training to  instances because only one gpu per instance is used. for a sample notebook that uses the amazon sagemaker k-means algorithm to segment the population of counties in the united states by attributes identified using principle component analysis, see . for instructions how to create and access jupyter notebook instances that you can use to run the example in amazon sagemaker, see . once you have created a notebook instance and opened it, select the sagemaker examples tab to see a list of all the amazon sagemaker samples. to open a notebook, click on its use tab and select create copy. 
 there are three ways that you can create a private workforce: create a new workforce while you are creating your labeling job. do this in the amazon sagemaker console in the ground truth section. create a new workforce before you create your labeling job. do this in the amazon sagemaker console in the ground truth section. import an existing workforce after creating a user pool in the amazon cognito console. once you create a private workforce, that workforce and all work teams and workers associated with it will be available to to use for all ground truth labeling job tasks and amazon augmented ai human review workflows tasks. if you are new to amazon sagemaker and want to test ground truth or amazon a2i, we suggest that you create a private work team consisting of people from your organization using the console. use this work team when creating labeling or human review workflows (flow definitions) to test your worker ui and job-workflow.  topics 
amazon sagemaker model monitor provides you the ability to continuously monitor the data collected from the endpoints on a schedule. you can create a monitoring schedule with the  api with a predefined periodic interval. for example, every x hours (x can range from 1 to 23). with a monitoring schedule, amazon sagemaker can kick off processing jobs at a specified frequency to analyze the data collected during a given period. amazon sagemaker provides a pre-built container for performing analysis on tabular datasets. in the processing job, amazon sagemaker compares the dataset for the current analysis with the baseline statistics, constraints provided and generate a violations report. in addition, cloudwatch metrics are emitted for each feature under analysis. alternatively, you could choose to bring your own container as outlined in the  topic.  you can create a model monitoring schedule for the endpoint created earlier. use the baseline resources (constraints and statistics) to compare against the real-time traffic. for this example, upload the training dataset that was used to train the pretrained model included in this example. if you already have it in amazon s3, you can point to it directly. create a model monitoring schedule for the endpoint using the baseline constraints and statistics to compare against real-time traffic. describe and inspect the schedule: after you describe it, observe that the  in  returned by the  api changes to . 
a small window that pops up on the display when it is opened.  the following is an example of the syntax that you can use with the  element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  the following attributes are supported by this element. the text to display for opening the modal. the default is "click to open modal". a string that specifies the type of trigger for the modal. the possible values are "link" (default) and "button". this element has the following parent and child elements. parent elements: child elements: nonefor more information, see the following. 
developing a machine learning model typically requires extensive experimenting with different datasets, algorithms, and hyperparameter values. to manage up to thousands of machine learning model experiments, use amazon sagemaker's search capabilities. you can use sagemaker search to: organize, find, and evaluate training jobs using properties, hyperparameters, performance metrics, or any metadata.find the best performing model by reviewing training job and model metrics, such as training loss or validation accuracy.trace a model's lineage to the training job and its related resources, such as the training datasets.this topic covers searching from the sagemaker console and the sagemaker api. for information on searching in amazon sagemaker studio, see . topics for a sample notebook that uses amazon sagemaker model tracking capability to manage ml experiments, see .  for instructions on how to create and access jupyter notebook instances that you can use to run the example in sagemaker, see . after you have created a notebook instance and opened it, choose the sagemaker examples tab to see a list of all of the sagemaker samples. the notebook for managing ml experiments is located in the advanced functionality section. to open a notebook, choose its use tab, and choose create copy. if you have questions, post them on the . to organize training jobs, assign one or more tags to them. to find a specific training job, model, or resource, use model tracking to search on keywords assigned to any searchable items. searchable items include training jobs, models, hyperparameters, metadata, tags, and urls. to refine your tracking results, you can search using multiple criteria. to choose the best model for deployment, evaluate how all models performed against one or more metrics. you can use model tracking results to list, sort, and evaluate the performance of the models in your experiments.  topics to group training jobs, create tags with descriptive keys and a value. for example, create tag keys for: project, owner, customer, and industry.  add tags to training jobs (console) open the . in the navigation pane, choose training jobs and create training job. scroll to the bottom of the page and enter a key and value for the tag. to add another tag, choose add tag, and add another key-value pair. you can search for training jobs using a variety of job attributes. note that some search parameters appear only if you have created a training job with that attribute. for example, tags appears only if you have added a tag for a training job. to find training jobs (console) open the . in the navigation pane, choose search. add parameters. in the search box, enter a parameter and choose a parameter type, for example trainingjobname. choose a conditional operation. for numeric values, use operators such as is equals to, lesser than, or or greater than. for text-based values, use operators such as equals to or contains. enter a value for the parameter. (optional) to refine your search, add additional search criteria. choose add row and enter the parameter values.  choose search. to evaluate a model's performance, review its metadata, hyperparameters, and metrics. to highlight metrics, adjust the view to show only metrics and important hyperparameters. to evaluate a model (console) open the . in the navigation pane, choose search and search for training jobs by specifying relevant parameters. the results are displayed in a table. open the preferences window by choosing the settings icon in the search results table. to show or hide a hyperparameter or metric, turn it on or off by choosing hyperparameter or metric . make necessary changes, then choose update view. after viewing metrics and important hyperparameters, you can compare and contrast the result. then, you can choose the best model to host or investigate the models that are performing poorly. to the find and evaluate training jobs or to get suggestions for items used in experiments that are searchable, you can use the  api. topics to find training jobs, create a search parameter using the  parameter. then use the search function in the  subprocess in the aws sdk for python (boto 3).  the following example shows how to use the  api to find training jobs.  to evaluate models, run a search as described in , review model metrics, then, use the aws sdk for python (boto 3) to create a table and plot it. the following example shows how to evaluate models and to display the results in a table. to get suggestions for a search, use the  api. the following example for aws sdk for python (boto 3) is a  request for items containing  the following is an example response for a  request. after getting search suggestions, you can use one of the property names in a search. you can use model tracking capability to verify which datasets were used in training, where holdout datasets were used, and other details about training jobs. for example, use model tracking capability to verify that a specific dataset was used in a training job for an audit or to verify compliance. to check whether a specific dataset was used in a training job, you search for the url to its location in amazon simple storage service (amazon s3). model tracking capability returns the training jobs that used the dataset that you specify. if your search doesn't return the dataset (the result is empty), the dataset wasn't used in a training job. an empty result confirms, for example, that a holdout dataset wasn't used. you can use model tracking capability to get information about the lineage of training jobs and the model resources that were used for them, including the dataset, algorithm, hyperparameters, and metrics. for example, if you find that the performance of a hosted model has declined, you can review its training job and the resources it used to determine what's causing the problem. topics to trace a model's lineage (console) open the . in the navigation pane, choose endpoints, and choose the relevant endpoint.  scroll to the endpoint configuration settings section. this section lists all of the model versions deployed at the endpoint, with a hyperlink to the training job that created each.  to trace a model's lineage, get the model's name, then use it to search for training jobs. the following example shows how to trace a model's lineage using the api. after finding the training job, you can review the resources used to train the model. 
finally you configure the bounding box tool to give instructions to your workers. you can configure a task title that describes the task and provides high-level instructions for the workers. you can provide both quick instructions and full instructions. quick instructions are displayed next to the image to be labeled. full instructions contain detailed instructions for completing the task. in this example, you only provide quick instructions. you can see an example of full instructions by choosing full instructions at the bottom of the section. to configure the bounding box tool in the task description field type in brief instructions for the task. for example: draw a box around any objects in the image. replace objects with the name of an object that appears in your images. in the labels field, type a category name for the objects that the worker should draw a bounding box around. for example, if you are asking the worker to draw boxes around football players, you could use "footballplayer" in this field. the short instructions section enables you to create instructions that are displayed on the page with the image that your workers are labeling. we suggest that you include an example of a correctly drawn bounding box and an example of an incorrectly drawn box. to create your own instructions, use these steps: select the text between good example and the image placeholder. replace it with the following text: draw the box around the object with a small border. select the first image placeholder and delete it. choose the image button and then enter the https url of one of the images that you created in step 1. select the text between bad example and the image placeholder. replace it with the following text: don't make the bounding box too large or cut into the object. select the second image placeholder and delete it. choose the image button and then enter the https url of the other image that you created in step 1. configuration of your labeling job is complete. to start your job, choose submit.  
to provide details for the monitoring schedule, use , which is a  expression that describes details about the monitoring schedule. amazon sagemaker model monitor supports the following  expressions: to set the job to start every hour:  to run the job daily:  for example, the following are valid  expressions: daily at 12 pm utc: daily at 12 am utc: to support running every 6, 12 hours, model monitoring supports the following expression:  for example, the following are valid  expressions: every 12 hours, starting at 5 pm utc: every two hours, starting at 12 am utc: notealthough the  expression is set to start at 5 pm utc, note that there could be a delay of 0-20 minutes from the actual requested time to run the execution. if you want to run on a daily schedule, don't provide this parameter. amazon sagemaker picks a time to run every day currently, amazon sagemaker only supports hourly integer rates between 1 hour and 24 hours. 
after your training and/or inference code is packaged in docker containers, create algorithm and model package resources that you can use in your amazon sagemaker account and, optionally, publish on aws marketplace. topics 
amazon sagemaker model monitor continuously monitors the quality of amazon sagemaker machine learning models in production. it enables developers to set alerts for when there are deviations in the model quality, such as data drift. early and pro-active detection of these deviations enables you to take corrective actions, such as retraining models, auditing upstream systems, or fixing data quality issues without having to monitor models manually or build additional tooling. you can use model monitor pre-built monitoring capabilities that do not require coding. you also have the flexibility to monitor models by coding to provide custom analysis. topics amazon sagemaker model monitor automatically monitors machine learning (ml) models in production and notifies you when data quality issues arise. ml models in production have to make predictions on real-life data that is not carefully curated like most training datasets. if the statistical nature of the data that your model receives while in production drifts away from the nature of the baseline data it was trained on, the model begins to lose accuracy in its predictions. model monitor uses rules to detect data drift and alerts you when it happens. the following figure shows how this process works.  to enable model monitoring, you take the following steps, which follow the path of the data through the various data collection, monitoring, and analysis processes. : enable the endpoint to capture data from incoming requests to a trained ml model and the resulting model predictions.: create a baseline from the dataset that was used to train the model. compute baseline schema constraints and statistics for each feature using , an open source library built on apache spark, which is used to measure data quality in large datasets.: create a monitoring schedule specifying what data to collect, how often to collect it, how to analyze it, and which reports to produce. : inspect the reports, which compare the latest data with the baseline, and watch for any violations reported and for metrics and notifications from amazon cloudwatch.noteamazon sagemaker model monitor currently supports only endpoints that host a single model and does not support monitoring multi-model endpoints. for information on using multi-model endpoints, see  . for a sample notebook that takes you through the full end-to-end workflow for model monitor, see the .  for a sample notebook that enables the model monitoring experience for an existing endpoint, see the .  for a sample notebook that visualizes the statistics.json file for a selected execution in a monitoring schedule, see the .  for instructions how to create and access jupyter notebook instances that you can use to run the example in amazon sagemaker, see . once you have created a notebook instance and opened it, choose the sagemaker examples tab to see a list of all the amazon sagemaker samples. to open a notebook, choose its use tab and choose create copy. 
this rule detects when the percentage of rectified linear unit (relu) activation functions in a trial are considered dead because their activation activity has dropped below a threshold. if the percent of inactive relus in a layer is greater than the  value of inactive relus, the rule returns . parameter descriptions for the deadrelu rule   for an example of how to configure and deploy a built-in rule, see . notethis rule can't be applied to the xgboost algorithm. 
use the topics on this page to learn about ground truth security features, and how to configure iam permissions to create a labeling job.  if you are new to ground truth or you do not require granular permissions to use the service, you can use the managed policy, amazonsagemakerfullaccess to grant permission to an iam role to create a labeling job. you can also attach amazonsagemakerfullaccess to an iam role to create an execution role. to learn more about this managed policy, see . importantwhen you create a custom labeling workflow, the amazonsagemakerfullaccess policy is restricted to invoking aws lambda functions with one of the following four strings as part of the function name: , , , or . this applies to both your pre-annotation and post-annotation lambda functions. if you choose to use names without those strings, you must explicitly provide  permission to the iam role used to create the labeling job. topics 
semantic segmentation involves classifying individual points of a 3d point cloud into pre-specified categories. use this task type when you want workers to create a point-level semantic segmentation mask for 3d point clouds. for example, if you specify the classes , , and , workers select one class at a time, and color all of the points that this class applies to the same color in the point cloud.  for this task type, the data object that workers label is a single point cloud frame. ground truth generates a 3d point cloud visualization using point cloud data you provide. you can also provide camera data to give workers more visual information about scenes in the frame, and to help workers paint objects. when a worker paints an object in either the 2d image or the 3d point cloud, the paint shows up in the other view.  you can adjust annotations created in a 3d point cloud object detection labeling job using the 3d point cloud semantic segmentation adjustment task type.  if you are a new user of the ground truth 3d point cloud labeling modality, we recommend you review . this labeling modality is different from other ground truth task types, and this topic provides an overview of important details you should be aware of when creating a 3d point cloud labeling job. topics ground truth provides workers with a web portal and tools to complete your 3d point cloud semantic segmentation annotation tasks. when you create the labeling job, you provide the amazon resource name (arn) for a pre-built ground truth ui in the  parameter. when you create a labeling job using this task type in the console, this ui is automatically used. you can preview and interact with the worker ui when you create a labeling job in the console. if you are a new use, it is recommended that you create a labeling job using the console to ensure your label attributes, point cloud frames, and if applicable, images, appear as expected.  the following is a gif of the 3d point cloud semantic segmentation worker task interface. if you provide camera data for sensor fusion, images are matched with scenes in the point cloud frame. workers can paint objects in either the 3d point cloud or the 2d image, and the paint appears in the corresponding location in the other medium. these images appear in the worker portal as shown in the following gif.   worker can navigate in the 3d scene using their keyboard and mouse. they can: double click on specific objects in the point cloud to zoom into them.use a mouse-scroller or trackpad to zoom in and out of the point cloud.use both keyboard arrow keys and q, e, a, and d keys to move up, down, left, right. use keyboard keys w and s to zoom in and out. the following video demonstrates movements around the 3d point cloud. workers can hide and re-expand all side views and menus. in this gif, the side-views and menus have been collapsed.   the following gif demonstrates how a worker can label multiple objects quickly, refine painted objects using the unpaint option and then view only points that have been painted.   additional view options and features are available. see the  for a comprehensive overview of the worker ui.  worker toolsworkers can navigate through the 3d point cloud by zooming in and out, and moving in all directions around the cloud using the mouse and keyboard shortcuts. when you create a semantic segmentation job, workers have the following tools available to them:  a paint brush to paint and unpaint objects. workers paint objects by selecting a label category and then painting in the 3d point cloud. workers unpaint objects by selecting the unpaint option from the label category menu and using the paint brush to erase paint. a polygon tool that workers can use to select and paint an area in the point cloud. a background paint tool, which enables workers to paint behind objects they have already annotated without altering the original annotations. for example, workers might use this tool to paint the road after painting all of the cars on the road. view options that enable workers to easily hide or view label text, a ground mesh, and additional point attributes like color or intensity. workers can also choose between perspective and orthogonal projections. you can create a 3d point cloud labeling job using the amazon sagemaker console or api operation, . to create a labeling job for this task type you need the following:  a single-frame input manifest file. to learn how to create this type of manifest file, see . if you are a new user of ground truth 3d point cloud labeling modalities, we recommend that you review . a work team from a private or vendor workforce. you cannot use amazon mechanical turk workers for 3d point cloud labeling jobs. to learn how to create workforces and work teams, see .a label category configuration file. for more information, see . additionally, make sure that you have reviewed and satisfied the .  use one of the following sections to learn how to create a labeling job using the console or an api.  this section covers details you need to know when you create a labeling job using the amazon sagemaker api operation . this api defines this operation for all aws sdks. to see a list of language-specific sdks supported for this operation, review the see also section of .  the page, , provides an overview of the  operation. follow these instructions and do the following while you configure your request:  you must enter an arn for . use . replace  with the aws region you are creating the labeling job in.  there should not be an entry for the  parameter.  your  must end in . for example, . your input manifest file must be a single-frame manifest file. for more information, see . you specify your labels and worker instructions in a label category configuration file. see  to learn how to create this file. you need to provide a pre-defined arns for the pre-annotation and post-annotation (acs) lambda functions. these arns are specific to the aws region you use to create your labeling job. to find the pre-annotation lambda arn, refer to . use the region you are creating your labeling job in to find the correct arn. for example, if you are creating your labeling job in us-east-1, the arn will be . to find the post-annotation lambda arn, refer to . use the region you are creating your labeling job in to find the correct arn. for example, if you are creating your labeling job in us-east-1, the arn will be . the number of workers specified in  should be . automated data labeling is not supported for 3d point cloud labeling jobs. you should not specify values for parameters in . 3d point cloud semantic segmentation labeling jobs can take multiple hours to complete. you can specify a longer time limit for these labeling jobs in  (up to 7 days, or 604800 seconds).  importantif you set your take time limit to be greater than 8 hours, you must set  for your iam execution role to at least 8 hours. to see how to update this value for your iam role, see  in the iam user guide, choose your preferred method to modify the role, and then follow the steps in . to create an adjustment labeling job, use the instructions in the previous section, with the following modifications:  in your label category configuration file, you must include . use this parameter to input the  used in the labeling job that generated the annotations you want your worker to adjust.  importantwhen you create a labeling job in the console, if you did not specify a label category attribute name, the name of your job is used as the labelattributename.  for example, if your label category attribute name was  in your first labeling job, add the following to your semantic segmentation adjustment labeling job label category configuration file. to learn how to create this file, see .  you need to provide a pre-defined arns for the pre-annotation and post-annotation (acs) lambda functions. these arns are specific to the aws region you use to create your labeling job. to find the pre-annotation lambda arn, refer to . use the region you are creating your labeling job in to find the correct arn that ends with .to find the post-annotation lambda arn, refer to . use the region you are creating your labeling job in to find the correct arn that ends with .the  parameter must contain the same label categories as the previous labeling job. adding new label categories or adjusting label categories is not supported.you can follow the instructions  in order to learn how to create a 3d point cloud semantic segmentation labeling job in the amazon sagemaker console. while you are creating your labeling job, be aware of the following:  your input manifest file must be a single-frame manifest file. for more information, see . automated data labeling and annotation consolidation are not supported for 3d point cloud labeling tasks. 3d point cloud semantic segmentation labeling jobs can take multiple hours to complete. you can specify a longer time limit for these labeling jobs when you select your work team (up to 7 days, or 604800 seconds).  importantif you set your take time limit to be greater than 8 hours, you must set  for your iam execution role to at least 8 hours. to see how to update this value for your iam role, see  in the iam user guide, choose your preferred method to modify the role, and then follow the steps in . you can create an adjustment labeling job in the console by chaining a successfully completed 3d point cloud semantic segmentation labeling job. to learn more, see . when you create a 3d point cloud semantic segmentation labeling job, tasks are sent to workers. when these workers complete their tasks, their annotations are written to the amazon s3 bucket you specified when you created the labeling job. the output data format determines what you see in your amazon s3 bucket when your labeling job status () is .  if you are a new user of ground truth, see  to learn more about the ground truth output data format. to learn about the 3d point cloud object detection output data format, see .  
you can use python and r natively in amazon sagemaker notebook kernels. there are also kernels that support specific frameworks. a very popular way to get started with amazon sagemaker is to use the . it provides open source python apis and containers that make it easy to train and deploy models in amazon sagemaker, as well as examples for use with several different machine learning and deep learning frameworks. for information about using specific frameworks or how to use r in amazon sagemaker, see the following topics. languages sdks and user guides: machine learning and deep learning frameworks guides: 
use a model package to create a deployable model that you can use to get real-time inferences by creating a hosted endpoint or to run batch transform jobs. you can create a deployable model from a model package by using the amazon sagemaker console, the low-level amazon sagemaker api), or the . topics to create a deployable model from a model package (console) open the amazon sagemaker console at . choose model packages. choose a model package that you created from the list on the my model packages tab or choose a model package that you subscribed to on the aws marketplace subscriptions tab. choose create model. for model name, type a name for the model. for iam role, choose an iam role that has the required permissions to call other services on your behalf, or choose create a new role to allow amazon sagemaker to create a role that has the  managed policy attached. for information, see . for vpc, choose a amazon vpc that you want to allow the model to access. for more information, see . leave the default values for container input options and choose model package. for environment variables, provide the names and values of environment variables you want to pass to the model container. for tags, specify one or more tags to manage the model. each tag consists of a key and an optional value. tag keys must be unique per resource. choose create model. after you create a deployable model, you can use it to set up an endpoint for real-time inference or create a batch transform job to get inferences on entire datasets. for information about hosted endpoints in amazon sagemaker, see . for information about batch transform jobs, see . to use a model package to create a deployable model by using the amazon sagemaker api, specify the name or the amazon resource name (arn) of the model package as the  field of the  object that you pass to the  api. after you create a deployable model, you can use it to set up an endpoint for real-time inference or create a batch transform job to get inferences on entire datasets. for information about hosted endpoints in amazon sagemaker, see . for information about batch transform jobs, see . to use a model package to create a deployable model by using the amazon sagemaker python sdk, initialize a  object, and pass the amazon resource name (arn) of the model package as the  argument. for example: after you create a deployable model, you can use it to set up an endpoint for real-time inference or create a batch transform job to get inferences on entire datasets. for information about hosted endpoints in amazon sagemaker, see . for information about batch transform jobs, see . 
to run your own training model using the amazon sagemaker containers, build a docker container through an amazon sagemaker notebook instance.  to create an amazon sagemaker notebook instance open .  choose notebook, notebook instances, and create notebook instance.  on the create notebook instance page, provide the following information:  for notebook instance name, enter runscriptnotebookinstance. for notebook instance type, choose ml.t2.medium. for iam role, choose create a new role. choose create a new role. on the create an iam role page, choose specific s3 buckets, specify an s3 bucket named sagemaker-run-script, and then choose create role. amazon sagemaker creates an iam role named . for example, . note that the execution role naming convention uses the date and time when the role was created, separated by a . record the role name because you'll need it later. for root access, choose enabled. choose create notebook instance.  it takes a few minutes for amazon sagemaker to launch a ml compute instance—in this case, a notebook instance—and attach an ml storage volume to it. the notebook instance has a preconfigured jupyter notebook server and a set of anaconda libraries. for more information, see the  api.  choose runscriptnotebookinstance, scroll down to permissions and encryption section, and record the iam role arn number in a notepad. the iam role arn number looks like the following:   when the status of the notebook instance is , from actions, choose open jupyterlab. to create and upload the dockerfile and python training scripts on the left top corner, choose new folder icon. name the folder .  create a  text file.  in the  directory, choose new launcher icon (+ mark) on the top left corner. in the right panel, select text file under the other section.  copy and paste the following  code into your newly created text file.  the dockerfile script performs the following tasks:  downloads the tensorflow library used to run the python script.  contains the common functionality necessary to create a container compatible with amazon sagemaker.  copies the script to the location inside the container that is expected by amazon sagemaker. the script must be located in this folder. defines train.py as the name of the entrypoint script that is located in the /opt/ml/code folder for the container. this is the only environmental variable that you must specify when, you are using your own container.make sure you rename the text file name . on the left directory navigation, the text file name is automatically set as . open a context manu (pop-up menu) for the file, choose rename, and rename the file as  without  extension. make sure you save the file by hitting  or . in the same way, create a  file with your own training script. you can test using the following example  script.  to build the container the jupyter notebook instance opens in the sagemaker directory. the docker build command must be run from the  directory you created, in this case . to open a new terminal window on your notebook instance, choose new launch icon, select terminal under other section, and run the following command to change into the  directory. this returns your current directory as follows.  to build the docker container, run the following docker build command including the final period. noteif you get an error that it can't find the dockerfile, make sure the files have correct names and are saved. remember that  is looking for a file called  in the current directory. if you named it something else, you can pass in the filename manually with the  argument. for example:    to test the container to test the container locally to the notebook instance, open a jupyter notebook. choose new launcher and select notebook in  framework.  copy and paste the following example script to the notebook code cell to configure a amazon sagemaker estimator. replace the  value with the iam role arn number you recorded while configuring the notebook instance. the arn should look like: . run the code cell. this test outputs the training environment configuration, the values used for the environmental variables, the source of the data, and the loss and accuracy obtained during training. after you successfully run this local mode test, you can push the image to amazon elastic container registry and use it to run training jobs. for examples that show how to complete these tasks, see  to clean up resources when done with the get started example open , select the notebook instance runscriptnotebookinstance, choose actions, and stop the instance. after the instance stops, delete will be activated. delete the notebook instance.  open  and delete the bucket that you created for storing model artifacts and the training dataset.  open  and delete the iam role. if you created permission policies, you can delete them, too.  notethe docker container shuts down automatically after it has run. you don't need to delete it. 
to specify the total number of data points to be sampled from the training dataset, use the parameter. for example, if the initial dataset has 1,000 data points and the  is set to 100, where the total number of instances is 2, each worker would sample 50 points. a total set of 100 data points would be collected. sampling runs in linear time with respect to the number of data points.  the current implementation of the k-nn algorithm has two methods of dimension reduction. you specify the method in the  hyperparameter. the  method specifies a random projection, which uses a linear projection using a matrix of random signs, and the  method specifies a fast johnson-lindenstrauss transform, a method based on the fourier transform. both methods preserve the l2 and inner product distances. the  method should be used when the target dimension is large and has better performance with cpu inference. the methods differ in their computational complexity. the  method requires o(ndk) time to reduce the dimension of a batch of n points of dimension d into a target dimension k. the  method requires o(nd log(d)) time, but the constants involved are larger. using dimension reduction introduces noise into the data and this noise can reduce prediction accuracy. during inference, the algorithm queries the index for the k-nearest-neighbors of a sample point. based on the references to the points, the algorithm makes the classification or regression prediction. it makes the prediction based on the class labels or values provided. k-nn provides three different types of indexes: a flat index, an inverted index, and an inverted index with product quantization. you specify the type with the  parameter. when the k-nn algorithm finishes training, it serializes three files to prepare for inference.  model_algo-1: contains the serialized index for computing the nearest neighbors.model_algo-1.labels: contains serialized labels (np.float32 binary format) for computing the predicted label based on the query result from the index.model_algo-1.json: contains the json-formatted model metadata which stores the  and  hyper-parameters from training for inference along with other relevant state.with the current implementation of k-nn, you can modify the metadata file to change the way predictions are computed. for example, you can change  to 10 or change  to regressor. 
a factorization machine is a general-purpose supervised learning algorithm that you can use for both classification and regression tasks. it is an extension of a linear model that is designed to capture interactions between features within high dimensional sparse datasets economically. for example, in a click prediction system, the factorization machine model can capture click rate patterns observed when ads from a certain ad-category are placed on pages from a certain page-category. factorization machines are a good choice for tasks dealing with high dimensional sparse datasets, such as click prediction and item recommendation. notethe amazon sagemaker implementation of factorization machines considers only pair-wise (2nd order) interactions between features. topics the factorization machine algorithm can be run in either in binary classification mode or regression mode. in each mode, a dataset can be provided to the test channel along with the train channel dataset. the scoring depends on the mode used. in regression mode, the testing dataset is scored using root mean square error (rmse). in binary classification mode, the test dataset is scored using binary cross entropy (log loss), accuracy (at threshold=0.5) and f1 score (at threshold =0.5). for training, the factorization machines algorithm currently supports only the  format with  tensors. because their use case is predominantly on sparse data,  is not a good candidate. both file and pipe mode training are supported for recordio-wrapped protobuf. for inference, factorization machines support the  and  formats.  for the binary classification problem, the algorithm predicts a score and a label. the label is a number and can be either  or . the score is a number that indicates how strongly the algorithm believes that the label should be . the algorithm computes score first and then derives the label from the score value. if the score is greater than or equal to 0.5, the label is .for the regression problem, just a score is returned and it is the predicted value. for example, if factorization machines is used to predict a movie rating, score is the predicted rating value.please see  for more details on training and inference file formats. the amazon sagemaker factorization machines algorithm is highly scalable and can train across distributed instances. we recommend training and inference with cpu instances for both sparse and dense datasets. in some circumstances, training with one or more gpus on dense data might provide some benefit. training with gpus is available only on dense data. use cpu instances for sparse data. for a sample notebook that uses the amazon sagemaker factorization machine learning algorithm to analyze the images of handwritten digits from zero to nine in the mnist dataset, see . for instructions how to create and access jupyter notebook instances that you can use to run the example in amazon sagemaker, see . once you have created a notebook instance and opened it, select the sagemaker examples tab to see a list of all the amazon sagemaker samples. the topic modeling example notebooks using the ntm algorithms are located in the introduction to amazon algorithms section. to open a notebook, click on its use tab and select create copy. 
neo is a capability of amazon sagemaker that enables machine learning models to train once and run anywhere in the cloud and at the edge. ordinarily, optimizing machine learning models for inference on multiple platforms is extremely difficult because you need to hand-tune models for the specific hardware and software configuration of each platform. if you want to get optimal performance for a given workload, you need to know the hardware architecture, instruction set, memory access patterns, and input data shapes among other factors. for traditional software development, tools such as compilers and profilers simplify the process. for machine learning, most tools are specific to the framework or to the hardware. this forces you into a manual trial-and-error process that is unreliable and unproductive. neo automatically optimizes gluon, keras, mxnet, pytorch, tensorflow, tensorflow-lite, and onnx models for inference on android, linux, and windows machines based on processors from ambarella, arm, intel, nvidia, nxp, qualcomm, texas instruments, and xilinx. neo is tested with computer vision models available in the model zoos across the frameworks.  neo can optimize models with parameters either in fp32 or quantized to int8 or fp16 bit-width. neo consists of a compiler and a runtime. first, the neo compilation api reads models exported from various frameworks. it converts the framework-specific functions and operations into a framework-agnostic intermediate representation. next, it performs a series of optimizations. then it generates binary code for the optimized operations, writes them to a shared object library, and saves the model definition and parameters into separate files. neo also provides a runtime for each target platform that loads and executes the compiled model. you can create a neo compilation job from either the amazon sagemaker console, aws command line interface (aws cli), python notebook, or the amazon sagemaker sdk. with a few cli commands, an api invocation, or a few clicks, you can convert a model for your chosen platform. you can deploy the model to an amazon sagemaker endpoint or on an aws iot greengrass device quickly. amazon sagemaker provides neo container images for amazon sagemaker xgboost and image classification models, and supports amazon sagemaker-compatible containers for your own compiled models. neo supports image classification models exported as frozen graphs from tensorflow, mxnet, or pytorch, and xgboost models. neo is available in the following  where amazon sagemaker is supported:  asia pacific (hong kong, mumbai, seoul, singapore, sydney, tokyo)canada (central)china (beijing, ningxia)eu (frankfurt, ireland, london, paris, stockholm)middle east (bahrain)north america (n. virginia, ohio, oregon, n. california)south america (sao paulo)aws govcloud (us-gov-west)the image classification model files need to be formatted as a tar.gz file that includes additional files that depend on the type of framework. : neo supports saved models and frozen models. for saved models, neo expects one .pb or one .pbtxt file and a variables directory that contains variables.  for frozen models, neo expect only one .pb or .pbtxt file. : neo expects one .h5 file containing the model definition.: neo expects one .pth file containing the model definition.: neo expects one symbol file (.json) and one parameter file (.params).: neo expects one xgboost model file (.model) where the number of nodes in a tree can't exceed 2^31.: neo expects one .onnx file.to find a full list of neo-supported frameworks and operators, see . topics for sample notebooks that uses amazon sagemaker neo to train, compile, optimize, and deploy machine learning models to make inferences, see:  for instructions on how to run these example notebooks in amazon sagemaker, see . if you need instructions on how to create a notebook instance to run these examples, see amazon sagemaker, see . to navigate to the relevant example in your notebook instance, choose the amazon sagemaker examples tab to see a list of all of the amazon sagemaker samples. to open a notebook, choose its use tab, then choose create copy. 
after creating and validating your algorithm or model in amazon sagemaker, list your product on aws marketplace. the listing process makes your products available in the aws marketplace and the amazon sagemaker console.  to list products on aws marketplace, you must be a registered seller. to register, use the self-registration process from the aws marketplace management portal (ammp). for information, see  in the user guide for aws marketplace providers. when you start the product listing process from the amazon sagemaker console, we check your seller registration status. if you have not registered, we direct you to do so. to start the listing process, do one of the following: from the amazon sagemaker console, choose the product, choose actions, and choose publish new ml marketplace listing. this carries over your product reference, the amazon resource name (arn), and directs you to the ammp to create the listing.go to , manually enter the amazon resource name (arn), and start your product listing. this process carries over the product metadata that you entered when creating the product in amazon sagemaker. for an algorithm listing, the information includes the supported instance types and hyperparameters. in addition, you can enter a product description, promotional information, and support information as you would with other aws marketplace products.
after training a model, evaluate it to determine whether its performance and accuracy allow you to achieve your business goals. you might generate multiple models using different methods and evaluate each. for example, you could apply different business rules for each model, and then apply various measures to determine each model's suitability. you might consider whether your model needs to be more sensitive than specific (or vice versa).  you can evaluate your model using historical data (offline) or live data: offline testing—use historical, not live, data to send requests to the model for inferences.    deploy your trained model to an alpha endpoint, and use historical data to send inference requests to it. to send the requests, use a jupyter notebook in your amazon sagemaker notebook instance and either the aws sdk for python (boto) or the high-level python library provided by amazon sagemaker.   online testing with live data—amazon sagemaker supports deploying multiple models (called production variants) to a single amazon sagemaker endpoint. you configure the production variants so that a small portion of the live traffic goes to the model that you want to validate. for example, you might choose to send 10% of the traffic to a model variant for evaluation. after you are satisfied with the model's performance, you can route 100% traffic to the updated model. for more information, see articles and books about how to evaluate models, for example, .  options for offline model evaluation include: validating using a "holdout set"—machine learning practitioners often set aside a part of the data as a "holdout set." they don’t use this data for model training. with this approach, you evaluate how well your model provides inferences on the holdout set. you then assess how effectively the model generalizes what it learned in the initial training, as opposed to using model "memory." this approach to validation gives you an idea of how often the model is able to infer the correct answer.    in some ways, this approach is similar to teaching elementary school students. first, you provide them with a set of examples to learn, and then test their ability to generalize from their learning. with homework and tests, you pose problems that were not included in the initial learning and determine whether they are able to generalize effectively. students with perfect memories could memorize the problems, instead of learning the rules.   typically, the holdout dataset is of 20-30% of the training data.   k-fold validation—in this validation approach, you split the example dataset into k parts. you treat each of these parts as a holdout set for k training runs, and use the other k-1 parts as the training set for that run. you produce k models using a similar process, and aggregate the models to generate your final model. the value k is typically in the range of 5-10.
aws addresses many common use cases by providing standalone iam policies that are created and administered by aws. these aws managed policies grant necessary permissions for common use cases so that you can avoid having to investigate which permissions are needed. for more information, see  in the iam user guide.  the following aws managed policies, which you can attach to users in your account, are specific to amazon sagemaker: amazonsagemakerreadonly – grants read-only access to amazon sagemaker resources. amazonsagemakerfullaccess – grants full access to amazon sagemaker resources and the supported operations. (this does not provide unrestricted s3 access, but supports buckets/objects with specific sagemaker tags.)the following aws managed policies can also be attached to users in your account:  – grants all actions for all aws services and for all resources in the account.  – grants a wide range of permissions to cover most of the use cases (primarily for analytics and business intelligence) encountered by data scientists.you can review these permissions policies by signing in to the iam console and searching for them. you can also create your own custom iam policies to allow permissions for amazon sagemaker actions and resources as you need them. you can attach these custom policies to the iam users or groups that require them.  
amazon sagemaker ground truth enables you to label highly sensitive data by allowing you to stay in control of your data and employing security best practices. use the topics on this page to learn about ground truth security features.  by default, ground truth encrypts data in an internal database used by the service and in amazon s3 buckets with amazon-owned customer master key (cmk).  optionally, you can provide an aws key management service (aws kms) key id when you create a labeling job, which ground truth uses to encrypt your output data. if you use a kms key id or an alias of your master key, your amazon sagemaker execution role must include permissions to call . to learn how to add this permission to an execution role, see .  if you don't provide a kms key id, amazon sagemaker uses the default aws kms key for amazon s3 for your role's account. amazon sagemaker uses server-side encryption with kms-managed keys for . for more information, see .  when you create a labeling job with automated labeling using the  api operation, you have the option to encrypt the storage volume attached to the ml compute instances that run the training job. to add encryption to your storage volume, use the parameter  to input a aws kms key. for more information about this parameter, see . if you use a kms key id or key arn, your amazon sagemaker execution role must include permissions to call . to learn how to add this permission to an execution role, see .  
noteamazon sagemaker studio is available only in specific aws regions. to view the list of supported regions, see . this procedure describes how to onboard to amazon sagemaker studio using aws sso authentication. for information on how to onboard using aws identity and access management (iam) authentication, see  or . to onboard to studio using aws sso open the . choose amazon sagemaker studio at the top left of the page. on the amazon sagemaker studio control panel, under get started, choose standard setup. for authentication method, choose aws single sign-on (sso). a message tells you whether you have an aws sso account in an aws region supported by amazon sagemaker studio. if you don't have an aws sso account in a supported region, you must create an aws sso account in a supported region before proceeding. to continue to onboard without creating a new aws sso account, choose the aws identity and access management (iam) authentication method or the quick start procedure, which also uses iam. for information about setting up aws sso for use with studio, see . to continue with sso, under permission, for execution role for all users, choose an option from the role selector. if you choose create a new role, the create an iam role dialog opens: for s3 buckets you specify, specify additional s3 buckets that users of your notebooks can access. if you don't want to add access to more buckets, choose none.choose create role. amazon sagemaker creates a new iam role with the  policy attached.choose submit. on the amazon sagemaker studio control panel, under studio summary, the status shows as pending while studio creates an amazon sagemaker studio application in your aws sso domain. when status changes to ready, the assign users button is enabled. choose assign users. the assign users page opens and displays a list of your organization's members. to assign users access to amazon sagemaker studio, choose the check box next to their user name and choose assign users.  send each assigned user the studio address link shown under studio summary. your aws sso users go to this address to access studio. to access studio after onboarding after you are given access to studio, you are sent an email inviting you to create a password and activate your aws sso account. the email also contains the url to sign in to studio. after you activate your account, go to the studio url, sign in, and wait for your user profile to be created. on subsequent visits, you only need to wait for studio to load. bookmark the studio url. the url is also available in the studio control panel. for information about using amazon sagemaker studio, see . 
a styled button that represents some action. the following attributes are supported by this element. the following is an example of a template that uses the  element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  a boolean switch that, if present, displays the button as disabled and prevents clicks. a switch that either submits its parent  element, if set to "submit", or resets its parent <crowd-form> element, if set to "reset". the url to an online resource. use this property if you need a link styled as a button. a string that specifies the icon to be displayed next to the button's text. the string must be the name of an icon from the open-source  set, which is pre-loaded. for example, to insert the  iron-icon, use the following: the icon is positioned to either the left or the right of the text, as specified by the icon-align attribute. to use a custom icon see icon-url. the left or right position of the icon relative to the button's text. the default is "left". a url to a custom image for the icon. a custom image can be used in place of a standard icon that is specified by the icon attribute. a boolean switch that, if present, displays the button as being in a loading state. this attribute has precedence over the disabled attribute if both attributes are present. when you use the  attribute to make the button act as a hyperlink to a specific url, the  attribute optionally targets a frame or window where the linked url should load. the general style of the button. use "primary" for primary buttons, "normal" for secondary buttons, "link" for tertiary buttons, or "icon" to display only the icon without text. this element has the following parent and child elements. parent elements: child elements: nonefor more information, see the following. 
all inter-network data in transit supports tls 1.2 encryption. amazon sagemaker ensures that machine learning (ml) model artifacts and other system artifacts are encrypted in transit and at rest. requests to the amazon sagemaker api and console are made over a secure (ssl) connection. you pass aws identity and access management roles to amazon sagemaker to provide permissions to access resources on your behalf for training and deployment. you can use encrypted amazon s3 buckets for model artifacts and data, as well as pass a aws kms key to amazon sagemaker instances to encrypt the attached ml storage volumes. some intra-network data in-transit (inside the service platform) is unencrypted. this includes: command and control communications between the service control plane and training job instances (not customer data).communications between nodes in distributed processing jobs (intra-network).communications between nodes in distributed training jobs (intra-network).there are no inter-node communications for batch processing. you can choose to encrypt internode training communications. enabling inter-container traffic encryption can increase training time, especially if you are using distributed deep learning algorithms. for affected algorithms, adding this additional level of security also increases cost. the training time for most amazon sagemaker built-in algorithms, such as xgboost, deepar, and linear learner, typically aren't affected. fips validated endpoints are available for the amazon sagemaker api and request router for hosted models (runtime). for information about fips compliant endpoints, see .  
a subtle notification that temporarily appears on the display. only one crowd-toast is visible. the following is an example of a liquid template that uses the  element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  the following attributes are supported by this element. a number that specifies the duration, in milliseconds, that the notification appears on the screen. the text to display in the notification. this element has the following parent and child elements. parent elements: child elements: nonefor more information, see the following.  
automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many jobs that test a range of hyperparameters on your dataset. you choose the tunable hyperparameters, a range of values for each, and an objective metric. you choose the objective metric from the metrics that the algorithm computes. automatic model tuning searches the hyperparameters chosen to find the combination of values that result in the model that optimizes the objective metric. for more information about model tuning, see . the sequence to sequence algorithm reports three metrics that are computed during training. choose one of them as an objective to optimize when tuning the hyperparameter values. you can tune the following hyperparameters for the amazon sagemaker sequence to sequence algorithm. the hyperparameters that have the greatest impact on sequence to sequence objective metrics are: , , , , and . 
pca is an unsupervised machine learning algorithm that attempts to reduce the dimensionality (number of features) within a dataset while still retaining as much information as possible. this is done by finding a new set of features called components, which are composites of the original features that are uncorrelated with one another. they are also constrained so that the first component accounts for the largest possible variability in the data, the second component the second most variability, and so on. in amazon sagemaker, pca operates in two modes, depending on the scenario:  regular: for datasets with sparse data and a moderate number of observations and features.randomized: for datasets with both a large number of observations and features. this mode uses an approximation algorithm. pca uses tabular data.  the rows represent observations you want to embed in a lower dimensional space. the columns represent features that you want to find a reduced approximation for. the algorithm calculates the covariance matrix (or an approximation thereof in a distributed manner), and then performs the singular value decomposition on this summary to produce the principal components.  topics for training, pca expects data provided in the train channel, and optionally supports a dataset passed to the test dataset, which is scored by the final algorithm. both  and  formats are supported for training. you can use either file mode or pipe mode to train models on data that is formatted as  or as . for inference, pca supports , , and . results are returned in either  or  format with a vector of "projections." for more details on training and inference file formats, see the  and the . for more information on input and output file formats, see  for inference and the . pca supports both gpu and cpu computation. which instance type is most performant depends heavily on the specifics of the input data. for a sample notebook that shows how to use the amazon sagemaker principal component analysis algorithm to analyze the images of handwritten digits from zero to nine in the mnist dataset, see . for instructions how to create and access jupyter notebook instances that you can use to run the example in amazon sagemaker, see . once you have created a notebook instance and opened it, select the sagemaker examples tab to see a list of all the amazon sagemaker samples. the topic modeling example notebooks using the ntm algorithms are located in the introduction to amazon algorithms section. to open a notebook, click on its use tab and select create copy. 
binary classification regression binary classification regression binary classification regression 
the following sections describe how to perform common tasks in amazon sagemaker studio. for an overview of the studio interface, see . topics amazon sagemaker studio can open only files listed in the studio file browser. the following procedure shows how to add files to the browser. to upload files in the left sidebar, choose the file browser icon ( ) to display the file browser. in the file browser, choose the upload files icon ( ) to open the dialog. in file upload, browse to the files, choose the files you want to upload, and then choose open. double-click a file to open the file in a new tab in sagemaker studio. for this example, we connect to a local clone of the  repository (repo). studio can't connect to a remote repo. to connect to a repo on the top menu, choose file > new > terminal. at the command prompt, run the following command and wait for the repo to finish cloning.  in the left sidebar, choose the git icon ( ). choose go find a repo. in file browser, the cloned repo appears in the list. double click the repo to open it. choose the git icon to view a list of git tools. when you create an amazon sagemaker autopilot experiment, amazon sagemaker analyzes your data and creates a notebook with candidate model definitions. if you choose to run the complete experiment, amazon sagemaker trains and tunes these models on your behalf. you can view statistics while the experiment is running. afterwards, you can compare trials and delve into the details. to create an autopilot experiment in the upper-left corner of sagemaker studio, choose amazon sagemaker studio to open the studio landing page. under build models automatically, choose create autopilot experiment. on the create experiment tab, enter the required information. choose create experiment. for more information, see . an experiment consists of multiple trials with a related objective. a trial consists of one or more trial components, such as a data preprocessing job and a training job. you can use the experiments browser to view detailed information about these entities. in the experiments browser, experiments, trials, and trial components are accessed as a hierarchy. double-click an entity to drill down the hierarchy. use the breadcrumb above the list to go up the hierarchy. to view an experiment in the left sidebar, choose the sagemaker experiment list icon ( ). in the experiments and trials browser, double-click an experiment to display the trials in the experiment. double-click a trial to display the components that make up the trial. double-click a component to open the describe trial component tab. choose the following headers to see available information about the trial component. charts – build your own chartsmetrics – metrics that are logged by a  during a trial runparameters – hyperparameter values and instance informationartifacts – amazon s3 storage locations for the input dataset and the output modelaws settings – job name, arn, status, creation time, training time, billable time, instance information, and othersdebugger – a list of debugger rules and any issues foundtrial mappingsfor more information, including a tutorial, see . when you stop a training job, its status changes to .  an algorithm can delay termination in order to save model artifacts after which the job status changes to . for more information, see the   api. to stop a training job follow the  procedure on this page until you open the describe trial component tab. at the upper-right side of the tab, choose stop training job. the status at the top left of the tab changes to stopped. to view the training time and billing time, choose aws settings. the first time a user on your team opens amazon sagemaker studio, amazon sagemaker creates a domain for the team. the domain includes an amazon elastic file system (amazon efs) volume with home directories for each of your users. notebook files and data files are stored in these directories. users can't access each other's home directories. importantdon't delete the amazon efs volume. if you delete it, the domain will no longer function and all of your users will lose their work. to find your amazon efs volume from the amazon sagemaker studio control panel, under studio summary, find the studio id. the id will be in the following format: . pass the , which is the same as , to the  api. in the response from , note the value in the  field. open the . under file systems, scroll through the list to find the  value, which is the same as the file system id. to provide feedback at the upper-right of sagemaker studio, choose feedback. choose a smiley emoji to let us know how satisfied you are with sagemaker studio and add any feedback you'd care to share with us. decide whether to share your identity with us, then choose submit. when you update amazon sagemaker studio to the latest release, amazon sagemaker shuts down and restarts the jupyterserver app. any unsaved notebook information is lost in the process. the user data in the amazon efs volume isn't touched. after the jupyterserver app is restarted, you must reopen studio through the amazon sagemaker studio control panel. to update studio (optional) choose amazon sagemaker studio on the top-left of studio to open the landing page. the studio version is shown on the bottom-left of the page. on the top menu, choose file then shut down. choose one of the following options: shutdown server – shuts down the jupyterserver app. terminal sessions, kernel sessions, sagemaker images, and instances aren't shut down. these resources continue to accrue charges.shutdown all – shuts down all apps, terminal sessions, kernel sessions, sagemaker images, and instances. these resources no longer accrue charges.reopen studio. 
you can create a labeling job in the amazon sagemaker console and by using an aws sdk in your preferred language to run . after a labeling job has been created, you can track worker metrics (for private workforces) and your labeling job status using . after you have chosen your task type, use the topics on this page to learn how to create a labeling job. if you are a new ground truth user, we recommend that you start by walking through the demo in . topics 
after you create your labeling job, you see a list of all the jobs that you have created. you can use this list to monitor that status of your labeling jobs. the list has the following fields: name – the name that you assigned the job when you created it.status – the completion status of the job. the status can be one of complete, failed, in progress, or stopped.labeled objects/total – shows the total number of objects in the labeling job and how many of them have been labeled.creation time – the date and time that you created the job.you can also clone, chain, or stop a job. select a job and then select one of the following from the actions menu: clone – creates a new labeling job with the configuration copied from the selected job. you can clone a job when you want to change to the job and run it again. for example, you can clone a job that was sent to a private workforce so that you can send it to the amazon mechanical turk workforce. or you can clone a job to rerun it against a new dataset stored in the same location as the original job.chain – creates a new labeling job that can build upon the data and models (if any) of a stopped, failed, or completed job. for more information about the use cases and how to use it, see .stop – stops a running job. you cannot restart a stopped job. you can clone a job to start over or chain the job to continue from where it left off. labels for any already labeled objects are written to the output file location. for more information, see .
use the procedures in this topic when you need to: create a topic that you want an existing work team to subscribe to.create a topic before you've created a work team.create or modify the work team with an api call, and you need to specify a topic amazon resource name (arn).if you create a work team using the console, the console provides an option to create a new topic for the team so that you don't have to perform these steps. the steps for creating amazon sns topics for work team notifications is similar to the steps in  in the amazon sns developer guide, with one significant addition—you must add an access policy so that amazon sagemaker can publish messages to the topic on your behalf. to add the policy when you create the topic open the amazon sns console at . in create topic, enter the name of your topic and then choose next steps. in access policy, choose advanced. in the json editor, find the  property, which displays the topic's arn. copy the  arn value. before the final closing brace (), add the following policy. create the topic. after you create the topic, it appears in your topics summary screen. for more information about creating topics, see  in the amazon sns developer guide. if you subscribe a work team to a topic after you've already created the work team, the individual work team members who were added to the team when the work team was created are not automatically subscribed to the topic. for information about subscribing workers' email addresses to the topic, see  in the amazon sns developer guide. the only situation where workers are automatically subscribed to your topic is when you create or import an amazon cognito user group at the time that you create a work team and you set up the topic subscription when you create that work team. for more information about creating and managing your workteams with amazon cognito, see . 
you can shut down individual resources, including notebooks, terminals, kernels, and images, or you can shut down all resources in one of these categories at the same time. topics you can shut down an open notebook from the amazon sagemaker studio file menu or from the sessions and images dialog box. notewhen you shut down a notebook, any unsaved information in the notebook is lost. the notebook is not deleted. to shut down an open notebook from the file menu save the notebook contents by choosing the disk icon on the left of the notebook menu. choose file then close and shutdown notebook. choose ok. the running terminals, kernels, and images browser consists of three sections: terminal sessions, kernel sessions, and images. each section lists all resources of that type. you can shut down each resource individually or shut down all resources in a section at the same time. when you choose shut down all in a section, the following are shut down: terminal sessions – all terminals are shut down and their tabs deleted.kernel sessions – all kernels (notebooks and consoles).running images – all sagemaker images, kernel sessions, sagemaker image terminals, and instances. system terminals aren't shut down.to shut down terminal sessions, kernel sessions, and images in the left sidebar, choose the running terminals, kernels, and images icon ( ). do either of the following: to shut down a specific resource, choose shut down on the same row as the resource. terminal sessions and kernel sessions are immediately shut down. for running images, a confirmation dialog is displayed listing what will be shut down. to shut down all resources in a section, choose shut down all. a confirmation dialog is displayed listing what will be shut down. choose shut down all to proceed. 
automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many jobs that test a range of hyperparameters on your dataset. you choose the tunable hyperparameters, a range of values for each, and an objective metric. you choose the objective metric from the metrics that the algorithm computes. automatic model tuning searches the hyperparameters chosen to find the combination of values that result in the model that optimizes the objective metric. for more information about model tuning, see . the object detection algorithm reports on a single metric during training: . when tuning a model, choose this metric as the objective metric. tune the amazon sagemaker object detection model with the following hyperparameters. the hyperparameters that have the greatest impact on the object detection objective metric are: , , and . 
for a sample notebook that shows you how to use a built-in rule when running a training job with a tensorflow model in amazon sagemaker, see .  the following code sample shows how to run an amazon sagemaker built-in rule using the  method from the . the first argument to this method is the base configuration associated with the rule. the built-in rules are configured with the  populated for all of the built-in rules. for details, see . there are provided default values for the parameters that are not required, but you have the option to modify these default values when using the built-in rules. you do not need to specify a pre-built docker image when using the , as the  handle this task.  you can use these rules or write your own rules using the amazon sagemaker debugger apis. you can also analyze raw tensor data without using rules in, for example, an amazon sagemaker notebook, using debugger's full set of apis.  
to protect your model-building data and model artifacts, you can use encrypted amazon simple storage service (amazon s3) buckets. to encrypt the machine learning (ml) storage volume that is attached to notebooks, processing jobs, training jobs, hyperparameter tuning jobs, batch transform jobs, and endpoints, you can pass a aws key management service (aws kms) key to amazon sagemaker . if you don't specify a kms key, amazon sagemaker encrypts storage volumes with a transient key and discards it immediately after encrypting the storage volume. you can use an aws managed kms key to encrypt all instance os volumes. you can encrypt all ml data volumes for all amazon sagemaker instances with a kms key that you specify. ml storage volumes are mounted as follows: notebooks - processing -  and  training -  and batch -  and endpoints -  and  processing, batch transform, and training job containers and their storage are ephemeral in nature. when the job completes, output is uploaded to amazon s3 using aws kms encryption (with an optional kms key you specify) and the instance is torn down.  importantsensitive data that needs to be encrypted with a kms key for compliance reasons should be stored in the ml storage volume or in amazon s3, both of which can be encrypted using a kms key you specify.  when you open a notebook instance, amazon sagemaker saves it and any files associated with it in the amazon sagemaker folder in the ml storage volume by default. when you stop a notebook instance, amazon sagemaker creates a snapshot of the ml storage volume. any customizations to the operating system of the stopped instance, such as installed custom libraries or operating system level settings, are lost. consider using a lifecycle configuration to automate customizations of the default notebook instance. when you terminate an instance, the snapshot and the ml storage volume are deleted. any data that you need to persist beyond the lifespan of the notebook instance should be transferred to an amazon s3 bucket. notecertain nitro-based amazon sagemaker instances include local storage, depending on the instance type. local storage volumes are encrypted using a hardware module on the instance. you can't use a kms key on an instance type with local storage. for a list of instance types that support local instance storage, see . for more information about storage volumes on nitro-based instances, see .for more information about local instance storage encryption, see . 
the following table contains the hyperparameters for the linear learner algorithm. these are parameters that are set by users to facilitate the estimation of model parameters from data. the required hyperparameters that must be set are listed first, in alphabetical order. the optional hyperparameters that can be set are listed next, also in alphabetical order. when a hyperparameter is set to , amazon sagemaker will automatically calculate and set the value of that hyperparameter.  
this topic describes how amazon sagemaker secures connections from the service to other locations. internetwork communications support tls 1.2 encryption between all components and clients. instances can be connected to customer vpc, providing access to s3 vpc endpoints or customer repositories. internet egress can be managed through this interface by the customer if service platform internet egress is disabled for notebooks. for training and hosting, egress through the service platform is not available when connected to the customer's vpc. by default, api calls made to published endpoints traverse the public network to the request router. amazon sagemaker supports amazon virtual private cloud interface endpoints powered by aws privatelink for private connectivity between the customer's vpc and the request router to access hosted model endpoints. for information about amazon vpc, see  
amazon sagemaker provides prebuilt docker images that include deep learning framework libraries and other dependencies needed for training and inference. with the , you can train and deploy models using these popular deep learning frameworks. for instructions on installing and using the sdk, see .  the following table provides links to the github repositories that contain the source code and dockerfiles for each framework and for tensorflow and mxnet serving. the instructions linked are for using the python sdk to run training algorithms and host models on amazon sagemaker. if you are not using the  and one of its estimators to retrieve the pre-built images, you have to retrieve them yourself. the amazon sagemaker prebuilt docker images are stored in amazon elastic container registry (amazon ecr). for a complete list of the available pre-built docker images, see . amazon sagemaker also provides prebuilt docker images for scikit-learn and spark ml. for information about docker images that enable using scikit-learn and spark ml solutions in amazon sagemaker, see . you can use prebuilt containers to deploy your custom models or models that have been trained in a framework other than amazon sagemaker. for an overview of the process of bringing the trained model artifacts into amazon sagemaker and hosting them at an endpoint, see . you can customize these prebuilt containers or extend them to handle any additional functional requirements for your algorithm or model that the prebuilt amazon sagemaker docker image doesn't support. for an example, see .  
you can use amazon augmented ai (amazon a2i) to incorporate a human review (human loop) into your custom machine learning workflow. this is known as a custom task type. to create a human loop using a flow definition, integrate it into your application, and monitor the results complete the amazon a2i . note the following:  the path the amazon simple storage service (amazon s3) bucket or buckets where you will store your input and output data. the amazon resource name (arn) of an aws identity and access management (iam) role with required permissions attached. (optional) if you plan to use a private workforce, the arn of your workforce. using html elements, create a custom worker template which amazon a2i uses to generate your worker task ui. to learn how to create a custom template, see .  use the custom worker template from step 2 to generate a worker task template in the amazon sagemaker console. to learn how, see . in the next step you will create a flow definition: if you want to create a flow definition using the amazon sagemaker api, note the arn of this worker task template for the next step.if you are creating a flow definition using the console, your template will automatically appear in worker task template section when you choose create human review workflow.when creating your flow definition, provide the path to your s3 buckets, your iam role arn, and your worker template.  learn how to create a flow definition using the amazon sagemaker  api: . learn how to create a flow definition using the amazon sagemaker console: .configure your human loop using the . to learn how, see .  to control when human reviews are initiated in your application, specify conditions under which  is called in your application. human loop activation conditions, such as confidence thresholds that trigger the human loop, are not available when using amazon a2i with custom task types. every  invocation results in a human review. once you have started a human loop, you can manage and monitor your loops using the amazon augmented ai runtime api and amazon eventbridge (also known as amazon cloudwatch events). to learn more, see . for an end-to-end example that demonstrates how to integrate a human review loop into a custom machine learning workflow using augmented ai, you can use a jupyter notebook from this  in an amazon sagemaker notebook instance.  use amazon amazon augmented ai (amazon a2i) integration with amazon comprehend [example] in the following procedure to integrate a human review look into an amazon comprehend sentimental anlaysis workflow. use amazon amazon augmented ai (amazon a2i) integration with amazon sagemaker hosted endpoint [example] in the following procedure to learn how to integrate a human review loop into a machine learning workflow that uses a hosted amazon sagemaker endpoint to deliver real-time predictions.to use an augmented ai custom task type sample notebook in an amazon sagemaker notebook if you do not have an active amazon sagemaker notebook instance, create one by following the instructions in . when your notebook instance is active, choose open jupyterlab to the right of the notebook instance's name. it may take a few moments for jupyterlab to load.  choose the  icon to clone a github repository into your workspace.  enter the  repository https url.  choose clone. open the notebook that you would like to run.  follow the instructions in the notebook to configure your flow definition and human loop and run the cells.  to avoid incurring unnecessary charges, when you are done with the demo stop and delete your notebook instance in addition to any amazon s3 buckets, iam roles, and cloudwatch events resources created during the walkthrough. 
you can create algorithms and model packages as resources in your amazon sagemaker account, and you can find and subscribe to algorithms and model packages on aws marketplace. use algorithms to: run training jobs. for information, see .run hyperparameter tuning jobs. for information, see .create model packages. after you use an algorithm resource to run a training job or a hyperparameter tuning job, you can use the model artifacts that these jobs output along with the algorithm to create a model package. for information, see . noteif you subscribe to an algorithm on aws marketplace, you must create a model package before you can use it to get inferences by creating hosted endpoint or running a batch transform job. use model packages to: create models that you can use to get real-time inference or run batch transform jobs. for information, see .create hosted endpoints to get real-time inference. for information, see .create batch transform jobs. for information, see .topics 
topics when you update an endpoint, application auto scaling checks to see whether any of the models on that endpoint are targets for automatic scaling. if the update would change the instance type for any model that is a target for automatic scaling, the update fails.  in the aws management console, you see a warning that you must deregister the model from automatic scaling before you can update it. if you are trying to update the endpoint by calling the  api, the call fails. before you update the endpoint, delete any scaling policies configured for it by calling the  application auto scaling api action, then call  to deregister the variant as a scalable target. after you update the endpoint, you can register the variant as a scalable target and attach an automatic scaling policy to the updated variant. there is one exception. if you change the model for a variant that is configured for automatic scaling, amazon sagemaker automatic scaling allows the update. this is because changing the model doesn't typically affect performance enough to change automatic scaling behavior. if you do update a model for a variant configured for automatic scaling, ensure that the change to the model doesn't significantly affect performance and automatic scaling behavior. when you update amazon sagemaker endpoints that have automatic scaling applied, complete the following steps: to update an endpoint that has automatic scaling applied deregister the endpoint as a scalable target by calling . because automatic scaling is blocked while the update operation is in progress (or if you turned off automatic scaling in the previous step), you might want to take the additional precaution of increasing the number of instances for your endpoint during the update. to do this, update the instance counts for the production variants hosted at the endpoint by calling . call  repeatedly until the value of the  field of the response is . call  to get the values of the current endpoint config. create a new endpoint config by calling . for the production variants where you want to keep the existing instance count or weight, use the same variant name from the response from the call to  in the previous step. for all other values, use the values that you got as the response when you called  in the previous step. update the endpoint by calling . specify the endpoint config you created in the previous step as the  field. if you want to retain the variant properties like instance count or weight, set the value of the  parameter to . this specifies that production variants with the same name will are updated with the most recent  from the response from the call to , regardless of the values of the  field in the new . (optional) re-enable automatic scaling by calling . notesteps 1 and 7 are required only if you are updating an endpoint with the following changes:changing the instance type for a production variant that has automatic scaling configured removing a production variant that has automatic scaling configured. if you delete an endpoint, application auto scaling checks to see whether any of the models on that endpoint are targets for automatic scaling. if any are and you have permission to deregister the model, application auto scaling deregisters those models as scalable targets without notifying you. if you use a custom permission policy that doesn't provide permission for the  and  actions, you must delete automatic scaling policies and deregister scalable targets and before deleting the endpoint. noteyou, as an iam user, might not have sufficient permission to delete an endpoint if another iam user configured automatic scaling for a variant on that endpoint. 
when you create a labeling job, you provide an input manifest file where each line of the manifest describes a unit of task to be completed by annotators. the format of your input manifest file depends on your task type.  if you are creating a 3d point cloud object detection or semantic segmentation labeling job, each line in your input manifest file contains information about a single 3d point cloud frame. this is called a point cloud frame input manifest. to learn more, see . if you are creating a 3d point cloud object tracking labeling job, each line of your input manifest file contains a sequence of 3d point cloud frames and associated data. this is called a point cloud sequence input manifest. to learn more, see . 
 amazon sagemaker ground truth logs worker events to amazon cloudwatch, such as when a worker starts or submits a task. use amazon cloudwatch metrics to measure and track throughput across a team or for individual workers.  importantworker event tracking is not available for amazon augmented ai human review workflows.   during the set-up process for a new work team, the permissions for amazon cloudwatch logging of worker events are created. since this feature was added in august of 2019, work teams created prior to that may not have the correct permissions. if all of your work teams were created before august 2019, create a new work team. it does not need any members and may be deleted after creation, but by creating it, the permissions will be established and will apply to all of your work teams, regardless of when they were created.   after tracking is enabled, the activity of your workers is logged. open the amazon cloudwatch console and choose logs in the navigation pane. you should see a log group named /aws/sagemaker/groundtruth/workeractivity.  each completed task is represented by a log entry, which contains information about the worker, their team, the job, when the task was accepted, and when it was submitted. example log entry   a useful data point in each event is the . you can match that to an individual worker. open the amazon sagemaker console at . under the ground truth section, choose workforces. choose private. choose the name of a team in the private teams section. in the team summary section, choose the user group identified under amazon cognito user group. that will take you to the group in the amazon cognito console. the group page lists the users in the group. choose any user's link in the username column to see more information about the user, including a unique sub id. to get information about all of the team's members, use the  action () in the amazon cognito api. if you don't want to write your own scripts to process and visualize the raw log information, amazon cloudwatch metrics provide insights into worker activity for you. to view metrics open the cloudwatch console at . in the navigation pane, choose metrics. choose the aws/sagemaker/workteam name space, then explore the . for example, selecting the workflow, workteam metrics lets you calculate the average time per submitted task for a specific labeling job. for more information, see . 
this previous release of the amazon sagemaker xgboost algorithm is based on the 0.72 release.  (extreme gradient boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining the estimates of a set of simpler, weaker models. xgboost has done remarkably well in machine learning competitions because it robustly handles a variety of data types, relationships, and distributions, and because of the large number of hyperparameters that can be tweaked and tuned for improved fits. this flexibility makes xgboost a solid choice for problems in regression, classification (binary and multiclass), and ranking. customers should consider using the new release of . they can use it as an amazon sagemaker built-in algorithm or as a framework to run scripts in their local environments as they would typically, for example, do with a tensorflow deep learning framework. the new implementation has a smaller memory footprint, better logging, improved hyperparameter validation, and an expanded set of metrics. the earlier implementation of xgboost remains available to customers if they need to postpone migrating to the new version. but this previous implementation will remain tied to the 0.72 release of xgboost. topics gradient boosting operates on tabular data, with the rows representing observations, one column representing the target variable or label, and the remaining columns representing features.  the amazon sagemaker implementation of xgboost supports csv and libsvm formats for training and inference: for training contenttype, valid inputs are text/libsvm (default) or text/csv.for inference contenttype, valid inputs are text/libsvm or (the default) text/csv.notefor csv training, the algorithm assumes that the target variable is in the first column and that the csv does not have a header record. for csv inference, the algorithm assumes that csv input does not have the label column.for libsvm training, the algorithm assumes that the label is in the first column. subsequent columns contain the zero-based index value pairs for features. so each row has the format:  : : ... inference requests for libsvm may or may not have labels in the libsvm format. this differs from other amazon sagemaker algorithms, which use the protobuf training input format to maintain greater consistency with standard xgboost data formats. for csv training input mode, the total memory available to the algorithm (instance count * the memory available in the ) must be able to hold the training dataset. for libsvm training input mode, it's not required, but we recommend it. sagemaker xgboost uses the python pickle module to serialize/deserialize the model, which can be used for saving/loading the model. to use a model trained with sagemaker xgboost in open source xgboost use the following python code: to differentiate the importance of labelled data points use instance weight supports amazon sagemaker xgboost allows customers to differentiate the importance of labelled data points by assigning each instance a weight value. for text/libsvm input, customers can assign weight values to data instances by attaching them after the labels. for example, . for text/csv input, customers need to turn on the  flag in the parameters and attach weight values in the column after labels. for example: ).amazon sagemaker xgboost currently only trains using cpus. it is a memory-bound (as opposed to compute-bound) algorithm. so, a general-purpose compute instance (for example, m4) is a better choice than a compute-optimized instance (for example, c4). further, we recommend that you have enough total memory in selected instances to hold the training data. although it supports the use of disk space to handle data that does not fit into main memory (the out-of-core feature available with the libsvm input mode), writing cache files onto disk slows the algorithm processing time. for a sample notebook that shows how to use the latest version of amazon sagemaker xgboost as a built-in algorithm to train and host a regression model, see . to use the 0.72 version of xgboost, you need to change the version in the sample code to 0.72. for instructions how to create and access jupyter notebook instances that you can use to run the example in amazon sagemaker, see . once you have created a notebook instance and opened it, select the sagemaker examples tab to see a list of all the amazon sagemaker samples. the topic modeling example notebooks using the xgboost algorithms are located in the introduction to amazon algorithms section. to open a notebook, click on its use tab and select create copy. the following table contains the hyperparameters for the xgboost algorithm. these are parameters that are set by users to facilitate the estimation of model parameters from data. the required hyperparameters that must be set are listed first, in alphabetical order. the optional hyperparameters that can be set are listed next, also in alphabetical order. the amazon sagemaker xgboost algorithm is an implementation of the open-source xgboost package. currently amazon sagemaker supports version 0.72. for more detail about hyperparameter configuration for this version of xgboost, see . automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many jobs that test a range of hyperparameters on your dataset. you choose the tunable hyperparameters, a range of values for each, and an objective metric. you choose the objective metric from the metrics that the algorithm computes. automatic model tuning searches the hyperparameters chosen to find the combination of values that result in the model that optimizes the objective metric.  for more information about model tuning, see . the xgboost algorithm based on version 0.72 computes the following nine metrics during training. when tuning the model, choose one of these metrics as the objective to evaluate the model.  tune the xgboost model with the following hyperparameters. the hyperparameters that have the greatest effect on xgboost objective metrics are: , , , , and .  
a widget for segmenting an image and assigning a label to each image segment. the following is an example of a liquid template that uses the  element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  the following attributes are supported by this element. the text to display above the image. this is typically a question or simple instruction for the worker. a json object containing the color mappings of a prior semantic segmentation job and a link to the overlay image output by the prior job. include this when you want a human worker to verify the results of a prior labeling job and adjust it if necessary. the attribute would appear as follows: while label mappings are included in individual worker output records, the overall result is represented as the  in the consolidated results. you can convert the  to  in a custom template using the liquid templating language: a json formatted array of strings, each of which is a label that a worker can assign to a segment of the image. the name of this widget. it is used as a key for the widget's input in the form output. the url of the image that is to be segmented. this element has the following parent and child elements. parent elements: child elements: , the following regions are supported by this element. general instructions about how to do image segmentation. important task-specific instructions that are displayed in a prominent place. the following output is supported by this element. a json object containing a base64 encoded png of the labels. a json object containing objects with named with the segmentation labels. color – the hexadecimal value of the label's rgb color in the  png.a boolean representing whether the initial values have been modified. this is only included when the output is from an adjustment task. a json object that specifies the dimensions of the image that is being annotated by the worker. this object contains the following properties. height – the height, in pixels, of the image.width – the width, in pixels, of the image.example : sample element outputsthe following is a sample of output from this element.   for more information, see the following. 
a box with an elevated appearance for displaying information. the following is an example of a template designed for sentiment analysis tasks that uses the  element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  the following attributes are supported by this element. the text displayed at the top of the box. a url to an image to be displayed within the box. this element has the following parent and child elements. parent elements: child elements: nonefor more information, see the following. 
you can access notebook metadata and app metadata using the amazon sagemaker ui. topics juypter notebooks contain optional metadata that you can access through the amazon sagemaker ui. to view the notebook metadata in the left sidebar, choose the notebook tools icon ( ). open the advanced tools section. the metadata should look similar to the following. when you create a notebook in amazon sagemaker studio, the app metadata is written to a file named  in the folder . you can get the app metadata by opening an image terminal from within the notebook. the metadata gives you the following information, which includes the sagemaker image and instance type the notebook runs in: apptype –  domainid – same as the studioiduserprofilename – the profile name of the current userresourcearn – the amazon resource name (arn) of the app, which includes the instance typeresourcename – the name of the sagemaker imagethe following screenshot shows the menu from a studio notebook.  to get the app metadata in the center of the notebook menu, choose the launch terminal icon ( ). this opens a terminal in the sagemaker image that the notebook runs in. run the following commands to display the contents of the  file. the file should look similar to the following. 
use warm start to start a hyperparameter tuning job using one or more previous tuning jobs as a starting point. the results of previous tuning jobs are used to inform which combinations of hyperparameters to search over in the new tuning job. hyperparameter tuning uses either bayesian or random search to choose combinations of hyperparameter values from ranges that you specify. for more information, see . using information from previous hyperparameter tuning jobs can help increase the performance of the new hyperparameter tuning job by making the search for the best combination of hyperparameters more efficient. notewarm start tuning jobs typically take longer to start than standard hyperparameter tuning jobs, because the results from the parent jobs have to be loaded before the job can start. the increased time depends on the total number of training jobs launched by the parent jobs. reasons you might want to consider warm start include: you want to gradually increase the number of training jobs over several tuning jobs based on the results you see after each iteration.you get new data, and want to tune a model using the new data.you want to change the ranges of hyperparameters that you used in a previous tuning job, change static hyperparameters to tunable, or change tunable hyperparameters to static values.you stopped a previous hyperparameter job early or it stopped unexpectedly.topics there are two different types of warm start tuning jobs: the new hyperparameter tuning job uses the same input data and training image as the parent tuning jobs. you can change the hyperparameter ranges to search and the maximum number of training jobs that the hyperparameter tuning job launches. you can also change hyperparameters from tunable to static, and from static to tunable, but the total number of static plus tunable hyperparameters must remain the same as it is in all parent jobs. you cannot use a new version of the training algorithm, unless the changes in the new version do not affect the algorithm itself. for example, changes that improve logging or adding support for a different data format are allowed.use identical data and algorithm when you use the same training data as you used in a previous hyperparameter tuning job, but you want to increase the total number of training jobs or change ranges or values of hyperparameters.when you run an warm start tuning job of type , there is an additional field in the response to  named . the value of this field is the  for the training job with the best objective metric value of all training jobs launched by this tuning job and all parent jobs specified for the warm start tuning job. the new hyperparameter tuning job can include input data, hyperparameter ranges, maximum number of concurrent training jobs, and maximum number of training jobs that are different than those of its parent hyperparameter tuning jobs. you can also change hyperparameters from tunable to static, and from static to tunable, but the total number of static plus tunable hyperparameters must remain the same as it is in all parent jobs. the training algorithm image can also be a different version from the version used in the parent hyperparameter tuning job. when you use transfer learning, changes in the dataset or the algorithm that significantly affect the value of the objective metric might reduce the usefulness of using warm start tuning. the following restrictions apply to all warm start tuning jobs: a tuning job can have a maximum of 5 parent jobs, and all parent jobs must be in a terminal state (, , or ) before you start the new tuning job.the objective metric used in the new tuning job must be the same as the objective metric used in the parent jobs.the total number of static plus tunable hyperparameters must remain the same between parent jobs and the new tuning job. because of this, if you think you might want to use a hyperparameter as tunable in a future warm start tuning job, you should add it as a static hyperparameter when you create a tuning job.the type of each hyperparameter (continuous, integer, categorical) must not change between parent jobs and the new tuning job.the number of total changes from tunable hyperparameters in the parent jobs to static hyperparameters in the new tuning job, plus the number of changes in the values of static hyperparameters cannot be more than 10. each value in a categorical hyperparameter counts against this limit. for example, if the parent job has a tunable categorical hyperparameter with the possible values  and , you change that hyperparameter to static in the new tuning job, that counts as 2 changes against the allowed total of 10. if the same hyperparameter had a static value of  in the parent job, and you change the static value to  in the new tuning job, it also counts as 2 changes.warm start tuning is not recursive. for example, if you create  as a warm start tuning job with  as a parent job, and  is itself an warm start tuning job with a parent job , the information that was learned when running  is not used for . if you want to use the information from , you must explicitly add it as a parent for .the training jobs launched by every parent job in a warm start tuning job count against the 500 maximum training jobs for a tuning job.hyperparameter tuning jobs created before october 1, 2018 cannot be used as parent jobs for warm start tuning jobs.for a sample notebook that shows how to use warm start tuning, see . for instructions how to create and access jupyter notebook instances that you can use to run the example in amazon sagemaker, see . once you have created a notebook instance and opened it, select the sagemaker examples tab to see a list of all the amazon sagemaker samples. the warm start tuning example notebook is located in the hyperparameter tuning section, and is named . to open a notebook, click on its use tab and select create copy. you can use either the low-level aws sdk for python (boto 3) or the high-level amazon sagemaker python sdk to create a warm start tuning job. topics to use warm start tuning, you specify the values of a  object, and pass that as the  field in a call to . the following code shows how to create a  object and pass it to  job by using the low-level amazon sagemaker api for python (boto 3). create the  object: create the warm start tuning job: to use the  to run a warm start tuning job, you: specify the parent jobs and the warm start type by using a  object.pass the  object as the value of the  argument of a  object.call the  method of the  object.for more information about using the  for hyperparameter tuning, see . this example uses an estimator that uses the  algorithm for training. the following code sets the hyperparameter ranges that the warm start tuning job searches within to find the best combination of values. for information about setting hyperparameter ranges, see . the following code configures the warm start tuning job by creating a  object. now set the values for static hyperparameters, which are hyperparameters that keep the same value for every training job that the warm start tuning job launches. in the following code,  is an estimator that was created previously. now create the  object and pass the  object that you previously created as the  argument. finally, call the  method of the  object to launch the warm start tuning job. 
before using a dataset to train a model, data scientists typically explore, analyze, and preprocess it. for example, in one of the exercises in this guide, you use the mnist dataset, a commonly available dataset of handwritten numbers, for model training. before you begin training, you transform the data into a format that is more efficient for training. for more information, see .  to preprocess data use one of the following methods: use a jupyter notebook on an amazon sagemaker notebook instance. you can also use the notebook instance to do the following: write code to create model training jobs deploy models to amazon sagemaker hosting test or validate your modelsfor more information, see   you can use a model to transform data by using amazon sagemaker batch transform. for more information, see . amazon sagemaker processing enables running jobs to preprocess and postprocess data, perform feature engineering, and evaluate models on amazon sagemaker easily and at scale. when combined with the other critical machine learning tasks provided by amazon sagemaker, such as training and hosting, processing provides you with the benefits of a fully managed machine learning environment, including all the security and compliance support built into amazon sagemaker. with processing, you have the flexibility to use the built-in data processing containers or to bring your own containers and submit custom jobs to run on managed infrastructure. after you submit a job, amazon sagemaker launches the compute instances, processes and analyzes the input data, and releases the resources upon completion. for more information, see . for information about how to run your own data processing scripts, .for information about how to build your own processing container to run scripts, see .
amazon sagemaker is a fully managed machine learning service. with amazon sagemaker, data scientists and developers can quickly and easily build and train machine learning models, and then directly deploy them into a production-ready hosted environment. it provides an integrated jupyter authoring notebook instance for easy access to your data sources for exploration and analysis, so you don't have to manage servers. it also provides common machine learning algorithms that are optimized to run efficiently against extremely large data in a distributed environment. with native support for bring-your-own-algorithms and frameworks, amazon sagemaker offers flexible distributed training options that adjust to your specific workflows. deploy a model into a secure and scalable environment by launching it with a few clicks from amazon sagemaker studio or the amazon sagemaker console. training and hosting are billed by minutes of usage, with no minimum fees and no upfront commitments. this guide includes information and tutorials on amazon sagemaker features. to learn how to build, train, and deploy models using amazon sagemaker, see .  topics amazon sagemaker includes the following features: an integrated machine learning environment where you can build, train, deploy, and analyze your models all in the same application. high-quality training datasets by using workers along with machine learning to create labeled datasets. human-in-the-loop reviews the next generation of amazon sagemaker notebooks that include sso integration, fast start-up times, and single-click sharing. analyze and pre-process data, tackle feature engineering, and evaluate models. experiment management and tracking. you can use the tracked data to reconstruct an experiment, incrementally build on experiments conducted by peers, and trace model lineage for compliance and audit verifications. inspect training parameters and data throughout the training process. automatically detect and alert users to commonly occurring errors such as parameter values getting too large or small. users without machine learning knowledge can quickly build classification and regression models. maximize the long-term reward that an agent receives as a result of its actions. preprocess datasets, run inference when you don't need a persistent endpoint, and associate input records with inferences to assist the interpretation of results. monitor and analyze models in production (endpoints) to detect data drift and deviations in model quality. train machine learning models once, then run anywhere in the cloud and at the edge. speed up the throughput and decrease the latency of getting real-time inferences. as with other aws products, there are no contracts or minimum commitments for using amazon sagemaker. for more information about the cost of using amazon sagemaker, see . if you are a first-time user of amazon sagemaker, we recommend that you do the following:  read  – this section provides an overview of amazon sagemaker, explains key concepts, and describes the core components involved in building ai solutions with amazon sagemaker. we recommend that you read this topic in the order presented.  – this section explains how to set up your aws account and onboard to amazon sagemaker studio. amazon sagemaker autopilot simplifies the machine learning experience by automating machine learning tasks. if you are new to amazon sagemaker, it provides the easiest learning path. it also serves as an excellent ml learning tool that provides visiblity into the code with notebooks generated for each of the automated ml tasks. for an introduction to its capabilities, see . to get started building, training, and deploying machine learning models, autopilot provides:  – this section walks you through training your first model using amazon sagemaker studio, or the amazon sagemaker console and the amazon sagemaker api. you use training algorithms provided by amazon sagemaker. explore other topics – depending on your needs, do the following: submit python code to train with deep learning frameworks – in amazon sagemaker, you can use your own training scripts to train models. for information, see .use amazon sagemaker directly from apache spark – for information, see .use amazon ai to train and/or deploy your own custom algorithms – package your custom algorithms with docker so you can train and/or deploy them in amazon sagemaker. see  to learn how amazon sagemaker interacts with docker containers, and for the amazon sagemaker requirements for docker images. view the  – this section describes the amazon sagemaker api operations. 
amazon textract enables you to add document text detection and analysis to your applications. amazon augmented ai (amazon a2i) directly integrated with amazon textract's  api operation. you can use  to analyze a document for relationships between detected items. when you add an amazon a2i human review loop to an  request, amazon a2i monitors the amazon textract results and sends a document to one or more human workers for review when the conditions specified in your flow definition are met. for example, if you want a human to review a specific key like "full name:" and their associated input-values you can create a trigger to start a human review anytime the key "full name:" is detected or when the inference confidence for that key falls within a range that you specify.  you can specify when amazon textract sends a task to a human worker for review when creating a human review workflow, or flow definition by specifying activation conditions.  you can set the following activation conditions when using the amazon textract task type: trigger a human review for specific form keys based on the form key confidence score. trigger a human review when specific form keys are missing. trigger human review for all form keys identified by amazon textract with confidence scores in a specified range.randomly send a sample of forms to humans for review.when your activation condition depends on form key confidence scores, you can use two types of prediction-confidence to trigger human loops: identification confidence – the confidence score for key-value pairs detected within a form.qualification confidence – the confidence score for text contained within key and value respectively in a form.in the image in the following section, full name: jane doe is the key-value pair, and full name and jane doe are the key and value respectively. you can set these activation conditions using the amazon sagemaker console when you create a human review workflow, or by creating a json for human loop activation conditions and specifying this as input in the  parameter of  api operation. to learn how specify activation conditions in json format, see  and . notewhen using augmented ai with amazon textract, create augmented ai resources in the same aws region you use to call .  to integrate a human review into an amazon textract text detection and analysis job, you need to create a flow definition, and then use the amazon textract api to integrate that flow definition into your workflow. to learn how to create a flow definition using the amazon sagemaker console or augmented ai api, see the following topics: after you've created your flow definition, see  to learn how to integrate your flow definition into your amazon textract task.  for an end-to-end example that demonstrates how to use amazon textract with augmented ai, you can use %20and%20textract%20analyzedocument.ipynb) in an amazon sagemaker notebook instance.  to use amazon textract with augmented ai using a amazon sagemaker notebook if you do not have an active amazon sagemaker notebook instance, create one by following the instructions in . when your notebook instance is active, choose open jupyterlab to the right of the notebook instance's name. it may take a few moments for jupyterlab to load.  choose the  icon to clone a github repository into your workspace.  enter the  repository https url.  choose clone. open the notebook amazon augmented ai (amazon a2i) integration with amazon textract's analyze document [example].  follow the instructions in the notebook to configure your flow definition and human loop and run the cells.  to avoid incurring unnecessary charges, when you are done with the demo stop and delete your notebook instance in addition to any amazon s3 buckets, iam roles, and cloudwatch events resources created during the walkthrough. when they're assigned a review task in an amazon textract workflow, workers might see ui similar to the following:  you can customize this ui in the amazon sagemaker console when you create your human review definition, or by creating and using a custom template. to learn more, see . 
here is a video series that provides a tour of amazon sagemaker autopilot capabilities using studio. they show how to start an automl job, analyze and preprocess data, how to do feature engineering and hyperparameter optimization on candidate models, and how to visualize and compare the resulting model metrics. topics this video shows you to how to start an automl job with autopilot. (length: 8:41)  this video shows you how to examine the data exploration and candidate definition notebooks generated by amazon sagemaker autopilot. (length: 10:04)  this video shows you how to optimize model performance during training using hyperparameter tuning. (length: 4:59)  this video shows you how to use job metrics to choose the best model and then how to deploy it. (length: 5:20)  this video walks you through an end to end demo where we first build a binary classification model automatically with amazon sagemaker autopilot. we see how candidate models have been built and optimized using auto-generated notebooks. we also look at the top candidates with amazon sagemaker experiments. finally, we deploy the top candidate (based on xgboost), and configure data capture with amazon sagemaker model monitor.  
in the  request, you specify the training algorithm. you can also specify algorithm-specific hyperparameters as string-to-string maps. the following table lists the hyperparameters for the pca training algorithm provided by amazon sagemaker. for more information about how pca works, see .  
this topic provides guidance on best practices for deploying machine learning models in amazon sagemaker. create robust endpoints when hosting your model. amazon sagemaker endpoints can help protect your application from  outages and instance failures. if an outage occurs or an instance fails, amazon sagemaker automatically attempts to distribute your instances across availability zones. for this reason, we strongly recommended that you deploy multiple instances for each production endpoint.  if you are using an , configure the vpc with at least two , each in a different availability zone. if an outage occurs or an instance fails, amazon sagemaker automatically attempts to distribute your instances across availability zones.  in general, to achieve more reliable performance, use more small  in different availability zones to host your endpoints. 
when used with amazon a2i, the amazon rekognition  operation supports the following inputs in the  parameters:  – use this condition type to create a human loop when inference confidence is low for one or more specified labels. – use this condition to specify a percentage of all inferences to send to humans for review. use this condition to do the following:audit your ml model by randomly sampling all of your model's inferences and sending a specified percentage to humans for review.using the  condition, randomly sample a percentage of the inferences that met the conditions specified in  to start a human loop and send only the specified percentage to humans for review. noteif you send the same request to  multiple times, the result of  will not change for the inference of that input. for example, if you make a  request once, and  does not trigger a humanloop, subsequent requests to  with the same configuration won't trigger a human loop.  when creating a flow definition, if you use the default worker task template that is provided in the human review workflows section of the amazon sagemaker console, inferences sent for human review by these activation conditions are included in the worker ui when a worker opens your task. if you use a custom worker task template, you need to include the  custom html element to access these inferences. for an example of a custom template that uses this html element, see . for the  , the following  are supported:  – the exact (case-sensitive) name of a  detected by the amazon rekognition  operation. you can specify the special catch-all value (*) to denote any moderation label.when you use the  , amazon a2i sends label inferences for the labels that you specified in  for human review. the   supports the  . the input for the  prameter should be real number between 0.01 and 100. this number represents the percentage of inferences that qualifies for a human review that are sent to humans for review. if you use the  condition without any other conditions, this number represents the percentage of all inferences that result from a single  request that are sent to humans for review. example 1: use moderationlabelconfidencecheck with the and operator the following example of a  condition triggers a humanloop when one or more of the following conditions are met: amazon rekognition detects the  moderation label with a confidence between 90 and 99.amazon rekognition detects the  moderation label with a confidence between 80 and 99.note the use of the or and and logical operators to model this logic. although only one of the two conditions under the  operator need to evaluate to true for a humanloop to be created, amazon augmented ai evaluates all conditions. human reviewers are asked to review the moderation labels for all the conditions that evaluated to true. example 2: use moderationlabelconfidencecheck with the catch-all value (*)  in the following example, if any moderation label with a confidence greater than or equal to 75 is detected, a humanloop is triggered. human reviewers are asked to review all moderation labels with confidence scores greater than or equal to 75. example 3: use sampling in the following example, 5% of amazon rekognition inferences from a  request will be sent to human workers. when using the default worker task template provided in the amazon sagemaker console, all moderation labels returned by amazon rekognition are sent to workers for review. example 4: use sampling and moderationlabelconfidencecheck with the and operator in this example, 5% of amazon rekognition inferences of the  moderation label with a confidence greater than 50 will be sent workers for review. when using the default worker task template provided in the amazon sagemaker console, only the  label will be sent to workers for review.  example 5: use sampling and moderationlabelconfidencecheck with the and operator use this example to configure your human review workflow to always send low confidence inferences of a specified label for human review and sample high confidence inference of a label at a specified rate.  in the following example, a human review is triggered in one of the following ways:  inferences for the  moderation label the with confidence scores less than 60 are always sent for human review. only the  label is sent to workers to review. 5% of all inferences for the  moderation label the with confidence scores greater than 90 will be sent for human review. only the  label is sent to workers to review. example 6: use sampling and moderationlabelconfidencecheck with the or operator in the following example, a human loop is created if the amazon rekognition inference response contains the 'graphic male nudity' label with inference confidence greater than 50. additionally, 5% of all other inferences will trigger a human loop.  
the following example describes the steps for developing rl models using amazon sagemaker rl. for complete code examples, see the sample notebooks at . formulate the rl problem—first, formulate the business problem into an rl problem. for example, auto scaling enables services to dynamically increase or decrease capacity depending on conditions that you define. currently, this requires setting up alarms, scaling policies, and thresholds, and other manual steps. to solve this with rl, we define the components of the markov decision process: objective—scale instance capacity so that it matches the desired load profile. environment—a custom environment that includes the load profile. it generates a simulated load with daily and weekly variations and occasional spikes. the simulated system has a delay between when new resources are requested and when they become available for serving requests. state—the current load, number of failed jobs, and number of active machines action—remove, add, or keep the same number of instances. reward—a positive reward for successful transactions, a high penalty for failing transactions beyond a specified threshold. define the rl environment—the rl environment can be the real world where the rl agent interacts or a simulation of the real world. you can connect open source and custom environments developed using gym interfaces, and commercial simulation environments such as matlab and simulink. define the presets—the presets configure the rl training jobs and define the hyperparameters for the rl algorithms. write the training code—write training code as a python script and pass the script to an amazon sagemaker training job. in your training code, import the environment files and the preset files, and then define the  function. train the rl model— use the amazon sagemaker  in the  to start an rl training job. if you are using local mode, the training job runs on the notebook instance. when you use amazon sagemaker for training, you can select gpu or cpu instances. store the output from the training job in a local directory if you train in local mode, or on amazon s3 if you use amazon sagemaker training. the  requires the following information as parameters.  the source directory where the environment, presets, and training code are uploaded. the path to the training script. the rl toolkit and deep learning framework you want to use. this automatically resolves to the amazon ecr path for the rl container. the training parameters, such as the instance count, job name, and s3 path for output. metric definitions that you want to capture in your logs. these can also be visualized in cloudwatch and in amazon sagemaker notebooks. visualize training metrics and output—after a training job that uses an rl model completes, you can view the metrics you defined in the training jobs in cloudwatch,. you can also plot the metrics in a notebook by using the  analytics library. visualizing metrics helps you understand how the performance of the model as measured by the reward improves over time. noteif you train in local mode, you can't visualize metrics in cloudwatch. evaluate the model—checkpointed data from the previously trained models can be passed on for evaluation and inference in the checkpoint channel. in local mode, use the local directory. in amazon sagemaker training mode, you need to upload the data to s3 first. deploy rl models—finally, deploy the trained model on an endpoint hosted on amazon sagemaker or on an edge device by using aws iot greengrass. for more information on rl with sagemaker, see . 
before you can use autoscaling, must have already created an amazon sagemaker model deployment. deployed models are referred to as a . this includes information about the model and the resources used to host it.  for more information about deploying a model endpoint, see . to enable autoscaling for a model, you can use the console, the aws cli, or the application auto scaling api. it is recommended to try to  to get familiar with the requirements and to test your first autoscaling configuration. when using the aws cli or application auto scaling the flow is to register the model, define the scaling policy, then apply it. the following overview provides further details on the prerequisites and components used with autoscaling. to use automatic scaling, you define and apply a scaling policy that uses amazon cloudwatch metrics and target values that you assign. automatic scaling uses the policy to increase or decrease the number of instances in response to actual workloads. you can use the aws management console to apply a scaling policy based on a predefined metric. a predefined metric is defined in an enumeration so that you can specify it by name in code or use it in the aws management console. alternatively, you can use either the aws command line interface (aws cli) or the application auto scaling api to apply a scaling policy based on a predefined or custom metric.  there are two types of supported scaling policies: target-tracking scaling and step scaling. it is recommended to use target-tracking scaling policies for your autoscaling configuration. you configure the target-tracking scaling policy by specifying a predefined or custom metric and a target value for the metric. for more information about using application auto scaling target-tracking scaling policies, see . you can use step scaling when you require an advanced configuration, such as specifying how many instances to deploy under what conditions. otherwise, using target-tracking scaling is preferred as it will be fully automated. for more information about using application auto scaling step scaling policies, see . a scaling policy has the following components: a target metric—the amazon cloudwatch metric that amazon sagemaker automatic scaling uses to determine when and how much to scale.minimum and maximum capacity—the minimum and maximum number of instances to use for scaling.a cool down period—the amount of time, in seconds, after a scale-in or scale-out activity completes before another scale-out activity can start.required permissions—permissions that are required to perform automatic scaling actions.a service-linked role—an aws identity and access management (iam) role that is linked to a specific aws service. a service-linked role includes all of the permissions that the service requires to call other aws services on your behalf. amazon sagemaker automatic scaling automatically generates this role, , for you.amazon cloudwatch alarms trigger the scaling policy, which calculate how to adjust scaling based on the metric and target value that you set. the scaling policy adds or removes endpoint instances as required to keep the metric at, or close to, the specified target value. in addition, a scaling policy also adjusts to fluctuations in the metric when a workload changes. the scaling policy minimizes rapid fluctuations in the number of available instances for your model. for example, a scaling policy that uses the predefined  metric with a target value of 70 can keep  at, or close to 70. you may specify the maximum number of endpoint instances for the model. the maximum value must be equal to or greater than the value specified for the minimum number of endpoint instances. amazon sagemaker automatic scaling does not enforce a limit for this value. you must also specify the minimum number of instances for the model. this value must be at least 1, and equal to or less than the value specified for the maximum number of endpoint instances. to determine the minimum and maximum number of instances that you need for typical traffic, test your autoscaling configuration with the expected rate of traffic to your model. importantscaling-in does not occur when there is no traffic: if a variant’s traffic becomes zero, amazon sagemaker automatic scaling doesn't scale in. this is because amazon sagemaker doesn't emit metrics with a value of zero.  tune the responsiveness of your scaling policy by adding a cooldown period. a cooldown period controls when your model is scaled-in (by reducing instances) or scaled-out (by increasing instances). it does this by blocking subsequent scale-in or scale-out requests until the period expires. this slows the deletion of instances for scale-in requests, and the creation of instances for scale-out requests. a cooldown period helps to ensure that the scaling policy doesn't launch or terminate additional instances before the previous scaling activity takes effect. after automatic scaling dynamically scales using a scaling policy, it waits for the cooldown period to complete before resuming scaling activities. you configure the cooldown period in your automatic scaling policy. you can specify the following cooldown periods: a scale-in activity reduces the number of instances. a scale-in cooldown period specifies the amount of time, in seconds, after a scale-in activity completes before another scale-in activity can start. a scale-out activity increases the number of instances. a scale-out cooldown period specifies the amount of time, in seconds, after a scale-out activity completes before another scale-out activity can start. if you don't specify a scale-in or a scale-out cooldown period automatic scaling use the default, which is 300 seconds for each. if instances are being added or removed too quickly when you test your automatic scaling configuration, consider increasing this value. you can see this behavior if the traffic to your model has a lot of spikes, or if you have multiple automatic scaling policies defined for a variant. if instances are not being added quickly enough to address increased traffic, consider decreasing this value. the  iam policy has all of the iam permissions required to perform autoscaling. for more information about amazon sagemaker iam permissions, see . if you are using a custom permission policy, you must include the following permissions: autoscaling uses the  service-linked role; it created for you automatically. a service-linked role is a unique type of iam role that is linked directly to an aws service. service-linked roles are predefined by the service and include all of the permissions that the service requires to call other aws services on your behalf. for more information, see . 
to create a pipeline model that can be deployed to an endpoint or used for a batch transform job, use the amazon sagemaker console or the  operation.  to create an inference pipeline (console) open the amazon sagemaker console at . choose models, and then choose create models from the inference group.  on the create model page, provide a model name, choose an iam role, and, if you want to use a private vpc, specify vpc values. to add information about the containers in the inference pipeline, choose add container, then choose next. complete the fields for each container in the order that you want to execute them, up to the maximum of five. complete the container input options, , location of inference code image, and, optionally, location of model artifacts, container host name, and environmental variables fields. . the myinferencepipelinemodel page summarizes the settings for the containers that provide input for the model. if you provided the environment variables in a corresponding container definition, amazon sagemaker shows them in the environment variables field. 
for an example of how to deploy a model to the amazon sagemaker hosting service, see . or, if you prefer, watch the following video tutorial:  amazon sagemaker provides model hosting services for model deployment, as shown in the following diagram. amazon sagemaker provides an https endpoint where your machine learning model is available to provide inferences.    deploying a model using amazon sagemaker hosting services is a three-step process: create a model in amazon sagemaker—by creating a model, you tell amazon sagemaker where it can find the model components. this includes the s3 path where the model artifacts are stored and the docker registry path for the image that contains the inference code. in subsequent deployment steps, you specify the model by name. for more information, see the  api. create an endpoint configuration for an https endpoint—you specify the name of one or more models in production variants and the ml compute instances that you want amazon sagemaker to launch to host each production variant. when hosting models in production, you can configure the endpoint to elastically scale the deployed ml compute instances. for each production variant, you specify the number of ml compute instances that you want to deploy. when you specify two or more instances, amazon sagemaker launches them in multiple availability zones. this ensures continuous availability. amazon sagemaker manages deploying the instances. for more information, see the  api. create an https endpoint—provide the endpoint configuration to amazon sagemaker. the service launches the ml compute instances and deploys the model or models as specified in the configuration. for more information, see the  api. to get inferences from the model, client applications send requests to the amazon sagemaker runtime https endpoint. for more information about the api, see the  api.  noteendpoints are scoped to an individual aws account, and are not public. the url does not contain the account id, but amazon sagemaker determines the account id from the authentication token that is supplied by the caller.for an example of how to use amazon api gateway and aws lambda to set up and deploy a web service that you can call from a client application that is not within the scope of your account, see  in the aws machine learning blog. notewhen you create an endpoint, amazon sagemaker attaches an amazon ebs storage volume to each ml compute instance that hosts the endpoint. the size of the storage volume depends on the instance type. for a list of instance types that amazon sagemaker hosting service supports, see . for a list of the sizes of the storage volumes that amazon sagemaker attaches to each instance, see . to increase a model's accuracy, you might choose to save the user's input data and ground truth, if available, as part of the training data. you can then retrain the model periodically with a larger, improved training dataset. when hosting models using amazon sagemaker hosting services, consider the following: typically, a client application sends requests to the amazon sagemaker https endpoint to obtain inferences from a deployed model. you can also send requests to this endpoint from your jupyter notebook during testing. you can deploy a model trained with amazon sagemaker to your own deployment target. to do that, you need to know the algorithm-specific format of the model artifacts that were generated by model training. for more information about output formats, see the section corresponding to the algorithm you are using in .  you can deploy multiple variants of a model to the same amazon sagemaker https endpoint. this is useful for testing variations of a model in production. for example, suppose that you've deployed a model into production. you want to test a variation of the model by directing a small amount of traffic, say 5%, to the new model. to do this, create an endpoint configuration that describes both variants of the model. you specify the  in your request to the . for more information, see .  you can configure a  to use application auto scaling. for information about configuring automatic scaling, see . you can modify an endpoint without taking models that are already deployed into production out of service. for example, you can add new model variants, update the ml compute instance configurations of existing model variants, or change the distribution of traffic among model variants. to modify an endpoint, you provide a new endpoint configuration. amazon sagemaker implements the changes without any downtime. for more information see,  and .  changing or deleting model artifacts or changing inference code after deploying a model produces unpredictable results. if you need to change or delete model artifacts or change inference code, modify the endpoint by providing a new endpoint configuration. once you provide the new endpoint configuration, you can change or delete the model artifacts corresponding to the old endpoint configuration. if you want to get inferences on entire datasets, consider using batch transform as an alternative to hosting services. for information, see  
reinforcement learning (rl) is a machine learning technique that attempts to learn a strategy, called a policy, that optimizes an objective for an agent acting in an environment. for example, the agent might be a robot, the environment might be a maze, and the goal might be to successfully navigate the maze in the smallest amount of time. in rl, the agent takes an action, observes the state of the environment, and gets a reward based on the value of the current state of the environment. the goal is to maximize the long-term reward that the agent receives as a result of its actions. rl is well-suited for solving problems where an agent can make autonomous decisions. topics rl is well-suited for solving large, complex problems. for example, supply chain management, hvac systems, industrial robotics, game artificial intelligence, dialog systems, and autonomous vehicles. because rl models learn by a continuous process of receiving rewards and punishments for every action taken by the agent, it is possible to train systems to make decisions under uncertainty and in dynamic environments. rl is based on models called markov decision processes (mdps). an mdp consists of a series of time steps. each time step consists of the following: environmentdefines the space in which the rl model operates. this can be either a real-world environment or a simulator. for example, if you train a physical autonomous vehicle on a physical road, that would be a real-world environment. if you train a computer program that models an autonomous vehicle driving on a road, that would be a simulator. statespecifies all information about the environment and past steps that is relevant to the future. for example, in an rl model in which a robot can move in any direction at any time step, then the position of the robot at the current time step is the state, because if we know where the robot is, it isn't necessary to know the steps it took to get there. actionwhat the agent does. for example, the robot takes a step forward. rewarda number that represents the value of the state that resulted from the last action that the agent took. for example, if the goal is for a robot to find treasure, the reward for finding treasure might be 5, and the reward for not finding treasure might be 0. the rl model attempts to find a strategy that optimizes the cumulative reward over the long term. this strategy is called a policy. observationinformation about the state of the environment that is available to the agent at each step. this might be the entire state, or it might be just a part of the state. for example, the agent in a chess-playing model would be able to observe the entire state of the board at any step, but a robot in a maze might only be able to observe a small portion of the maze that it currently occupies. typically, training in rl consists of many episodes. an episode consists of all of the time steps in an mdp from the initial state until the environment reaches the terminal state. to train rl models in amazon sagemaker rl, use the following components:  a deep learning (dl) framework. currently, amazon sagemaker supports rl in tensorflow and apache mxnet.an rl toolkit. an rl toolkit manages the interaction between the agent and the environment, and provides a wide selection of state of the art rl algorithms. amazon sagemaker supports the intel coach and ray rllib toolkits. for information about intel coach, see . for information about ray rllib, see .an rl environment. you can use custom environments, open-source environments, or commercial environments. for information, see .the following diagram shows the rl components that are supported in amazon sagemaker rl.  
training a model produces the following the model training datamodel artifacts, which amazon sagemaker generates during model training you save these in an amazon simple storage service (amazon s3) bucket: you can store datasets that you use as your training data and model artifacts that are the output of a training job in a single bucket or in two separate buckets. for this exercise and others in this guide, one bucket is sufficient. if you already have s3 buckets, you can use them, or you can create new ones.  to create a bucket, follow the instructions in  in the amazon simple storage service console user guide. include  in the bucket name. for example, . noteamazon sagemaker needs permission to access these buckets. you grant permission with an iam role, which you create in the next step when you create an amazon sagemaker notebook instance. this iam role automatically gets permissions to access any bucket that has  in the name. it gets these permissions through the  policy, which amazon sagemaker attaches to the role. if you add a policy to the role that grants the sagemaker service principal  permission, the name of the bucket does not need to contain sagemaker.  
there are three steps involved in the implementation of the linear learner algorithm: preprocess, train, and validate.  normalization, or feature scaling, is an important preprocessing step for certain loss functions that ensures the model being trained on a dataset does not become dominated by the weight of a single feature. the amazon sagemaker linear learner algorithm has a normalization option to assist with this preprocessing step. if normalization is turned on, the algorithm first goes over a small sample of the data to learn the mean value and standard deviation for each feature and for the label. each of the features in the full dataset is then shifted to have mean of zero and scaled to have a unit standard deviation. notefor best results, ensure your data is shuffled before training. training with unshuffled data may cause training to fail.  you can configure whether the linear learner algorithm normalizes the feature data and the labels using the  and  hyperparameters, respectively. normalization is enabled by default for both features and labels for regression. only the features can be normalized for binary classification and this is the default behavior.  with the linear learner algorithm, you train with a distributed implementation of stochastic gradient descent (sgd). you can control the optimization process by choosing the optimization algorithm. for example, you can choose to use adam, adagrad, stochastic gradient descent, or other optimization algorithms. you also specify their hyperparameters, such as momentum, learning rate, and the learning rate schedule. if you aren't sure which algorithm or hyperparameter value to use, choose a default that works for the majority of datasets.  during training, you simultaneously optimize multiple models, each with slightly different objectives. for example, you vary l1 or l2 regularization and try out different optimizer settings.  when training multiple models in parallel, the models are evaluated against a validation set to select the most optimal model once training is complete. for regression, the most optimal model is the one that achieves the best loss on the validation set. for classification, a sample of the validation set is used to calibrate the classification threshold. the most optimal model selected is the one that achieves the best binary classification selection criteria on the validation set. examples of such criteria include the f1 measure, accuracy, and cross-entropy loss.  noteif the algorithm is not provided a validation set, then evaluating and selecting the most optimal model is not possible. to take advantage of parallel training and model selection ensure you provide a validation set to the algorithm.  
follow the steps described in the running the training job section of the  sample to produce a machine learning model train using amazon sagemaker. then you can use neo to further optimize the model with the following code:  this code compiles the model and saves the optimized model in . sample notebooks of using sdk are provided in the  section. 
an embedding is a mapping from discrete objects, such as words, to vectors of real numbers. due to gpu memory scarcity, the  environment variable can be specified to optimize on whether the  or the encoder embedding inference network is loaded into gpu. if the majority of your inference is for encoder embeddings, specify . the following is a batch transform example of using 4 instances of p3.2xlarge that optimizes for encoder embedding inference: content-type: application/json; infer_max_seqlens=<fwd-length>,<bck-length> where <fwd-length> and <bck-length> are integers in the range [1,5000] and define the maximum sequence lengths for the forward and backward encoder. content-type: application/jsonlines; infer_max_seqlens=<fwd-length>,<bck-length> where <fwd-length> and <bck-length> are integers in the range [1,5000] and define the maximum sequence lengths for the forward and backward encoder. in both of these formats, you specify only one input type:  or  the inference service then invokes the corresponding encoder and outputs the embeddings for each of the instances.  content-type: application/json content-type: application/jsonlines the vector length of the embeddings output by the inference service is equal to the value of one of the following hyperparameters that you specify at training time: , , or . 
this section explains how amazon sagemaker interacts with a docker container that runs your own inference code for hosting services. use this information to write inference code and create a docker image.  topics to configure a container to run as an executable, use an  instruction in a dockerfile. note the following:  for model inference, amazon sagemaker runs the container as: amazon sagemaker overrides default  statements in a container by specifying the  argument after the image name. the  argument overrides arguments that you provide with the  command in the dockerfile.   we recommend that you use the  form of the  instruction: for example: the  form of the  instruction starts the executable directly, not as a child of . this enables it to receive signals like  and  from the amazon sagemaker apis, which is a requirement.    for example, when you use the  api to create an endpoint, amazon sagemaker provisions the number of ml compute instances required by the endpoint configuration, which you specify in the request. amazon sagemaker runs the docker container on those instances.    if you reduce the number of instances backing the endpoint (by calling the  apis), amazon sagemaker runs a command to stop the docker container on the instances being terminated. the command sends the  signal, then it sends the  signal thirty seconds later.   if you update the endpoint (by calling the  api), amazon sagemaker launches another set of ml compute instances and runs the docker containers that contain your inference code on them. then it runs a command to stop the previous docker containers. to stop a docker container, command sends the  signal, then it sends the  signal thirty seconds later.    amazon sagemaker uses the container definition that you provided in your  request to set environment variables and the dns hostname for the container as follows:   it sets environment variables using the  string-to-string map.it sets the dns hostname using the . if you plan to use gpu devices for model inferences (by specifying gpu-based ml compute instances in your  request), make sure that your containers are  compatible. don't bundle nvidia drivers with the image. for more information about , see .  you can't use the  initializer as your entry point in amazon sagemaker containers because it gets confused by the train and serve arguments.in your  request, the container definition includes the  parameter, which identifies the s3 location where model artifacts are stored. amazon sagemaker uses this information to determine where to copy the model artifacts from. it copies the artifacts to the  directory for use by your inference code. the  must point to a tar.gz file. otherwise, amazon sagemaker won't download the file.  if you trained your model in amazon sagemaker, the model artifacts are saved as a single compressed tar file in amazon s3. if you trained your model outside amazon sagemaker, you need to create this single compressed tar file and save it in a s3 location. amazon sagemaker decompresses this tar file into /opt/ml/model directory before your container starts. containers need to implement a web server that responds to  and  on port 8080.  to obtain inferences, the client application sends a post request to the amazon sagemaker endpoint. for more information, see the  api. amazon sagemaker passes the request to the container, and returns the inference result from the container to the client. note the following: amazon sagemaker strips all  headers except those supported by . amazon sagemaker might add additional headers. inference containers must be able to safely ignore these additional headers.to receive inference requests, the container must have a web server listening on port 8080 and must accept  requests to the  endpoint. a customer's model containers must accept socket connection requests within 250 ms.a customer's model containers must respond to requests within 60 seconds. the model itself can have a maximum processing time of 60 seconds before responding to the /invocations. if your model is going to take 50-60 seconds of processing time, the sdk socket timeout should be set to be 70 seconds.the  and  api calls result in amazon sagemaker starting new inference containers. soon after container startup, amazon sagemaker starts sending periodic get requests to the /ping endpoint. the simplest requirement on the container is to respond with an http 200 status code and an empty body. this indicates to amazon sagemaker that the container is ready to accept inference requests at the /invocations endpoint. if the container does not begin to pass health checks, by consistently responding with 200s, during the 4 minutes after startup,  will fail, leaving endpoint in a failed state, and the update requested by  will not be completed. while the minimum bar is for the container to return a static 200, a container developer can use this functionality to perform deeper checks. the request timeout on /ping attempts is 2 seconds. 
to get started, you or your organization's administrator need to complete the amazon sagemaker studio onboarding process. for more information, see . you can access an amazon sagemaker studio notebook in any of the following ways: you receive an email invitation to access studio through your organization's aws sso account, which includes a direct link to login to studio without having to use the amazon sagemaker console. you can proceed to the .you receive a link to a shared studio notebook, which includes a direct link to log in to studio without having to use the amazon sagemaker console. you can proceed to the . you onboard to studio and then log in to the amazon sagemaker console. for more information, see .to log in from the amazon sagemaker console onboard to amazon sagemaker studio. if you've already onboarded, skip to the next step. open the amazon sagemaker . choose amazon sagemaker studio. the amazon sagemaker studio control panel opens. in the amazon sagemaker studio control panel, you'll see a list of user names. next to your user name, choose open studio. now that you're in studio, you can try any of the following options: create a amazon sagemaker studio notebook – continue to the next section.familiarize yourself with the amazon sagemaker studio interface – see .follow a studio end-to-end tutorial – see .
amazon sagemaker supports automatic scaling (autoscaling) for your hosted models. autoscaling dynamically adjusts the number of instances provisioned for a model in response to changes in your workload. when the workload increases, autoscaling brings more instances online. when the workload decreases, autoscaling removes unnecessary instances so that you don't pay for provisioned instances that you aren't using. topics 
to monitor the progress of a hyperparameter tuning job and the training jobs that it launches, use the amazon sagemaker console. topics to view the status of the hyperparameter tuning job open the amazon sagemaker console at . choose hyperparameter tuning jobs. in the list of hyperparameter tuning jobs, check the status of the hyperparameter tuning job you launched. a tuning job can be: —the hyperparameter tuning job successfully completed.—the hyperparameter tuning job is in progress. one or more training jobs are still running.—the hyperparameter tuning job failed.—the hyperparameter tuning job was manually stopped before it completed. all training jobs that the hyperparameter tuning job launched are stopped.—the hyperparameter tuning job is in the process of stopping.to view the status of the training jobs that the hyperparameter tuning job launched in the list of hyperparameter tuning jobs, choose the job that you launched. choose training jobs. view the status of each training job. to see more details about a job, choose it in the list of training jobs. to view a summary of the status of all of the training jobs that the hyperparameter tuning job launched, see training job status counter. a training job can be: —the training job successfully completed.—the training job is in progress.—the training job was manually stopped before it completed.—the training job failed, but can be retried. a failed training job can be retried only if it failed because an internal service error occurred.—the training job failed and can't be retried. a failed training job can't be retried when a client error occurs.a hyperparameter tuning job uses the objective metric that each training job returns to evaluate training jobs. while the hyperparameter tuning job is in progress, the best training job is the one that has returned the best objective metric so far. after the hyperparameter tuning job is complete, the best training job is the one that returned the best objective metric. to view the best training job, choose best training job.  to deploy the best training job as a model that you can host at an amazon sagemaker endpoint, choose create model.  
you can create a human review workflow or a worker task template programmatically. the apis you use depend on whether you are creating a amazon rekognition, amazon textract, or custom task type. this topic provides links to api reference documentation for each task type and programming task. the following apis can be used with augmented ai: amazon augmented aiuse the augmented ai api to start, stop, and delete human review loops. you can also list all human review loops and return information about human review loops in your account.learn more about human review loop apis in the . amazon rekognitionuse the humanloopconfig parameter of the  api to trigger a human review workflow using amazon rekognition. amazon sagemakeruse the amazon sagemaker api to create a , also known as a human review workflow. you can also create a , or worker task template.for more information, see the  or the  api documentation. amazon textractuse the humanloopconfig parameter of the  api to trigger a human review workflow using amazon textract. the following walkthroughs and tutorials provide example code and step-by-step instructions for creating human review workflows and worker task templates programmatically.  in the amazon rekognition developer guide in the amazon textract developer guide
before you begin using the amazon sagemaker console to create a labeling job, you must set up the dataset for use. do this: save two images at publicly available http urls. the images are used when creating instructions for completing a labeling task. the images should have an aspect ratio of around 2:1. for this exercise, the content of the images is not important. create an amazon s3 bucket to hold the input and output files. the bucket must be in the same region where you are running ground truth. make a note of the bucket name because you use it during step 2. place 5–10 png images in the bucket. create a manifest file for the dataset and store it in the s3 bucket. use these steps:  using a text editor, create a new text file. add a line similar to the following for each image file in your dataset: add one line for each png file in your s3 bucket. save the file in the s3 bucket containing your source files. record the name because you use it in step 2. noteit is not necessary to store the manifest file in the same bucket as the source file. you use the same bucket in this exercise because it is easier. for more information, see . assign the following permissions policy to the user that is creating the labeling job:  
to extract information from unstructured text and classify it into predefined categories, use an amazon sagemaker ground truth named entity recognition (ner) labeling task. traditionally, ner involves sifting through text data to locate noun phrases, called named entities, and categorizing each with a label, such as "person," "organization," or "brand." you can broaden this task to label longer spans of text and categorize those sequences with predefined labels that you specify.  when tasked with a named entity recognition labeling job, workers apply your labels to specific words or phrases within a larger text block. they choose a label, then apply it by using the cursor to highlight the part of the text to which the label applies. workers can't apply multiple labels to the same text, and labels can't overlap.  you can create a named entity recognition labeling job using the ground truth section of the amazon sagemaker console or the  operation. importantfor this task type, if you create your own manifest file, use  to identify the location of text files in amazon s3 that you want labeled. if you provide the text that you want labeled directly in the input manifest file, use . for more information, see . you can follow the instructions  to learn how to create a named entity recognition labeling job in the amazon sagemaker console. in step 10, choose text from the task category drop down menu, and choose named entity recognition  as the task type.  ground truth provides a worker ui similar to the following for labeling tasks. when you create the labeling job with the console, you specify instructions to help workers complete the job and labels that workers can choose from.   to create a named entity recognition labeling job, using the amazon sagemaker api operation . this api defines this operation for all aws sdks. to see a list of language-specific sdks supported for this operation, review the see also section of . follow the instructions on  and do the following while you configure your request:  pre-annotation lambda functions for this task type end with . to find the pre-annotation lambda arn for your region, see  . annotation-consolidation lambda functions for this task type end with . to find the annotation-consolidation lambda arn for your region, see . the following is an example of an  to create a labeling job in the us east (n. virginia) region. all parameters in red should be replaced with your specifications and resources.  if you create a labeling job using the api, you must supply a worker task template in . copy and modify the following template. only modify the , , and .  upload this template to s3, and provide the s3 uri for this file in . once you have created a named entity recognition labeling job, your output data will be located in the amazon s3 bucket specified in the  parameter when using the api or in the output dataset location field of the job overview section of the console.  to learn more about the output manifest file generated by ground truth and the file structure the ground truth uses to store your output data, see .  
the following sagemaker images are available in amazon sagemaker studio. sagemaker images contain the latest  and the latest version of the kernel. the name in brackets ([ ]) is the resource identifier of the sagemaker image as specified in the amazon resource name (arn) for the sagemaker image. data science [datascience-1.0]  is a  image with the most commonly used python packages and libraries, such as numpy and scikit learn. base python [python-3.6]mxnet (optimized for cpu) [mxnet-1.6-cpu-py36]mxnet (optimized for gpu) [mxnet-1.6-gpu-py36]pytorch (optimized for cpu) [pytorch-1.4-cpu-py36]pytorch (optimized for gpu) [pytorch-1.4-gpu-py36]tensorflow (optimized for cpu) [tensorflow-1.15-cpu-py36]tensorflow (optimized for gpu) [tensorflow-1.15-gpu-py36]tensorflow 2 (optimized for cpu) [tensorflow-2.1-cpu-py36]tensorflow 2 (optimized for gpu) [tensorflow-2.1-gpu-py36]
to create a labeling job using the amazon sagemaker api, you use the  operation. for specific instructions on creating a labeling job for a built-in task type, see that .  to use this operation, you need the following: a worker task template () or human task ui arn () in amazon s3. for 3d point cloud labeling jobs, use the arn listed in  for your task type. if you are using a built-in task type other than 3d point cloud tasks, you can add your worker instructions to one of the pre-built templates and save the template (using a .html or .liquid extension) in your s3 bucket. find the pre-build templates on your .if you are using a custom labeling workflow, you can create a custom template and save the template in your s3 bucket. to learn how to built a custom worker template, see . for custom html elements that you can use to customize your template, see . for a repository of demo templates for a variety of labeling tasks, see .at least one s3 bucket to store your input and output data. an input manifest file that specifies your input data. for information about creating an input manifest, see . a label category configuration file. for 3d point cloud task type, use the format in . for all other built-in task types and custom tasks, your label category configuration file must be a json file in the following format. identify the labels you want to use by replacing , ,, with your label categories. an aws identity and access management (iam) role with the amazonsagemakerfullaccess iam policy attached and with permissions to access your s3 buckets. if you require more granular permissions, see . note that amazonsagemakerfullaccess will grant your role permission to access all s3 buckets with  in the name.  if your input or output bucket name does not contain , you can attach a policy similar to the following to the role that is passed to the  operation. a pre-annotation and post-annotation (or annotation-consolidation) aws lambda function amazon resource name (arn) to process your input and output data. lambda functions are predefined in each aws region for built-in task types. to find the pre-annotation lambda arn for your region, see . to find the annotation-consolidation lambda arn for your region, see . for custom labeling workflows, you must provide a custom pre- and post-annotation lambda arn. to learn how to create these lambda functions, see .a work team arn. to learn more about work teams and workforces, see .  if you use the , use the  parameter in  to declare that your content is free of personally identifiable information or adult content. if your content contains personally identifiable information or adult content, amazon sagemaker might restrict the amazon mechanical turk workers that can view your task.  if you are creating a labeling job for a point cloud task type, you cannot use the amazon mechanical turk workforce.  (optional) for , you can have multiple workers label a single data object by inputting a number greater than one for the  parameter. for more information about annotation consolidation, see .the following is an example of an  to create a labeling job for a built-in task type in the us east (n. virginia) region.  importantif you set  to be greater than one hour (3,600 seconds), you must increases the max session duration of your execution role to be greater than or equal to the task timeout.you can modify the max session duration of your execution rule using the iam console, aws cli, and iam api. to modify your execution role, go to  in the iam user guide, select your preferred method (console, cli, or api) to modify the role from the topics list, and then select modifying a role maximum session duration to view the instructions. for 3d point cloud task types, refer to . for more information about this operation, see . for information about how to use other language-specific sdks, see  in the  topic.  
after you train your model, you can deploy it to get predictions in one of two ways: to set up a persistent endpoint to get one prediction at a time, use amazon sagemaker hosting services.to get predictions for an entire dataset, use amazon sagemaker batch transform.topics 
additional examples of using amazon sagemaker with apache spark are available at .  
you can use a worker template to customize the interface and instructions that your workers see when working on your tasks. use the instructions on this page to create a worker task template in the augmented ai area of the amazon sagemaker console. a starter template is provided for amazon textract and amazon rekognition tasks. to learn how to customize your template using html crowd elements, see . when you create a worker template in the worker task templates page of the augmented ai area of the amazon sagemaker console, a worker task template arn will be generated. use this arn as the input to when you create a flow definition using the api operation . you can choose this template when creating a flow definition on the human review workflows page of the console.  if you are creating a template for an amazon textract or amazon rekognition task type, and you want to preview your worker template, you will need to attach the policy described in  to the iam role that you use in this procedure.  to create a worker task template in the amazon sagemaker console open the amazon sagemaker console at . in amazon augmented ai in the left navigation pane, choose worker task templates. choose create template. in template name, enter a unique name. (optional) enter an iam role that grants a2i the permissions necessary to call services on your behalf.  in template type, choose a template type from the drop-down menu. if you are creating a template for a textract-form extraction or rekognition-image moderation task, choose the appropriate option.  enter your custom template elements as follows: if you selected the amazon textract or amazon rekognition task template, the template editor autopopulates with a default template that you can customize. if you are using a custom template, enter your predefined template in the editor. (optional) to complete this step, you must provided an iam role arn with permission to read amazon s3 objects that get rendered on your user interface in step 5.  you can only preview your template if you are creating templates for amazon textract or amazon rekognition.  choose see preview to preview the interface and instructions that workers will see. this is an interactive preview. after you complete the sample task and choose submit, you see the resulting output from the task that you just performed.  if you are creating a worker task template for a custom task type, you can preview your worker task ui using . for more information, see . when you're satisfied with your template, choose create. after you've created your template, you can select that template when you create a human review workflow in the console. your template also appears in the amazon augmented ai section of the amazon sagemaker console under worker task templates. choose your template to view its arn. use this arn when using the api operation .  
for this example, you use a training dataset of information about bank customers that includes the customer's job, marital status, and how they were contacted during the bank's direct marketing campaign. to use a dataset for a hyperparameter tuning job, you download it, transform the data, and then upload it to an amazon s3 bucket. for more information about the dataset and the data transformation that the example performs, see the hpo_xgboost_direct_marketing_sagemaker_apis notebook in the hyperparameter tuning section of the sagemaker examples tab in your notebook instance. to download and explore the dataset, run the following code in your notebook: before creating the hyperparameter tuning job, prepare the data and upload it to an s3 bucket where the hyperparameter tuning job can access it. run the following code in your notebook:  
the form wrapper for all custom tasks. sets and implements important actions for the proper submission of your form data. if a  of type "submit" is not included inside the  element, it will automatically be appended within the  element.  the following is an example of an image classification template that uses the  element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  this element has the following parent and child elements. parent elements: nonechild elements: any of the  elementsthe  element extends the  and inherits its events, such as  and . for more information, see the following. 
this section provides examples of the available input and output data formats used by the ip insights algorithm during training and inference. topics 
amazon sagemaker experiments is a capability of amazon sagemaker that lets you organize, track, compare, and evaluate your machine learning experiments. machine learning is an iterative process. you need to experiment with multiple combinations of data, algorithm and parameters, all the while observing the impact of incremental changes on model accuracy. over time this iterative experimentation can result in thousands of model training runs and model versions. this makes it hard to track the best performing models and their input configurations. it’s also difficult to compare active experiments with past experiments to identify opportunities for further incremental improvements. sagemaker experiments automatically tracks the inputs, parameters, configurations, and results of your iterations as trials. you can assign, group, and organize these trials into experiments. sagemaker experiments is integrated with amazon sagemaker studio providing a visual interface to browse your active and past experiments, compare trials on key performance metrics, and identify the best performing models. sagemaker experiments comes with its own  which makes the analytics capabilities easily accessible in amazon sagemaker notebooks. because sagemaker experiments enables tracking of all the steps and artifacts that went into creating a model, you can quickly revisit the origins of a model when you are troubleshooting issues in production, or auditing your models for compliance verifications. topics amazon sagemaker experiments offers a structured organization scheme to help users group and organize their machine learning iterations. the top level entity, an experiment, is a collection of trials that are observed, compared, and evaluated as a group. a trial is a set of steps called trial components. each trial component can include a combination of inputs such as datasets, algorithms, and parameters, and produce specific outputs such as models, metrics, datasets, and checkpoints. examples of trial components are data pre-processing jobs, training jobs, and batch transform jobs. the goal of an experiment is to determine the trial that produces the best model. multiple trials are performed, each one isolating and measuring the impact of a change to one or more inputs, while keeping the remaining inputs constant. by analyzing the trials, you can determine which features have the most effect on the model. amazon sagemaker experiments enables tracking of experiments. automated tracking sagemaker experiments automatically tracks amazon sagemaker autopilot jobs as experiments with their underlying training jobs tracked as trials. sagemaker experiments also automatically tracks sagemaker independently executed training, batch transform, and processing jobs as trial components, whether assigned to a trial or left unassigned. unassigned trial components can be associated with a trial at a later time. all experiment artifacts including datasets, algorithms, hyperparameters, and model metrics are tracked and recorded. this data allows customers to trace the complete lineage of a model which helps with model governance, auditing, and compliance verifications. manual tracking sagemaker experiments provides tracking apis for recording and tracking machine learning workflows running locally on sagemaker studio notebooks, including classic sagemaker notebooks. these experiments must be part of a sagemaker training, batch transform, or processing job. amazon sagemaker experiments is integrated with amazon sagemaker studio. when you use sagemaker studio, sagemaker experiments automatically tracks your experiments and trials, and presents visualizations of the tracked data and an interface to search the data. sagemaker experiments automatically organizes, ranks, and sorts trials based on a chosen metric using the concept of a trial leaderboard. sagemaker studio produces real-time data visualizations, such as metric charts and graphs, to quickly compare and identify the best performing models. these are updated in real-time as the experiment progresses. amazon sagemaker experiments is integrated with amazon sagemaker autopilot. when you perform an automated machine learning job using autopilot, sagemaker experiments creates an experiment for the job, and trials for each of the different combinations of trial components, parameters, and artifacts that autopilot tries for the job. you can visually drill into all trials and components using sagemaker. 
for a list of the aws regions supported by amazon sagemaker and the amazon elastic compute cloud (amazon ec2) instance types that are available in each region, see . each aws region is divided into sub-regions known as availability zones. for a given region, the availability zones in that region don't always contain the same instance types supported by amazon sagemaker. if any availability zone in the region contains a given instance type, then that instance type is listed as available in the region. for a list of the aws regions supported by amazon sagemaker studio, see . for a list of the amazon sagemaker service endpoints for each region and the amazon sagemaker service quotas for each instance type, see . you can request a quota increase using service quotas or the aws support center. to request an increase, see  in the aws general reference. topics 
a widget for drawing rectangles on an image and assigning a label to the portion of the image that is enclosed in each rectangle. the following is an example of a liquid template that uses the  element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template. for more examples, see this .  the following attributes are supported by this element. the text to display above the image. this is typically a question or simple instruction for the worker. an array of json objects, each of which sets a bounding box when the component is loaded. each json object in the array contains the following properties. bounding boxes set via the  property can be adjusted and whether or not a worker answer was adjusted is tracked via an  boolean in the worker answer output. height – the height of the box in pixels.label – the text assigned to the box as part of the labeling task. this text must match one of the labels defined in the labels attribute of the <crowd-bounding-box> element.left – distance of the top-left corner of the box from the left side of the image, measured in pixels.top – distance of the top-left corner of the box from the top of the image, measured in pixels.width – the width of the box in pixels. you can extract the bounding box initial value from a manifest file of a previous job in a custom template using the liquid templating language: a json formatted array of strings, each of which is a label that a worker can assign to the image portion enclosed by a rectangle. limit: 10 labels. the name of this widget. it's used as a key for the widget's input in the form output. the url of the image on which to draw bounding boxes.  this element has the following parent and child elements. parent elements: child elements: , the following regions are required by this element. general instructions about how to draw bounding boxes. important task-specific instructions that are displayed in a prominent place. the following output is supported by this element. an array of json objects, each of which specifies a bounding box that has been created by the worker. each json object in the array contains the following properties. height – the height of the box in pixels.label – the text assigned to the box as part of the labeling task. this text must match one of the labels defined in the labels attribute of the <crowd-bounding-box> element.left – distance of the top-left corner of the box from the left side of the image, measured in pixels.top – distance of the top-left corner of the box from the top of the image, measured in pixels.width – the width of the box in pixels.a json object that specifies the dimensions of the image that is being annotated by the worker. this object contains the following properties. height – the height, in pixels, of the image.width – the width, in pixels, of the image.example : sample element outputsthe following are samples of outputs from common use scenarios for this element.single label, single box / multiple label, single box   single label, multiple box   multiple label, multiple box   you could have many labels available, but only the ones that are used appear in the output. for more information, see the following. 
you can use a vendor-managed workforce to label your data using amazon sagemaker ground truth (ground truth) and amazon augmented ai (amazon a2i). vendors have extensive experience in providing data labeling services for the purpose of machine learning. vendor workforces for these two services must be created and managed seperately through the amazon sagemaker console.  vendors make their services available via the aws marketplace. you can find details of the vendor's services on their detail page, such as the number of workers and the hours that they work. you can use these details to make estimates of how much the labeling job will cost and the amount of time that you can expect the job to take. once you have chosen a vendor you subscribe to their services using the aws marketplace. a subscription is an agreement between you and the vendor. the agreement spells out the details of the agreement, such as price, schedule, or refund policy. you work directly with the vendor if there are any issues with your labeling job. you can subscribe to any number of vendors to meet your data annotation needs. when you create a labeling job or human review worklow you can specify that the job be routed to a specific vendor. before you send sensitive data to a vendor, check the vendor's security practices on their detail page and review the end user license agreement (eula) that is part of your subscription agreement.  you must use the console to subscribe to a vendor workforce. once you have a subscription, you can use the  operation to list your subscribed vendors. to subscribe to a vendor workforce open the amazon sagemaker console at . choose the appropriate page in the amazon sagemaker console. for ground truth labeling jobs, choose labeling workforces, choose vendor, and then choose find data labeling services.for amazon a2i human review workflows, choose human review workforces, choose vendor, and then choose find human review services. the console opens the aws marketplace with: data labeling services category selected for ground truthhuman review services category selected for amazon a2ihere you see a list of the vendor services available for this service.  choose a vendor. the aws marketplace shows detailed information about the data labeling or human review service. use this information to determine if the vendor meets your requirements for your task. if the vendor meets your requirements, choose continue to subscribe. review the details of the subscription. if you agree to the terms, choose subscribe to complete your subscription to the service. 
you can use amazon sagemaker to train and deploy a model using custom pytorch code. the amazon sagemaker python sdk pytorch estimators and models and the amazon sagemaker open-source pytorch container make writing a pytorch script and running it in amazon sagemaker easier. i want to train a custom pytorch model in amazon sagemaker.for a sample jupyter notebook, see .for documentation, see . i have a pytorch model that i trained in amazon sagemaker, and i want to deploy it to a hosted endpoint.. i have a pytorch model that i trained outside of amazon sagemaker, and i want to deploy it to an amazon sagemaker endpoint. i want to see the api documentation for  pytorch classes. i want to see information about amazon sagemaker pytorch containers..  for general information about writing pytorch training scripts and using pytorch estimators and models with amazon sagemaker, see . for information about pytorch versions supported by the amazon sagemaker pytorch container, see . 
when you onboard to amazon sagemaker studio using iam authentication, studio creates a domain for your account. a domain consists of a list of authorized users, configuration settings, and an amazon elastic file system (amazon efs) volume, which contains data for the users, including notebooks, resources, and artifacts. a user can have multiple applications (apps) which support the reading and execution experience of the user’s notebooks, terminals, and consoles. to return studio to the state it was in before you onboarded, you must delete this domain. you must delete the domain if you want to switch authentication modes from iam to aws sso. to delete a domain, the domain cannot contain any user profiles. to delete a user profile, the profile cannot contain any non-failed apps. when you delete these resources, the following occurs: app – the data (files and notebooks) in a user's home directory is saved. unsaved notebook data is lost.user profile – the user is no longer able to sign in to studio and loses access to their home directory but the data is not deleted. an admin can retrieve the data from the amazon efs volume where it is stored under the user's aws account.noteyou must have admin permission to delete a domain. you can only delete an app whose status is , which is displayed as ready in studio. an app whose status is  doesn't need to be deleted to delete the containing domain. in studio, an attempt to delete an app in the failed state results in an error. topics to delete a domain open the . choose amazon sagemaker studio at the top left of the page to open the amazon sagemaker studio control panel. repeat the following steps for each user in the user name list. choose the user. on the user details page, for each non-failed app in the apps list, choose delete app. on the delete app dialog, choose yes, delete app, type delete in the confirmation field, and then choose delete. when the status for all apps show as deleted, choose delete user. notewhen a user is deleted, they lose access to the amazon efs volume that contains their data, including notebooks and other artifacts. when all users are deleted, choose delete studio. on the delete studio dialog, choose yes, delete studio, type delete in the confirmation field, and then choose delete. for a list of aws regions supported by amazon sagemaker studio, see . to delete a domain retrieve the list of domains in your account. retrieve the list of applications for the domain to be deleted. delete each application in the list. retrieve the list of user profiles in the domain. delete each user profile in the list. delete the domain. 
as a managed service, amazon sagemaker performs operations on your behalf on the aws hardware that is managed by amazon sagemaker. amazon sagemaker can perform only operations that the user permits. an amazon sagemaker user can grant these permissions with an iam role (referred to as an execution role). the user passes the role when making these api calls: , , , , and . you attach the following trust policy to the iam role which grants amazon sagemaker principal permissions to assume the role, and is the same for all of the execution roles:  the permissions that you need to grant to the role vary depending on the api that you call. the following sections explain these permissions. noteinstead of managing permissions by crafting a permission policy, you can use the aws-managed  permission policy. the permissions in this policy are fairly broad, to allow for any actions you might want to perform in amazon sagemaker. for a listing of the policy including information about the reasons for adding many of the permissions, see . if you prefer to create custom policies and manage permissions to scope the permissions only to the actions you need to perform with the execution role, see the following topics. for more information about iam roles, see  in the iam user guide. topics the permissions that you grant to the execution role for calling the  api depend on what you plan to do with the notebook instance. if you plan to use it to invoke amazon sagemaker apis and pass the same role when calling the  and  apis, attach the following permissions policy to the role: to tighten the permissions, limit them to specific amazon s3 and amazon ecr resources, by restricting , as follows: if you plan to access other resources, such as amazon dynamodb or amazon relational database service, add the relevant permissions to this policy. in the preceding policy, you scope the policy as follows: scope the  permission to the specific bucket that you specify as  in a  request.scope , , and  permissions as follows: scope to the following values that you specify in a  request:   scope to the following values that you specify in a  request:   scope  permissions as follows:scope to the  value that you specify in a  request.scope to the  value that you specify in a  request:the  and  actions are applicable for "*" resources. for more information, see  in the amazon cloudwatch user guide. for an execution role that you can pass in a  api request, you can attach the following permission policy to the role: instead of the specifying , you could scope these permissions to specific amazon s3 and amazon ecr resources: if the training container associated with the hyperparameter tuning job needs to access other data sources, such as dynamodb or amazon rds resources, add relevant permissions to this policy. in the preceding policy, you scope the policy as follows: scope the  permission to a specific bucket that you specify as the  in a  request.scope the and  permissions to the following objects that you specify in the input and output data configuration in a  request:   scope amazon ecr permissions to the registry path () that you specify in a  request.the  and  actions are applicable for "*" resources. for more information, see  in the amazon cloudwatch user guide. if you specify a private vpc for your hyperparameter tuning job, add the following permissions: if your input is encrypted using server-side encryption with an aws kms–managed key (sse-kms), add the following permissions: if you specify a kms key in the output configuration of your hyperparameter tuning job, add the following permissions: if you specify a volume kms key in the resource configuration of your hyperparameter tuning job, add the following permissions: for an execution role that you can pass in a  api request, you can attach the following permission policy to the role: instead of the specifying , you could scope these permissions to specific amazon s3 and amazon ecr resources: if  needs to access other data sources, such as dynamodb or amazon rds resources, add relevant permissions to this policy. in the preceding policy, you scope the policy as follows: scope the  permission to a specific bucket that you specify as the  in a  request.scope the and  permissions to the objects that will be downloaded or uploaded in the  and  in a  request.scope amazon ecr permissions to the registry path () that you specify in a  request.the  and  actions are applicable for "*" resources. for more information, see  in the amazon cloudwatch user guide. if you specify a private vpc for your processing job, add the following permissions: if your input is encrypted using server-side encryption with an aws kms–managed key (sse-kms), add the following permissions: if you specify a kms key in the output configuration of your processing job, add the following permissions: if you specify a volume kms key in the resource configuration of your processing job, add the following permissions: for an execution role that you can pass in a  api request, you can attach the following permission policy to the role: instead of the specifying , you could scope these permissions to specific amazon s3 and amazon ecr resources: if  needs to access other data sources, such as dynamodb or amazon rds resources, add relevant permissions to this policy. in the preceding policy, you scope the policy as follows: scope the  permission to a specific bucket that you specify as the  in a  request.scope the and  permissions to the following objects that you specify in the input and output data configuration in a  request:   scope amazon ecr permissions to the registry path () that you specify in a  request.the  and  actions are applicable for "*" resources. for more information, see  in the amazon cloudwatch user guide. if you specify a private vpc for your training job, add the following permissions: if your input is encrypted using server-side encryption with an aws kms–managed key (sse-kms), add the following permissions: if you specify a kms key in the output configuration of your training job, add the following permissions: if you specify a volume kms key in the resource configuration of your training job, add the following permissions: for an execution role that you can pass in a  api request, you can attach the following permission policy to the role: instead of the specifying , you can scope these permissions to specific amazon s3 and amazon ecr resources: if  need to access other data sources, such as amazon dynamodb or amazon rds resources, add relevant permissions to this policy. in the preceding policy, you scope the policy as follows: scope s3 permissions to objects that you specify in the  in a  request.scope amazon ecr permissions to a specific registry path that you specify as the  and  in a  request.the  and  actions are applicable for "*" resources. for more information, see  in the amazon cloudwatch user guide. if you specify a private vpc for your model, add the following permissions: the  managed policy includes all of the necessary permissions to perform most actions in amazon sagemaker. you can use attach this policy to any role that you pass to an amazon sagemaker execution role. you can also create more narrowly-scoped policies if you want more granular control of the permissions that you grant to your execution role. the following list explains why some of the categories of permissions in the  policy are needed. needed for automatically scaling an amazon sagemaker real-time inference endpoint. needed to view aws ai marketplace subscriptions. needed to post cloudwatch metrics, interact with alarms, and upload cloudwatch logs logs in your account. needed for aws codecommit integration with amazon sagemaker notebook instances. needed for amazon sagemaker ground truth to define your private workforce and work teams. needed to manage elastic network interfaces when you specify a amazon vpc for your amazon sagemaker jobs and notebook instances. all amazon sagemaker services launch amazon ec2 instances and require this permission set. needed to pull and store docker artifacts for training and inference. this is required only if you use your own container in amazon sagemaker. needed to integrate amazon elastic inference with amazon sagemaker. needed for inference pipeline pre-processing from within amazon sagemaker notebook instances. needed for amazon sagemaker ground truth. needed to give the amazon sagemaker console access to list available roles. needed to give the amazon sagemaker console access to list the available aws kms keys. needed to allow amazon sagemaker jobs and endpoints to publish log streams. 
this page contains links to the documentation for previous versions of amazon sagemaker xgboost. topics 
the  sparkml serving model and predictor and the amazon sagemaker open-source sparkml serving container support deploying apache spark ml pipelines serialized with mleap in amazon sagemaker to get inferences. for information about using the sparkml serving container to deploy models to amazon sagemaker, see . for information about the  sparkml serving model and predictors, see . 
amazon sagemaker is a fully managed service that enables you to quickly and easily integrate machine learning-based models into your applications. this section provides an overview of machine learning and explains how amazon sagemaker works. if you are a first-time user of amazon sagemaker, we recommend that you read the following sections in order: topics 
a custom rule can be configured for a training job using the  and  objects in the  api. the following code sample shows how to configure a custom  rule written with the smdebug library using this amazon sagemaker api. this example assumes that you’ve written the custom rule in custom_rules.py file and uploaded it to an amazon s3 bucket. the example provides pre-built docker images that you can use to run your custom rules. these are listed at . you specify the url registry address for the pre-built docker image in the  parameter. 
when you create an aws account, you get a single sign-in identity that has complete access to all of the aws services and resources in the account. this identity is called the aws account root user. signing in to the aws console using the email address and password that you used to create the account gives you complete access to all of the aws resources in your account.  we strongly recommend that you not use the root user for everyday tasks, even the administrative ones. instead, adhere to the , an aws identity and access management (iam) administrator user. then securely lock away the root user credentials and use them to perform only a few account and service management tasks.  to create an administrator user create an administrator user in your aws account. for instructions, see  in the iam user guide. notewe assume that you use administrator user credentials for the exercises and procedures in this guide. if you choose to create and use another iam user, grant that user minimum permissions. for more information, see .
to specify the metrics and target values for a scaling policy, you configure a target-tracking scaling policy. you can use either a predefined metric or a custom metric. scaling policy configuration is represented by a json block. you save your scaling policy configuration as a json block in a text file. you use that text file when invoking the aws cli or the application auto scaling api. for more information about policy configuration syntax, see  in the application auto scaling api reference.  the following options are available for defining a target-tracking scaling policy configuration. to quickly define a target-tracking scaling policy for a variant, use the   predefined metric.  is the average number of times per minute that each instance for a variant is invoked. we strongly recommend using this metric. to use a predefined metric in a scaling policy, create a target tracking configuration for your policy. in the target tracking configuration, include a  for the predefined metric and a  for the target value of that metric. examplethe following example is a typical policy configuration for target-tracking scaling for a variant. in this configuration, we use the  predefined metric to adjust the number of variant instances so that each instance has a  metric of 70.    if you need to define a target-tracking scaling policy that meets your custom requirements, define a custom metric. you can define a custom metric based on any production variant metric that changes in proportion to scaling.  not all amazon sagemaker metrics work for target tracking. the metric must be a valid utilization metric, and it must describe how busy an instance is. the value of the metric must increase or decrease in inverse proportion to the number of variant instances. that is, the value of the metric should decrease when the number of instances increases. importantbefore deploying automatic scaling in production, you must test automatic scaling with your custom metric. examplethe following example is a target-tracking configuration for a scaling policy. in this configuration, for a variant named , a custom metric adjusts the variant based on an average cpu utilization of 50 percent across all instances.   to add a cooldown period for scaling-out your model, specify a value, in seconds, for . similarly, to add a cooldown period for scaling-in your model, add a value, in seconds, for . for more information about  and , see  in the application auto scaling api reference.  examplethe following is an example target-tracking configuration for a scaling policy. in this configuration, the  predefined metric is used to adjust scaling based on an average of 70 across all instances of that variant. the configuration provides a scale-in cooldown period of 10 minutes and a scale-out cooldown period of 5 minutes.    
this rule detects when the loss is not decreasing in value at an adequate rate. these losses must be scalars.  this rule can be applied either to one of the supported deep learning frameworks (tensorflow, mxnet, and pytorch) or to the xgboost algorithm. you must specify either the  or  parameter. if both the parameters are specified, the rule inspects the union of tensors from both sets. for an example of how to configure and deploy a built-in rule, see . parameter descriptions for the lossnotdecreasing rule   
the amazon sagemaker deepar forecasting algorithm is a supervised learning algorithm for forecasting scalar (one-dimensional) time series using recurrent neural networks (rnn). classical forecasting methods, such as autoregressive integrated moving average (arima) or exponential smoothing (ets), fit a single model to each individual time series. they then use that model to extrapolate the time series into the future.  in many applications, however, you have many similar time series across a set of cross-sectional units. for example, you might have time series groupings for demand for different products, server loads, and requests for webpages. for this type of application, you can benefit from training a single model jointly over all of the time series. deepar takes this approach. when your dataset contains hundreds of related time series, deepar outperforms the standard arima and ets methods. you can also use the trained model to generate forecasts for new time series that are similar to the ones it has been trained on. the training input for the deepar algorithm is one or, preferably, more  time series that have been generated by the same process or similar processes. based on this input dataset, the algorithm trains a model that learns an approximation of this process/processes and uses it to predict how the target time series evolves. each target time series can be optionally associated with a vector of static (time-independent) categorical features provided by the  field and a vector of dynamic (time-dependent) time series provided by the  field. amazon sagemaker trains the deepar model by randomly sampling training examples from each target time series in the training dataset. each training example consists of a pair of adjacent context and prediction windows with fixed predefined lengths. to control how far in the past the network can see, use the  hyperparameter. to control how far in the future predictions can be made, use the  hyperparameter. for more information, see . topics deepar supports two data channels. the required  channel describes the training dataset. the optional  channel describes a dataset that the algorithm uses to evaluate model accuracy after training. you can provide training and test datasets in  format. files can be in gzip or  file format. when specifying the paths for the training and test data, you can specify a single file or a directory that contains multiple files, which can be stored in subdirectories. if you specify a directory, deepar uses all files in the directory as inputs for the corresponding channel, except those that start with a period (.) and those named _success. this ensures that you can directly use output folders produced by spark jobs as input channels for your deepar training jobs. by default, the deepar model determines the input format from the file extension (, , or ) in the specified input path. if the path does not end in one of these extensions, you must explicitly specify the format in the sdk for python. use the  parameter of the  class. the records in your input files should contain the following fields: —a string with the format . the start timestamp can't contain time zone information.—an array of floating-point values or integers that represent the time series. you can encode missing values as  literals, or as  strings in json, or as  floating-point values in parquet. (optional)—an array of arrays of floating-point values or integers that represents the vector of custom feature time series (dynamic features). if you set this field, all records must have the same number of inner arrays (the same number of feature time series). in addition, each inner array must have the same length as the associated  value. missing values are not supported in the features. for example, if target time series represents the demand of different products, an associated  might be a boolean time-series which indicates whether a promotion was applied (1) to the particular product or not (0):   (optional)—an array of categorical features that can be used to encode the groups that the record belongs to. categorical features must be encoded as a 0-based sequence of positive integers. for example, the categorical domain {r, g, b} can be encoded as {0, 1, 2}. all values from each categorical domain must be represented in the training dataset. that's because the deepar algorithm can forecast only for categories that have been observed during training. and, each categorical feature is embedded in a low-dimensional space whose dimensionality is controlled by the  hyperparameter. for more information, see .if you use a json file, it must be in  format. for example: in this example, each time series has two associated categorical features and one time series features. for parquet, you use the same three fields as columns. in addition,  can be the  type. you can compress parquet files using gzip () or the snappy compression library (). if the algorithm is trained without  and  fields, it learns a "global" model, that is a model that is agnostic to the specific identity of the target time series at inference time and is conditioned only on its shape. if the model is conditioned on the  and  feature data provided for each time series, the prediction will probably be influenced by the character of time series with the corresponding  features. for example, if the  time series represents the demand of clothing items, you can associate a two-dimensional  vector that encodes the type of item (e.g. 0 = shoes, 1 = dress) in the first component and the color of an item (e.g. 0 = red, 1 = blue) in the second component. a sample input would look as follows: at inference time, you can request predictions for targets with  values that are combinations of the  values observed in the training data, for example: the following guidelines apply to training data: the start time and length of the time series can differ. for example, in marketing, products often enter a retail catalog at different dates, so their start dates naturally differ. but all series must have the same frequency, number of categorical features, and number of dynamic features. shuffle the training file with respect to the position of the time series in the file. in other words, the time series should occur in random order in the file.make sure to set the  field correctly. the algorithm uses the  timestamp to derive the internal features. if you use categorical features (), all time series must have the same number of categorical features. if the dataset contains the  field, the algorithm uses it and extracts the cardinality of the groups from the dataset. by default,  is . if the dataset contains the  field, but you don't want to use it, you can disable it by setting  to . if a model was trained using a  feature, you must include it for inference.if your dataset contains the  field, the algorithm uses it automatically. all time series have to have the same number of feature time series. the time points in each of the feature time series correspond one-to-one to the time points in the target. in addition, the entry in the  field should have the same length as the . if the dataset contains the  field, but you don't want to use it, disable it by setting( to ). if the model was trained with the  field, you must provide this field for inference. in addition, each of the features has to have the length of the provided target plus the . in other words, you must provide the feature value in the future.if you specify optional test channel data, the deepar algorithm evaluates the trained model with different accuracy metrics. the algorithm calculates the root mean square error (rmse) over the test data as follows:  y**i,t is the true value of time series i at the time t. ŷ**i,t is the mean prediction. the sum is over all n time series in the test set and over the last τ time points for each time series, where τ corresponds to the forecast horizon. you specify the length of the forecast horizon by setting the  hyperparameter. for more information, see . in addition, the algorithm evaluates the accuracy of the forecast distribution using weighted quantile loss. for a quantile in the range [0, 1], the weighted quantile loss is defined as follows:   q**i,t(τ) is the τ-quantile of the distribution that the model predicts. to specify which quantiles to calculate loss for, set the  hyperparameter. in addition to these, the average of the prescribed quantile losses is reported as part of the training logs. for information, see .  for inference, deepar accepts json format and the following fields: , which includes one or more time series in json lines formata name of , which includes parameters for generating the forecast for more information, see . when preparing your time series data, follow these best practices to achieve the best results: except for when splitting your dataset for training and testing, always provide the entire time series for training, testing, and when calling the model for inference. regardless of how you set , don't break up the time series or provide only a part of it. the model uses data points further back than the value set in  for the lagged values feature.when tuning a deepar model, you can split the dataset to create a training dataset and a test dataset. in a typical evaluation, you would test the model on the same time series used for training, but on the future  time points that follow immediately after the last time point visible during training. you can create training and test datasets that satisfy this criteria by using the entire dataset (the full length of all time series that are available) as a test set and removing the last  points from each time series for training. during training, the model doesn't see the target values for time points on which it is evaluated during testing. during testing, the algorithm withholds the last  points of each time series in the test set and generates a prediction. then it compares the forecast with the withheld values. you can create more complex evaluations by repeating time series multiple times in the test set, but cutting them at different endpoints. with this approach, accuracy metrics are averaged over multiple forecasts from different time points. for more information, see .avoid using very large values (>400) for the  because it makes the model slow and less accurate. if you want to forecast further into the future, consider aggregating your data at a higher frequency. for example, use  instead of .because lags are used, a model can look further back in the time series than the value specified for . therefore, you don't need to set this parameter to a large value. we recommend starting with the value that you used for .we recommend training a deepar model on as many time series as are available. although a deepar model trained on a single time series might work well, standard forecasting algorithms, such as arima or ets, might provide more accurate results. the deepar algorithm starts to outperform the standard methods when your dataset contains hundreds of related time series. currently, deepar requires that the total number of observations available across all training time series is at least 300.you can train deepar on both gpu and cpu instances and in both single and multi-machine settings. we recommend starting with a single cpu instance (for example, ml.c4.2xlarge or ml.c4.4xlarge), and switching to gpu instances and multiple machines only when necessary. using gpus and multiple machines improves throughput only for larger models (with many cells per layer and many layers) and for large mini-batch sizes (for example, greater than 512). for inference, deepar supports only cpu instances. specifying large values for , , , , or  can create models that are too large for small instances. in this case, use a larger instance type or reduce the values for these parameters. this problem also frequently occurs when running hyperparameter tuning jobs. in that case, use an instance type large enough for the model tuning job and consider limiting the upper values of the critical parameters to avoid job failures.  for a sample notebook that shows how to prepare a time series dataset for training the amazon sagemaker deepar algorithm and how to deploy the trained model for performing inferences, see  as well as , which illustrates the advanced features of deepar on a real world dataset. for instructions on creating and accessing jupyter notebook instances that you can use to run the example in amazon sagemaker, see . after creating and opening a notebook instance, choose the sagemaker examples tab to see a list of all of the amazon sagemaker examples. to open a notebook, choose its use tab, and choose create copy. 
query a trained model by using the model's endpoint. the endpoint takes .jpg and .png image formats with  and  content-types. the response is the class index with a confidence score and bounding box coordinates for all objects within the image encoded in json format. the following is an example of response .json file: each row in this .json file contains an array that represents a detected object. each of these object arrays consists of a list of six numbers. the first number is the predicted class label. the second number is the associated confidence score for the detection. the last four numbers represent the bounding box coordinates [xmin, ymin, xmax, ymax]. these output bounding box corner indices are normalized by the overall image size. note that this encoding is different than that use by the input .json format. for example, in the first entry of the detection result, 0.3088374733924866 is the left coordinate (x-coordinate of upper-left corner) of the bounding box as a ratio of the overall image width, 0.07030484080314636 is the top coordinate (y-coordinate of upper-left corner) of the bounding box as a ratio of the overall image height, 0.7110607028007507 is the right coordinate (x-coordinate of lower-right corner) of the bounding box as a ratio of the overall image width, and 0.9345266819000244 is the bottom coordinate (y-coordinate of lower-right corner) of the bounding box as a ratio of the overall image height.  to avoid unreliable detection results, you might want to filter out the detection results with low confidence scores. in the , we provide scripts to remove the low confidence detections. scripts are also provided to plot the bounding boxes on the original image. for batch transform, the response is in json format, where the format is identical to the json format described above. the detection results of each image is represented as a json file. for example: for more details on training and inference, see the . accept: application/json;annotation=1 
amazon rekognition makes it easy to add image analysis to your applications. the amazon rekognition api operation is directly integrated with amazon a2i so that you can easily create a human loop to review unsafe images, such as explicit adult or violent content. you can use  to configure a human loop using a flow definition arn. this enables amazon a2i to analyze predictions made by amazon rekognition and sent results to a human for review they meet the conditions set in your flow definition. you can set the following activation conditions when using the amazon rekognition task type: trigger human review for labels identified by amazon rekognition based on the label confidence score.randomly send a sample of images to humans for review.you can set these activation conditions using the amazon sagemaker console when you create a human review workflow, or by creating a json for human loop activation conditions and specifying this as input in the  parameter of  api operation. to learn how specify activation conditions in json format, see  and . notewhen using augmented ai with amazon rekognition, create augmented ai resources in the same aws region you use to call .  to integrate a human review into an amazon rekognition, see the following topics: after you've created your flow definition, see  to learn how to integrate your flow definition into your amazon rekognition task.  for an end-to-end example that demonstrates how to use amazon rekognition with augmented ai, you can use %20and%20rekognition%20detectmoderationlabels.ipynb) in an amazon sagemaker notebook instance.  to use amazon textract with augmented ai using a amazon sagemaker notebook if you do not have an active amazon sagemaker notebook instance, create one by following the instructions in . when your notebook instance is active, choose open jupyterlab to the right of the notebook instance's name. it may take a few moments for jupyterlab to load.  select the  icon to clone a github repository into your workspace.  enter the  repository https url.  choose clone. open the notebook amazon augmented ai (amazon a2i) integration with amazon rekognition [example].  follow the instructions in the notebook to configure your flow definition and human loop and run the cells.  to avoid incurring unnecessary charges, when you are done with the demo stop and delete your notebook instance in addition to any amazon s3 buckets, iam roles, and cloudwatch events resources created during the walkthrough. when they're assigned a review task in an amazon rekognition workflow, workers might see ui similar to the following:  you can customize this interface in the amazon sagemaker console when you create your human review definition, or by creating and using a custom template. to learn more, see . 
after you build and train your models, you can deploy them to get predictions in one of two ways: to set up a persistent endpoint to get predictions from your models, use amazon sagemaker hosting services. for an overview on deploying a single model or multiple models with amazon sagemaker hosting services, see . for an example of how to deploy a model to the amazon sagemaker hosting service, see . or, if you prefer, watch the following video tutorial: to get predictions for an entire dataset, use amazon sagemaker batch transform. for an overview on deploying a model with amazon sagemaker batch transform, see . for an example of how to deploy a model with batch transform, see . or, if you prefer, watch the following video tutorial: these topics assume that you have built and trained one or more machine learning models and are ready to deploy them. if you are new to amazon sagemaker and have not completed these prerequisite tasks, work through the steps in the  tutorial to familiarize yourself with an example of how amazon sagemaker manages the data science process and how it handles model deployment. for more information about building a model, see . for information about training a model, see . amazon sagemaker provides features to manage resources and optimize inference performance when deploying machine learning models. for guidance on using inference pipelines, compiling and deploying models with neo, elastic inference, and automatic model scaling, see the following topics. to manage data processing and real-time predictions or to process batch transforms in a pipeline, see . if you want to deploy a model on inf1 instances, see .to train tensorflow, apache mxnet, pytorch, onnx, and xgboost models once and optimize them to deploy on arm, intel, and nvidia processors, see .to preprocess entire datasets quickly or to get inferences from a trained model for large datasets when you don't need a persistent endpoint, see .to speed up the throughput and decrease the latency of getting real-time inferences from your deep learning models that are deployed as amazon sagemaker hosted models using a gpu instance for your endpoint, see .to dynamically adjust the number of instances provisioned in response to changes in your workload, see .to create an endpoint that can host multiple models using a shared serving container, see .to test multiple models in production, see .for guidance on managing model deployments, including monitoring, troubleshooting, and best practices, and for information on storage associated with inference hosting instances: for tools that can be used to monitor model deployments, see .for troubleshooting model deployments, see .for model deployment best practices, see .for information about the size of storage volumes provided for different sizes of hosting instances, see .for developers that need more advanced guidance on how to run your own inference code: to run your own inference code hosting services, see . to run your own inference code for batch transforms, see . topics 
due to gpu memory scarcity, the  environment variable can be specified to optimize on whether the classification/regression or the  inference network is loaded into gpu. if the majority of your inference is for classification or regression, specify . the following is a batch transform example of using 4 instances of p3.2xlarge that optimizes for classification/regression inference: content-type: application/json content-type: application/jsonlines for classification problems, the length of the scores vector corresponds to . for regression problems, the length is 1. accept: application/json accept: application/jsonlines in both the classification and regression formats, the scores apply to individual labels.  

a widget for classifying various forms of content—such as audio, video, or text—into one or more categories. the content to classify is referred to as an object.  the following is an example of an html worker task template built using this crowd element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  the following attributes are supported by the  element. each attribute accepts a string value or string values.  required. a json-formatted array of strings, each of which is a category that a worker can assign to the object.  required. the text to display above the image. this is typically a question or simple instruction for workers. required. the name of this widget. in the form output, the name is used as a key for the widget's input. optional. a json-formatted string with the following format: . this attribute sets a default value that workers can choose if none of the labels applies to the object shown in the worker ui. this element has the following parent and child elements: parent elements: child elements: , , this element uses the following regions. the content to be classified by the worker. content can be plain text or an object that you specify in the template using html. for example, you can use html elements to include a video or audio player, embedding a pdf file, or include a comparison of two or more images. general instructions about how to classify text. important task-specific instructions. these instructions are displayed prominently. the output of this element is an object that uses the specified  value as a property name, and a string from  as the property's value. example : sample element outputsthe following is a sample of output from this element.   for more information, see the following: 
amazon sagemaker debugger is designed to be aware that tensors required to execute a rule may not be available at every step. hence, it raises a few exceptions, which allow you to control what happens when a tensor is missing. these exceptions are available in the . you can import them as follows: the following exceptions are available: : the tensor requested is not available for the step. this might mean that this step might not be saved at all by the hook, or that this step might have saved some tensors but the requested tensor is not part of them. note that when you see this exception, it means that this tensor can never become available for this step in the future. if the tensor has reductions saved for the step, it notifies you they can be queried.: this tensor is not being saved or has not been saved by the  api. this means that this tensor is never seen for any step in .: the step was not saved and debugger has no data from the step.: the step has not yet been seen by . it may be available in the future if the training is still going on. debugger automatically loads new data as it becomes available.: raised when the training ends. once you see this, you know that there are no more steps and no more tensors to be saved.: the index reader is not valid.: a worker was invoked that was not valid.: evaluation of the rule at the step resulted in the condition being met.: insufficient information was provided to invoke the rule.
when the labels on a dataset need to be validated, amazon sagemaker ground truth provides functionality to have workers verify that labels are correct or to adjust previous labels. these types of jobs fall into two distinct categories: label verification – workers indicate if the existing labels are correct, or rate their quality, and can add comments to explain their reasoning.label adjustment – workers adjust prior annotations to correct them.you can use ground truth to adjust or verify only labels that resulted from bounding box and semantic segmentation labeling jobs. you can start a label verification and adjustment jobs using the amazon sagemaker console or the api. to start a label verification job (console) open the amazon sagemaker console:  and choose labeling jobs. start a new labeling job by  a prior job or start from scratch, specifying an input manifest that contains labeled data objects. choose the label verification task type and choose next. in the display existing labels pane, the system shows the available label attribute names in your manifest. choose the label attribute name for the labeling job that you want to verify. use the instructions areas of the tool designer to provide context about what the previous labelers were asked to do and what the current verifiers need to check. choose see preview to check that the tool is displaying the prior labels correctly and presents the label verification task clearly. select create. this will create and start your labeling job. use the amazon sagemaker console to start a label verification or adjustment job. to start a label adjustment job (console) open the amazon sagemaker console:  and choose labeling jobs. start a new labeling job by  a prior job or start from scratch, specifying an input manifest that contains labeled data objects. choose the correct task type for your data and choose next. after choosing the workers, expand display existing labels. if it isn't expanded, choose the arrow next to the title to expand it. check the box next to i want to display existing labels from the dataset for this job. for label attribute name, choose the name from your manifest that corresponds to the labels that you want to display for adjustment. ground truth tries to detect and populate these values by analyzing the manifest, but you might need to set the correct value. use the instructions areas of the tool designer to provide context about what the previous labelers were tasked with doing and what the current verifiers need to check and adjust. choose see preview to check that the tool shows the prior labels correctly and presents the task clearly. select create. this will create and start your labeling job. start a label verification or adjustment job by chaining a successfully completed job or starting a new job from scratch using the  operation. to learn how to start a new labeling job by chaining a previous job, see .  to create a label verification or adjustment job, use the following guidelines to specify api attributes for the  operation: use the  parameter to specify the output label name that you want to use for verified or adjusted labels.if you are chaining the job, the labels from the previous labeling job to be adjusted or verified will be specified in the custom ui template. to learn how to create a custom template, see . identify the location of the ui template in the  parameter. amazon sagemaker provides widgets that you can use in your custom template to display old labels. use the  attribute in one of the following crowd elements to extract the labels that need verification or adjustment and include them in your task template: —use this crowd element in your custom ui task template to specify semantic segmentation labels that need to be verified or adjusted.—use this crowd element in your custom ui task template to specify bounding box labels that need to be verified or adjusted.the  parameter must contain the same label categories as the previous labeling job. adding new label categories or adjusting label categories is not supported.ground truth stores the output data from a label verification or adjustment job in the s3 bucket that you specified in the  parameter of the  operation. for more information about the output data from a label verification or adjustment labeling job, see . amazon sagemaker ground truth writes label verification data to the output manifest within the metadata for the label. it adds two properties to the metadata: a  property, with a value of ".a  property, with an array of  values. this property is added when the worker enters comments. if there are no comments, the field doesn't appear.the following example output manifest shows how label verification data appears: the worker output of adjustment tasks resembles the worker output of the original task, except that it contains the adjusted values and an  property with the value of  or  to indicate whether an adjustment was made. for more examples of the output of different tasks, see . to get expected behavior when creating a label verification or adjustment job, carefully verify your input data.  if you are using image data, verify that your manifest file contains hexadecimal rgb color information. to save money on processing costs, filter your data to ensure you are not including unwanted objects in your labeling job input manifest.add required amazon s3 permissions to ensure your input data is processed correctly. to properly reproduce color information in verification or adjustment tasks, the tool requires hexadecimal rgb color information in the manifest (for example, #ffffff for white). when you set up a semantic segmentation verification or adjustment job, the tool examines the manifest to determine if this information is present. if it can't find it,amazon sagemaker ground truth displays an error message and the ends job setup. in prior iterations of the semantic segmentation tool, category color information wasn't output in hexadecimal rgb format to the output manifest. that feature was introduced to the output manifest at the same time the verification and adjustment workflows were introduced. therefore, older output manifests aren't compatible with this new workflow. amazon sagemaker ground truth processes all objects in your input manifest. if you have a partially labeled data set, you might want to create a custom manifest using an  on your input manifest. unlabeled objects individually fail, but they don't cause the job to fail, and they might incur processing costs. filtering out objects you don't want verified reduces your costs. if you create a verification job using the console, you can use the filtering tools provided there. if you create jobs using the api, make filtering your data part of your workflow where needed. due to browser security models, some image markup tasks—like keypoints, polygons, bounding boxes, and semantic segmentation—require you to add a cross-origin resource sharing (cors) specification to the amazon simple storage service (amazon s3) bucket where you store the images. you must apply the cors specification prior to marking up the images. applying cors to your bucket open the amazon s3 console at . select the bucket in which you are storing your images. select the permissions tab, then cors configuration. add the following block of xml and save. 
topics dr. nathalie rauschmayr, aws applied scientist | length: 49 minutes 26 seconds  find out how amazon sagemaker experiments and debugger make your training jobs easy to manage. amazon sagemaker debugger provides transparent visibility into training jobs and saves training metrics into your amazon s3 bucket. amazon sagemaker experiments enables you to call the training information as trials through amazon sagemaker studio and supports visualization of the training job. this helps you keep model quality while reducing less important parameters based on importance rank. this video demonstrates a model pruning technique that makes pre-trained resnet50 and alexnet models lighter and affordable while keeping high standards for model accuracy. amazon sagemaker estimator trains those algorithms supplied from pytorch model zoo in an aws deep learning container with pytorch framework, and debugger extracts training metrics from the training process. the video also demonstrates how to set up a debugger custom rule to watch the accuracy of a pruned model, to trigger an aws cloudwatch event and a lambda function when the accuracy hits a threshold, and to automatically stop the pruning process to avoid redundant iterations.  learning objectives are as follows:  learn how to use amazon sagemaker to accelerate ml model training and improve model quality. understand how to manage training iterations with amazon sagemaker experiments by automatically capturing input parameters, configurations, and results. discover how debugger makes the training process transparent by automatically capturing real-time tensor data from metrics such as weights, gradients, and activation outputs of convolutional neural networks.use cloudwatch to trigger lambda when debugger catches issues.master the amazon sagemaker training process using amazon sagemaker experiments and debugger.you can find the notebooks and training scripts used in this video from . the following image shows how the iterative model pruning process reduces the size of alexnet by cutting out the 100 least significant filters based on importance rank evaluated by activation outputs and gradients. the pruning process reduced the initial 50 million parameters to 18 million. it also reduced the estimated model size from 201 mb to 73 mb.   you also need to track model accuracy, and the following image shows how you can plot the model pruning process to visualize changes in model accuracy based on the number of parameters in amazon sagemaker studio.  from the experiments tab in the amazon sagemaker studio interface, select a list of tensors saved by debugger from the pruning process and compose a trial component list panel. select all ten iterations and choose add chart to create a trial component chart. once you decide on a model to deploy, select the trial component and open a drop-down menu or choose deploy model. noteto deploy a model through amazon sagemaker studio using this notebook example, add a line at the end of the  function in the  script.   this notebook demonstrates how amazon sagemaker debugger visualizes tensors from an unsupervised (or self-supervised) learning process on a mnist image dataset of handwritten numbers. the training model in this notebook is a convolutional autoencoder with the mxnet framework. the convolutional autoencoder has a bottleneck-shaped convolutional neural network that consists of an encoder part and a decoder part.  the encoder in this example has two convolution layers to produce compressed representation (latent variables) of the input images. in this case, the encoder produces a latent variable of size (1, 20) from an original input image of size (28, 28) and significantly reduces the size of data for training by 40 times. the decoder has two deconvolutional layers and ensures that the latent variables preserve key information by reconstructing output images. the convolutional encoder powers clustering algorithms with smaller input data size and, as well as performance of clustering algorithms such as k-means, k-nn, and t-distributed stochastic neighbor embedding (t-sne). this notebook example demonstrates how to visualize the latent variables using debugger, as shown in the following animation. it also demonstrates how the t-sne algorithm classifies the latent variables into ten clusters and projects them into a two-dimensional space. the scatter plot color scheme on the right side of the image reflects the true values to show how well the bert model and t-sne algorithm organize the latent variables into the clusters.  bidirectional encode representations from transformers (bert) is a language representation model. as the name of model reflects, the bert model builds on transfer learning and the transformer model for natural language processing (nlp). the bert model is pre-trained on unsupervised tasks such as predicting missing words in a sentence or predicting the next sentence that naturally follows a previous sentence. the training data contains 3.3 billion words (tokens) of english text, such as wikipedia and electronic books. for a simple example, the bert model can give a high attention to appropriate verb tokens or pronoun tokens from a subject token. the pre-trained bert model can be fine-tuned with an additional output layer to achieve state-of-the-art model training in nlp tasks, such as automated response to questions, text classification, and many others.  debugger collects tensors from the fine-tuning process; in context of nlp, the weight of neurons is called attention.  this notebook demonstrates how to use  on the stanford question and answering dataset and how to set up amazon sagemaker debugger to monitor the training job. plotting attention scores and individual neurons in the query and key vectors can help to identify causes of incorrect model predictions. with amazon sagemaker debugger, you can easily retrieve the tensors and plot the attention-head view in real-time as training progresses and understand what the model is learning. the following animation shows the attention scores of the first 20 input tokens for ten iterations in the training job provided in the notebook example.  this notebook demonstrates how to use amazon sagemaker debugger to plot class activation maps for image detection and classification. one of applications that adopts the class activation maps is self-driving cars, which require instantaneous detection and classification of images such as traffic signs, roads, and obstacles. in this notebook, the pytorch resnet model is trained on , which contains more than 40 classes of traffic-related objects and more than 50,000 images in total.  during the training process, amazon sagemaker debugger collects tensors to plot the class activation maps in real-time. as shown in the animated image, the class activation map (also called as a saliency map) highlights regions with high activation in red color.  using tensors captured by debugger, you can visualize how the activation map evolves during the model training. the model starts by detecting the edge on the left bottom corner at the beginning of the training job. as the training progresses, the focus shifts to the center and detects the speed limit sign, and the model successfully predicts the input image as class 3, which is a class of speed limit 60km/h signs, with a 97% confidence level. amazon sagemaker studio provides visualizations to interpret tensor outputs that are captured by debugger.  the following screenshot shows visualizations of loss curves for training. the training is in progress.  amazon sagemaker studio allows simple comparison across multiple jobs (in this case, the loss). this helps identify the best-performing training jobs.  when rules are triggered for anomalous conditions, amazon sagemaker studio presents logs for the failing rule, allowing easy analysis of the causes of the condition.  
the image classification algorithm takes an image as input and classifies it into one of the output categories. deep learning has revolutionized the image classification domain and has achieved great performance. various deep learning networks such as resnet [1], densenet, inception, and so on, have been developed to be highly accurate for image classification. at the same time, there have been efforts to collect labeled image data that are essential for training these networks. imagenet[2] is one such large dataset that has more than 11 million images with about 11,000 categories. once a network is trained with imagenet data, it can then be used to generalize with other datasets as well, by simple re-adjustment or fine-tuning. in this transfer learning approach, a network is initialized with weights (in this example, trained on imagenet), which can be later fine-tuned for an image classification task in a different dataset.  image classification in amazon sagemaker can be run in two modes: full training and transfer learning. in full training mode, the network is initialized with random weights and trained on user data from scratch. in transfer learning mode, the network is initialized with pre-trained weights and just the top fully connected layer is initialized with random weights. then, the whole network is fine-tuned with new data. in this mode, training can be achieved even with a smaller dataset. this is because the network is already trained and therefore can be used in cases without sufficient training data. 
the following table lists parameters for each of the algorithms provided by amazon sagemaker. algorithms that are parallelizable can be deployed on multiple compute instances for distributed training. for the training image and inference image registry path column, use the  version tag to ensure that you are using a stable version of the algorithm. you can reliably host a model trained using an image with the  tag on an inference image that has the  tag. using the  tag in the registry path provides you with the most up-to-date version of the algorithm, but might cause problems with backward compatibility. avoid using the  tag for production purposes. for the training image and inference image registry path column, depending on algorithm and region use one of the following values for <ecr_path>. algorithms: blazingtext, image classification, object detection, semantic segmentation, seq2seq, and xgboost (0.72) algorithms: deepar forecasting algorithms: factorization machines, ip insights, k-means, k-nearest-neighbor, linear learner, object2vec, neural topic model,pca, and random cut forest algorithms: latent dirichlet allocation (lda) algorithms: xgboost (0.90) use the paths and training input mode as follows: to create a training job (with a request to the  api), specify the docker registry path and the training input mode for the training image. you create a training job to train a model using a specific dataset.  to create a model (with a  request), specify the docker registry path for the inference image. amazon sagemaker launches machine learning compute instances that are based on the endpoint configuration and deploys the model, which includes the artifacts (the result of model training).
a constraints.json file is used to express the constraints that a dataset must satisfy. amazon sagemaker model monitor containers can use the constraints.json file to evaluate datasets against. pre-built containers provide the ability to generate the constraints.json file automatically for a baseline dataset. if you bring your own container, you can provide it with similar abilities or you can create the constraints.json file in some other way. here is the schema for the constraint file that the prebuilt container uses. bring our own containers can adopt the same format or enhance it as required. table: monitoring constraints 
you can use amazon sagemaker to train and deploy a model using custom chainer code. the amazon sagemaker python sdk chainer estimators and models and the amazon sagemaker open-source chainer container make writing a chainer script and running it in amazon sagemaker easier. i want to train a custom chainer model in amazon sagemaker.for a sample jupyter notebook, see .for documentation, see . i have a chainer model that i trained in amazon sagemaker, and i want to deploy it to a hosted endpoint.. i have a chainer model that i trained outside of amazon sagemaker, and i want to deploy it to an amazon sagemaker endpoint. i want to see the api documentation for  chainer classes. i want to see information about amazon sagemaker chainer containers..  for information about supported chainer versions, and for general information about writing chainer training scripts and using chainer estimators and models with amazon sagemaker, see .  
this rule evaluates the goodness of a confusion matrix for a classification problem. it creates a matrix of size  and populates it with data coming from (, ) pairs. for each (, ) pair, the count in  is incremented by 1. when the matrix is fully populated, the ratio of data on-diagonal values and off-diagonal values are evaluated as follows: for elements on the diagonal: for elements off the diagonal: this rule can be applied either to one of the supported deep learning frameworks (tensorflow, mxnet, and pytorch) or to the xgboost algorithm. for an example of how to configure and deploy a built-in rule, see . parameter descriptions for the confusion rule   notethis rule infers default values for the optional parameters if their values aren't specified. 
before you can create algorithm and model package resources to use in amazon sagemaker or list on aws marketplace, you have to develop them and package them in docker containers. notewhen algorithms and model packages are created for listing on aws marketplace, amazon sagemaker scans the containers for security vulnerabilities on supported operating systems.only the following operating system versions are supported:debian: 6.0, 7, 8, 9, 10 ubuntu: 12.04, 12.10, 13.04, 14.04, 14.10, 15.04, 15.10, 16.04, 16.10, 17.04, 17.10, 18.04, 18.10 centos: 5, 6, 7 oracle linux: 5, 6, 7 alpine: 3.3, 3.4, 3.5 amazon linux topics an algorithm should be packaged as a docker container and stored in amazon ecr to use it in amazon sagemaker. the docker container contains the training code used to run training jobs and, optionally, the inference code used to get inferences from models trained by using the algorithm. for information about developing algorithms in amazon sagemaker and packaging them as containers, see . for a complete example of how to create an algorithm container, see the sample notebook at . you can also find the sample notebook in an amazon sagemaker notebook instance. the notebook is in the advanced functionality section, and is named . for information about using the sample notebooks in a notebook instance, see . always thoroughly test your algorithms before you create algorithm resources to publish on aws marketplace. notewhen a buyer subscribes to your containerized product, the docker containers run in an isolated (internet-free) environment. when you create your containers, do not rely on making outgoing calls over the internet. calls to aws services are also not allowed. a deployable model in amazon sagemaker consists of inference code, model artifacts, an iam role that is used to access resources, and other information required to deploy the model in amazon sagemaker. model artifacts are the results of training a model by using a machine learning algorithm. the inference code must be packaged in a docker container and stored in amazon ecr. you can either package the model artifacts in the same container as the inference code, or store them in amazon s3.  you create a model by running a training job in amazon sagemaker, or by training a machine learning algorithm outside of amazon sagemaker. if you run a training job in amazon sagemaker, the resulting model artifacts are available in the  field in the response to a call to the  operation. for information about how to develop an amazon sagemaker model container, see . for a complete example of how to create a model container from a model trained outside of amazon sagemaker, see the sample notebook at . you can also find the sample notebook in an amazon sagemaker notebook instance. the notebook is in the advanced functionality section, and is named . for information about using the sample notebooks in a notebook instance, see . always thoroughly test your models before you create model packages to publish on aws marketplace. notewhen a buyer subscribes to your containerized product, the docker containers run in an isolated (internet-free) environment. when you create your containers, do not rely on making outgoing calls over the internet. calls to aws services are also not allowed. 
to train a machine learning model, you need a large, high-quality, labeled dataset. ground truth helps you build high-quality training datasets for your machine learning models. with ground truth, you can use workers from either amazon mechanical turk, a vendor company that you choose, or an internal, private workforce along with machine learning to enable you to create a labeled dataset. you can use the labeled dataset output from ground truth to train your own models. you can also use the output as a training dataset for an amazon sagemaker model. depending on your ml application, you can choose from one of the ground truth built-in task types to have workers generate specific types of labels for your data. you can also build a custom labeling workflow to provide your own ui and tools to workers labeling your data. to learn more about the ground truth built in task types, see . to learn how to create a custom labeling workflow, see . in order to automate labeling your training dataset, you can optionally use automated data labeling, a ground truth process that uses machine learning to decide which data needs to be labeled by humans. automated data labeling may reduce the labeling time and manual effort required. for more information, see . to create a custom labeling job, see . use either pre-built or custom tools to assign the labeling tasks for your training dataset. a labeling ui template is a webpage that ground truth uses to present tasks and instructions to your workers. the amazon sagemaker console provides built-in templates for labeling data. you can use these templates to get started , or you can build your own tasks and instructions by using our html 2.0 components. for more information, see .   use the workforce of your choice to label your dataset. you can choose your workforce from: the amazon mechanical turk workforce of over 500,000 independent contractors worldwide.a private workforce that you create from your employees or contractors for handling data within your organization.a vendor company that you can find in the aws marketplace that specializes in data labeling services.for more information, see . you store your datasets in amazon s3 buckets. the buckets contain three things: the data to be labeled, an input manifest file that ground truth uses to read the data files, and an output manifest file. the output file contains the results of the labeling job. for more information, see . events from your labeling jobs appear in amazon cloudwatch under the  group. cloudwatch uses the labeling job name as the name for the log stream. if you are a first-time user of ground truth, we recommend that you do the following: read —this section walks you through setting up your first ground truth labeling job. explore other topics—depending on your needs, do the following: create instruction pages for your labeling jobs—create a custom instruction page that makes it easier for your workers to understand the requirements of the job. for more information, see .manage your labeling workforce—create new work teams and manage your existing workforce. for more information, see .create a custom ui—make it easier for your workers to quickly and correctly label your data by creating a custom ui for them to use. for more information, see .see the —this section describes operations to automate ground truth operations. 
in this step, you set which aws lambda functions to trigger on each dataset object prior to sending it to workers and which function will be used to process the results once the task is submitted. these functions are required. you will first need to visit the aws lambda console or use aws lambda's apis to create your functions. the amazonsagemakerfullaccess policy is restricted to invoking aws lambda functions with one of the following four strings as part of the function name: , , , or . this applies to both your pre-annotation and post-annotation lambdas. if you choose to use names without those strings, you must explicitly provide  permission to the iam role used for creating the labeling job. select your lambdas from the lambda functions section that comes after the code editor for your custom html in the ground truth console. if you need an example, there is an end-to-end demo, including python code for the lambdas, in the "" document. before a labeling task is sent to the worker, your aws lambda function will be sent a json formatted request to provide details. example of a pre-annotation request   the  will contain the json formatted properties from your manifest's data object. for a very basic image annotation job, it might just be a  property specifying the image to be annotated. the json line objects in your manifest can be up to 100 kilobytes in size and contain a variety of data. in return, ground truth will require a response formatted like this: example of expected return data   in the previous example, the  needs to contain all the data your custom form will need. if you're doing a bounding box task where the instructions stay the same all the time, it may just be the http(s) or s3 resource for your image file. if it's a sentiment analysis task and different objects may have different choices, it would be the object reference as a string and the choices as an array of strings. implications of  this value is optional because it will default to . the primary use case for explicitly setting it is when you want to exclude this data object from being labeled by human workers.  if you have a mix of objects in your manifest, with some requiring human annotation and some not needing it, you can include a  value in each data object. you can then use code in your pre-annotation lambda to read the value from the data object and set the value in your lambda output. the pre-annotation lambda runs first before any tasks are available to workers, your entire manifest will be processed into an intermediate form, using your lambda. this means you won't be able to change your lambda part of the way through a labeling job and see that have an impact on the remaining tasks.  when all workers have annotated the data object or when  has been reached, whichever comes first, ground truth will send those annotations to your post-annotation lambda. this lambda is generally used for . the request object will come in like this: example of a post-labeling task request    if no worker work on the data object and  has been reached, data object will be marked as failed and not included as part of post annotation lambda invocation. the actual annotation data will be in a file designated by the  string in the  object. to process the annotations as they come in, even for a simple pass through function, you need to assign the necessary permissions to your lambda to read files from your s3 bucket. in the console page for creating your lambda, scroll to the execution role panel. select create a new role from one or more templates. give the role a name. from the policy templates drop-down, choose amazon s3 object read-only permissions. save the lambda and the role will be saved and selected. example of an annotation data file   essentially, all the fields from your form will be in the  object. at this point you can start running data consolidation algorithms on the data, using an aws database service to store results. or you can pass some processed/optimized results back to ground truth for storage in your consolidated annotation manifests in the s3 bucket you specify for output during the configuration of the labeling job. in return, ground truth will require a response formatted like this: example of expected return data   at this point, all the data you're sending to your s3 bucket, other than the  will be in the  object. that will result in an entry in your job's consolidation manifest like this: example of label format in output manifest   because of the potentially complex nature of a custom template and the data it collects, ground truth does not offer further processing of the data or insights into it.  
when you're starting a new notebook, we recommend that you create the notebook in amazon sagemaker studio instead of launching a notebook instance from the amazon sagemaker console. there are many benefits to using a sagemaker studio notebook, including the following: starting a studio notebook is faster than launching an instance-based notebook. typically, it is 5-10 times faster than instance-based notebooks.notebook sharing is an integrated feature in sagemaker studio. users can generate a shareable link that reproduces the notebook code and also the sagemaker image required to execute it, in just a few clicks.sagemaker studio notebooks come pre-installed with the latest .sagemaker studio notebooks are accessed from within studio. this enables you to build, train, debug, track, and monitor your models without leaving studio.each member of a studio team gets their own home directory to store their notebooks and other files. the directory is automatically mounted onto all instances and kernels as they're started, so their notebooks and other files are always available. the home directories are stored in amazon elastic file system (amazon efs) so that you can access them from other services.when using aws sso, you use your sso credentials through a unique url to directly access sagemaker studio. you don't have to interact with the aws management console to run your notebooks.studio notebooks are equipped with a set of predefined sagemaker image settings to get you started faster.notestudio notebooks don't support local mode. however, you can use a notebook instance to train a sample of your dataset locally, and then use the same code in a studio notebook to train on the full dataset. when you open a notebook in sagemaker studio, the view is an extension of the jupyterlab interface. the primary features are the same, so you'll find the typical features of a jupyter notebook and jupyterlab. for more information about the studio interface, see . 
 to enable amazon sagemaker debugger, use the pre-built containers supported by amazon sagemaker deep learning containers. debugger built-in features are implemented in the deep learning containers, and you can simply use your training script without making any changes. if you want to use debugger in frameworks other than the ones covered by deep learning containers, you have another option, script mode, in amazon sagemaker containers. enable debugger by adding a few lines of code to your training script. you can use amazon sagemaker debugger with the following machine learning frameworks and algorithms: tensorflow, apache mxnet, pytorch, and xgboost. topics to enable amazon sagemaker debugger without making any changes to your training script, use one of the aws deep learning containers in frameworks listed in the following table.  each of these containers automatically adds the debugger hook, a callback that debugger uses to save requested tensors throughout the training process. for a full list of available regions and image urls of deep learning containers, see . to learn more about frameworks and versions supported by debugger, see . if you want to use debugger with a framework of a version not listed in the table, you need to run your training job in script mode using aws-managed containers or build your own container. for more information about using debugger in script mode, see  . for example, the following code example calls a deep learning container in tensorflow 2.1.0: in the sample code, the lines in bold text activate the pre-built container in the tf 2.1.0 framework with your original training script. this example assumes the training script is located at .  for more examples using debugger in tensorflow containers, see the following notebook sample: for more examples using debugger in mxnet containers, see the following notebook samples: for more examples using debugger in mxnet containers, see the following notebook sample: the xgboost algorithm can be used as a built-in algorithm or as a framework like mxnet, pytorch, or tensorflow. if amazon sagemaker xgboost is used as a built-in algorithm in container version 0.90-2 or later, debugger is available by default (zero code change experience). for more examples using debugger in xgboost containers, see the following notebook samples: to learn more about variations of debugger configuration, refer to the . to learn more about the debugger hook and rule features, see  and . to use script mode and enable debugger to run your training script out of the frameworks supported by the pre-built aws deep learning containers, you need to make changes to your training script. script mode enables you to run your training scripts inside amazon sagemaker containers. debugger and  support a wider range of frameworks. you can enable script mode by adding a single line in your estimator and debugger by adding a few lines in your training python script.  to learn more about the frameworks, see .  the following example code shows how to enable script mode in debugger.  after enabling script mode, you need to make a minimal changes inside the training script. the following notebook examples show how to apply the changes to training scripts in detail. to see the difference between using debugger in a deep learning container and in script mode, open this notebook and put it and  side by side.   in script mode, the hook configuration part is removed from the script in which you set the estimator. instead, the debugger hook feature is merged into the training script, . the training script imports the  library in the required tensorflow keras environment to communicate with the tensorflow resnet50 algorithm. it also manually implements the  hook functionality by adding the  argument inside the  function (in line 49), and by adding the manual hook configuration (in line 89) provided through amazon sagemaker python sdk. this script mode example runs the training job in the tf 2.1 framework for direct comparison with the zero script change in the tf 2.1 example. the benefit of setting up debugger in script mode is the flexibility to choose framework versions not covered by aws deep learning containers.  this notebook enables debugger in script mode in pytorch v1.3.1 framework. pytorch v1.3.1 is supported by amazon sagemaker containers, and this example shows details of how to modify a training script.  the amazon sagemaker pytorch estimator is already in script mode by default. in the notebook, the line to activate  is not included in the estimator configuration. this notebook shows detailed steps to change  to . additionally, this example shows how you can use debugger built-in rules to detect training issues such as the vanishing gradients problem, and the debugger trial features to call and analyze the saved tensors.  
all amazon sagemaker built-in algorithms adhere to the common input inference format described in . this topic contains a list of the available output formats for the amazon sagemaker k-means algorithm. the first value in each line corresponds to . the second value in each line corresponds to . 
amazon sagemaker notebook instances are internet-enabled by default. this allows you to download popular packages and notebooks, customize your development environment, and work efficiently. however, this provides an additional avenue for unauthorized access to your data. for example, a malicious user or code that you accidentally install on the computer (in the form of a publicly available notebook or a publicly available source code library) could access your data. you can choose to launch your notebook instance in your virtual private cloud (vpc) to restrict which traffic can go through public internet. when launched with your vpc attached, the notebook instance can either be configured with or without direct internet access. when your notebook allows direct internet access, amazon sagemaker provides a network interface that allows the notebook to communicate with the internet through a vpc managed by amazon sagemaker. traffic within your vpc's cidr will go through elastic network interface (eni) created in your vpc. all the other traffic will go through the eni created by amazon sagemaker, which is essentially through the public internet. traffic to gateway vpc endpoints like amazon s3 and dynamodb will go through the public internet, while traffic to interface vpc endpoints will still go through your vpc. if you want to use gateway vpc endpoints, you may want to disable direct internet access.  to disable direct internet access, you can specify a vpc for your notebook instance. doing so will prevent amazon sagemaker from providing internet access to your notebook instance. as a result, the notebook instance won't be able to train or host models unless your vpc has an interface endpoint (privatelink) or a nat gateway and your security groups allow outbound connections.  for information about creating a vpc interface endpoint to use privatelink for your notebook instance, see . for information about setting up a nat gateway for your vpc, see  in the in the amazon virtual private cloud user guide. for information about security groups, see . for more information about networking configurations in each networking mode and configuring network on premise, refer to .  an amazon sagemaker notebook instance is designed to work best for an individual user. it is designed to give data scientists and other users the most power for managing their development environment. a notebook instance user has root access for installing packages and other pertinent software. we recommend that you exercise judgement when granting individuals access to notebook instances that are attached to a vpc that contains sensitive information. for example, you might grant a user access to a notebook instance with an iam policy, as in the following example: 
principal component analysis (pca) is a learning algorithm that reduces the dimensionality (number of features) within a dataset while still retaining as much information as possible.  pca reduces dimensionality by finding a new set of features called components, which are composites of the original features, but are uncorrelated with one another. the first component accounts for the largest possible variability in the data, the second component the second most variability, and so on. it is an unsupervised dimensionality reduction algorithm. in unsupervised learning, labels that might be associated with the objects in the training dataset aren't used. given the input of a matrix with rows  each of dimension , the data is partitioned into mini-batches of rows and distributed among the training nodes (workers). each worker then computes a summary of its data. the summaries of the different workers are then unified into a single solution at the end of the computation.  modes the amazon sagemaker pca algorithm uses either of two modes to calculate these summaries, depending on the situation: regular: for datasets with sparse data and a moderate number of observations and features.randomized: for datasets with both a large number of observations and features. this mode uses an approximation algorithm. as the algorithm's last step, it performs the singular value decomposition on the unified solution, from which the principal components are then derived. the workers jointly compute both ![[\sum x_i^t x_i]]() and  . notebecause  are  row vectors, ![[x_i^t x_i]]() is a matrix (not a scalar). using row vectors within the code allows us to obtain efficient caching. the covariance matrix is computed as ![[\sum x_i^t x_i - (1/n) (\sum x_i)^t \sum x_i]]() , and its top  singular vectors form the model. noteif  is , we avoid computing and subtracting  . use this algorithm when the dimension  of the vectors is small enough so that ![[d^2]]() can fit in memory. when the number of features in the input dataset is large, we use a method to approximate the covariance metric. for every mini-batch  of dimension , we randomly initialize a  matrix that we multiply by each mini-batch, to create a  matrix. the sum of these matrices is computed by the workers, and the servers perform svd on the final  matrix. the top right  singular vectors of it are the approximation of the top singular vectors of the input matrix. let  . given a mini-batch  of dimension , the worker draws a random matrix  of dimension  . depending on whether the environment uses a gpu or cpu and the dimension size, the matrix is either a random sign matrix where each entry is  or a fjlt (fast johnson lindenstrauss transform; for information, see  and the follow-up papers). the worker then computes  and maintains  . the worker also maintains ![[h^t]]() , the sum of columns of  ( being the total number of mini-batches), and , the sum of all input rows. after processing the entire shard of data, the worker sends the server , , , and  (the number of input rows). denote the different inputs to the server as ![[b^1, h^1, s^1, n^1,…]]() the server computes , , ,  the sums of the respective inputs. it then computes ![[c = b – (1/n) h^t s]]() , and finds its singular value decomposition. the top-right singular vectors and singular values of  are used as the approximate solution to the problem. 
the amazon sagemaker blazingtext algorithm provides highly optimized implementations of the word2vec and text classification algorithms. the word2vec algorithm is useful for many downstream natural language processing (nlp) tasks, such as sentiment analysis, named entity recognition, machine translation, etc. text classification is an important task for applications that perform web searches, information retrieval, ranking, and document classification. the word2vec algorithm maps words to high-quality distributed vectors. the resulting vector representation of a word is called a word embedding. words that are semantically similar correspond to vectors that are close together. that way, word embeddings capture the semantic relationships between words.  many natural language processing (nlp) applications learn word embeddings by training on large collections of documents. these pretrained vector representations provide information about semantics and word distributions that typically improves the generalizability of other models that are later trained on a more limited amount of data. most implementations of the word2vec algorithm are not optimized for multi-core cpu architectures. this makes it difficult to scale to large datasets.  with the blazingtext algorithm, you can scale to large datasets easily. similar to word2vec, it provides the skip-gram and continuous bag-of-words (cbow) training architectures. blazingtext's implementation of the supervised multi-class, multi-label text classification algorithm extends the fasttext text classifier to use gpu acceleration with custom  kernels. you can train a model on more than a billion words in a couple of minutes using a multi-core cpu or a gpu. and, you achieve performance on par with the state-of-the-art deep learning text classification algorithms.  the amazon sagemaker blazingtext algorithms provides the following features: accelerated training of the fasttext text classifier on multi-core cpus or a gpu and word2vec on gpus using highly optimized cuda kernels. for more information, see . by learning vector representations for character n-grams. this approach enables blazingtext to generate meaningful vectors for out-of-vocabulary (oov) words by representing their vectors as the sum of the character n-gram (subword) vectors.a   for the word2vec algorithm that allows faster training and distributed computation across multiple cpu nodes. the   does mini-batching using the negative sample sharing strategy to convert level-1 blas operations into level-3 blas operations. this efficiently leverages the multiply-add instructions of modern architectures. for more information, see .to summarize, the following modes are supported by blazingtext on different types instances: for more information about the mathematics behind blazingtext, see . topics the blazingtext algorithm expects a single preprocessed text file with space-separated tokens. each line in the file should contain a single sentence. if you need to train on multiple text files, concatenate them into one file and upload the file in the respective channel. for word2vec training, upload the file under the train channel. no other channels are supported. the file should contain a training sentence per line. for supervised mode, you can train with file mode or with the augmented manifest text format. for  mode, the training/validation file should contain a training sentence per line along with the labels. labels are words that are prefixed by the string __label__. here is an example of a training/validation file: notethe order of labels within the sentence doesn't matter.  upload the training file under the train channel, and optionally upload the validation file under the validation channel. the supervised mode also supports the augmented manifest format, which enables you to do training in pipe mode without needing to create recordio files. while using the format, an s3 manifest file needs to be generated that contains the list of sentences and their corresponding labels. the manifest file format should be in  format in which each line represents one sample. the sentences are specified using the  tag and the label can be specified using the  tag. both  and  tags should be provided under the  parameter value as specified in the request. multi-label training is also supported by specifying a json array of labels. for more information on augmented manifest files, see . for word2vec training, the model artifacts consist of vectors.txt, which contains words-to-vectors mapping, and vectors.bin, a binary used by blazingtext for hosting, inference, or both. vectors.txt stores the vectors in a format that is compatible with other tools like gensim and spacy. for example, a gensim user can run the following commands to load the vectors.txt file: if the evaluation parameter is set to , an additional file, eval.json, is created. this file contains the similarity evaluation results (using spearman’s rank correlation coefficients) on ws-353 dataset. the number of words from the ws-353 dataset that aren't there in the training corpus are reported. for inference requests, the model accepts a json file containing a list of strings and returns a list of vectors. if the word is not found in vocabulary, inference returns a vector of zeros. if subwords is set to  during training, the model is able to generate vectors for out-of-vocabulary (oov) words. mime-type: training with supervised outputs creates a model.bin file that can be consumed by blazingtext hosting. for inference, the blazingtext model accepts a json file containing a list of sentences and returns a list of corresponding predicted labels and probability scores. each sentence is expected to be a string with space-separated tokens, words, or both. mime-type: by default, the server returns only one prediction, the one with the highest probability. for retrieving the top k predictions, you can set k in the configuration, as follows: for blazingtext, the and  parameters must be equal. for batch transform, they both need to be . if they differ, the  field is ignored. the format for input follows: the format for output follows: for both supervised (text classification) and unsupervised (word2vec) modes, the binaries (\.bin*) produced by blazingtext can be cross-consumed by fasttext and vice versa. you can use binaries produced by blazingtext by fasttext. likewise, you can host the model binaries created with fasttext using blazingtext. however, the binaries are only supported when training on cpu and single gpu; training on multi-gpu will not produce binaries. for more details on dataset formats and model hosting, see the example notebooks , , and . for  and  modes, blazingtext supports single cpu and single gpu instances. both of these modes support learning of  embeddings. to achieve the highest speed without compromising accuracy, we recommend that you use an ml.p3.2xlarge instance.  for  mode, blazingtext supports single or multiple cpu instances. when training on multiple instances, set the value of the  field of the  object that you pass to  to . blazingtext takes care of distributing data across machines. for the supervised text classification mode, a c5 instance is recommended if the training dataset is less than 2 gb. for larger datasets, use an instance with a single gpu (ml.p2.xlarge or ml.p3.2xlarge). for a sample notebook that uses the amazon sagemaker blazingtext algorithm to train and deploy supervised binary and multiclass classification models, see . for instructions for creating and accessing jupyter notebook instances that you can use to run the example in amazon sagemaker, see . after creating and opening a notebook instance, choose the sagemaker examples tab to see a list of all the amazon sagemaker examples. the topic modeling example notebooks that use the blazing text are located in the introduction to amazon algorithms section. to open a notebook, choose its use tab, then choose create copy. 
amazon augmented ai uses amazon cloudwatch events (cloudwatch events) to alert you when a human review loop changes status. when a review loop changes to the , , or  status, augmented ai sends an event to cloudwatch events similar to the following: the details in the json output include the following: the timestamp when augmented ai created the human loop. a failure code denoting a specific type of failure. the reason why a human loop has failed. the failure reason is only returned when the human review loop status is . the amazon resource name (arn) of the flow definition, or human review workflow. the amazon resource name (arn) of the human loop. the name of the human loop. an object containing information about the output of the human loop. the location of the amazon s3 object where augmented ai stores your human loop output. the status of the human loop. to configure a cloudwatch events rule to get status updates, or events, for your amazon a2i human loops, use the aws command line interface (aws cli)  command. when using the  command, specify the following to receive human loop statuses:  to configure a cloudwatch events rule to watch for all status changes, use the following command and replace the placeholder text. for example, replace  with a unique cloudwatch events rule name and  with the amazon resource number (arn) of an iam role with an events.amazonaws.com trust policy attached. replace region with the aws region you want to create the rule in.   to learn more about the  request, see  in the amazon cloudwatch events user guide. to process events, you need to set up a target. for example, if you want to receive an email when a human loop status changes, use a procedure in  in the amazon cloudwatch user guide to set up an amazon sns topic and subscribe your email to it. once you have create a topic, you can use it to create a target.  to add a target to your cloudwatch events rule open the cloudwatch console:  in the navigation pane, choose rules. choose the rule that you want to add a target to.  choose actions, and then choose edit. under targets, choose add target and choose the aws service you want to act when a human loop status change event is detected.  configure your target. for instructions, see the topic for configuring a target in the . choose configure details. for name, enter a name and, optionally, provide details about the purpose of the rule in description.  make sure that the check box next to state is selected so that your rule is listed as enabled.  choose update rule. after you receive human review results, you can analyze the results and compare them to machine learning predictions. the json that is stored in the amazon s3 bucket contains both the machine learning predictions and the human review results.  
an annotation is the result of a single worker's labeling task. annotation consolidation combines the annotations of two or more workers into a single label for your data objects. a label, which is assigned to each object in the dataset, is a probabilistic estimate of what the true label should be. each object in the dataset typically has multiple annotations, but only one label or set of labels. you can decide how many workers should annotate each object in your dataset. more workers can increase the accuracy of your labels, but also increases the cost of labeling. if you use the amazon sagemaker console to create a labeling job, the following are the defaults for the number of workers who can annotate objects:  text classification—3 workersimage classification—3 workersbounding boxes—5 workerssemantic segmentation—3 workersnamed entity recognition—3 workerswhen you use the  operation, you set the number of workers to annotate each data object with the  parameter. you can override the default number of workers that annotate a data object using the console or the  operation. ground truth provides an annotation consolidation function for each of its predefined labeling tasks: bounding box, image classification, name entity recognition, semantic segmentation, and text classification. these are the functions: multi-class annotation consolidation for image and text classification uses a variant of the  approach to annotations. it estimates parameters for each worker and uses bayesian inference to estimate the true class based on the class annotations from individual workers. bounding box annotation consolidates bounding boxes from multiple workers. this function finds the most similar boxes from different workers based on the , or intersection over union, of the boxes and averages them. semantic segmentation annotation consolidation treats each pixel in a single image as a multi-class classification. this function treats the pixel annotations from workers as "votes," with more information from surrounding pixels incorporated by applying a smoothing function to the image.named entity recognition clusters text selections by jaccard similarity and calculates selection boundaries based on the mode, or median if the mode isn't clear. the label resolves to the most assigned entity label in the cluster, breaking ties by random selection.you can use other algorithms to consolidate annotations. for information, see .  you can choose to use your own annotation consolidation function to determine the final labels for your labeled objects. there are many possible approaches for writing a function and the approach that you take depends on the nature of the annotations to consolidate. broadly, consolidation functions look at the annotations from workers, measure the similarity between them, and then use some form of probabilistic judgment to determine what the most probable label should be. if you want to use other algorithms to create annotation consolidations functions, you can find the worker responses in the  folder of the amazon s3 bucket where you direct the job output. to assess the similarity between labels, you can use one of the following strategies, or you can use one that meets your data labeling needs: for label spaces that consist of discrete, mutually exclusive categories, such as multi-class classification, assessing similarity can be straightforward. discrete labels either match or not. for label spaces that don't have discrete values, such as bounding box annotations, find a broad measure of similarity. for bounding boxes, one such measure is the jaccard index. this measures the ratio of the intersection of two boxes with the union of the boxes to assess how similar they are. for example, if there are three annotations, then there can be a function that determines which annotations represent the same object and should be consolidated.with one of the above strategies in mind, make some sort of probabilistic judgment on what the consolidated label should be. in the case of discrete, mutually exclusive categories, this can be straightforward. one of the most common ways to do this is to take the results of a majority vote between the annotations. this weights the annotations equally.  some approaches attempt to estimate the accuracy of different annotators and weight their annotations in proportion to the probability of correctness. an example of this is the expectation maximization method, which is used in the default ground truth consolidation function for multi-class annotations.  for more information about creating an annotation consolidation function, see . 
amazon sagemaker provides prebuilt docker images that install the scikit-learn and spark ml libraries and the dependencies they need to build docker images that are compatible with amazon sagemaker using the . with the sdk, you can use scikit-learn for machine learning tasks and use spark ml to create and tune machine learning pipelines. for instructions on installing and using the sdk, see . the following table contains links to the github repositories with the source code and the dockerfiles for scikit-learn and spark ml frameworks and to instructions that show how use the python sdk estimators to run your own training algorithms on amazon sagemaker learner and your own models on amazon sagemaker hosting. if you are not using the sm python sdk and one of its estimators to manage the container, you have to retrieve the relevant pre-build container. the amazon sagemaker prebuilt docker images are stored in amazon elastic container registry (amazon ecr). you can push or pull them using their fullname registry addresses. amazon sagemaker uses the following docker image url patterns for scikit-learn and spark m:  for example,   for example,  the following table lists the supported values for account ids and corresponding aws region names. the supported values listed in the table are also available on the  page of the  github repository. amazon sagemaker also provides prebuilt docker images for popular deep learning frameworks. for information about docker images that enable using deep learning frameworks in amazon sagemaker, see . for information on docker images for developing reinforcement learning (rl) solutions in amazon sagemaker, see . 
during training, deepar accepts a training dataset and an optional test dataset. it uses the test dataset to evaluate the trained model. in general, the datasets don't have to contain the same set of time series. you can use a model trained on a given training set to generate forecasts for the future of the time series in the training set, and for other time series. both the training and the test datasets consist of one or, preferably, more target time series. each target time series can optionally be associated with a vector of feature time series and a vector of categorical features. for more information, see .  for example, the following is an element of a training set indexed by i which consists of a target time series, zi,t, and two associated feature time series, xi,1,t and xi,2,t:  the target time series might contain missing values, which are represented by line breaks in the time series. deepar supports only feature time series that are known in the future. this allows you to run "what if?" scenarios. what happens, for example, if i change the price of a product in some way?  each target time series can also be associated with a number of categorical features. you can use these features to encode which groupings a time series belongs to. categorical features allow the model to learn typical behavior for groups, which it can use to increase model accuracy. deepar implements this by learning an embedding vector for each group that captures the common properties of all time series in the group.  to facilitate learning time-dependent patterns, such as spikes during weekends, deepar automatically creates feature time series based on the frequency of the target time series. for example, deepar creates two feature time series (day of the month and day of the year) for a weekly time series frequency. it uses these derived feature time series with the custom feature time series that you provide during training and inference. the following figure shows two of these derived time series features: ui,1,t represents the hour of the day and ui,2,t the day of the week.  the deepar algorithm automatically generates these feature time series. the following table lists the derived features for the supported basic time frequencies. deepar trains a model by randomly sampling several training examples from each of the time series in the training dataset. each training example consists of a pair of adjacent context and prediction windows with fixed predefined lengths. the  hyperparameter controls how far in the past the network can see, and the  hyperparameter controls how far in the future predictions can be made. during training, the algorithm ignores training set elements containing time series that are shorter than a specified prediction length. the following figure represents five samples with context lengths of 12 hours and prediction lengths of 6 hours drawn from element i. for brevity, we've omitted the feature time series xi,1,t and ui,2,t.  to capture seasonality patterns, deepar also automatically feeds lagged values from the target time series. in the example with hourly frequency, for each time index, t = t, the model exposes the zi,t values, which occurred approximately one, two, and three days in the past.  for inference, the trained model takes as input target time series, which might or might not have been used during training, and forecasts a probability distribution for the next  values. because deepar is trained on the entire dataset, the forecast takes into account patterns learned from similar time series. for information on the mathematics behind deepar, see .  
for the latest aws terminology, see the  in the aws general reference. 
amazon sagemaker periodically tests and releases software that is installed on notebook instances. this includes: kernel updatessecurity patchesaws sdk updates updatesopen source software updatesamazon sagemaker does not automatically update software on a notebook instance when it is in service. to ensure that you have the most recent software updates, stop and restart your notebook instance, either in the amazon sagemaker console or by calling . you can also manually update software installed on your notebook instance while it is running by using update commands in a terminal or in a notebook. noteupdating kernels and some packages might depend on whether root access is enabled for the notebook instance. for more information, see . notebook instances do not notify you if you are running outdated software. you can check the  or the security bulletin at  for updates. 
with amazon sagemaker studio notebooks, you can change the amazon elastic compute cloud (amazon ec2) instance type that your notebook runs on from within the notebook. when you open a new notebook for the first time, you are assigned a default instance type to run the notebook. when you open additional notebooks on the same instance type, the notebooks run on the same instance as the first notebook, even if the notebooks use different kernels. importantif you change the instance type, unsaved information and existing settings for the notebook are lost, and installed packages must be re-installed. the following screenshot shows the menu from a studio notebook. the processor and memory of the instance type powering the notebook are displayed as 2 vcpu + 4 gib.  to change the instance type choose the instance type. in select instance, choose one of the fast launch instance types that are listed. or to see all instance types, switch off fast launch only. the list can be sorted by any column. after choosing a type, choose save and continue. wait for the new instance to become enabled, and then the new instance type information is displayed. for a list of the available instance types, see .  
to get inferences on an entire dataset you run a batch transform on a trained model. to run inferences on a full dataset, you can use the same inference pipeline model created and deployed to an endpoint for real-time processing in a batch transform job. to run a batch transform job in a pipeline, you download the input data from amazon s3 and send it in one or more http requests to the inference pipeline model. for an example that shows how to prepare data for a batch transform, see the "preparing data for batch transform" section of the . for information about amazon sagemaker batch transforms, see .  noteto use custom docker images in a pipeline that includes , you need an . your amazon ecr repository must grant amazon sagemaker permission to pull the image. for more information, see . the following example shows how to run a transform job using the . in this example,  is the inference pipeline that combines sparkml and xgboost models (created in previous examples). the amazon s3 location specified by  contains the input data, in csv format, to be downloaded and sent to the spark ml model. after the transform job has finished, the amazon s3 location specified by  contains the output data returned by the xgboost model in csv format. 
to configure a docker container to run as an executable, use an  instruction in a dockerfile. note the following:  for model training, amazon sagemaker runs the container as follows:  amazon sagemaker overrides any default  statement in a container by specifying the  argument after the image name. the  argument also overrides arguments that you provide using  in the dockerfile.    in your dockerfile, use the  form of the  instruction:  for example: the  form of the  instruction starts the executable directly, not as a child of . this enables it to receive signals like  and  from amazon sagemaker apis. note the following:   the  api has a stopping condition that directs amazon sagemaker to stop model training after a specific time.  the  api issues the equivalent of the , with a 2 minute timeout, command to gracefully stop the specified container: the command attempts to stop the running container by sending a  signal. after the 2 minute timeout, sigkill is sent and the containers are forcibly stopped. if the container handles the sigterm gracefully and exits within 120 seconds from receiving it, no sigkill is sent.  noteif you want access to the intermediate model artifacts after amazon sagemaker stops the training, add code to handle saving artifacts in your  handler. if you plan to use gpu devices for model training, make sure that your containers are  compatible. only the cuda toolkit should be included on containers; don't bundle nvidia drivers with the image. for more information about , see .you can't use the  initializer as your entry point in amazon sagemaker containers because it gets confused by the train and serve arguments. and all sub-directories are reserved by amazon sagemaker training. when building your algorithm’s docker image, please ensure you don't place any data required by your algorithm under them as the data may no longer be visible during training.
once you've started a human review loop, you can check the results of and manage the loop using the . additionally, amazon a2i integrates with amazon eventbridge (also known as amazon cloudwatch events) to alert you when a human review loop changes status.  use the procedures below to learn how to use the amazon augmented ai runtime api to monitor and manage your human loops. see  to learn how amazon a2i integrates with amazon eventbridge.  to check your output data: check the results of your human loop by calling the  operation. the result of this api operation contains information about the reason for and outcome of the loop activation. check the output data from your human loop in amazon simple storage service (amazon s3). the path to the data uses the following pattern where  represents the human loop creation date with year (), month () and day () and the creation time with hour (), minute () and second ().  you can integrate this structure with aws glue or amazon athena to partition and analyze your output data. for more information, see .  in your output data, you can use the following to track individual workers: the  is unique to each worker. in , you will see the following.  – the service which augmented ai uses to manage the private workforce. – the cognito user pool associated with the work team assigned to this human review task. – a unique identifier that refers to the worker. you can retrieve details about this worker (such as the name or username) using this id using amazon cognito. to learn how, see  in . to stop and delete your human loop: once a human loop has been started, you can stop your human loop by calling the  operation using the . if a human loop was successfully stopped, the server sends back an http 200 response.  to delete a human loop for which the status equals , , or , use the  operation.  to list human loops: you can list all active human loops by calling the  operation. you can filter human loops by the creation date of the loop using the  and  parameters.  if successful,  will return  and  objects in the response element.  contains information about a single human loop. for example, it will list a loop's status and if applicable, failure reason.  use the string returned in  as an input in a subsequent call to  to see the next page of human loops.  
a widget for classifying an image, which can be a jpg, png, or gif, with no size limit. the following is an example of an image classification template that uses the  element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  the following attributes are required by this element. a json formatted array of strings, each of which is a category that a worker can assign to the image. you should include "other" as a category, so that the worker can provide an answer. you can specify up to 10 categories. the text to display above the image. this is typically a question or simple instruction for the worker. the name of this widget. it is used as a key for the widget's input in the form output. information to be overlaid on the source image. this is for verification workflows of bounding-box and semantic-segmentation tasks. it is a json object containing an object with the name of the task-type in camelcase as the key. that key's value is an object that contains the labels and other necessary information from the previous task. an example of a  element with attributes for verifying a semantic segmentation task follows: a bounding-box verification task would use the  value like follows: the url of the image to be classified.  this element has the following parent and child elements. parent elements: child elements: , , the following regions are used by this element. general instructions for the worker on how to classify an image. important task-specific instructions that are displayed in a prominent place. use this in verification workflows when you need workers to explain why they made the choice they did. use the text between the opening and closing tags to provide instructions for workers on what information should be included in the comment. it uses the following attributes: a phrase with a call to action for leaving a comment. used as the title text for a modal window where the comment is added. optional. defaults to "add a comment." this text appears below the categories in the widget. when clicked, it opens a modal window where the worker may add a comment. optional. defaults to "add a comment." an example text in the comment text area that is overwritten when worker begins to type. this does not appear in output if the worker leaves the field blank. optional. defaults to blank. the output of this element is a string that specifies one of the values defined in the categories attribute of the <crowd-image-classifier> element. example : sample element outputsthe following is a sample of output from this element.   for more information, see the following. 
when using amazon augmented ai (amazon a2i) to create a human review workflow for your ml/ai application, you create and configure resources in amazon sagemaker such as a human workforce and worker task templates. to configure and start a human loop, you will either integrate amazon a2i with other aws services such as amazon textract or amazon rekognition or use the amazon augmented ai runtime api. to create a human review workflow and start a human loop, you will need to attach certain policies to your aws identity and access management (iam) role or user. specifically:  when you create a flow definition, you need to provide a role that grants amazon a2i permission to access amazon s3 both for reading objects that will be rendered in a human task ui and for writing the results of the human review.  this role will also need to have a trust policy attached to give amazon sagemaker permission to assume the role. this allows amazon a2i to perform actions in accordance with permissions that you attach to the role.  see  for example policies that you can modify and attach to the role you use to create a flow definition. these are the policies that will be attached to the iam role that is created in the human review workflows section of the amazon a2i area of the amazon sagemaker console.  to create and start human loops, you either use an api operation from a built-in task type (such as  or ) or the amazon a2i runtime api operation  in a custom ml application. you need to attach the amazonaugmentedaifullaccess managed policy to the iam user that invokes these api operations to grant permission to these services to use amazon a2i operations. to learn how, see . this policy does not grant permission to invoke the api operations of the aws service associated with built-in task types. for example, amazonaugmentedaifullaccess does not grant permission to call amazon rekognition api operation  or amazon textract api operation . you can use the more general policy, amazonaugmentedaiintegratedapiaccess, to grant these permissions. for more information, see . this is a good option when you want to grant an iam user broad permissions to use amazon a2i and integrated aws services' api operations.  if you want to configure more granular permissions, see  and  for identity-based policies you can use to grant permission to use these individual services. to preview your custom worker task ui template, you need an iam role with permissions to read amazon s3 objects that get rendered on your user interface. see a policy example in .topics to create a flow definition, attach the policies in this section to the role that you use when creating a human review workflow in the amazon sagemaker console, or using the  api operation. if you are using the console to create a human review workflow, enter the role amazon resource name (arn) in the iam role field when . when creating a flow definition using the api, attach these policies to the role that is passed to the  parameter of the  operation. when you create a human review workflow (flow definition), amazon a2i invokes amazon s3 to complete your task. to grant amazon a2i permission to retrieve and store your files in your amazon s3 bucket, create the following policy and attach it to your role. for example, if the images, documents, and other files that you are sending for human review are stored in an s3 bucket named , and if you want the human reviews to be stored in a bucket named , you would create the following policy.  in addition, the iam role must have the following trust policy to give amazon sagemaker permission to assume the role. to learn more about iam trust policies, see  section of policies and permissions in the aws identity and access management documentation. for more information about creating and managing iam roles and policies, see the following topics in the aws identity and access management user guide:  to create iam role, see . to learn how to create iam policies, see . to learn how to attach an iam policy to a role, see .to use amazon a2i to create and start human loops for amazon rekognition, amazon textract, or the amazon a2i runtime api, you must use an iam user that has permissions to invoke amazon a2i operations. to do this, use the iam console to attach the  managed policy to a new or existing iam user.  this policy grants permission to an iam user to invoke api operations from the amazon sagemaker api for flow definition creation and management and the amazon augmented ai runtime api for human loop creation and management. to learn more about these api operations, see . amazonaugmentedaifullaccess does not grant permissions to use amazon rekognition or amazon textract api operations.  noteyou can also attach the amazonaugmentedaifullaccess to an iam role that is used to create and start a human loop.  to create the required iam user sign in to the iam console at . choose users and choose an existing user, or create a new user by choosing add user. to learn how to create a new user, see  in the aws identity and access management user guide. if you chose to attach the policy to an existing user, choose add permissions.while creating a new user, follow the next step on the set permissions page.choose attach existing policies directly. in the search bar, enter amazonaugmentedaifullaccess and check the box next to that policy.  to enable this iam user to create a flow definition with the public work team, also attach the amazonsagemakermechanicalturkaccess managed policy.  after attaching the policy or policies: if you are using an existing user, choose next: review, and then choose add permissions. if you are creating a new user, choose next: tags and complete the process of creating your user.  for more information, see  in the aws identity and access management user guide. to create an iam user that has permission to invoke the api operations used by the built-in task types (that is,  for amazon rekognition and  for amazon textract) and permission to use all amazon a2i api operations, attach the iam managed policy, amazonaugmentedaiintegratedapiaccess. you may want to use this policy when you want to grant broad permissions to an iam user using amazon a2i with more than one task type. to learn more about these api operations, see . noteyou can also attach the amazonaugmentedaiintegratedapiaccess to an iam role that is used to create and start a human loop.  to create the required iam user sign in to the iam console at . choose users and choose an existing user, or create a new user by choosing add user. to learn how to create a new user, see  in the aws identity and access management user guide. if you chose to attach the policy to an existing user, choose add permissions.while creating a new user, follow the next step on the set permissions page.choose attach existing policies directly. in the search bar, enter amazonaugmentedaiintegratedapiaccess and select the box next to that policy.  to allow this iam user to create a flow definition using amazon mechanical turk, also attach the amazonsagemakermechanicalturkaccess managed policy.  after attaching the policy or policies: if you are using an existing user, choose next: review, and then choose add permissions. if you are creating a new user, choose next: tags and complete the process of creating your user.  for more information, see  in the aws identity and access management user guide. to customize the interface and instructions that your workers see when working on your tasks, you create a worker task template. you can create the template using the  operation or the amazon sagemaker console.  to preview your template, you need an iam role with the following permissions to read amazon s3 objects that get rendered on your user interface.  for amazon rekognition and amazon textract task types, you can preview your template using the amazon augmented ai section of the amazon sagemaker console. for custom task types, you preview your template by invoking the  operation. to preview your template, follow the instructions for your task type: amazon rekognition and amazon textract task types – in the amazon sagemaker console, use the role 's amazon resource name (arn) in the procedure documented in .custom task types – in the  operation, use the role's arn in the  parameter..
amazon sagemaker ground truth can reuse datasets from prior jobs in two ways: cloning and chaining. cloning copies the set-up of a prior labeling job and allows you to make additional changes, before setting it to run. chaining uses not only the setup of the prior job, but also the results. this allows you to continue an incomplete job, and add labels or data objects to a completed job. chaining is a more complex operation.  for data processing:  cloning uses the prior job's input manifest, with optional modifications, as the new job's input manifest. chaining uses the prior job's output manifest as the new job's input manifest. chaining is useful when you need to: continue a labeling job that was manually stopped.continue a labeling job that failed mid-job, after fixing issues.switch to automated data labeling after manually labeling part of a job (or vice-versa).add more data objects to a completed job and start the job from there.add another annotation to a completed job. for example, you have a collection of phrases labeled for topic, then want to run the set again, categorizing them by the topic's implied audience.in amazon sagemaker ground truth you can configure a chained labeling job with either the console or the api. the label attribute name ( in the api) is a string used as the key for the key-value pair formed with the label that a worker assigns to the data object. the following rules apply for the label attribute name: it can't end with .the names  and  are reserved and can't be used.for semantic segmentation labeling jobs, , it must end with . for all other labeling jobs, it can't end with . if you use the console to create the job, amazon sagemaker ground truth automatically appends  to all label attribute names except for semantic segmentation jobs.for a chained labeling job, if you're using the same label attribute name from the originating job and you configure the chained job to use auto-labeling, then if it had been in auto-labeling mode at any point, ground truth uses the model from the originating job.in an output manifest, the label attribute name appears similar to the following. if you're creating a job in the console and don't explicitly set the label attribute name value, ground truth uses the job name as the label attribute name for the job. choose a stopped, failed, or completed labeling job from the list of your existing jobs. this enables the actions menu. from the actions menu, choose chain. in the job overview panel, a new job name is set based on the title of the job from which you are chaining this one. you can change it. you may also specify a label attribute name different from the labeling job name. if you're chaining from a completed job, the label attribute name uses the name of the new job you're configuring. to change the name, select the check box. if you're chaining from a stopped or failed job, the label attribute name uses to the name of the job from which you're chaining. its easy to see and edit the value because the name check box is checked. attribute label naming considerationsthe default uses the label attribute name ground truth has selected. all data objects without data connected to that label attribute name are labeled. using a label attribute name not present in the manifest causes the job to process all the objects in the dataset. the input dataset location in this case is automatically selected as the output manifest of the chained job. the input field is not available, so you cannot change it. adding data objects to a labeling jobyou cannot specify an alternate manifest file. manually edit the output manifest from the previous job to add new items before starting a chained job. the s3 uri helps you locate where you are storing the manifest in your s3 bucket. download the manifest file from there, edit it locally on your computer, then upload the new version to replace it. make sure you are not introducing errors during editing. we recommend you use json linter to check your json. many popular text editors and ides have linter plugins available. the procedure is almost the same as setting up a new labeling job with , except for two primary differences: manifest location: rather than use your original manifest from the prior job, the value for the  in the  should point to the s3 uri of the output manifest from the prior labeling job.label attribute name: setting the correct  value is important here. as pointed out, this is the key portion of a key-value pair where labeling data is the value. sample use cases include:adding new or more-specific labels to a completed job — set a new label attribute name.labeling the unlabeled items from a prior job — use the label attribute name from the prior job.you can get some chaining benefits if you use an augmented manifest that has already been partially labeled. check the label attribute name check box and set the name so that it matches the name in your manifest. if you're using the api, the instructions are the same as starting a chained job. however, be sure to upload your manifest to an s3 bucket and use it instead of using the output manifest from a prior job. the label attribute name value in the manifest has to conform to the naming considerations discussed above. 
you can create a custom kernel in any of the sagemaker images. the kernel will display in the sagemaker studio launcher when the sagemaker image that hosts the kernel is chosen. the kernel will also display in the select kernel dialog when changing a notebook's kernel. conda must be installed in the sagemaker image in order to create the kernel. the  sagemaker image comes with conda preinstalled. to create a kernel in any of the other sagemaker images, you must first install conda. for more information, see . a kernel is defined by a conda environment. for more information about conda environments, see . to create a custom kernel choose file and then new launcher (). from the studio launcher, choose the sagemaker image that will host the kernel. choose image terminal. the terminal opens inside the chosen sagemaker image. if the chosen sagemaker image doesn't have conda installed, run the following command to install conda. run the following command to create the kernel. we're showing  as it's available on all jupyter notebooks. refer to the  documentation for other options. you can use the  option to ignore the package installation confirmation. noteyou don't need to activate the kernel after creating it. the kernel isn't persisted when the sagemaker image hosting the kernel is shutdown. to save the kernel, export the kernel as a yml file and save the file to your amazon efs volume. to export and recreate a custom kernel run the following command to export the kernel as a yml file. run the following command to recreate the kernel from the yml file. 
a message that alerts the worker to a current situation. the following is an example of a liquid template that uses the  element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  the following attributes are supported by this element. a boolean switch that, if present, allows the message to be closed by the worker. a string that specifies the type of message to be displayed. the possible values are "info" (the default), "success", "error", and "warning". this element has the following parent and child elements. parent elements: child elements: nonefor more information, see the following. 
 amazon cognito is used to define and manage your private workforce and your work teams. it is a service that you can use to create identities for your workers and authenticate these identities with identity providers.  a private workforce corresponds to a single amazon cognito user pool. private work teams correspond to amazon cognito user groups within that user pool.    example identity providers supported by amazon cognito:  social sign-in providers such as facebook and google openid connect (oidc) providers security assertion markup language (saml) providers such as active directory the amazon cognito built-in identity provider  for more information, see   to create a private workforce using amazon cognito, you must have an existing amazon cognito user pool containing at least one user group. see  to learn how to create a user pool. see  to learn how to add a user group to a pool.  once your user pool has been created, follow the steps below to create a private workforce by importing that user pool into amazon sagemaker. to create a private workforce by importing a amazon cognito user pool open the amazon sagemaker console at .  in the navigation pane, choose labeling workforces.  choose private. choose create private team. this creates a private workforce and a work team.  choose import workers from existing amazon cognito user groups.  choose a user pool that you have created. user pools require a domain and an existing user group. if you get an error that the domain is missing, set it in the domain name options on the app integration page of the amazon cognito console for your group. choose an app client. we recommend using a client generated by amazon sagemaker.  choose a user group from your pool to import its members.  optionally choose an amazon simple notification service (amazon sns) topic to which to subscribe the team so that workers are notified by email when new labeling jobs become available. amazon sns notifications are supported by ground truth and are not supported by augmented ai. if you subscribe workers to receive sns notifications, they will only receive notifications about ground truth labeling jobs. they will not receive notifications about augmented ai tasks.  choose create private team.  importantafter you create a workforce using a amazon cognito user pool, it should not be deleted without first deleting all work teams associated with that pool in the amazon sagemaker console.    after you import your private workforce, refresh the page to see the private workforce summary page. on this page, you can see information about the amazon cognito user pool for your workforce, a list of work teams for your workforce, and a list of all of the members of your private workforce. this workforce will now be available to use in both amazon augmented ai and amazon sagemaker ground truth for human review tasks and data labeling jobs respectively.  
 in this overview, you learn how to get started with a deep graph network by using one of the dgl containers in amazon elastic container registry (amazon ecr). you can also see links to practical examples for deep graph networks.   deep graph networks refer to a type of neural network that is trained to solve graph problems. a deep graph network uses an underlying deep learning framework like pytorch or mxnet. the potential for graph networks in practical ai applications are highlighted in the amazon sagemaker tutorials for  (dgl). examples for training models on graph datasets include social networks, knowledge bases, biology, and chemistry.      figure 1. the dgl ecosystem  several examples are provided using amazon sagemaker’s deep learning containers that are preconfigured with dgl. if you have special modules you want to use with dgl, you can also build your own container. the examples involve heterographs, which are graphs that have multiple types of nodes and edges, and draw on a variety of applications across disparate scientific fields, such as bioinformatics and social network analysis. dgl provides a wide array of . some of the highlights include:  gcn - graph convolutional network r-gcn - relational graph convolutional network gat - graph attention network dgmg - deep generative models of graphs jtnn - junction tree neural network dgl is available as a deep learning container in amazon ecr. you can select deep learning containers when you write your estimator function in an amazon sagemaker notebook. you can also craft your own custom container with dgl by following the  guide. the easiest way to get started with a deep graph network uses one of the dgl containers in amazon ecr.   note backend framework support is limited to pytorch and mxnet.  setupif you are using amazon sagemaker studio, you need to clone the examples repo first. if you are using a notebook instance, you can find the examples choosing the sagemaker icon at bottom of the left toolbar.  to clone the amazon sagemaker sdk and notebook examples repository from the jupyter lab view in amazon sagemaker, go to the file browser at the top of the left toolbar. from the file browser panel you can see a new navigation at the top of the panel.  choose the icon on the far right to clone a git repository.  add the repository url:   browse the newly added folder and its contents. the dgl examples are stored in the sagemaker-python-sdk folder.  to train a deep graph network from the jupyter lab view in amazon sagemaker, browse the  and look for dgl folders. several files may be included to support an example. examine the readme for any prerequisites.  run the .ipynb notebook example.   find the estimator function, and note the line where it is using an amazon ecr container for dgl and a specific instance type. you may want to update this to use a container in your preferred region.  run the function to launch the instance and use the dgl container for training a graph network. charges are incurred for launching this instance. the instance self-terminates when the training is complete.   an example of knowledge graph embedding (kge) is provided. it uses the freebase dataset, a knowledge base of general facts. an example use case would be to graph the relationships of persons and predict their nationality.    an example implementation of a graph convolutional network (gcn) shows how you can train a graph network to predict toxicity. a physiology dataset, tox21, provides toxicity measurements for how substances affect biological responses.    another gcn example shows you how to train a graph network on a scientific publications bibliography dataset, known as cora. you can use it to find relationships between authors, topics, and conferences.   the last example is a recommender system for movie reviews. it uses a graph convolutional matrix completion (gcmc) network trained on the movielens datasets. these datasets consist of movie titles, genres, and ratings by users.   the following examples use preconfigured deep learning containers. these are the easiest to try since they work out-of-the-box on amazon sagemaker.     the following examples enable you to bring your own container (byoc). read the  and familiarize yourself with that process before trying these. configuration is required.    
a group of radio buttons. only one radio button within the group can be selected. choosing one radio button clears any previously chosen radio button within the same group. for an example of a custom ui template that uses the  element, see this . the following is an example of the syntax that you can use with the  element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  no special attributes are supported by this element. this element has the following parent and child elements. parent elements: child elements: outputs an array of objects representing the  elements within it. example sample of element output   for more information, see the following. 
amazon sagemaker model monitor provides a prebuilt container with ability to analyze the data captured from endpoints for tabular datasets. if you would like to bring your own container, model monitor provides extension points which you can leverage. under the hood, when you create a , amazon sagemaker model monitor ultimately kicks off processing jobs. hence the container needs to be aware of the processing job contract documented in the  topic. note that amazon sagemaker model monitor kicks off the processing job on your behalf per the schedule. while invoking, model monitor sets up additional environment variables for you so that your container has enough context to process the data for that particular execution of the scheduled monitoring. for additional information on container inputs, see the . in the container, using the above environment variables/context, you can now analyze the dataset for the current period in your custom code. once this analysis is completed, you can chose to emit your reports to be uploaded to s3. the reports that the pre-built container generates are documented in . if you would like the visualization of the reports to work in amazon sagemaker studio, you should follow the same format. you can also choose to emit completely custom reports. you also have an option to emit cloudwatch metrics from the container by following the instructions in . topics 
the following topics explain the data formats for the algorithms provided by amazon sagemaker. topics 
third-party auditors assess the security and compliance of amazon sagemaker as part of multiple aws compliance programs. these include soc, pci, fedramp, hipaa, and others. for a list of aws services in scope of specific compliance programs, see . for general information, see . you can download third-party audit reports using aws artifact. for more information, see . your compliance responsibility when using amazon sagemaker is determined by the sensitivity of your data, your company's compliance objectives, and applicable laws and regulations. aws provides the following resources to help with compliance:  – these deployment guides discuss architectural considerations and provide steps for deploying security- and compliance-focused baseline environments on aws. – this whitepaper describes how companies can use aws to create hipaa-compliant applications. – this collection of workbooks and guides might apply to your industry and location. – this aws service assesses how well your resource configurations comply with internal practices, industry guidelines, and regulations. – this aws service provides a comprehensive view of your security state within aws that helps you check your compliance with security industry standards and best practices.
by default, a workforce isn't restricted to specific ip addresses. you can use the  operation to restrict access to tasks to workers at allowable ip address ranges (). if you specify one or more cidrs, workers who attempt to access tasks using any ip address outside the specified ranges are denied access and get a  error message on the worker portal. you can specify up to four cidr values using .  after you have restricted your workforce to one or more cidrs, you can view all allowable cidrs using the  operation. 
a workforce is the group of workers that you have selected to label your dataset. you can choose either the amazon mechanical turk workforce, a vendor-managed workforce, or you can create your own private workforce to label or review your dataset. whichever workforce type you choose, amazon sagemaker takes care of sending tasks to workers.  when you use a private workforce, you also create work teams, a group of workers from your workforce that are assigned to specific jobs— amazon sagemaker ground truth labeling jobs or amazon augmented ai human review tasks. you can have multiple work teams and can assign one or more work teams to each job. ground truth and amazon augmented ai use amazon cognito to manage your private workforce and work teams. for more information about the permissions required to manage your workforce this way, see . topics 
an element that displays instructions on three tabbed pages, summary, detailed instructions, and examples, when the worker clicks on a link or button. the following is an example of a liquid template that used the  element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  the following attributes are supported by this element. the text to display for opening the instructions. the default is click for instructions. a string that specifies the type of trigger for the instructions. the possible values are "link" (default) and "button". this element has the following parent and child elements. parent elements: child elements: nonethe following regions are supported by this element. content that provides specific instructions for a task. this appears on the page of the "detailed instructions" tab. content that provides examples of inadequate task completion. this appears on the page of the "examples" tab. more than one example may be provided within this element. content that provides examples of proper task completion. this appears on the page of the "examples" tab. a brief statement that summarizes the task to be completed. this appears on the page of the "summary" tab. more than one example may be provided within this element. for more information, see the following. 
point cloud data is always located in a coordinate system. this coordinate system may be local to the vehicle or the device sensing the surroundings, or it may be a world coordinate system. when you use ground truth 3d point cloud labeling jobs, all the annotations are generated using the coordinate system of your input data. for some labeling job task types and features, you must provide data in a world coordinate system.  in this topic, you'll learn the following: when you must provide data in a world coordinate system or global frame of reference for your ground truth labeling job.what a world coordinate is and how you can convert point cloud data to a world coordinate system. how you can use your sensor and camera extrinsic matrices to provide pose data when using sensor fusion. if your point cloud data was collected in a local coordinate system, you can use an extrinsic matrix of the sensor used to collect the data to convert it to a world coordinate system or a global frame of reference. if you cannot obtain an extrinsic for your point cloud data and, as a result, cannot obtain point clouds in a world coordinate system, you can provide point cloud data in a local coordinate system for 3d point cloud object detection and semantic segmentation task types.  for object tracking, you must provide point cloud data in a world coordinate system. this is because when you are tracking objects across multiple frames, the ego vehicle itself is moving in the world and so all of the frames need a point of reference.  if you include camera data for sensor fusion, it is recommended that you provide camera poses in the same world coordinate system as the 3d sensor (such as a lidar sensor).  this section explains what a world coordinate system (wcs), also referred to as a global frame of reference, is and explains how you can provide point cloud data in a world coordinate system. a wcs or global frame of reference is a fixed universal coordinate system in which vehicle and sensor coordinate systems are placed. for example, if multiple point cloud frames are located in different coordinate systems because they were collected from two sensors, a wcs can be used to translate all of the coordinates in these point cloud frames into a single coordinate system, where all frames have the same origin, (0,0,0). this transformation is done by translating the origin of each frame to the origin of the wcs using a translation vector, and rotating the three axes (typically x, y, and z) to the right orientation using a rotation matrix. this rigid body transformation is called a homogeneous transformation. a world coordinate system is important in global path planning, localization, mapping, and driving scenario simulations. ground truth uses the right-handed cartesian world coordinate system such as the one defined in , where the x axis is forward toward the car’s movement, y axis is left, and the z axis points up from the ground.  the global frame of reference depends on the data. some datasets use the lidar position in the first frame as the origin. in this scenario, all the frames use the first frame as a reference and device heading and position will be near the origin in the first frame. for example, kitti datasets have the first frame as a reference for world coordinates. other dataset us a device position that is different from the origin. note that this is not the gps/imu coordinate system, which is typically rotated by 90 degrees along the z-axis. if your point cloud data is in a gps/imu coordinate system (such as oxts in the open source av kitti dataset), then you need to transform the origin to a world coordinate system (typically the vehicle's reference coordinate system). you apply this transformation by multiplying your data with transformation metrics (the rotation matrix and translation vector). this will transform the data from its original coordinate system to a global reference coordinate system. learn more about this transformation in the next section.  ground truth assumes that your point cloud data has already been transformed into a reference coordinate system of your choice. for example, you can choose the reference coordinate system of the sensor (such as lidar) as your global reference coordinate system. you can also take point clouds from various sensors and transform them from the sensor's view to the vehicle's reference coordinate system view. you use the a sensor's extrinsic matrix, made up of a rotation matrix and translation vector, to convert your point cloud data to a wcs or global frame of reference.  collectively, the translation vector and rotation matrix can be used to make up an extrinsic matrix, which can be used to convert data from a local coordinate system to a wcs. for example, your lidar extrinsic matrix may be composed as follows, where  is the rotation matrix and  is the translation vector: for example, the autonomous driving kitti dataset includes a rotation matrix and translation vector for the lidar extrinsic transformation matrix for each frame. the  python module can be used for loading the kitti data, and in the dataset  gives the lidar extrinsic transform for the th frame with can be multiplied with points in that frame to convert them to a world frame - . multiplying a point in lidar frame with a lidar extrinsic matrix transforms it into world coordinates. multiplying a point in the world frame with the camera extrinsic matrix gives the point coordinates in the camera's frame of reference. the following code example demonstrates how you can convert point cloud frames from the kitti dataset into a wcs.  ground truth supports sensor fusion of point cloud data with up to 8 video camera inputs. this feature allows human labellers to view the 3d point cloud frame side-by-side with the synchronized video frame. in addition to providing more visual context for labeling, sensor fusion allows workers to adjust annotations in the 3d scene and in 2d images and the adjustment are projected into the other view. the following video demonstrates a 3d point cloud labeling job with lidar and camera sensor fusion.   for best results, when using sensor fusion, your point cloud should be in a wcs. ground truth uses your sensor (such as lidar), camera, and ego vehicle pose information to compute extrinsic and intrinsic matrices for sensor fusion.  ground truth uses sensor (such as lidar) extrinsic and camera extrinsic and intrinsic matrices to project objects to and from the point cloud data's frame of reference to the camera's frame of reference.  for example, in order to project a label from the 3d point cloud to camera image plane, ground truth transforms 3d points from lidar’s own coordinate system to the camera's coordinate system. this is typically done by first transforming 3d points from lidar’s own coordinate system to a world coordinate system (or a global reference frame) using the lidar extrinsic matrix. ground truth then uses the camera inverse extrinsic (which converts points from a global frame of reference to the camera's frame of reference) to transform the 3d points from world coordinate system obtained in previous step into the camera image plane. the lidar extrinsic matrix can also be used to transform 3d data into a world coordinate system. if your 3d data is already transformed into world coordinate system then the first transformation doesn’t have any impact on label translation, and label translation only depends on the camera inverse extrinsic. a view matrix is used to visualize projected labels. to learn more about these transformations and the view matrix, see .  ground truth computes these extrinsic matrices by using lidar and camera pose data that you provide:  ( in quaternions: , , , and ) and  (, , ). for the vehicle, typically the heading and position are described in vehicle's reference frame in a world coordinate system and are called a ego vehicle pose. for each camera extrinsic, you can add pose information for that camera. for more information, see . ground truth use the camera extrinsic and intrinsic matrices to compute view metrics to transform labels to and from the 3d scene to camera images. ground truth computes the camera intrinsic matrix using camera focal length (, ) and optical center coordinates (,) that you provide. for more information, see . image distortion can occur for a variety of reasons. for example, images may be distorted due to barrel or fish-eye effects. ground truth uses intrinsic parameters along with distortion co-efficient to undistort images you provide when creating 3d point cloud labeling jobs. if a camera image is already been undistorted, all distortion coefficients should be set to 0. for more information about the transformations ground truth performs to undistort images, see . to collect data for autonomous driving applications, the measurements used to generate point cloud data and are taken from sensors mounted on a vehicle, or the ego vehicle. to project label adjustments to and from the 3d scene and 2d images, ground truth needs your ego vehicle pose in a world coordinate system. the ego vehicle pose is comprised of position coordinates and orientation quaternion.   ground truth uses your ego vehicle pose to compute rotation and transformations matrices. rotations in 3 dimensions can be represented by a sequence of 3 rotations around a sequence of axes. in theory, any three axes spanning the 3d euclidean space are enough. in practice, the axes of rotation are chosen to be the basis vectors. the three rotations are expected to be in a global frame of reference (extrinsic). ground truth does not a support body centered frame of reference (intrinsic) which is attached to, and moves with, the object under rotation. to track objects, ground truth needs to measure from a global reference where all vehicles are moving. when using ground truth 3d point cloud labeling jobs, z specifies the axis of rotation (extrinsic rotation) and yaw euler angles are in radians (rotation angle). ground truth uses pose information for 3d visualizations and sensor fusion. pose information you input through your manifest file is used to compute extrinsic matrices. if you already have an extrinsic matrix, you can use it to extract sensor and camera pose data.  for example in the autonomous driving kitti dataset, the  python module can be used for loading the kitti data. in the dataset  gives the lidar extrinsic transform for the th frame and it can be multiplied with the points to get them in a world frame - . this transform can be converted into position (translation vector) and heading (in quaternion) of lidar for the input manifest file json format. camera extrinsic transform for  in th frame can be calculated by  and this can be converted into heading and position for . positionin the input manifest file,  refers to the position of the sensor with respect to a world frame. if you are unable to put the device position in a world coordinate system, you can use lidar data with local coordinates. similarly, for mounted video cameras you can specify the position and heading in a world coordinate system. for camera, if you do not have position information, please use (0, 0, 0).  the following are the fields in the position object:  (float) – x coordinate of ego vehicle, sensor, or camera position in meters.   (float) – y coordinate of ego vehicle, sensor, or camera position in meters.   (float) – z coordinate of ego vehicle, sensor, or camera position in meters.  the following is an example of a  json object:  headingin the input manifest file,  is an object that represents the orientation of a device with respect to world frame. heading values should be in quaternion. a  is a representation of the orientation consistent with geodesic spherical properties. if you are unable to put the sensor heading in world coordinates, please use the identity quaternion . similarly, for cameras, specify the heading in quaternions. if you are unable to obtain extrinsic camera calibration parameters, please also use the identity quaternion.  fields in  object are as follows:  (float) - x component of ego vehicle, sensor, or camera orientation.   (float) - y component of ego vehicle, sensor, or camera orientation.   (float) - z component of ego vehicle, sensor, or camera orientation.   (float) - w component of ego vehicle, sensor, or camera orientation.  the following is an example of a  json object:  to learn more, see . ground truth requires that all orientation, or heading, data be given in quaternions. a  is a representation of the orientation consistent with geodesic spherical properties that can be used to approximate of rotation. compared to  they are simpler to compose and avoid the problem of . compared to rotation matrices they are more compact, more numerically stable, and more efficient.  you can compute quaternions from a rotation matrix or a transformation matrix. if you have a rotation matrix (made up of the axis rotations) and translation vector (or origin) in world coordinate system instead of a single 4x4 rigid transformation matrix, then you can directly use the rotation matrix and translation vector to compute quaternions. libraries like  and  can help. the following code-block shows an example using these libraries to compute quaternion from a rotation matrix.  a ui tool like  can also be useful. if you have a 4x4 extrinsic transformation matrix, note that the transformation matrix is in the form  where is the rotation matrix and is the origin translation vector. that means you can extract rotation matrix and translation vector from the transformation matrix as follows. with your own setup, you can compute an extrinsic transformation matrix using the gps/imu position and orientation (latitude, longitude, altitude and roll, pitch, yaw) with respect to the lidar sensor on the ego vehicle. for example, you can compute pose from kitti raw data using  to transform the oxts data into a local euclidean poses, specified by 4x4 rigid transformation matrices. you can then transform this pose transformation matrix to a global reference frame using the reference frames transformation matrix in the world coordinate system. the following sections go into greater detail about the ground truth sensor fusion transformations that are performed using the pose data you provide. in order to project to and from a 3d lidar scene to a 2d camera image, ground truth computes the rigid transformation projection metrics using the ego vehicle pose and heading. ground truth computes rotation and translation of a world coordinates into the 3d plane by doing a simple sequence of rotations and translation.  ground truth computes rotation metrics using the heading quaternions as follows:  here,  corresponds to parameters in the  json object, . ground truth computes the translation column vector as . then the extrinsic metrics is simply as follows: geometric camera calibration, also referred to as camera resectioning, estimates the parameters of a lens and image sensor of an image or video camera. you can use these parameters to correct for lens distortion, measure the size of an object in world units, or determine the location of the camera in the scene. camera parameters include intrinsics and distortion coefficients. if the camera pose is given, then ground truth computes the camera extrinsic based on a rigid transformation from the 3d plane into the camera plane. the calculation is the same as the one used for the , except that ground truth uses camera pose ( and ) and computes the inverse extrinsic. cameras have been around for a long-long time. however, with the introduction of the cheap pinhole cameras in the late 20th century, they became a common occurrence in our everyday life. unfortunately, this cheapness comes with its price—significant distortion. luckily, these are constants and with calibration and some remapping image distortion can be corrected. furthermore, with calibration you can also determine the relationship between the camera’s natural units (pixels) and the real world units (for example, millimeters). radial distortion occurs when light rays bend more near the edges of a lens than they do at its optical center. the smaller the lens, the greater the distortion. the presence of the radial distortion manifests in form of the barrel or fish-eye effect and ground truth uses formula 1 to undistort it.  formula 1:  tangential distortion occurs because the lenses used to take the images are not perfectly parallel to the imaging plane. this can be corrected with formula 2.  formula 2:  in the input manifest file, you can provide distortion coefficients and ground truth will undistort your images. all distortion coefficients are floats.  , , ,  – radial distortion coefficients. supported for both fisheye and pinhole camera models. , – tangential distortion coefficients. supported for pinhole camera models.if images are already undistorted, all distortion coefficients should be 0 in your input manifest.  in order to correctly reconstruct the corrected image, ground truth does a unit conversion of the images based on focal lengths. if a common focal length is used with a given aspect ratio for both axes, such as 1, in the upper formula we will have a single focal length. the matrix containing these four parameters is referred to as the in camera intrinsic calibration matrix.   while the distortion coefficients are the same regardless of the camera resolutions used, these should be scaled with the current resolution from the calibrated resolution.  the following are float values.   - focal length in x direction. - focal length in y direction. - x coordinate of principal point. - y coordinate of principal point.ground truth use the camera extrinsic and camera intrinsic to compute view metrics as shown in the following code block to transform labels between the 3d scene and 2d images. 
this rule detects if the gradients in a trial become extremely small or drop to a zero magnitude. if the mean of the absolute values of the gradients drops below a specified , the rule returns . parameters descriptions for the vanishinggradient rule   for an example of how to configure and deploy a built-in rule, see . notethis rule can't be applied to the xgboost algorithm. 
this document guides you through the process of setting up a workflow with a custom labeling template. for more information about starting a labeling job, see . in that section, when you choose the task type, select custom labeling task, and then follow this section's instructions to configure it. topics 
custom elastic container registry (ecr) images deployed in amazon sagemaker are expected to adhere to the basic contract described in  that govern how amazon sagemaker interacts with a docker container that runs your own inference code. for a container to be capable of loading and serving multiple models concurrently, there are additional apis and behaviors that must be followed. this additional contract includes new apis to load, list, get, and unload models, and a different api to invoke models. there are also different behaviors for error scenarios that the apis need to abide by. to indicate that the container complies with the additional requirements, you can add the following command to your docker file: amazon sagemaker also injects an environment variable into the container if you are creating a multi-model endpoint for a serial inference pipline, your docker file must have the required labels for both multi-models and serial inference pipelines. for more information about serial information pipelines, see . to help you implement these requirements for a custom container, two libraries are available:  is an open source framework for serving machine learning models that can be installed in containers to provide the front end that fulfills the requirements for the new multi-model endpoint container apis. it provides the http front end and model management capabilities required by multi-model endpoints to host multiple models within a single container, load models into and unload models out of the container dynamically, and performs inference on a specified loaded model. it also provides a pluggable backend that supports a pluggable custom backend handler where you can implement your own algorithm. is a library that bootstraps multi model server with a configuration and settings that make it compatible with amazon sagemaker multi-model endpoints. it also allows you to tweak important performance parameters, such as the number of workers per model, depending on the needs of your scenario. currently, the only pre-built containers that support multi-model endpoints are the mxnet inference container an the pytorch inference container. if you want to use any other framework or algorithm, you need to build a container. the easiest way to do this is to use the  to extend an existing pre-built container. the amazon sagemaker inference toolkit is an implementation for the multi-model server (mms) that creates endpoints that can be deployed in amazon sagemaker. for a sample notebook that shows how to set up and deploy a custom container that supports multi-model endpoints in amazon sagemaker, see the . notethe amazon sagemaker inference toolkit supports only python model handlers. if you want to implement your handler in any other language, you must build your own container that implements the additional multi-model endpoint apis. for information, see . to extend a container by using the amazon sagemaker inference toolkit create a model handler. mms expects a model handler, which is a python file that implements functions to pre-process, get preditions from the model, and process the output in a model handler. for an example of a model handler, see  from the sample notebook. import the inference toolkit and use its  function to start mms. the following example is from the  file from the sample notebook. notice that the call to  passes the model handler described in the previous step: in your , copy the model handler from the first step and specify the python file from the previous step as the entrypoint in your . the following lines are from the  used in the sample notebook: build and register your container. the following shell script from the sample notebook builds the container and uploads it to an amazon elastic container registry repository in your aws account: you can now use this container to deploy multi-model endpoints in amazon sagemaker. topics 
when used with amazon a2i, the  operation supports the following inputs in the  parameter:  – use this condition to create a human loop when inference confidence is within a specified range for document form keys and word blocks. a form key is any word in a document that is associated with an input. the input is called a value. together, form keys and values are referred to as key-value pairs. a word block refers to the words that amazon textract recognizes inside of a detected block of text. to learn more about amazon textract document blocks, see  in the amazon textract developer guide. – use this condition to create a human loop when textract did not identify the key or its associated aliases within the document.  – use this condition to specify a percentage of forms to send to humans for review, regardless of inference confidence scores. use this condition to do the following:audit your ml model by randomly sampling all forms analyzed by your model and sending a specified percentage to humans for review.using the  condition, randomly sample a percentage of the inferences that met the conditions specified in  to start a human loop and send only the specified percentage to humans for review. noteif you send the same request to  multiple times, the result of  will not change for the inference of that input. for example, if you make an  request once, and  doesn't trigger a humanloop, subsequent requests to  with the same configuration will not trigger a human loop. the   supports the following :  – a string representing a key in a key-value pair detected by amazon textract that needs to be reviewed by human workers. if the value of this parameter is the special catch-all value (*), then all keys are considered to be matched to the condition. you can use this to model the case where any key-value pair satisfying certain confidence thresholds needs human review. – an array that represents alternate spellings or logical equivalents for the important form key. when you use the  , amazon a2i sends the key-value block and word block inferences of the key-value blocks and associated aliases that you specified in  and  for human review. when creating a flow definition, if you use the default worker task template that is provided in the human review workflows section of the amazon sagemaker console, key-value and block inferences sent for human review by this activation condition are included in the worker ui. if you use a custom worker task template, you need to include the  element to include initial-value input data (inferences) from amazon textract. for an example of a custom template that uses this input element, see . the   supports the following :  – a string representing a key in a key-value pair detected by amazon textract that needs to be reviewed by human workers. – an array that represents alternate spellings or logical equivalents for the important form key. when you use the  , if the key in  or aliases in  are not included in amazon textract inference, that form will be sent to human for review and no predicted key-value pairs will be included. for example, if amazon textract only identified  and  in a form, but was missing the   (in the  condition type) that form would be sent to humans for review without any of the form keys detected ( and ). if you use the default worker task template that is provided in the amazon sagemaker console, a task will be created asking workers identify the key in  and associated value. if you use a custom worker task template, you need to include the  custom html element to configure this task.  the   supports the  . the input for  must be real number between 0.01 and 100. this number represents the percentage of data that qualifies for a human review and will be sent to humans for review. if you use the  condition without any other conditions, this number represents the percentage of all resulting inferences made by the  operation from a single request that will be sent to humans for review. if you specify the  condition without any other condition type, all key-value and block inferences are sent to workers for review.  when creating a flow definition, if you use the default worker task template that is provided in the human review workflows section of the amazon sagemaker console, all key-value and block inferences sent for human review by this activation condition are included in the worker ui. if you use a custom worker task template, you need to include the  element to include initial-value input data (inferences) from amazon textract. for an example of a custom template that uses this input element, see . while only one condition needs to evaluate to true to trigger a human loop, amazon a2i will evaluate all conditions for each object analyzed by amazon textract. the human reviewers are asked to review the important form keys for all the conditions that evaluated to true. example 1: detect important form keys with confidence scores in a specified range trigger humanloop following is an example of a  json that triggers a humanloop if any one of the following three conditions is met: textract analyzedocument api returns a key-value pair whose key is one of , , or , with the confidence of the key-value block being less than 60 and the confidences of each of the word blocks making up the key and value being less than 85.textract analyzedocument api returns a key-value pair whose key is one of , , , or , with the confidence of the key-value block being less than 65 and the confidences of each of the word blocks making up the key and value being less than 85.textract analyzedocument api returns a key-value pair whose key is one of , , or , with the confidence of the key-value block being less than 60 and the confidences of each of the word blocks making up the key and value being less than 85.example 2: use importantformkeyconfidencecheck in the following example, if amazon textract detects a key-value pair whose confidence for the key-value block is less than 60 and is less than 90 for any underlying word blocks, a humanloop is created. the human reviewers are asked to review all the form key-value pairs that matched the confidence value comparisons. example 3: use sampling in the following example, 5% of inferences resulting from an amazon textract  request will be sent to human workers for review. all detected key-value pairs returned by amazon textract are sent to workers for review. example 4: use missingimportantformkey in the following example, if  or its alias,  is missing from keys detected by amazon textract, a human review will be triggered. when using the default worker task template, the worker ui asks workers to identify the key  or  and its associated value.  example 5: use sampling and importantformkeyconfidencecheck with the and operator in this example, 5% of key-value pairs detected by amazon textract whose key is one of , , , or , with the confidence of the key-value block less than 65 and the confidences of each of the word blocks making up the key and value less than 85 are sent to workers for review. example 6: use sampling and importantformkeyconfidencecheck with the and operator use this example to configure your human review workflow to always send low confidence inferences of a specified key-value pair for human review and sample high confidence inference of a key-value pair at a specified rate.  in the following example, a human review is triggered in one of the following ways:  key-value pairs detected whose key is one of , , , or , with key-value and word block confidences less than 60 will be sent for human review. only the  form key (and its aliases) and associated values are sent to workers to review. 5% of key-value pairs detected whose key is one of , , , or , with key-value and word block confidences greater than 90 will be sent for human review. only the  form key (and its aliases) and associated values are sent to workers to review. example 7: use sampling and importantformkeyconfidencecheck with the or operator in the following example, the amazon textract  operation returns a key-value pair whose key is one of , , , or , with the confidence of the key-value block less than 65 and the confidences of each of the word blocks making up the key and value less than 85. additionally, 5% of all other forms will trigger a human loop. for each form randomly chosen, all key-value pairs detected for that form will be sent to humans for review. 
you can use the amazon sagemaker console to create and manage the work teams and individual workers that make up a private workforce.  use a work team to assign members of your private workforce to a job - a labeling or human review job. when you create your workforce using the amazon sagemaker console, there is a work team called everyone-in-private-workforce that you can use if you want to assign your entire workforce to a job. because an imported amazon cognito user pool may contain members that you don't want to include in your work teams, a similar work team is not created for amazon cognito user pools.  you have two choices to create a new work team:  you can create a work team in the amazon sagemaker console and add members from your workforce to the team. you can create a user group by using the amazon cognito console and then create a work team by importing the user group. you can import more than one user group into each work team. you manage the members of the work team by updating the user group in the amazon cognito console. see  for more information.  you can create a workteam using the amazon sagemaker console, on the labeling workforces page. you can create a workteam by creating a new amazon cognito user group or by importing an existing user group. for more information on creating a user group in the amazon cognito console, see . to create a work team using the amazon sagemaker console: open the amazon sagemaker console at   choose labeling workforces from the left menu. under private, choose create private team.  under team details, enter a team name. the name must be unique in your account in an aws region.  under add workers, choose a method to add workers to the team using a user group. if you chose create a team by adding workers to a new amazon cognito user group, select the workers to add to the team. if you chose create a team by importing existing amazon cognito user groups, choose the user groups that are part of the new team. if you select an sns topic, all workers added to the team are subscribed to the amazon sns topic and notified when new work items are available to the team. select from a list of your existing ground truth related amazon sns topics or select create new topic to open a topic-creation dialog.  amazon sns notifications are supported by ground truth and are not supported by augmented ai. if you subscribe workers to receive sns notifications, they will only receive notifications about ground truth labeling jobs. they will not receive notifications about augmented ai tasks.  workers in a workteam subscribed to a topic receive notifications when a new ground truth labeling job for that team becomes available and when one is about to expire.   read  for more information about using amazon sns topic. after you have created a work team, you can see more information about the team and change or set the amazon sns topic to which its members are subscribed by visiting the amazon cognito console. any members of the team who were added to the team prior to the team being subscribed to a topic will need to be subscribed to that topic manually. read  for more information on creating and managing the amazon sns topic.   a work team is a group of workers within your workforce that you can assign jobs to. a worker can be added to more than one work team. once a worker has been added to a work team, that worker can be disabled or removed.  adding a worker to the workforce will allow you to add that worker to any work team within that work force.   to add workers using the private workforce summary page: open the amazon sagemaker console at  choose labeling workforces to navigate to your private workforce summary page.  choose private  choose invite new workers  paste or type a list of email addresses, separated by commas, into the email addresses box. you can have up to 50 email addresses in this list.   a worker must be added to the workforce before being added to a work team. to add a worker to a work team, first navigate to the private workforce summary page using the steps above.  to add a worker to a work team from the workforce summary page in the private teams section, choose the team that you want to add the workers to  choose the workers tab.  choose add workers to team and choose the boxes next to the workers that you want to add click add workers to team  disabling a worker stops the worker from receiving a job. this action does not remove the worker from the workforce, or any work team the worker is associated with. to disable or remove a worker to a work team, first navigate to the private workforce summary page using the steps above.  to deactivate a worker using the private workforce summary page in the workers section, choose the worker that you would like to disable.  choose disable.  if desired, you can subsequently enable a worker after they have been disabled.  you can remove workers from your private workforce directly in the amazon sagemaker console if that worker was added in this console. if you added the worker (user) in the amazon cognito console, see  to learn how to remove the worker in the amazon cognito console.  to remove a worker using the private workforce summary page in the workers section, choose the worker that you would like to delete.  if the worker has not been disabled, choose disable.   select the worker and choose delete.  
a widget for classifying an image into one ore more categories. the image can be a jpg, png, or gif file, and has no size limit.  the following is an example of an html worker task template built using this crowd element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  the following attributes are supported by the  element. each attribute accepts a string value or string values. required. a json-formatted array of strings, each of which is a category that a worker can assign to the image. a worker must choose at least one category and can choose all categories.  required. the text to display above the image. this is typically a question or simple instruction for workers. required. the name of this widget. in the form output, the name is used as a key for the widget's input. required. the url of the image to be classified.  optional. a json-formatted string with the following format: . this attribute sets a default value that workers can choose if none of the labels applies to the image shown in the worker ui. this element has the following parent and child elements: parent elements: child elements: , , this element uses the following regions general instructions for the worker on how to classify an image. important task-specific instructions. these instructions are displayed prominently. the output of this element is a string that specifies one or more of the values defined in the  attribute of the  element. example : sample element outputsthe following is a sample of output from this element.   for more information, see the following: 
amazon sagemaker autopilot provides samples, videos, and tutorials to get started with amazon sagemaker autopilot topics 
content-type: application/jsonlines the “in0” and “in1” are the inputs for encoder0 and encoder1, respectively. the same format is valid for both classification and regression problems. for regression, the field  can accept real valued inputs. 
the following are the available input and output formats for the ip insights algorithm. amazon sagemaker built-in algorithms adhere to the common input inference format described in . however, the amazon sagemaker ip insights algorithm does not currently support recordio format. the csv file must have two columns. the first column is an opaque string that corresponds to an entity's unique identifier. the second column is the ipv4 address of the entity's access event in decimal-dot notation.  content-type: text/csv json data can be provided in different formats. ip insights follows the common amazon sagemaker formats. for more information about inference formats, see . content-type: application/json the json lines content type is useful for running batch transform jobs. for more information on amazon sagemaker inference formats, see . for more information on running batch transform jobs, see . content-type: application/jsonlines the default output of the amazon sagemaker ip insights algorithm is the  between the input entity and ip address. the dot_product signifies how compatible the model considers the entity and ip address. the  is unbounded. to make predictions about whether an event is anomalous, you need to set a threshold based on your defined distribution. for information about how to use the  for anomaly detection, see the . accept: application/json advanced users can access the model's learned entity and ip embeddings by providing the additional content-type parameter  to the accept heading. you can use the  and  for debugging, visualizing, and understanding the model. additionally, you can use these embeddings in other machine learning techniques, such as classification or clustering. accept: application/json;verbose=true accept: application/jsonlines  accept: application/jsonlines; verbose=true  
amazon sagemaker ground truth sends data objects to your workers in batches. there are one or more tasks for each data object. for each task, a worker annotates one of your data objects. a batch does the following: sets the number of data objects that are available to workers. after the objects are annotated, another batch is sent.breaks the work into smaller chunks to avoid overloading your workforce.provides chunks of data for the iterative training of automated data labeling models.ground truth first sends a batch of 10 tasks to your workers. it uses this small batch to set up the labeling job and to make sure that the job is correctly configured. ground truth then sends larger batches to your workers.  you configure batch size when you create the job using the  operation. if you use the amazon sagemaker console to create the labeling job, ground truth automatically configures your job to use 1,000 tasks in each batch. 
the following sagemaker kernels are available in amazon sagemaker studio. the name in parentheses is the sagemaker image hosting the kernel. python 3 (data science)python 3 (base python)python 3 (mxnet cpu optimized)python 3 (mxnet gpu optimized)python 3 (pytorch cpu optimized)python 3 (pytorch gpu optimized)python 3 (tensorflow cpu optimized)python 3 (tensorflow gpu optimized)python 3 (tensorflow 2 cpu optimized)python 3 (tensorflow 2 gpu optimized)no kernel
use an amazon sagemaker ground truth multi-label image classification labeling task when you need workers to classify multiple objects in an image. for example, the following image features a dog and a cat. you can use multi-label image classification to associate the labels "dog" and "cat" with this image.  when working on a multi-label image classification task, workers should choose all applicable labels, but must choose at least one. when creating a job using this task type, you can provide up to 50 label-categories.  when creating a labeling job in the console, ground truth doesn't provide a "none" category for when none of the labels applies to an image. to provide this option to workers, include a label similar to "none" or "other" when you create a multi-label image classification job.  to restrict workers to choosing a single label for each image, use the  task type. importantfor this task type, if you create your own manifest file, use  to identify the location of each image file in amazon s3 that you want labeled. for more information, see . you can follow the instructions  to learn how to create a multi-label image classification labeling job in the amazon sagemaker console. in step 10, choose image from the task category drop down menu, and choose image classification (multi-label) as the task type.  ground truth provides a worker ui similar to the following for labeling tasks. when you create a labeling job in the console, you specify instructions to help workers complete the job and labels that workers can choose from.   to create a multi-label image classification labeling job, use the amazon sagemaker api operation . this api defines this operation for all aws sdks. to see a list of language-specific sdks supported for this operation, review the see also section of . follow the instructions on  and do the following while you configure your request:  pre-annotation lambda functions for this task type end with . to find the pre-annotation lambda arn for your region, see  . annotation-consolidation lambda functions for this task type end with . to find the annotation-consolidation lambda arn for your region, see . the following is an example of an  to create a labeling job in the us east (n. virginia) region. all parameters in red should be replaced with your specifications and resources.  if you create a labeling job using the api, you must supply a worker task template in . copy and modify the following template. only modify the , , and .  upload this template to s3, and provide the s3 uri for this file in . once you have created a multi-label image classification labeling job, your output data will be located in the amazon s3 bucket specified in the  parameter when using the api or in the output dataset location field of the job overview section of the console.  to learn more about the output manifest file generated by ground truth and the file structure the ground truth uses to store your output data, see .  to see an example of output manifest files for multi-label image classification labeling job, see . 
to get predictions, deploy your model. the method you use depends on how you want to generate inferences: to get one inference at a time in real time, set up a persistent endpoint using amazon sagemaker hosting services.to get inferences for an entire dataset, use amazon sagemaker batch transform.topics 
in a production environment, you might have an internet-facing application sending requests to the endpoint for inference. the following high-level example shows how to integrate your model endpoint into your application. for an example of how to use amazon api gateway and aws lambda to set up and deploy a web service that you can call from a client application, see  in the aws machine learning blog. create an iam role that the aws lambda service principal can assume. give the role permissions to call the amazon sagemaker  api. create a lambda function that calls the amazon sagemaker  api. call the lambda function from a mobile application. for an example of how to call a lambda function from a mobile application using amazon cognito for credentials, see .  next step 
 when you create a new hyperparameter optimization (hpo) job with amazon sagemaker, you have the option of using the console or the api. you provide one or more job specifications for the different algorithms you’re testing. these are called training definitions. each training definition has a name, an algorithm source, metrics selection, an objective metric, and a configuration for a set of hyperparameter values. it also has a data configuration for setting up the input data channels for the algorithm you choose, and a setting for the output data location. you select the resources you want to use for the training run.  topics 
the  (extreme gradient boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining an ensemble of estimates from a set of simpler and weaker models. the xgboot algorithm performs well in machine learning competitions because of its robust handling of a variety of data types, relationships, distributions, and the variety of hyperparameters that you can fine-tune. you can use xgboost for regression, classification (binary and multiclass), and ranking problems.  you can use the new release of the xgboost algorithm either as an amazon sagemaker built-in algorithm or as a framework to run training scripts in your local environments. this implementation has a smaller memory footprint, better logging, improved hyperparameter validation, and an expanded set of metrics than the original versions. it provides an xgboost  that executes a training script in a managed xgboost environment. the current release of amazon sagemaker xgboost is based on the original xgboost versions 0.90 and 1.0. framework (open source) mode: 0.90-1, 0.90-2, 1.0-1algorithm mode: 0.90-1, 0.90-2, 1.0-1topics with amazon sagemaker, you can use xgboost as a built-in algorithm or framework. by using xgboost as a framework, you have more flexibility and access to more advanced scenarios, such as k-fold cross-validation, because you can customize your own training scripts.  use xgboost as a framework use xgboost as a framework to run your customized training scripts that can incorporate additional data processing into your training jobs. in the following code example, you can find how amazon sagemaker python sdk provides the xgboost api as a framework in the same way it provides other framework apis, such as tensorflow, mxnet, and pytorch. for an end-to-end example of using sagemaker xgboost as a framework, see  use xgboost as a built-in algorithm use the xgboost built-in algorithm to build an xgboost training container as shown in the following code example. you can automatically spot the xgboost built-in algorithm image uri using the amazon sagemaker  api. if you want to ensure if the  api finds the correct uri, see  and look up  from the full list of built-in algorithm image uris and available regions. after specifying the xgboost image uri, you can use the xgboost container to construct an estimator using the sagemaker estimator api and initiate a training job. this xgboost built-in algorithm mode does not incorporate your own xgboost training script and runs directly on the input datasets.  if you do not specify the xgboost version in the  function, it picks up the xgboost version 1.0-1 by default.  for more information about how to set up the xgboost as a built-in algorithm, see the following notebook examples. gradient boosting operates on tabular data, with the rows representing observations, one column representing the target variable or label, and the remaining columns representing features.  the amazon sagemaker implementation of xgboost supports csv and libsvm formats for training and inference: for training contenttype, valid inputs are text/libsvm (default) or text/csv.for inference contenttype, valid inputs are text/libsvm (default) or text/csv.notefor csv training, the algorithm assumes that the target variable is in the first column and that the csv does not have a header record.for csv inference, the algorithm assumes that csv input does not have the label column.for libsvm training, the algorithm assumes that the label is in the first column. subsequent columns contain the zero-based index value pairs for features. so each row has the format:  : : ... inference requests for libsvm might not have labels in the libsvm format. this differs from other amazon sagemaker algorithms, which use the protobuf training input format to maintain greater consistency with standard xgboost data formats. for csv training input mode, the total memory available to the algorithm (instance count * the memory available in the ) must be able to hold the training dataset. for libsvm training input mode, it's not required, but we recommend it. sagemaker xgboost uses the python pickle module to serialize/deserialize the model, which can be used for saving/loading the model. to use a model trained with sagemaker xgboost in open source xgboost use the following python code: to differentiate the importance of labelled data points use instance weight supports amazon sagemaker xgboost allows customers to differentiate the importance of labelled data points by assigning each instance a weight value. for text/libsvm input, customers can assign weight values to data instances by attaching them after the labels. for example, . for text/csv input, customers need to turn on the  flag in the parameters and attach weight values in the column after labels. for example: ).amazon sagemaker xgboost currently only trains using cpus. it is a memory-bound (as opposed to compute-bound) algorithm. so, a general-purpose compute instance (for example, m5) is a better choice than a compute-optimized instance (for example, c4). further, we recommend that you have enough total memory in selected instances to hold the training data. although it supports the use of disk space to handle data that does not fit into main memory (the out-of-core feature available with the libsvm input mode), writing cache files onto disk slows the algorithm processing time. for a sample notebook that shows how to use amazon sagemaker xgboost as a built-in algorithm to train and host a regression model, see . for instructions how to create and access jupyter notebook instances that you can use to run the example in amazon sagemaker, see . once you have created a notebook instance and opened it, select the sagemaker examples tab to see a list of all the amazon sagemaker samples. the topic modeling example notebooks using the xgboost algorithms are located in the introduction to amazon algorithms section. to open a notebook, click on its use tab and select create copy. 
the following table contains the hyperparameters for the factorization machines algorithm. these are parameters that are set by users to facilitate the estimation of model parameters from data. the required hyperparameters that must be set are listed first, in alphabetical order. the optional hyperparameters that can be set are listed next, also in alphabetical order. 
models and data in a multi-model endpoint are co-located on instance storage volume and in container memory. all instances for amazon sagemaker endpoints run on a single tenant container that you own. only your models can run on your multi-model endpoint. it's your responsibility to manage the mapping of requests to models and to provide access for users to the correct target models. amazon sagemaker uses  to provide iam identity-based policies that you use to specify allowed or denied actions and resources and the conditions under which actions are allowed or denied. by default, an iam principal with  permissions on a multi-model endpoint can invoke any model at the address of the s3 prefix defined in the  operation, provided that the iam execution role defined in operation has permissions to download the model. if you need to restrict  access to a limited set of models in s3, you can do one of the following: restrict  calls to specific models hosted at the endpoint by using the  iam condition key. for example, the following policy allows  requests only when the value of the  field matches one of the specified regular expressions: for information about amazon sagemaker condition keys, see  in the aws identity and access management user guide. create multi-model endpoints with more restrictive s3 prefixes. for more information about how amazon sagemaker uses roles to manage access to endpoints and perform operations on your behalf, see . your customers might also have certain data isolation requirements dictated by their own compliance requirements that can be satisfied using iam identities. 
in the  request, you specify the training algorithm. you can also specify algorithm-specific hyperparameters as string-to-string maps. the following table lists the hyperparameters for the amazon sagemaker rcf algorithm. for more information, including recommendations on how to choose hyperparameters, see . 
amazon sagemaker sequence to sequence is a supervised learning algorithm where the input is a sequence of tokens (for example, text, audio) and the output generated is another sequence of tokens. example applications include: machine translation (input a sentence from one language and predict what that sentence would be in another language), text summarization (input a longer string of words and predict a shorter string of words that is a summary), speech-to-text (audio clips converted into output sentences in tokens). recently, problems in this domain have been successfully modeled with deep neural networks that show a significant performance boost over previous methodologies. amazon sagemaker seq2seq uses recurrent neural networks (rnns) and convolutional neural network (cnn) models with attention as encoder-decoder architectures.  topics training amazon sagemaker seq2seq expects data in recordio-protobuf format. however, the tokens are expected as integers, not as floating points, as is usually the case. a script to convert data from tokenized text files to the protobuf format is included in . in general, it packs the data into 32-bit integer tensors and generates the necessary vocabulary files, which are needed for metric calculation and inference. after preprocessing is done, the algorithm can be invoked for training. the algorithm expects three channels: : it should contain the training data (for example, the  file generated by the preprocessing script).: it should contain the validation data (for example, the  file generated by the preprocessing script).: it should contain two vocabulary files ( and ) if the algorithm doesn't find data in any of these three channels, training results in an error. inference for hosted endpoints, inference supports two data formats. to perform inference using space separated text tokens, use the  format. otherwise, use the  format to work with the integer encoded data. both mode supports batching of input data.  format also allows you to visualize the attention matrix. : expects the input in json format and returns the output in json format. both content and accept types should be . each sequence is expected to be a string with whitespace separated tokens. this format is recommended when the number of source sequences in the batch is small. it also supports the following additional configuration options: : {: }: returns the attention matrix for the particular input sequence. : expects the input in  format and returns the output in . both content and accept types should be . for this format, the source sequences must be converted into a list of integers for subsequent protobuf encoding. this format is recommended for bulk inference.for batch transform, inference supports json lines format. batch transform expects the input in json lines format and returns the output in json lines format. both content and accept types should be . the format for input is as follows: the format for response is as follows: for additional details on how to serialize and deserialize the inputs and outputs to specific formats for inference, see the  . currently amazon sagemaker seq2seq is only supported on gpu instance types and is only set up to train on a single machine. but it does also offer support for multiple gpus.  for a sample notebook that shows how to use the amazon sagemaker sequence to sequence algorithm to train a english-german translation model, see . for instructions how to create and access jupyter notebook instances that you can use to run the example in amazon sagemaker, see . once you have created a notebook instance and opened it, select the sagemaker examples tab to see a list of all the amazon sagemaker samples. the topic modeling example notebooks using the ntm algorithms are located in the introduction to amazon algorithms section. to open a notebook, click on its use tab and select create copy. 
use this page to become familiarize with the user interface and tools available to complete your 3d point cloud object detection task. topics when you work on a 3d point cloud object tracking task, you need to select a category from the annotations menu on the right side of your worker portal using the label categories menu. after you've selected a category, use the add cuboid and fit cuboid tools to fit a cuboid around objects in the 3d point cloud that this category applies to. after you place a cuboid, you can modify its location, dimensions, and orientation directly in the point cloud, and the three panels shown on the right. if you see one or more images in your worker portal, you can also modify cuboids in the images or in the 3d point cloud and the edits will show up in the other medium.  importantif you see cuboids have already been added to the 3d point cloud frames when you open your task, adjust those cuboids and add additional cuboids as needed.  to edit a cuboid, including moving, re-orienting, and changing cuboid dimensions, you must use shortcut keys. you can see a full list of shortcut keys in the shortcuts menu in your ui. the following are important key-combinations that you should become familiar with before starting your labeling task.  when you open your task, two frames will be loaded. if your task includes more than two frames, you need to use the navigation bar in the lower-left corner, or the load frames icon to load additional frames. you should annotate and adjust labels in all frames before submitting.  after you fit a cuboid tightly around the boundaries of an object, navigate to another frame using the navigation bar in the lower-right corner of the ui. if that same object has moved to a new location, add another cuboid and fit it tightly around the boundaries of the object. each time you manually add a cuboid, you see the frame sequence bar in the lower-left corner of the screen turn red where that frame is located temporally in the sequence.  the following video shows how, if you add a cuboid in one frame, and then adjust it in another, your ui will automatically infer the location of the cuboid in all of the frames in-between.  your ui automatically infers the location of that object in all other frames after you've placed a cuboid. you can see the movement of that object, and the inferred and manually created cuboids using the arrows. adjust inferred cuboids as needed. the following video demonstrates how to navigate between frames.   you can navigate in the 3d scene using your keyboard and mouse. you can: double click on specific objects in the point cloud to zoom into them.use a mouse-scroller or trackpad to zoom in and out of the point cloud.use both keyboard arrow keys and q, e, a, and d keys to move up, down, left, right. use keyboard keys w and s to zoom in and out. once you place a cuboids in the 3d scene, a side-view will appear with three projected views: top, side, and back. these side-views show points in and around the placed cuboid and help workers refine cuboid boundaries in that area. workers can zoom in and out of each of those side-views using their mouse.  the following video demonstrates movements around the 3d point cloud and in the side-view.   when you are in the worker ui, you see the following menus: instructions – review these instructions before starting your task.shortcuts – use this menu to view keyboard shortcuts that you can use to navigate the point cloud and use the annotation tools provided. label – use this menu to modify a cuboid. first, select a cuboid, and then choose an option from this menu. this menu includes assistive labeling tools like setting a cuboid to the ground and automatically fitting the cuboid to the object's boundaries. view – use this menu to toggle different view options on and off. for example, you can use this menu to add a ground mesh to the point cloud, and to choose the projection of the point cloud.3d point cloud – use this menu to add additional attributes to the points in the point cloud, such as color, and pixel intensity. note that these options may not be available.when you open a task, the move scene icon is on, and you can move around the point cloud using your mouse and the navigation buttons in the point cloud area of the screen. to return to the original view you see when you first opened the task, choose the reset scene icon.  after you select the add cuboid icon, you can add cuboids to the point cloud and images (if included). you must select the move scene icon again to move to another area in the 3d point cloud or image.  to collapse all panels on the right and make the 3d point cloud full-screen, choose the full screen icon.  if camera images are included, you may have the following view options: c – view the camera angle on point cloud view.f – view the frustum, or field of view, of the camera used to capture that image on point cloud view. p – view the point cloud overlaid on the image.b – view cuboids in the image. the following video demonstrates how to use these view options. the f option is used to view the field of view of the camera (the gray area), the c options shows the direction the camera is facing and angle of the camera (blue lines), and the b option is used to view the cuboid.   use this table to learn about the icons you see in your worker task portal.  the shortcuts listed in the shortcuts menu can help you navigate the 3d point cloud and use tools to add and edit cuboids. before you start your task, we recommend that you review the shortcuts menu and become acquainted with these commands. you need to use some of the 3d cuboid controls to edit your cuboid.  you should periodically save your work. ground truth will automatically save your work ever 15 minutes.  when you open a task, you must complete your work on it before pressing submit. if you select stop working you will loose that task, and other workers will be able to start working on it.  
you can create a neo compilation job in the amazon sagemaker console. in the amazon sagemaker console, choose compilation jobs, and then choose create compilation job.  on the create compilation job page, for job name, enter a name. then select an iam role.  if you don’t have an iam role, choose create a new role.  on the create an iam role page, choose any s3 bucket, and choose create role.  in the input configuration section, for location of model artifacts, enter the path of the s3 bucket that contains your model artifacts. for data input configuration, enter the json string that specifies how many data matrix inputs you and the shape of each data matrices. for machine learning framework, choose the framework.  in the output configuration section, for s3 output location, enter the path to the s3 bucket or folder where you want to store the model. for target device, choose which device you want to deploy your model to, and choose create job.  check the status of the compilation job when started.  check the status of the compilation job when completed.  
after you deploy a model into your production environment, use amazon sagemaker model monitor to continuously monitor the quality of your machine learning models in real time. amazon cloudwatch model monitor enables you to set up an automated alert triggering system when there are deviations in the model quality, such as data drift and anomalies. amazon cloudwatch logs collects log files of monitoring the model status and notifies when the quality of your model hits certain thresholds that you preset. aws cloudtrail stores the log files to an amazon s3 bucket you specify. early and pro-active detection of model deviations through aws model monitor products enables you to take prompt actions to maintain and improve the quality of your deployed model.  for more information about amazon sagemaker model monitoring products, see . to start your machine learning journey with amazon sagemaker, sign up for an aws account at .  
this rule detects if all or a specified percentage of the values in the tensors are zero. this rule can be applied either to one of the supported deep learning frameworks (tensorflow, mxnet, and pytorch) or to the xgboost algorithm. you must specify either the  or  parameter. if both the parameters are specified, the rule inspects the union of tensors from both sets. for an example of how to configure and deploy a built-in rule, see . parameters descriptions for the allzero rule   
this section explains how amazon sagemaker makes training information, such as training data, hyperparameters, and other configuration information, available to your docker container.  when you send a  request to amazon sagemaker to start model training, you specify the amazon elastic container registry path of the docker image that contains the training algorithm. you also specify the amazon simple storage service (amazon s3) location where training data is stored and algorithm-specific parameters. amazon sagemaker makes this information available to the docker container so that your training algorithm can use it. this section explains how we make this information available to your docker container. for information about creating a training job, see . topics  amazon sagemaker makes the hyperparameters in a  request available in the docker container in the  file. training_job_name—the training job name stored in the  parameter in a  request.training_job_arn—the amazon resource name (arn) of the training job returned as the  response element for .you specify data channel information in the  parameter in a  request. amazon sagemaker makes this information available in the  file in the docker container. for example, suppose that you specify three data channels (, , and ) in your request. amazon sagemaker provides the following json: noteamazon sagemaker provides only relevant information about each data channel (for example, the channel name and the content type) to the container, as shown.  will be set as  if specify efs or fsxlustre as input data sources. the  parameter in a  request specifies how to make data available for model training: in  mode or  mode. depending on the specified input mode, amazon sagemaker does the following:  mode—amazon sagemaker makes the data for the channel available in the  directory in the docker container. for example, if you have three channels named , , and , amazon sagemaker makes three directories in the docker container: notechannels that use file system data sources such as amazon elastic file system (efs) and amazon fsx must use file mode. also to utilize an fsx file server, you must specify a path that begins with . if a file system is specified, the directory path provided in the channel is mounted at . mode—amazon sagemaker makes data for the channel available from the named pipe: . for example, if you have three channels named , , and , you will need to read from the following pipes: read the pipes sequentially. for example, if you have a channel called training, read the pipes in this sequence:  open  in read mode and read it to end-of-file (eof), or if you are done with the first epoch, close the pipe file early.  after closing the first pipe file, look for  and read it until you have completed the second epoch, and so on. if the file for a given epoch doesn't exist yet, your code may need to retry until the pipe is created. there is no sequencing restriction across channel types. that is, you can read multiple epochs for the training channel, for example, and only start reading the validation channel when you are ready. or, you can read them simultaneously if your algorithm requires that.  if you're performing distributed training with multiple containers, amazon sagemaker makes information about all containers available in the  file. to enable inter-container communication, this json file contains information for all containers. amazon sagemaker makes this file available for both file and pipe mode algorithms. the file provides the following information: —the name of the current container on the container network. for example, . host values can change at any time. don't write code with specific values for this variable.—the list of names of all containers on the container network, sorted lexicographically. for example,  for a three-node cluster. containers can use these names to address other containers on the container network. host values can change at any time. don't write code with specific values for these variables.—the name of the network interface that is exposed to your container. for example, containers running the message passing interface (mpi) can use this information to set the network interface name.do not use the information in  or  because it might be inaccurate.hostname information may not be immediately available to the algorithm container. we recommend adding a retry policy on hostname resolution operations as nodes become available in the cluster.the following is an example file on node 1 in a three-node cluster: 
this rule measures sampling imbalances between classes and throws errors if the imbalance exceeds a threshold or if too many mispredictions for underrepresented classes occur as a result of the imbalance. classification models require well-balanced classes in the training dataset or a proper weighting/sampling of classes during training. the rule performs two checks: it counts the occurrences per class. if the ratio of number of samples between smallest and largest class is larger than the , an error is thrown.it checks the prediction accuracy per class. if resampling or weighting has not been correctly applied, then the model can reach high accuracy for the class with many training samples, but low accuracy for the classes with few training samples. if a fraction of mispredictions for a certain class is above , an error is thrown.this rule can be applied either to one of the supported deep learning frameworks (tensorflow, mxnet, and pytorch) or to the xgboost algorithm. for an example of how to configure and deploy a built-in rule, see . parameter descriptions for the classimbalance rule   
when you delete a flow definition, your flow definition status changes to  and all associated human loops will automatically be stopped and deleted by augmented ai if workers have not started tasks created by those human loops. if human workers are already working on a task, that task will continue to be available until it is completed or expires. as long as workers are still working on a task, your flow definition's status will be . if these tasks are completed, the results are stored in the amazon s3 bucket specified in your flow definition. deleting a flow definition won't remove any worker answers from your amazon s3 bucket.  after all human loops have been deleted, the flow definition will be permanently deleted. when a flow definition has been deleted, you can reuse its name to create a new flow definition.  you might want to delete a flow definition for any of the following reasons: you have sent data to a set of human reviewers and you want to delete all non-started human loops because you do not want those workers to work on those tasks any longer.the worker task template used to generate your worker ui does not render correctly or is not functioning as expected. after you delete a flow definition, the following changes occur: the flow definition no longer appears on the human review workflows page in the augmented ai area of the amazon sagemaker console. when you use the flow definition name as input to the api operations  or , augmented ai returns a  error. when you use , deleted flow definitions won't be included in the results. when use the flow definition arn as input to the augmented ai runtime api operation , augmented ai returns a .you can delete a flow definition on the human review workflows page in the augmented ai area of the amazon sagemaker console or by using the amazon sagemaker api.  flow definitions can only be deleted if their status is .  delete a flow definition (console) navigate to the amazon sagemaker console at . in the navigation pane, under the augmented ai section, choose human review workflows. choose the hyperlinked name of the human review workflow that you want to delete.  on the summary page of your human review workflow, choose delete in the upper-right corner.  in the dialog box asking you to confirm that you want to delete your human review workflow, choose delete.  you're automatically redirected to the human review workflows page. while your human review workflow is being deleted, the status deleting appears in the status column for that workflow. after it's deleted, it won't appear in the list of workflows on this page.  delete a flow definition (api)you can delete a flow definition using the amazon sagemaker  api operation. this api operation is supported through the  and a . the following table shows example requests using sdk for python (boto 3) and the aws cli to delete the flow definition, .  the following request example uses the sdk for python (boto 3) to delete the flow definition. for more information, see  in the aws sdk for python (boto) api reference. the following request example uses the aws cli to delete the flow definition. for more information, see  in the .  if the action is successful, augmented ai sends back an http 200 response with an empty http body. 
 amazon sagemaker debugger provides full visibility into model training by monitoring, recording, analyzing, and visualizing training process tensors. a tensor is a high-dimensional array of machine learning and deep learning metrics such as weights, gradients, and losses; in other words, it is a collection of metrics continuously updated during the backpropagation and optimization process of training deep learning models.   debugger can dramatically reduce the time, resources, and cost needed to train models. using debugger in amazon sagemaker studio or amazon sagemaker notebook instances makes inspecting training job issues easier with supported features and frameworks and provides a visual interface for analyzing your tensor data.   amazon sagemaker debugger python sdk and its client library  are designed to create python objects that enable you to interact with the saved tensors. debugger provides tools to set up hooks and rules to save and access tensors, as well as to make the tensors available for analysis through its trial feature, all through flexible and powerful apis. it supports the machine learning frameworks tensorflow, pytorch, mxnet, and xgboost on python 3.6 and above.  if you want to learn more about the debugger and  apis, see the following documentation:  the following amazon sagemaker debugger sample notebooks show how to set up amazon sagemaker training jobs to configure debugger hooks to save the tensors, apply debugger rules to the tensors to monitor the status of training jobs, and visualize them.  the notebooks are best used in the following order: the following video series provides a tour of amazon sagemaker debugger capabilities using amazon sagemaker studio and amazon sagemaker notebook instances.  topics julien simon, aws technical evangelist | length: 14 minutes 17 seconds this tutorial video demonstrates how to use amazon sagemaker debugger to capture and inspect debugging information from a training model. the example training model used in this video is a simple convolutional neural network (cnn) based on keras with the tensorflow backend. amazon sagemaker in a tensorflow framework and debugger enable you to build an estimator directly using the training script and debug the training job.  you can find the example notebook in the video in  provided by the author. you need to clone the  notebook file and a training script  example file to your amazon sagemaker studio or a amazon sagemaker notebook instance. after you clone the two files, specify the path  to the  file inside the  notebook. if you cloned the two files in the same directory, set it as . julien simon, aws technical evangelist | length: 44 minutes 34 seconds this video session explores advanced features of debugger and amazon sagemakermodel monitor that help boost productivity and the quality of your models. first, this video shows how to detect and fix training issues, visualize tensors, and improve models with debugger. next, at 22:41, the video shows how to monitor models in production and identify prediction issues such as missing features or data drift using amazon sagemaker model monitor. finally, it offers cost optimization tips to help you make the most of your machine learning budget.  you can find the example notebook in the video in  offered by the author. topics 
this topic provides an overview of the ground truth worker portal and the tools available to complete your 3d point cloud labeling task. first, select the type of task you are working on from topics.  for adjustment jobs, select the original labeling job task type that produced the labels you are adjusting. review and adjust the labels in your task as needed.  topics 
associate git repositories with your notebook instance to save your notebooks in a source control environment that persists even if you stop or delete your notebook instance. you can associate one default repository and up to three additional repositories with a notebook instance. the repositories can be hosted in aws codecommit, github, or on any other git server. associating git repositories with your notebook instance can be useful for: persistence - notebooks in a notebook instance are stored on durable amazon ebs volumes, but they do not persist beyond the life of your notebook instance. storing notebooks in a git repository enables you to store and use notebooks even if you stop or delete your notebook instance.collaboration - peers on a team often work on machine learning projects together. storing your notebooks in git repositories allows peers working in different notebook instances to share notebooks and collaborate on them in a source-control environment.learning - many jupyter notebooks that demonstrate machine learning techniques are available in publicly hosted git repositories, such as on github. you can associate your notebook instance with a repository to easily load jupyter notebooks contained in that repository.there are two ways to associate a git repository with a notebook instance: add a git repository as a resource in your amazon sagemaker account. then, to access the repository, you can specify an aws secrets manager secret that contains credentials. that way, you can access repositories that require authentication.associate a public git repository that is not a resource in your account. if you do this, you cannot specify credentials to access the repository.topics to manage your github repositories, easily associate them with your notebook instances, and associate credentials for repositories that require authentication, add the repositories as resources in your amazon sagemaker account. you can view a list of repositories that are stored in your account and details about each repository in the amazon sagemaker console and by using the api. you can add git repositories to your amazon sagemaker account in the amazon sagemaker console or by using the aws cli. noteyou can use the amazon sagemaker api  to add git repositories to your amazon sagemaker account, but step-by-step instructions are not provided here. to add a git repository as a resource in your amazon sagemaker account open the amazon sagemaker console at . choose git repositories, then choose add repository. to add an codecommit repository, choose aws codecommit. to add a github or other git-based repository, choose github/other git-based repo. to add an existing codecommit repository choose use existing repository. for repository, choose a repository from the list. enter a name to use for the repository in amazon sagemaker. the name must be 1 to 63 characters. valid characters are a-z, a-z, 0-9, and - (hyphen). choose add repository. to create a new codecommit repository choose create new repository. enter a name for the repository that you can use in both codecommit and amazon sagemaker. the name must be 1 to 63 characters. valid characters are a-z, a-z, 0-9, and - (hyphen). choose create repository. to add a git repository hosted somewhere other than codecommit choose github/other git-based repo. enter a name of up to 63 characters. valid characters include alpha-numeric characters, a hyphen (-), and 0-9. enter the url for the repository. do not provide a user name in the url. add the username and password in aws secrets manager as described in the next step. for git credentials, choose the credentials to use to authenticate to the repository. this is necessary only if the git repository is private. noteif you have two-factor authentication enabled for your git repository, use a personal access token generated by your git service provider instead of a password. to use an existing aws secrets manager secret, choose use existing secret, and then choose a secret from the list. for information about creating and storing a secret, see  in the aws secrets manager user guide. the name of the secret you use must contain the string . notethe secret must have a staging label of  and must be in the following format:for github repositories, we recommend using a personal access token instead of your account password. for information, see . to create a new aws secrets manager secret, choose create secret, enter a name for the secret, and then enter the username and password to use to authenticate to the repository. the name for the secret must contain the string . notethe iam role you use to create the secret must have the  permission in its iam policy.the secret must have a staging label of  and must be in the following format:for github repositories, we recommend using a personal access token instead of your account password. to not use any credentials, choose no secret. choose create secret. use the  aws cli command. specify a name for the repository as the value of the  argument. the name must be 1 to 63 characters. valid characters are a-z, a-z, 0-9, and - (hyphen). also specify the following: the default branchthe url of the git repository notedo not provide a user name in the url. add the username and password in aws secrets manager as described in the next step.the amazon resource name (arn) of an aws secrets manager secret that contains the credentials to use to authenticate the repository as the value of the  argumentfor information about creating and storing a secret, see  in the aws secrets manager user guide. the following command creates a new repository named  in your amazon sagemaker account that points to a git repository hosted at . for linux, os x, or unix: for windows: notethe secret must have a staging label of  and must be in the following format:for github repositories, we recommend using a personal access token instead of your account password. you can associate git repositories with a notebook instance when you create the notebook instance by using the aws management console, or the aws cli. if you want to use a codecommit repository that is in a different aws than the notebook instance, set up cross-account access for the repository. for information, see . topics to create a notebook instance and associate git repositories in the amazon sagemaker console follow the instructions at . for git repositories, choose git repositories to associate with the notebook instance. for default repository, choose a repository that you want to use as your default repository. amazon sagemaker clones this repository as a subdirectory in the jupyter startup directory at . when you open your notebook instance, it opens in this repository. to choose a repository that is stored as a resource in your account, choose its name from the list. to add a new repository as a resource in your account, choose add a repository to amazon sagemaker (opens the add repository flow in a new window) and then follow the instructions at . to clone a public repository that is not stored in your account, choose clone a public git repository to this notebook instance only, and then specify the url for that repository. for additional repository 1, choose a repository that you want to add as an additional directory. amazon sagemaker clones this repository as a subdirectory in the jupyter startup directory at . to choose a repository that is stored as a resource in your account, choose its name from the list. to add a new repository as a resource in your account, choose add a repository to amazon sagemaker (opens the add repository flow in a new window) and then follow the instructions at . to clone a repository that is not stored in your account, choose clone a public git repository to this notebook instance only, and then specify the url for that repository. repeat this step up to three times to add up to three additional repositories to your notebook instance. to create a notebook instance and associate git repositories by using the aws cli, use the  command as follows: specify the repository that you want to use as your default repository as the value of the  argument. amazon sagemaker clones this repository as a subdirectory in the jupyter startup directory at . when you open your notebook instance, it opens in this repository. to use a repository that is stored as a resource in your amazon sagemaker account, specify the name of the repository as the value of the  argument. to use a repository that is not stored in your account, specify the url of the repository as the value of the  argument.specify up to three additional repositories as the value of the  argument. amazon sagemaker clones this repository as a subdirectory in the jupyter startup directory at , and the repository is excluded from the default repository by adding it to the  directory of the default repository. to use repositories that are stored as resources in your amazon sagemaker account, specify the names of the repositories as the value of the  argument. to use repositories that are not stored in your account, specify the urls of the repositories as the value of the  argument.for example, the following command creates a notebook instance that has a repository named , that is stored as a resource in your amazon sagemaker account, as a default repository, and an additional repository that is hosted on github: noteif you use an aws codecommit repository that does not contain "sagemaker" in its name, add the  and  permissions to the role that you pass as the  argument to the  command. for information about how to add permissions to a role, see  in the aws identity and access management user guide.  to associate a codecommit repository in a different aws account with your notebook instance, set up cross-account access for the codecommit repository. to set up cross-account access for a codecommit repository and associate it with a notebook instance: in the aws account that contains the codecommit repository, create an iam policy that allows access to the repository from users in the account that contains your notebook instance. for information, see  in the codecommit user guide. in the aws account that contains the codecommit repository, create an iam role, and attach the policy that you created in the previous step to that role. for information, see  in the codecommit user guide. create a profile in the notebook instance that uses the role that you created in the previous step: open the notebook instance. open a terminal in the notebook instance. edit a new profile by typing the following in the terminal: edit the file with the following profile information: where codecommitaccount is the account that contains the codecommit repository, crossaccountaccessprofile is the name of the new profile, and crossaccountrepositorycontributorrole is the name of the role you created in the previous step. on the notebook instance, configure git to use the profile you created in the previous step: open the notebook instance. open a terminal in the notebook instance. edit the git configuration file typing the following in the terminal: edit the file with the following profile information: where crossaccountaccessprofile is the name of the profile that you created in the previous step. when you open a notebook instance that has git repositories associated with it, it opens in the default repository, which is installed in your notebook instance directly under . you can open and create notebooks, and you can manually run git commands in a notebook cell. for example: to open any of the additional repositories, navigate up one folder. the additional repositories are also installed as directories under . if you open the notebook instance with a jupyterlab interface, the jupyter-git extension is installed and available to use. for information about the jupyter-git extension for jupyterlab, see . when you open a notebook instance in jupyterlab, you see the git repositories associated with it on the left menu:  you can use the jupyter-git extension to manage git visually, instead of using the command line:  
tensors define the state of a training job at any particular point in its lifecycle. to manage tensors, use amazon sagemaker debugger  class to group them into collections.   the following topics include examples that show how to set up the hook configuration  object and include it in an estimator object to run your training script in aws deep learning containers in a tensorflow framework. the argument  of  is where you specify the directory to your training script, and you can simply replace  with your training script without making any changes (zero script change experience).  topics you can define your collection of tensors using amazon sagemaker debugger.  the following example shows how to use the default setting for debugger hook configuration for an estimator in a deep learning container with a tensorflow framework. to see a list of debugger built-in collections, see . you can also save a reduced number of tensors instead of the full set of tensors; for example, if you want to reduce the amount of data saved in your amazon s3 bucket.  the following example shows how to modify the debugger hook configuration to specify target tensors that you want to save. noteif you save reductions, only the reductions are available for analysis. if you also want to save the raw tensor, pass the  flag,  amazon sagemaker debugger can automatically generate tensorboard scalar summaries, distributions, and histograms for saved tensors. you enable this by passing a  object when you create an , as shown in the following example. you can also choose to disable or enable histograms for individual collections. by default, the  flag for a collection is set to . debugger adds scalar summaries to tensorboard for all  and scalars saved through . for more information about scalar collections and the  method, see the . the following example saves weights and gradients as full tensors, and also saves the gradients as histograms and distributions that can be visualized with tensorboard. amazon sagemaker debugger saves them in the amazon s3 location passed in the  object. the following two notebook examples show advanced use of amazon sagemaker debugger for visualizing tensors. debugger provides a completely transparent view into training neural network models. topics  this notebook example shows how to visualize saved tensors using amazon sagemaker debugger. by visualizing the tensors, you can easily see how the tensor values change while training deep learning algorithms. this notebook includes a training job with a poorly configured neural network and uses amazon sagemaker debugger to aggregate and analyze tensors, including gradients, activation outputs, and weights. for example, the following plot shows the distribution of gradients of a convolutional layer that is suffering from a vanishing gradient problem.   this notebook also illustrates how a good initial hyperparameter setting improves the training process by generating the same tensor distribution plots.   this notebook example shows how to save and visualize tensors from an mxnet gluon model training job using amazon sagemaker debugger. it illustrates that debugger is set to save all tensors to an amazon s3 bucket and retrieves relu activation outputs for the visulization. the following figure shows a three-dimensional visualization of the relu activation outputs. the color scheme is set for blue to indicate values close to 0 and yellow to indicate values close to 1.   in this notebook, the  class imported from  is designed to plot convolutional neural networks that take two-dimentional images for inputs. the  script provided with the notebook retrieves tensors using debugger and visualizes the convolutional neural network. you can run this notebook on amazon sagemaker studio to reproduce the tensor visualization and implement your own convolutional neural network model to it.  
amazon sagemaker ip insights is an unsupervised learning algorithm that learns the usage patterns for ipv4 addresses. it is designed to capture associations between ipv4 addresses and various entities, such as user ids or account numbers. you can use it to identify a user attempting to log into a web service from an anomalous ip address, for example. or you can use it to identify an account that is attempting to create computing resources from an unusual ip address. trained ip insight models can be hosted at an endpoint for making real-time predictions or used for processing batch transforms. amazon sagemaker ip insights ingests historical data as (entity, ipv4 address) pairs and learns the ip usage patterns of each entity. when queried with an (entity, ipv4 address) event, an amazon sagemaker ip insights model returns a score that infers how anomalous the pattern of the event is. for example, when a user attempts to log in from an ip address, if the ip insights score is high enough, a web login server might decide to trigger a multi-factor authentication system. in more advanced solutions, you can feed the ip insights score into another machine learning model. for example, you can combine the ip insight score with other features to rank the findings of another security system, such as those from . the amazon sagemaker ip insights algorithm can also learn vector representations of ip addresses, known as embeddings. you can use vector-encoded embeddings as features in downstream machine learning tasks that use the information observed in the ip addresses. for example, you can use them in tasks such as measuring similarities between ip addresses in clustering and visualization tasks. topics training and validation the amazon sagemaker ip insights algorithm supports training and validation data channels. it uses the optional validation channel to compute an area-under-curve (auc) score on a predefined negative sampling strategy. the auc metric validates how well the model discriminates between positive and negative samples. training and validation data content types need to be in  format. the first column of the csv data is an opaque string that provides a unique identifier for the entity. the second column is an ipv4 address in decimal-dot notation. ip insights currently supports only file mode. for more information and some examples, see . inference for inference, ip insights supports , , and  data content types. for more information about the common data formats for inference provided by amazon sagemaker, see . ip insights inference returns output formatted as either  or . each record in the output data contains the corresponding  (or compatibility score) for each input data point. for more information and some examples, see . the amazon sagemaker ip insights algorithm can run on both gpu and cpu instances. for training jobs, we recommend using gpu instances. however, for certain workloads with large training datasets, distributed cpu instances might reduce training costs. for inference, we recommend using cpu instances. ip insights supports all available gpus. if you need to speed up training, we recommend starting with a single gpu instance, such as ml.p3.2xlarge, and then moving to a multi-gpu environment, such as ml.p3.8xlarge and ml.p3.16xlarge. multi-gpus automatically divide the mini batches of training data across themselves. if you switch from a single gpu to multiple gpus, the  is divided equally into the number of gpus used. you may want to increase the value of the  to compensate for this. the type of cpu instance that we recommend depends largely on the instance's available memory and the model size. the model size is determined by two hyperparameters:  and . the maximum supported model size is 8 gb. the following table lists typical ec2 instance types that you would deploy based on these input parameters for various model sizes. in table 1, the value for  in the first column range from 32 to 2048 and the values for  in the first row range from 10,000 to 50,000,000. the values for the , , , and  hyperparameters also affect the amount of memory required. if these values are large, you might need to use a larger instance type than normal. for a sample notebook that shows how to train the amazon sagemaker ip insights algorithm and perform inferences with it, see . for instructions how to create and access jupyter notebook instances that you can use to run the example in amazon sagemaker, see . after creating a notebook instance, choose the sagemaker examples tab to see a list of all the amazon sagemaker examples. to open a notebook, choose its use tab and choose create copy. 
a ui component that can be checked or unchecked allowing a user to select multiple options from a set. the following is an example of a liquid template that uses the  element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  the following attributes are supported by this element. a boolean switch that, if present, displays the check box as checked. the following is an example of the syntx used to check a checkbox by default. a boolean switch that, if present, displays the check box as disabled and prevents it from being checked. the following is an example of the syntax used to disable a checkbox.  a string that is used to identify the answer submitted by the worker. this value will match a key in the json object that specifies the answer. a boolean switch that, if present, requires the worker to provide input. the following is an example of the syntax used to require a checkbox be selected. a string used as the name for the check box state in the output. defaults to "on" if not specified.  this element has the following parent and child elements. parent elements: child elements: noneprovides a json object. the  string is the object name and the string is the property name for a boolean value based on the check box state; true if checked, false if not checked. example : sample element outputsusing the same  value for multiple boxes.   note that all three color values are properties of a single object.using different  values for each box.   for more information, see the following. 
use an amazon augmented ai (amazon a2i) human review workflow, or flow definition, to specify the following: for the amazon textract and amazon rekognition built-in task types, the conditions under which your human loop will be called.the workforce that your tasks will be sent to.the instructions that your workforce will receive, which is called a worker task template.the configuration of your worker tasks, including the number of workers that receive a task and time limits to complete tasks. where your output data will be stored. you can create a flow definition in the amazon sagemaker console or using the amazon sagemaker  operation. you can build a worker task template using the console for amazon textract and amazon rekognition task types while creating your flow definition. importanthuman loop activation conditions, which trigger the human loop—for example, confidence thresholds—aren't available for amazon a2i custom task types. when using the console to create a flow definition for a custom task type, you can't specify activation conditions. when using the amazon a2i api to create a flow definition for a custom task type, you can't set the  attribute of the  parameter. to control when human reviews are initiated, specify conditions under which  is called in your custom application. in this case, every  invocation results in a human review. for more information, see . prerequisites to create a flow definition, you must have completed the prerequisites described in .  if you use the api to create a flow definition for any task type, or if you use a custom task type when creating a flow definition in the console, first you will need to create a worker task template. for more information, see . if you want to preview your worker task template while creating a flow definition for a built-in task type in the console, ensure that you grant the role that you use to create the flow definition permission to access the amazon s3 bucket that contains your template artifacts using a policy like the one described in . topics use this procedure to create a amazon augmented ai (amazon a2i) human review workflow using the amazon sagemaker console. if you are new to amazon a2i, we recommend that you create a private work team using people in your organization, and use this work team's arn when creating your flow definition. to learn how to set up a private workforce and create a work team, see . if you have already set up a private workforce, see  to learn how to add a work team to that workforce. if you are using amazon a2i with one of the built-in task types, you can create worker instructions using a default worker task template provided by augmented ai while creating a human review workflow in the console. to see samples of the default templates provided by augmented ai, see the built-in task types in . to create flow definition (console) open the amazon sagemaker console at . in the navigation pane, under the augmented ai section, choose human review workflows and then choose create human review workflow. in overview, do the following: for name, enter a unique workflow name. the name must be lowercase, unique within the aws region in your account, and can have up to 63 characters. valid characters include: a-z, 0-9, and - (hyphen). for s3 location for output, enter the s3 bucket where you want to store the human review results. the bucket must be located in the same aws region as the workflow. for iam role, choose the role that has the required permissions. if you choose a built-in task type and want to preview your worker template in the console, provide a role with the type of policy described in  attached. for task type, choose the task type that you want the human worker to perform.  if you chose the amazon rekognition or amazon textract task type, specify the conditions that will invoke human review. for amazon rekognition image moderation tasks, choose an inference confidence score threshold interval that triggers human review. for amazon textract tasks, you can trigger a human review when specific form keys are missing or when form key detection confidence is low. you can also trigger a human review if, after evaluating all of the form keys in the text, confidence is lower than your required threshold for any form key. you will see two variables that you can use to specify your confidence thresholds: identification confidence and qualification confidence. to learn more about these variables, see .for both task types, you can randomly send a percentage of data objects (images or forms) and their labels to humans for review. configure and specify your worker task template: if you are using the amazon rekognition or amazon textract task type: in the create template section:  to create instructions for your workers using the amazon a2i default template for amazon rekognition and amazon textract task types, choose build from a default template. if you choose build from a default template, create your instructions under worker task design:  provide a template name that is unique in the aws region you are in. in the instructions section, provide detailed instructions on how to complete your task. to help workers achieve greater accuracy, provide good and bad examples. (optional) in additional instructions, provide your workers with additional information and instructions.  for information on creating effective instructions, see . to select a custom template that you've created, choose it from the template menu and provide a task description to briefly describe the task for your workers. to learn how to create a custom template, see .if you are using the custom task type: in the worker task template section, choose your template from the list. all of the templates that you have created in the amazon sagemaker console appear in this list. to learn how to create a template for custom task types, see .(optional) preview your worker template:  for amazon rekognition and amazon textract task types, you have the option to choose see a sample worker task to preview your worker task ui. if you are creating a flow definition for a custom task type, you can preview your worker task ui using the  operation. for more information, see . for workers, choose a workforce type. choose create. after you've created a human review workflow, it appears in the console under human review workflows. to see your flow definition's amazon resource name (arn) and configuration details, choose the workflow by selecting its name.  if you are using a built-in task type, you can use the flow definition arn to start a human loop using that aws service's api (for example, the amazon textract api). for custom task types, you can use the arn to start a human loop using the amazon augmented ai runtime api. to learn more about both options, see . to create a flow definition using the amazon sagemaker api, you use the  operation. for an overview of the  operation, and details about each parameter, see .  to create a flow definition (api) for , enter a unique name. the name must be unique within the aws region in your account, and can have up to 63 characters. valid characters include: a-z, 0-9, and - (hyphen). for , enter the arn of the role that you configured to grant access to your data sources. for , enter information about the workers and what they should see. for information about each parameter in , see . (optional) if you are using a built-in task type, provide conditions that trigger a human loop in . to learn how to create the input required for the  parameter, see . if you do not specify conditions here, when you provide a flow definition to the aws service associated with a built-in task type (for example, amazon textract or amazon rekognition), that service will send every task to a human worker for review.  if you are using a custom task type,  is disabled. to learn how to control when tasks are sent to human workers using a custom task type, see . (optional) if you are using a built-in task type, specify the integration source (for example, amazon rekognition or amazon textract) in the  parameter. for , indicate where in amazon simple storage service (amazon s3) to store the output of the human loop. (optional) use  to enter key value pairs to help you categorize and organize a flow definition. each tag consists of a key and a value, both of which you define. the following is an example of a request to create an amazon rekognition human loop using the aws python sdk (boto3). replace  with  to create a amazon textract human loop. for more information, see the  documentation. the return value of a successful call of the  api operation is a flow definition amazon resource name (arn). if you are using a built-in task type, you can use the flow definition arn to start a human loop using that aws service's api (i.e. the amazon textract api). for custom task types, you can use the arn to start a human loop using the amazon augmented ai runtime api. to learn more about both of these options, see . 
the amazon sagemaker image classification algorithm is a supervised learning algorithm that supports multi-label classification. it takes an image as input and outputs one or more labels assigned to that image. it uses a convolutional neural network (resnet) that can be trained from scratch or trained using transfer learning when a large number of training images are not available.  the recommended input format for the amazon sagemaker image classification algorithms is apache mxnet . however, you can also use raw images in .jpg or .png format. refer to  for a broad overview of efficient data preparation and loading for machine learning systems.  noteto maintain better interoperability with existing deep learning frameworks, this differs from the protobuf data formats commonly used by other amazon sagemaker algorithms. for more information on convolutional networks, see:   kaiming he, et al., 2016 ieee conference on computer vision and pattern recognitiontopics the amazon sagemaker image classification algorithm supports both recordio () and image (, , and ) content types for training in file mode, and supports the recordio () content type for training in pipe mode. however, you can also train in pipe mode using the image files (, , and ), without creating recordio files, by using the augmented manifest format. distributed training is supported for file mode and pipe mode. when using the recordio content type in pipe mode, you must set the  of the  to . the algorithm supports , , and  for inference. if you use the recordio format for training, specify both  and  channels as values for the  parameter of the  request. specify one recordio () file in the  channel and one recordio file in the  channel. set the content type for both channels to .  if you use the image format for training, specify , , , and  channels as values for the  parameter of the  request. specify the individual image data ( or  files) for the  and  channels. specify one  file in each of the  and  channels. set the content type for all four channels to .  noteamazon sagemaker reads the training and validation data separately from different channels, so you must store the training and validation data in different folders. a  file is a tab-separated file with three columns that contains a list of image files. the first column specifies the image index, the second column specifies the class label index for the image, and the third column specifies the relative path of the image file. the image index in the first column must be unique across all of the images. the set of class label indices are numbered successively and the numbering should start with 0. for example, 0 for the cat class, 1 for the dog class, and so on for additional classes.   the following is an example of a  file:  for example, if your training images are stored in , , and so on, specify the path for your  channel as , which is the top-level directory for your data. in the  file, specify the relative path for an individual file named  in the  class directory as . you can also store all your image files under one subdirectory inside the  directory. in that case, use that subdirectory for the relative path. for example, .  the augmented manifest format enables you to do training in pipe mode using image files without needing to create recordio files. you need to specify both train and validation channels as values for the  parameter of the  request. while using the format, an s3 manifest file needs to be generated that contains the list of images and their corresponding annotations. the manifest file format should be in  format in which each line represents one sample. the images are specified using the  tag that points to the s3 location of the image. the annotations are provided under the  parameter value as specified in the  request. it can also contain additional metadata under the  tag, but these are ignored by the algorithm. in the following example, the  are contained in the list of image and annotation references . the corresponding label value is  for the first image and  for the second image: the order of  in the input files matters when training the imageclassification algorithm. it accepts piped data in a specific order, with  first, followed by . so the "attributenames" in this example are provided with  first, followed by . when using the imageclassification algorithm with augmented manifest, the value of the  parameter must be . multi-label training is also supported by specifying a json array of values. the  hyperparameter must be set to match the total number of classes. there are two valid label formats: multi-hot and class-id.  in the multi-hot format, each label is a multi-hot encoded vector of all classes, where each class takes the value of 0 or 1. in the following example, there are three classes. the first image is labeled with classes 0 and 2, while the second image is labeled with class 2 only:  in the class-id format, each label is a list of the class ids, from [0, ), which apply to the data point. the previous example would instead look like this: the multi-hot format is the default, but can be explicitly set in the content type with the  parameter:  the class-id format, which is the format outputted by groundtruth, must be set explicitly:  for more information on augmented manifest files, see . you can also seed the training of a new model with the artifacts from a model that you trained previously with amazon sagemaker. incremental training saves training time when you want to train a new model with the same or similar data. amazon sagemaker image classification models can be seeded only with another build-in image classification model trained in amazon sagemaker. to use a pretrained model, in the  request, specify the  as "model" in the  parameter. set the  for the model channel to . the input hyperparameters of both the new model and the pretrained model that you upload to the model channel must have the same settings for the ,  and  input parameters. these parameters define the network architecture. for the pretrained model file, use the compressed model artifacts (in .tar.gz format) output by amazon sagemaker. you can use either recordio or image formats for input data. for a sample notebook that shows how to use incremental training with the amazon sagemaker image classification algorithm, see the . for more information on incremental training and for instructions on how to use it, see .  the generated models can be hosted for inference and support encoded  and  image formats as , and  content-type. the output is the probability values for all classes encoded in json format, or in  for batch transform. the image classification model processes a single image per request and so outputs only one line in the json or json lines format. the following is an example of a response in json lines format: for more details on training and inference, see the image classification sample notebook instances referenced in the introduction. for image classification, we support the following gpu instances for training: , , , , and . we recommend using gpu instances with more memory for training with large batch sizes. however, both cpu (such as c4) and gpu (such as p2 and p3) instances can be used for the inference. you can also run the algorithm on multi-gpu and multi-machine settings for distributed training. both p2 and p3 instances are supported in the image classification algorithm. for a sample notebook that uses the amazon sagemaker image classification algorithm to train a model on the  and then to deploy it to perform inferences, see the . for instructions how to create and access jupyter notebook instances that you can use to run the example in amazon sagemaker, see . once you have created a notebook instance and opened it, select the sagemaker examples tab to see a list of all the amazon sagemaker samples. the example image classification notebooks are located in the introduction to amazon algorithms section. to open a notebook, click on its use tab and select create copy. 
before you use iam to manage access to amazon sagemaker, you should understand what iam features are available to use with amazon sagemaker. to get a high-level view of how amazon sagemaker and other aws services work with iam, see  in the iam user guide. topics with iam identity-based policies, you can specify allowed or denied actions and resources as well as the conditions under which actions are allowed or denied. amazon sagemaker supports specific actions, resources, and condition keys. to learn about all of the elements that you use in a json policy, see  in the iam user guide. the  element of an iam identity-based policy describes the specific action or actions that will be allowed or denied by the policy. policy actions usually have the same name as the associated aws api operation. the action is used in a policy to grant permissions to perform the associated operation.  policy actions in amazon sagemaker use the following prefix before the action: . for example, to grant someone permission to run an amazon sagemaker training job with the amazon sagemaker  api operation, you include the  action in their policy. policy statements must include either an  or  element. amazon sagemaker defines its own set of actions that describe tasks that you can perform with this service. to specify multiple actions in a single statement, separate them with commas as follows: you can specify multiple actions using wildcards (*). for example, to specify all actions that begin with the word , include the following action: to see a list of amazon sagemaker actions, see  in the iam user guide. amazon sagemaker does not support specifying resource arns in a policy. the  element (or  block) lets you specify conditions in which a statement is in effect. the  element is optional. you can build conditional expressions that use , such as equals or less than, to match the condition in the policy with values in the request.  if you specify multiple  elements in a statement, or multiple keys in a single  element, aws evaluates them using a logical  operation. if you specify multiple values for a single condition key, aws evaluates the condition using a logical  operation. all of the conditions must be met before the statement's permissions are granted.  you can also use placeholder variables when you specify conditions. for example, you can grant an iam user permission to access a resource only if it is tagged with their iam user name. for more information, see  in the iam user guide.  amazon sagemaker defines its own set of condition keys and also supports using some global condition keys. to see all aws global condition keys, see  in the iam user guide. amazon sagemaker supports a number of service-specific condition keys that you can use for fine-grained access control for the following operations: to see a list of amazon sagemaker condition keys, see  in the iam user guide. to learn with which actions and resources you can use a condition key, see . for examples of using amazon sagemaker condition keys, see the following: . to view examples of amazon sagemaker identity-based policies, see . amazon sagemaker does not support resource-based policies. you can attach tags to amazon sagemaker resources or pass tags in a request to amazon sagemaker. to control access based on tags, you provide tag information in the  of a policy using the , , or  condition keys. for more information about tagging amazon sagemaker resources, see . to view an example identity-based policy for limiting access to a resource based on the tags on that resource, see . an  is an entity within your aws account that has specific permissions. you can use temporary credentials to sign in with federation, assume an iam role, or to assume a cross-account role. you obtain temporary security credentials by calling aws sts api operations such as  or .  amazon sagemaker supports using temporary credentials. amazon sagemaker doesn't support service-linked roles. this feature allows a service to assume a  on your behalf. this role allows the service to access resources in other services to complete an action on your behalf. service roles appear in your iam account and are owned by the account. this means that an iam administrator can change the permissions for this role. however, doing so might break the functionality of the service. amazon sagemaker supports service roles.  when you create a notebook instance, processing job, training job, hosted endpoint, or batch transform job resource in amazon sagemaker, you must choose a role to allow amazon sagemaker to access amazon sagemaker on your behalf. if you have previously created a service role or service-linked role, then amazon sagemaker provides you with a list of roles to choose from. it's important to choose a role that allows access to the aws operations and resources you need. for more information, see . 
this rule detects if you have tensors with very high or low variances. very high or low variances in a tensor could lead to neuron saturation, which reduces the learning ability of the neural network. very high variance in tensors can also eventually lead to exploding tensors. use this rule to detect such issues early. this rule can be applied either to one of the supported deep learning frameworks (tensorflow, mxnet, and pytorch) or to the xgboost algorithm. you must specify either the  or  parameter. if both the parameters are specified, the rule inspects the union of tensors from both sets. for an example of how to configure and deploy a built-in rule, see . parameter descriptions for the tensorvariance rule   
to avoid incurring unnecessary charges, use the aws management console to delete the resources that you created for this exercise.  noteif you plan to explore other exercises in this guide, you might want to keep some of these resources, such as your notebook instance, s3 bucket, and iam role. open the amazon sagemaker console at  and delete the following resources: the endpoint. deleting the endpoint also deletes the ml compute instance or instances that support it. under inference, choose endpoints. choose the endpoint that you created in the example, then choose actions | delete. choose delete. the endpoint configuration. under inference, choose endpoint configurations. choose the endpoint configuration that you created in the example, then choose actions | delete. choose delete. the model. under inference, choose models. choose the model that you created in the example, then choose actions | delete. choose delete. the notebook instance. before deleting the notebook instance, stop it. under notebook, choose notebook instances. choose the notebook instance that you created in the example, then choose actions | stop. the notebook instance takes up to several minutes to stop. when status changes to stopped, move on to the next step. choose actions | delete. choose delete. open the amazon s3 console at  and delete the bucket that you created for storing model artifacts and the training dataset.  open the amazon cloudwatch console at  and delete all of the log groups that have names starting with . 
to train, deploy, and validate a model in amazon sagemaker, you can use either the amazon sagemaker python sdk or the aws sdk for python (boto 3). (you can also use the console, but for this exercise, you will use the notebook instance and one of the sdks.) this exercise provides code examples for each library.  the  abstracts several implementation details, and is easy to use. if you're a first-time amazon sagemaker user, we recommend that you use it to train, deploy, and validate the model. for more information, see . topics to choose the right algorithm for your model, you typically follow an evaluation process. for this exercise, you use the  provided by amazon sagemaker, so no evaluation is required. for information about choosing algorithms, see . the  includes the  estimator. you can use this class, in the  module, with any algorithm. for more information, see .  to run a model training job () import the  and get the xgboost container. download the training and validation data from the amazon s3 location where you uploaded it in , and set the location where you store the training output. create an instance of the  class.  in the constructor, you specify the following parameters:  – the aws identity and access management (iam) role that amazon sagemaker can assume to perform tasks on your behalf (for example, reading training results, called model artifacts, from the s3 bucket and writing training results to amazon s3). this is the role that you got in . and  – the type and number of ml compute instances to use for model training. for this exercise, you use only a single training instance. – the size, in gb, of the amazon elastic block store (amazon ebs) storage volume to attach to the training instance. this must be large enough to store training data if you use  mode ( mode is the default). – the  path to the s3 bucket where amazon sagemaker stores the training results. – the session object that manages interactions with amazon sagemaker apis and any other aws service that the training job uses.set the hyperparameter values for the xgboost training job by calling the  method of the estimator. for a description of xgboost hyperparameters, see . create the training channels to use for the training job. for this example, we use both  and  channels. to start model training, call the estimator's  method.  this is a synchronous operation. the method displays progress logs and waits until training completes before returning. for more information about model training, see . model training for this exercise can take up to 15 minutes. next step to train a model, amazon sagemaker uses the  api. the aws sdk for python (boto 3) provides the corresponding  method.  when using this method, you provide the following information: the training algorithm – specify the registry path of the docker image that contains the training code. for the registry paths for the algorithms provided by amazon sagemaker, see .algorithm-specific hyperparameters – specify algorithm-specific hyperparameters to influence the final quality of the model. for information, see .the input and output configuration – provide the s3 bucket where training data is stored and where amazon sagemaker saves the results of model training (the model artifacts). to run a model training job (aws sdk for python (boto 3)) import the  utility function  and get the location of the xgboost container. set up the training information for the job. you pass this information when you call . for more information about the information that you need to send to a training job, see . name your training job, and finish configuring the parameters that you send to it. call  to start the training job, and wait for it to complete. if the training job fails, print the reason that it failed. ``` %%time region = boto3.session().region_name sm = boto3.session().client('sagemaker') sm.create_training_job(**training_job_params)    status = sm.describe_training_job(trainingjobname=training_job_name)['trainingjobstatus']    print(status)    sm.get_waiter('training_job_completed_or_stopped').wait(trainingjobname=training_job_name)    status = sm.describe_training_job(trainingjobname=training_job_name)['trainingjobstatus']    print("training job ended with status: " + status)    if status == 'failed':        message = sm.describe_training_job(trainingjobname=training_job_name)['failurereason']        print('training failed with the following error: {}'.format(message))        raise exception('training job failed')    ``` you now have a trained model. amazon sagemaker stores the resulting artifacts in your s3 bucket.  next step 
in , you use the  because the example uses the k-means algorithm provided by amazon sagemaker for model training. you might choose to use your own custom algorithm for model training instead. assuming that you have already created a docker image, you can create your own  and specify the amazon elastic container registry path for your custom image.  the following example shows how to create a  from the . in the new estimator, you explicitly specify the docker registry path to your training and inference code images. in the code, the parameters in the  constructor include:  —identifies the docker registry path to the training image containing your custom code. —identifies the docker registry path to the image containing inference code. —implements . this parameter serializes rows in the input  to send them to the model hosted in amazon sagemaker for inference.  —implements  . this parameter deserializes responses from the model, hosted in amazon sagemaker, back into a .  —specifies the data format that spark uses when uploading training data from a  to s3. for example,  for protobuf format,  for comma-separated values, and  for libsvm format. you can implement your own  and  to serialize and deserialize rows from a data format that your inference code supports, such as .libsvm or ..csv. 
amazon sagemaker autopilot provides the following sample notebooks. : this notebook demonstrates how uses the  to predict whether a customer will enroll for a term deposit at a bank. you can use autopilot on this dataset to get the most accurate ml pipeline by exploring options contained in various candidate pipelines. autopilot generates each candidate in a two step procedure. the first step performs automated feature engineering on the dataset. the second step trains and tunes an algorithm to produce a model. the notebook contains instructions on how to train the model as well as how to deploy the model to perform batch inference using the best candidates.: this notebook describes using machine learning (ml) for the automated identification of unhappy customers, also known as customer churn prediction. the sample shows how to analyze a publicly available dataset and perform feature engineering on it. next it shows how to tune a model by selecting the best performing pipeline along with the optimal hyperparameters for the training algorithm. finally it shows how to deploy the model to a hosted endpoint and evaluate its predictions against ground truth. ml models rarely give perfect predictions though, so this notebook is also about how to incorporate the relative costs of prediction mistakes when determining the financial outcome of using ml.
use an amazon sagemaker ground truth image classification labeling task when you need workers to classify images using predefined labels that you specify. workers are shown images and are asked to choose one label for each image.  you can create an image classification labeling job using the ground truth section of the amazon sagemaker console or the  operation.  importantfor this task type, if you create your own manifest file, use  to identify the location of each image file in amazon s3 that you want labeled. for more information, see . you can follow the instructions  to learn how to create a image classification labeling job in the amazon sagemaker console. in step 10, choose image from the task category drop down menu, and choose image classification (single label) as the task type.  ground truth provides a worker ui similar to the following for labeling tasks. when you create the labeling job with the console, you specify instructions to help workers complete the job and labels that workers can choose from.   to create an image classification labeling job, use the amazon sagemaker api operation . this api defines this operation for all aws sdks. to see a list of language-specific sdks supported for this operation, review the see also section of . follow the instructions on  and do the following while you configure your request:  pre-annotation lambda functions for this task type end with . to find the pre-annotation lambda arn for your region, see  . annotation-consolidation lambda functions for this task type end with . to find the annotation-consolidation lambda arn for your region, see . the following is an example of an  to create a labeling job in the us east (n. virginia) region. all parameters in red should be replaced with your specifications and resources.  if you create a labeling job using the api, you must supply a worker task template in . copy and modify the following template. only modify the , , and .  upload this template to s3, and provide the s3 uri for this file in . once you have created an image classification labeling job, your output data will be located in the amazon s3 bucket specified in the  parameter when using the api or in the output dataset location field of the job overview section of the console.  to learn more about the output manifest file generated by ground truth and the file structure the ground truth uses to store your output data, see .  to see an example of an output manifest file from an image classification labeling job, see . 
if you choose a custom template, you'll reach the custom labeling task panel. there you can select from multiple starter templates that represent some of the more common tasks. the templates provide a starting point to work from in building your customized labeling task's template. in this demonstration, you work with the intent detection template, which uses the  element, and the aws lambda functions needed for processing your data before and after the task. topics this is the intent detection template that is provided as a starting point. the custom templates use the , and each of the items between double curly braces is a variable. the pre-annotation aws lambda function should provide an object named  and that object's properties can be accessed as  in your template. in the starter template, there are two variables: the  property in the  element opening tag and the  in the  region's content. unless you need to offer different sets of labels with different utterances, avoiding a variable and just using text will save processing time and creates less possibility of error. the template used in this demonstration will remove that variable, but variables and filters like  are explained in more detail in the  article. two parts of these custom elements that sometimes get overlooked are the  and  regions. good instructions generate good results. in the elements that include these regions, the  appear automatically in the "instructions" pane on the left of the worker's screen. the  are linked from the "view full instructions" link near the top of that pane. clicking the link opens a modal pane with more detailed instructions. you can not only use html, css, and javascript in these sections, you are encouraged to if you believe you can provide a strong set of instructions and examples that will help workers complete your tasks with better speed and accuracy.  example try out a sample with jsfiddle try out an . the example is rendered by jsfiddle, therefore all the template variables are replaced with hard-coded values. click the "view full instructions" link to see a set of examples with extended css styling. you can fork the project to experiment with your own changes to the css, adding sample images, or adding extended javascript functionality. example : final customized intent detection templatethis uses the , but with a variable for the . if you are trying to keep a consistent css design among a series of different labeling jobs, you can include an external stylesheet using a  element the same way you'd do in any other html document.   example : your manifest fileif you are preparing your manifest file manually for a text-classification task like this, have your data formatted in the following manner.   this differs from the manifest file used for the "" demonstration in that  was used as the property name instead of . the use of  designates s3 uris for images or other files that must be converted to http. otherwise,  should be used like it is with the text strings above. as part of the job set-up, provide the arn of an aws lambda that can be called to process your manifest entries and pass them to the template engine.  this lambda function is required to have one of the following four strings as part of the function name: , , , or . this applies to both your pre-annotation and post-annotation lambdas. when you're using the console, if you have lambdas that are owned by your account, a drop-down list of functions meeting the naming requirements will be provided to choose one. in this very basic sample, where you have only one variable, it's primarily a pass-through function. here's a sample pre-labeling lambda using python 3.7. the  property of the  contains the properties from a data object in your manifest. in this demonstration, which is a simple pass through, you just pass that straight through as the  value. if you add properties with those values to the  object, they will be available to your html template as liquid variables with the format . as part of the job set up, provide the arn of an lambda function that can be called to process the form data when a worker completes a task. this can be as simple or complex as you want. if you want to do answer-consolidation and scoring as data comes in, you can apply the scoring or consolidation algorithms of your choice. if you want to store the raw data for offline processing, that is an option. set permissions for your post-annotation lambda functionthe annotation data will be in a file designated by the  string in the  object. to process the annotations as they come in, even for a simple pass through function, you need to assign  access to your lambda so it can read the annotation files.in the console page for creating your lambda, scroll to the execution role panel. select create a new role from one or more templates. give the role a name. from the policy templates drop-down, choose amazon s3 object read-only permissions. save the lambda and the role will be saved and selected. the following sample is for python 3.7. the post-annotation lambda will often receive batches of task results in the event object. that batch will be the  object the lambda should iterate through. you'll find the output of the job in a folder named after your labeling job in the target s3 bucket you specified. it will be in a subfolder named . for an intent detection task, the output in the output manifest will look a bit like the demo below. the example has been cleaned up and spaced out to be easier for humans to read. the actual output will be more compressed for machine reading. example : json in your output manifest   this should help you create and use your own custom template. 
the following is an example for how to enable autoscaling on an endpoint using aws cloudformation. for more details, refer to cloudfront's . 
this rule checks if input images have been correctly normalized. specifically, it detects if the mean of the sample data differs by more than a threshold value from zero. many computer vision models require that input data has a zero mean and unit variance. this rule is applicable to deep learning applications. for an example of how to configure and deploy a built-in rule, see . parameter descriptions for the checkinputimages rule   
amazon sagemaker studio is a web-based, integrated development environment (ide) for machine learning that lets you build, train, debug, deploy, and monitor your machine learning models. studio provides all the tools you need to take your models from experimentation to production while boosting your productivity. in a single unified visual interface, customers can write and execute code in jupyter notebooksbuild and train machine learning modelsdeploy the models and monitor the performance of their predictionstrack and debug the machine learning experimentsthe following sections provide an overview of the user interface and a description of studio's main features. for information on the onboarding steps to sign-on to amazon sagemaker studio, see . for a tutorial that demonstrates the basic features of amazon sagemaker studio, see . topics 
when you create a notebook instance, amazon sagemaker creates a json file on the instance at the location  that contains the  and  of the notebook instance. you can access this metadata from anywhere within the notebook instance, including in lifecycle configurations. for information about notebook instance lifecycle configurations, see . the  file has the following structure: you can use this metadata from within the notebook instance to get other information about the notebook instance. for example, the following commands get the tags associated with the notebook instance: the out put looks like the following: 
a widget for drawing polygons on an image and assigning a label to the portion of the image that is enclosed in each polygon. the following is an example of a liquid template that uses the  element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  the following attributes are supported by this element. the text to display above the image. this is typically a question or simple instruction for the worker. a json formatted array of strings, each of which is a label that a worker can assign to the image portion enclosed by a polygon. the name of this widget. it's used as a key for the widget's input in the form output. the url of the image on which to draw polygons.  an array of json objects, each of which defines a polygon to be drawn when the component is loaded. each json object in the array contains the following properties. label – the text assigned to the polygon as part of the labeling task. this text must match one of the labels defined in the labels attribute of the <crowd-polygon> element.vertices – an array of json objects. each object contains an x and y coordinate value for a point in the polygon.examplean  attribute might look something like this.   because this will be within an html element, the json array must be enclosed in single or double quotes. the example above uses single quotes to encapsulate the json and double quotes within the json itself. if you must mix single and double quotes inside your json, replace them with their html entity codes ( for double quote,  for single) to safely escape them. this element has the following parent and child elements. parent elements: child elements: , the following regions are required. general instructions about how to draw polygons. important task-specific instructions that are displayed in a prominent place. the following output is supported by this element. an array of json objects, each of which describes a polygon that has been created by the worker. each json object in the array contains the following properties. label – the text assigned to the polygon as part of the labeling task.vertices – an array of json objects. each object contains an x and y coordinate value for a point in the polygon. the top left corner of the image is 0,0.a json object that specifies the dimensions of the image that is being annotated by the worker. this object contains the following properties. height – the height, in pixels, of the image.width – the width, in pixels, of the image.example : sample element outputsthe following are samples of outputs from common use scenarios for this element.single label, single polygon   single label, multiple polygons   multiple labels, multiple polygons   you could have many labels available, but only the ones that are used appear in the output. for more information, see the following. 
step intervals: by default, the python sdk uses step interval as 500 for emitting tensors. if you want to emit more frequently, choose specific tensors and run for fewer epochs to avoid stressing the cpu/disk.  
the following sections contain reference documentation on the apis, exceptions, and some known limitations for debugger. topics 
you can add autoscaling for a model with the aws cli or the application auto scaling api. you first must register the model, then you must define an autoscaling policy. with the aws cli, you can configure autoscaling based on either a predefined or a custom metric. to register your endpoint, use the  aws cli command with the following parameters: —set this value to .—the resource identifier for the model (specifically, the production variant). for this parameter, the resource type is  and the unique identifier is the name of the production variant. for example, .—set this value to .—the minimum number of instances that for this model. set  to at least 1. it must be equal to or less than the value specified for .—the maximum number of instances that application auto scaling should manage. set  to a minimum of 1, it must be equal to or greater than the value specified for .examplethe following example shows how to register a model named  that is dynamically scaled to have one to eight instances:   to define the scaling limits for the model, register your model with application auto scaling. application auto scaling dynamically scales the number of production variant instances. to register your model with application auto scaling, use the  application auto scaling api action with the following parameters: —set this value to .—the resource identifier for the production variant. for this parameter, the resource type is  and the unique identifier is the name of the variant, for example .—set this value to .—the minimum number of instances to be managed by application auto scaling. this value must be set to at least 1 and must be equal to or less than the value specified for .—the maximum number of instances to be managed by application auto scaling. this value must be set to at least 1 and must be equal to or greater than the value specified for .examplethe following example shows how to register an amazon sagemaker production variant that is dynamically scaled to use one to eight instances:    
amazon sagemaker ground truth has numerous built-in task types. these built-in task types streamline the process of creating image, text, video, and point cloud labeling jobs using the amazon sagemaker console, api, associated language specific sdk's. for each task type, ground truth provides a worker task templates, and pre- and post-annotation lambda functions to process your input and output data respectively.  the following topics describe each built-in task type, demo the worker ui provided by ground truth, and provide instructions to help you create a labeling job using that task type.  topics 
 you can create a private workforce in the amazon sagemaker console in one of two ways: when creating a labeling job in the labeling jobs page of the amazon sagemaker ground truth sectionusing the labeling workforces page of the amazon sagemaker ground truth section. if you are creating a private workforce for an amazon a2i human review workflow, use this method.both of these methods also create a default work team containing all of the members of the workforce.  this private workforce will be available to use for both ground truth and amazon augmented ai jobs.  if you haven't created a private workforce when you create your labeling job, you are prompted to create one.  to create a workforce while creating a labeling job (console) open the amazon sagemaker console at . in the navigation pane, choose labeling jobs and fill in all required fields. for instructions on how to start a labeling job, see . choose next. choose private for the workforce type.  in the workers section, enter: the team name.  email addresses for up to 100 workforce members. email addresses are case sensitive. your workers must log in using the same case used when the address was initially entered. you can add additional workforce members after the job has been created.  the name of your organization. amazon sagemaker uses this to customize the email sent to the workers. a contact email address for workers to report issues related to the task. when you create the labeling job, an email is sent to each worker inviting them to join the workforce. after creating the workforce, you can add, delete, and disable workers using the amazon sagemaker console or the amazon cognito console.  to create and manage your private workforce, you can use the labeling workforces page. when following the instructions below, you will have the option to create a private workforce by entering worker emails or importing a pre-existing workforce from an amazon cognito user pool. to import a workforce, see .  to create a private workforce using worker emails open the amazon sagemaker console at .  in the navigation pane, choose labeling workforces.  choose private, then choose create private team.  choose invite new workers by email. paste or type a list of up to 50 email addresses, separated by commas, into the email addresses box.  enter an organization name and contact email.  optionally choose an sns topic to subscribe the team to so workers are notified by email when new ground truth labeling jobs become available. amazon sns notifications are supported by ground truth and are not supported by augmented ai. if you subscribe workers to receive sns notifications, they will only receive notifications about ground truth labeling jobs. they will not receive notifications about augmented ai tasks.  click the create private team button.  after you import your private workforce, refresh the page. on the private workforce summary page, you can see information about the amazon cognito user pool for your workforce, a list of work teams for your workforce, and a list of all of the members of your private workforce.  noteif you delete all of your private work teams, you have to repeat this process to use a private workforce in that region.  
automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many jobs that test a range of hyperparameters on your dataset. you choose the tunable hyperparameters, a range of values for each, and an objective metric. you choose the objective metric from the metrics that the algorithm computes. automatic model tuning searches the hyperparameters chosen to find the combination of values that result in the model that optimizes the objective metric. for more information about model tuning, see . the blazingtext word2vec algorithm (, , and  modes) reports on a single metric during training: . this metric is computed on . when tuning the hyperparameter values for the word2vec algorithm, use this metric as the objective. the blazingtext text classification algorithm ( mode), also reports on a single metric during training: the . when tuning the hyperparameter values for the text classification algorithm, use these metrics as the objective. tune an amazon sagemaker blazingtext word2vec model with the following hyperparameters. the hyperparameters that have the greatest impact on word2vec objective metrics are: , , , , and . tune an amazon sagemaker blazingtext text classification model with the following hyperparameters. 
automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many jobs that test a range of hyperparameters on your dataset. you choose the tunable hyperparameters, a range of values for each, and an objective metric. you choose the objective metric from the metrics that the algorithm computes. automatic model tuning searches the hyperparameters chosen to find the combination of values that result in the model that optimizes the objective metric. for more information about model tuning, see . the factorization machines algorithm has both binary classification and regression predictor types. the predictor type determines which metric you can use for automatic model tuning. the algorithm reports a  regressor metric, which is computed during training. when tuning the model for regression tasks, choose this metric as the objective. the factorization machines algorithm reports three binary classification metrics, which are computed during training. when tuning the model for binary classification tasks, choose one of these as the objective. you can tune the following hyperparameters for the factorization machines algorithm. the initialization parameters that contain the terms bias, linear, and factorization depend on their initialization method. there are three initialization methods: , , and . these initialization methods are not themselves tunable. the parameters that are tunable are dependent on this choice of the initialization method. for example, if the initialization method is , then only the  parameters are tunable. specifically, if , then , , and  are tunable. similarly, if the initialization method is , then only  parameters are tunable. if the initialization method is , then only  parameters are tunable. these dependencies are listed in the following table.  
use this task type when you want workers to add and fit 3d cuboids around objects to track their movement across 3d point cloud frames. for example, you can use this task type to ask workers to track the movement of vehicles across multiple point cloud frames.  for this task type, the data object that workers label is a sequence of point cloud frames. a sequence is defined as a temporal series of point cloud frames. ground truth renders a series of 3d point cloud visualizations using a sequence you provide and workers can switch between these 3d point cloud frames in the worker task interface.  ground truth providers workers with tools to annotate objects with 9 degrees of freedom: (x,y,z,rx,ry,rz,l,w,h) in three dimensions in both 3d scene and projected side views (top, side, and back). when a worker draws a cuboid around an object, that cuboid is given a unique id, for example  for one car in the sequence and  for another. workers use that id to label the same object in multiple frames.  you can also provide camera data to give workers more visual information about scenes in the frame, and to help workers draw 3d cuboids around objects. when a worker adds a 3d cuboid to identify an object in either the 2d image or the 3d point cloud, and the cuboid shows up in the other view.  you can adjust annotations created in a 3d point cloud object detection labeling job using the 3d point cloud object tracking adjustment task type.  if you are a new user of the ground truth 3d point cloud labeling modality, we recommend you review . this labeling modality is different from other ground truth task types, and this page provides an overview of important details you should be aware of when creating a 3d point cloud labeling job. topics ground truth provides workers with a web portal and tools to complete your 3d point cloud object tracking annotation tasks. when you create the labeling job, you provide the amazon resource name (arn) for a pre-built ground truth ui in the  parameter. when you create a labeling job using this task type in the console, this ui is automatically used. you can preview and interact with the worker ui when you create a labeling job in the console. if you are a new use, it is recommended that you create a labeling job using the console to ensure your label attributes, point cloud frames, and if applicable, images, appear as expected.  the following is a gif of the 3d point cloud object tracking worker task interface and demonstrates how the worker can navigate the point cloud frames in the sequence.   once workers add a single cuboid, that cuboid is replicated in all frames of the sequence with the same id. once workers adjust the cuboid in another frame, ground truth will interpolate the movement of that object and adjust all cuboids between the manually adjusted frames. the following gif demonstrates this interpolation feature. in the navigation bar on the bottom-left, red-areas indicate manually adjusted frames.   if you provide camera data for sensor fusion, images are matched up with scenes in point cloud frames. these images appear in the worker portal as shown in the following gif.  worker can navigate in the 3d scene using their keyboard and mouse. they can: double click on specific objects in the point cloud to zoom into them.use a mouse-scroller or trackpad to zoom in and out of the point cloud.use both keyboard arrow keys and q, e, a, and d keys to move up, down, left, right. use keyboard keys w and s to zoom in and out. once a worker places a cuboids in the 3d scene, a side-view will appear with the three projected side views: top, side, and back. these side-views show points in and around the placed cuboid and help workers refine cuboid boundaries in that area. workers can zoom in and out of each of those side-views using their mouse.  the following video demonstrates movements around the 3d point cloud and in the side-view.   additional view options and features are available. see the  for a comprehensive overview of the worker ui.  workers can navigate through the 3d point cloud by zooming in and out, and moving in all directions around the cloud using the mouse and keyboard shortcuts. if workers click on a point in the point cloud, the ui will automatically zoom into that area. workers can use various tools to draw 3d cuboid around objects. for more information, see assistive labeling tools.  after workers have placed a 3d cuboid in the point cloud, they can adjust these cuboids to fit tightly around cars using a variety of views: directly in the 3d cuboid, in a side-view featuring three zoomed-in perspectives of the point cloud around the box, and if you include images for sensor fusion, directly in the 2d image.  view options that enable workers to easily hide or view label text, a ground mesh, and additional point attributes. workers can also choose between perspective and orthogonal projections.  assistive labeling toolsground truth helps workers annotate 3d point clouds faster and more accurately using ux, machine learning and computer vision powered assistive labeling tools for 3d point cloud object tracking tasks. the following assistive labeling tools are available for this task type: label autofill – when a worker adds a cuboid to a frame, that cuboid is automatically added to all frames in the sequence. these autofilled labels automatically adjust as the worker manually adjusts annotations for the same object. for more information, see label interpolation in the next bullet point. label interpolation – after a worker has labeled a single object in two frames, ground truth uses those annotations to interpolate the movement of that object between those two frames.bulk label management – workers can add, delete, and rename annotations in bulk. workers can manually delete annotations for a given object before or after a frame. for example, a worker can delete all labels for an object after frame 10 if that object is no longer located in the scene after that frame. if a worker accidentally bulk deletes all annotations for a object, they can add them back. for example, if a worker deletes all annotations for an object before frame 100, they can bulk add them to those frames. workers can rename a label in one frame and all 3d cuboids assigned that label are updated with the new name across all frames. snapping – workers can add a cuboid around an object and use a keyboard shortcut or menu option to have ground truth's autofit tool snap the cuboid tightly around the object's boundaries. fit to ground – after a worker adds a cuboid to the 3d scene, the worker can automatically snap the cuboid to the ground. for example, the worker can use this feature to snap a cuboid to the road or sidewalk in the scene. multi-view labeling – after a worker adds a 3d cuboid to the 3d scene, a side -panel displays front and two side perspectives to help the worker adjust the cuboid tightly around the object. workers can annotation the 3d point cloud, the side panel and the adjustments appear in the other views in real time. sensor fusion – if you provide data for sensor fusion, workers can adjust annotations in the 3d scenes and in 2d images, and the annotations will be projected into the other view in real time. auto-merge cuboids – workers can automatically merge two cuboids across all frames if they determine that cuboids with different labels actually represent a single object. view options – enables workers to easily hide or view label text, a ground mesh, and additional point attributes like color or intensity. workers can also choose between perspective and orthogonal projections. you can create a 3d point cloud labeling job using the amazon sagemaker console or api operation, . to create a labeling job for this task type you need the following:  a sequence input manifest file. to learn how to create this type of manifest file, see . if you are a new user of ground truth 3d point cloud labeling modalities, we recommend that you review . a work team from a private or vendor workforce. you cannot use amazon mechanical turk for 3d point cloud labeling jobs. to learn how to create workforces and work teams, see .a label category configuration file. for more information, see . additionally, make sure that you have reviewed and satisfied the .  to learn how to create a labeling job using the console or an api, see the following sections.  this section covers details you need to know when you create a labeling job using the amazon sagemaker api operation . this api defines this operation for all aws sdks. to see a list of language-specific sdks supported for this operation, review the see also section of .   provides an overview of the  operation. follow these instructions and do the following while you configure your request:  you must enter an arn for . use . replace  with the aws region you are creating the labeling job in.  there should not be an entry for the  parameter.  your  must end in . for example, . your input manifest file must be a point cloud frame sequence manifest file. for more information, see . you specify your labels and worker instructions in a label category configuration file. for more information, see  to learn how to create this file. you need to provide a pre-defined arns for the pre-annotation and post-annotation (acs) lambda functions. these arns are specific to the aws region you use to create your labeling job. to find the pre-annotation lambda arn, refer to . use the region you are creating your labeling job in to find the correct arn that ends with . to find the post-annotation lambda arn, refer to . use the region you are creating your labeling job in to find the correct arn that ends with . the number of workers specified in  should be . automated data labeling is not supported for 3d point cloud labeling jobs. you should not specify values for parameters in . 3d point cloud object tracking labeling jobs can take multiple hours to complete. you can specify a longer time limit for these labeling jobs in  (up to 7 days, or 604,800 seconds).  importantif you set your take time limit to be greater than 8 hours, you must set  for your iam execution role to at least 8 hours. to see how to update this value for your iam role, see  in the iam user guide, choose your preferred method to modify the role, and then follow the steps in . to create an adjustment labeling job, use the instructions in the previous section, with the following modifications:  in your label category configuration file, you must include . use this parameter to input the  used in the labeling job that generated the annotations you want your worker to adjust.  importantwhen you create a labeling job in the console, if you did not specify a label category attribute name, the name of your job is used as the labelattributename.  for example, if your label category attribute name was  in your first labeling job, add the following to your object tracking adjustment labeling job label category configuration file. to learn how to create this file, see .  you need to provide a pre-defined arns for the pre-annotation and post-annotation (acs) lambda functions. these arns are specific to the aws region you use to create your labeling job. to find the pre-annotation lambda arn, refer to . use the region you are creating your labeling job in to find the correct arn that ends with .to find the post-annotation lambda arn, refer to . use the region you are creating your labeling job in to find the correct arn that ends with .the  parameter must contain the same label categories as the previous labeling job. adding new label categories or adjusting label categories is not supported.you can follow the instructions  in order to learn how to create a 3d point cloud object tracking labeling job in the amazon sagemaker console. while you are creating your labeling job, be aware of the following:  your input manifest file must be a sequence manifest file. for more information, see . optionally, you can provide label category attributes. workers can assign one or more of these attributes to annotations to provide more information about that object. for example, you might want to use the attribute occluded to have workers identify when an object is partially obstructed.automated data labeling and annotation consolidation are not supported for 3d point cloud labeling tasks. 3d point cloud object tracking labeling jobs can take multiple hours to complete. you can specify a longer time limit for these labeling jobs when you select your work team (up to 7 days, or 604800 seconds).  importantif you set your take time limit to be greater than 8 hours, you must set  for your iam execution role to at least 8 hours. to see how to update this value for your iam role, see  in the iam user guide, choose your preferred method to modify the role, and then follow the steps in . you can create an adjustment labeling job in the console by chaining a successfully completed object tracking labeling job. to learn more, see . when you create a 3d point cloud object tracking labeling job, tasks are sent to workers. when these workers complete their tasks, their annotations are written to the amazon s3 bucket you specified when you created the labeling job. the output data format determines what you see in your amazon s3 bucket when your labeling job status () is .  if you are a new user of ground truth, see  to learn more about the ground truth output data format. to learn about the 3d point cloud object tracking output data format, see .  
use this topic to learn how to create an aws identity and access management (iam) user or role with required permissions to create a labeling job and how to create an execution role for labeling jobs. administrators can use these policies to restrict access to services that are specific to ground truth.  if you are new to ground truth and do not require the granular permissions described here, you can use the aws managed policy, amazonsagemakerfullaccess, to grant access to an iam entity to create a labeling job. you can also attach amazonsagemakerfullaccess to an iam role to create an execution role. to learn more about this managed policy, see .  importantwhen you create a custom labeling workflow, the amazonsagemakerfullaccess policy is restricted to invoking aws lambda functions with one of the following four strings as part of the function name: , , , or . this applies to both your pre-annotation and post-annotation lambda functions. if you choose to use names without those strings, you must explicitly provide  permission to the iam role used to create the labeling job. the following is an overview of the topics you'll find on this page:  use the policy in  to grant access to the ground truth area of the amazon sagemaker console. this policy includes permissions to create and modify private work teams. to learn more about these permissions, see .when you create a labeling job, you must provide an execution role. use  to learn about the permissions required for this role. for more information about iam users and roles, see  in the iam user guide.  to access the amazon sagemaker console, you must have a minimum set of permissions. to use the ground truth console, you need to grant permissions for additional resources. speciﬁcally, the console needs permissions for the aws marketplace to view subscriptions, amazon s3 actions for access to your input and output ﬁles. amazon cognito permission is required for initial work team setup.  to grant permission to an iam user or role to use the ground truth area of the amazon sagemaker console to create a labeling job, attach the following policy to the user or role.  to learn more about the permissions required to use the amazon sagemaker console, see . when added to a permissions policy, the following permission grants access to create and manage a private workforce and workteam. to learn more about private workforces, see . when you configure your labeling job, you need to provide an execution role which is a role that amazon sagemaker has permission to assume to start and run your labeling job. this role must give amazon sagemaker permission to access the following:  amazon s3 to retrieve your input data and write output data to an amazon s3 bucket. you can either grant permission for an iam role to access an entire bucket by providing the bucket arn, or you can grant access to the role to access specific resources in a bucket. for example, the arn for a bucket may look similar to  and the arn of a resource in an amazon s3 bucket may look similar to . for more information, see  in the amazon simple storage service developer guide. cloudwatch to log worker metrics and labeling job statuses. (optional) aws kms for data encryption.when you create a custom labeling workflow, aws lambda for processing input and output data. all of the permissions above can be granted with the  managed policy except for data and storage volume encryption if your s3 buckets, objects, and lambda functions meet the conditions specified in the policy.  use the following policy examples to create an execution role that fits your specific use case.  the following policy grants permission to create a labeling job for a built-in task type. this execution policy does not include permissions for aws kms data encryption or decryption.  you can use the following policy to create an execution role that works with an automated labeling job. replace  with your role arn. you can find your iam role arn in the iam console under roles.  to create a custom labeling workflow, you need to add permission for aws lambda to process input and output data. modify the following policy by adding the lambda function arns for your pre- and post-annotation lambda functions in a list under . attach the policy to an execution role that you use to create custom labeling workﬂow. to add input data decryption or output data encryption using aws kms to any type of labeling job, modify the following policy by listing kms key arns you want to grant permissions to use under . attach the policy to an execution role.  to encrypt the storage volume attached to the ml compute instances that run the training job for automated labeling, include the following in one of the execution policies above in the  section. storage volume encryption is only available when you create a labeling job using the api operation . for more information, see .  
amazon sagemaker studio notebooks are collaborative notebooks that you can launch quickly because you don't need to set up compute instances and file storage beforehand. a set of instance types, known as fast launch types are designed to launch in under two minutes. sagemaker studio notebooks provide persistent storage, which enables you to view and share notebooks even if the instances that the notebooks run on are shut down. you can share your notebooks with others, so that they can easily reproduce your results and collaborate while building models and exploring your data. you provide access to a read-only copy of the notebook through a secure url. dependencies for your notebook are included in the notebook's metadata. when your colleagues copy the notebook, it opens in the same environment as the original notebook. a sagemaker studio notebook runs in an environment defined by the following: instance type – the hardware configuration the notebook runs on. the configuration includes the number and type of processors (vcpu and gpu), and the amount and type of memory. the instance type determines the pricing rate.sagemaker image – a container image that is compatible with sagemaker studio. the image consists of the kernels, language packages, and other files required to run a notebook in studio.kernel – the process that runs the code contained in the notebook. a kernel is defined by a conda environment.you can change any of these resources from within the notebook. sample sagemaker studio notebooks are available in the  folder of the . each notebook comes with the necessary sagemaker image that opens the notebook with the appropriate kernel. we recommend that you familiarize yourself with the amazon sagemaker studio interface before creating or using a sagemaker studio notebook. for more information, see . topics 
all amazon sagemaker built-in algorithms adhere to the common input inference format described in . this topic contains a list of the available output formats for the amazon sagemaker pca algorithm. accept—application/json accept—application/jsonlines accept—application/x-recordio-protobuf 
you can connect to your notebook instance from your vpc through an  in your virtual private cloud (vpc) instead of connecting over the internet. when you use a vpc interface endpoint, communication between your vpc and the notebook instance is conducted entirely and securely within the aws network. amazon sagemaker notebook instances support  (amazon vpc) interface endpoints that are powered by . each vpc endpoint is represented by one or more  (enis) with private ip addresses in your vpc subnets. notebefore you create an interface vpc endpoint to connect to a notebook instance, create an interface vpc endpoint to connect to the amazon sagemaker api. that way, when users call  to get the url to connect to the notebook instance, that call also goes through the interface vpc endpoint. for information, see . you can create an interface endpoint to connect to your notebook instance with either the aws console or aws command line interface (aws cli) commands. for instructions, see . make sure that you create an interface endpoint for all of the subnets in your vpc from which you want to connect to the notebook instance. when you create the interface endpoint, specify aws.sagemaker.region.notebook as the service name. after you create a vpc endpoint, enable private dns for your vpc endpoint. anyone using the amazon sagemaker api, the aws cli, or the console to connect to the notebook instance from within the vpc will connect to the notebook instance through the vpc endpoint instead of the public internet. amazon sagemaker notebook instances support vpc endpoints in all aws regions where both  and  are available. topics to connect to your notebook instance through your vpc, you either have to connect from an instance that is inside the vpc, or connect your private network to your vpc by using an amazon virtual private network (vpn) or aws direct connect. for information about amazon vpn, see  in the amazon virtual private cloud user guide. for information about aws direct connect, see  in the aws direct connect user guide. you can create a policy for amazon vpc endpoints for amazon sagemaker notebook instances to specify the following: the principal that can perform actions.the actions that can be performed.the resources on which actions can be performed.for more information, see  in the amazon vpc user guide. the following example of a vpc endpoint policy specifies that all users that have access to the endpoint are allowed to access the notebook instance named . access to other notebook instances is denied. even if you set up an interface endpoint in your vpc, individuals outside the vpc can connect to the notebook instance over the internet. importantif you apply an iam policy similar to one of the following, users can't access the specified amazon sagemaker apis or the notebook instance through the console. to restrict access to only connections made from within your vpc, create an aws identity and access management policy that restricts access to only calls that come from within your vpc. then add that policy to every aws identity and access management user, group, or role used to access the notebook instance. notethis policy allows connections only to callers within a subnet where you created an interface endpoint. if you want to restrict access to the notebook instance to only connections made using the interface endpoint, use the  condition key instead of : both of these policy examples assume that you have also created an interface endpoint for the amazon sagemaker api. for more information, see . in the second example, one of the values for  is the id of the interface endpoint for the notebook instance. the other is the id of the interface endpoint for the amazon sagemaker api. the policy examples here include  because typically you would call  to make sure that the  is  before you try to connect to it. for example: for both of these calls, if you did not enable private dns hostnames for your vpc endpoint, or if you are using a version of the aws sdk that was released before august 13, 2018, you must specify the endpoint url in the call. for example, the call to  would be: 
when you create a notebook in amazon sagemaker studio or open a non-shared notebook in studio for the first time, you have to select a sagemaker image and kernel for the notebook. amazon sagemaker launches the notebook on a default instance of a type based on the chosen sagemaker image. for cpu based images, the default instance type is  (available as part of the ). for gpu based images, the default instance type is . if you create or open additional notebooks that use the same instance type, whether or not the notebooks use the same kernel, the notebooks run on the same instance of that instance type. after a notebook is launched, you can change its instance type, and sagemaker image and kernel from within the notebook. for more information, see  and . billing occurs per instance and starts when the first instance of a given instance type is launched. if you want to create or open a notebook without the risk of incurring charges, open the notebook from the file menu and choose no kernel from the select kernel dialog. you can read and edit a notebook without a running kernel but you can't run cells. billing ends when the sagemaker image for the instance is shut down. for more information, see . for information on shutting down the notebook, see . after you're logged in to amazon sagemaker studio, you can create a notebook in the following ways: from the file menufrom the amazon sagemaker studio launchertopics to create a notebook from the file menu from the menu at the top of studio, choose file, choose new, and then choose notebook. on the select kernel dialog, to use the default kernel, python 3 (data science), choose select. otherwise, use the dropdown menu to select a different kernel. for a list of the available kernels, see . to create a notebook from the launcher in the left sidebar, choose the file browser icon ( ). on the file browser menu, choose the plus (+) sign to display the launch in a new tab. the keyboard shortcut for the these steps is . on the launcher, keep the default sagemaker image, data science, or use the dropdown menu to select a different image. under notebook, choose python3. for a list of the available images, see . after you choose the kernel, your new notebook launches and opens in a new studio tab. to view the notebook's kernel session, in the left sidebar, choose the running terminals, kernels, and images icon ( ). you can stop the notebook's kernel session from this view. 
to configure and launch a hyperparameter tuning job, complete the following steps. topics to specify settings for the hyperparameter tuning job, you define a json object. you pass the object as the value of the  parameter to  when you create the tuning job. in this json object, you specify: the ranges of hyperparameters that you want to tune. for more information, see the limits of the resource that the hyperparameter tuning job can consume.the objective metric for the hyperparameter tuning job. an objective metric is the metric that the hyperparameter tuning job uses to evaluate the training job that it launches. noteto use your own algorithm for hyperparameter tuning, you need to define metrics for your algorithm. for information,see .the hyperparameter tuning job defines ranges for the , , , and  hyperparameters of the  built-in algorithm. the objective metric for the hyperparameter tuning job maximizes the  metric that the algorithm sends to cloudwatch logs. to configure the training jobs that the tuning job launches, define a json object that you pass as the value of the  parameter of the  call. in this json object, you specify: optional—metrics that the training jobs emit. notedefine metrics only when you use a custom training algorithm. because this example uses a built-in algorithm, you don't specify metrics. for information about defining metrics, see .the container image that specifies the training algorithm.the input configuration for your training and test data.the storage location for the algorithm's output. specify the s3 bucket where you want to store the output of the training jobs.the values of algorithm hyperparameters that are not tuned in the tuning job.the type of instance to use for the training jobs.the stopping condition for the training jobs. this is the maximum duration for each training job.in this example, we set static values for the , , , , and  parameters of the  built-in algorithm. now you can provide a name for the hyperparameter tuning job and then launch it by calling the  api. pass , and  that you created in previous steps as the values of the parameters.  
the only cost incurred for onboarding to amazon sagemaker studio is the one-time cost for the amazon elastic file system (amazon efs) volume created to store user data, such as files and notebooks. in the sagemaker studio control panel, when the status displays as , the amazon efs volume has been created and the cost incurred. additional storage charges for the amazon efs volume will be incurred as user data is created in studio. for pricing information on amazon efs, see . additional costs are incurred when other operations are run inside studio, for example, creating an amazon sagemaker autopilot job, running a notebook, running training jobs, and hosting a model. for information on the costs associated with using studio notebooks, see . for more information about billing along with pricing examples, see . on the pricing page, look under one of the aws regions supported by amazon sagemaker studio. for the regions supported by studio, see . 
a hyperparameter tuning job finds the best version of a model by running many training jobs on your dataset using the algorithm and ranges of hyperparameters that you specify. it then chooses the hyperparameter values that result in a model that performs the best, as measured by a metric that you choose. for more information, see . you can create use an algorithm resource to create a hyperparameter tuning job by using the amazon sagemaker console, the low-level amazon sagemaker api, or the . topics to use an algorithm to run a hyperparameter tuning job (console) open the amazon sagemaker console at . choose algorithms. choose an algorithm that you created from the list on the my algorithms tab or choose an algorithm that you subscribed to on the aws marketplace subscriptions tab. choose create hyperparameter tuning job. the algorithm you chose will automatically be selected. on the create hyperparameter tuning job page, provide the following information: for warm start, choose enable warm start to use the information from previous hyperparameter tuning jobs as a starting point for this hyperparameter tuning job. for more information, see . choose identical data and algorithm if your input data is the same as the input data for the parent jobs of this hyperparameter tuning job, or choose transfer learning to use additional or different input data for this hyperparameter tuning job. for parent hyperparameter tuning job(s), choose up to 5 hyperparameter tuning jobs to use as parents to this hyperparameter tuning job. for hyperparameter tuning job name, type a name for the tuning job. for iam role, choose an iam role that has the required permissions to run hyperparameter tuning jobs in amazon sagemaker, or choose create a new role to allow amazon sagemaker to create a role that has the  managed policy attached. for information, see . for vpc, choose a amazon vpc that you want to allow the training jobs that the tuning job launches to access. for more information, see . choose next. for objective metric, choose the metric that the hyperparameter tuning job uses to determine the best combination of hyperparameters, and choose whether to minimize or maximize this metric. for more information, see . for hyperparameter configuration, choose ranges for the tunable hyperparameters that you want the tuning job to search, and set static values for hyperparameters that you want to remain constant in all training jobs that the hyperparameter tuning job launches. for more information, see . choose next. for input data configuration, specify the following values for each channel of input data to use for the hyperparameter tuning job. you can see what channels the algorithm you're using for hyperparameter tuning supports, and the content type, supported compression type, and supported input modes for each channel, under channel specification section of the algorithm summary page for the algorithm. for channel name, type the name of the input channel. for content type, type the content type of the data that the algorithm expects for the channel. for compression type, choose the data compression type to use, if any. for record wrapper, choose  if the algorithm expects data in the  format. for s3 data type, s3 data distribution type, and s3 location, specify the appropriate values. for information about what these values mean, see . for input mode, choose file to download the data from to the provisioned ml storage volume, and mount the directory to a docker volume. choose pipeto stream data directly from amazon s3 to the container. to add another input channel, choose add channel. if you are finished adding input channels, choose done. for output location, specify the following values: for s3 output path, choose the s3 location where the training jobs that this hyperparameter tuning job launches store output, such as model artifacts. noteyou use the model artifacts stored at this location to create a model or model package from this hyperparameter tuning job. for encryption key, if you want amazon sagemaker to use a aws kms key to encrypt output data at rest in the s3 location. for resource configuration, provide the following information: for instance type, choose the instance type to use for each training job that the hyperparameter tuning job launches. for instance count, type the number of ml instances to use for each training job that the hyperparameter tuning job launches. for additional volume per instance (gb), type the size of the ml storage volume that you want to provision each training job that the hyperparameter tuning job launches. ml storage volumes store model artifacts and incremental states. for encryption key, if you want amazon sagemaker to use an aws key management service key to encrypt data in the ml storage volume attached to the training instances, specify the key. for resource limits, provide the following information: for maximum training jobs, specify the maximum number of training jobs that you want the hyperparameter tuning job to launch. a hyperparameter tuning job can launch a maximum of 500 training jobs. for maximum parallel training jobs, specify the maximum number of concurrent training jobs that the hyperparameter tuning job can launch. a hyperparameter tuning job can launch a maximum of 10 concurrent training jobs. for stopping condition, specify the maximum amount of time in seconds, minutes, hours, or days, that you want each training job that the hyperparameter tuning job launches to run. for tags, specify one or more tags to manage the hyperparameter tuning job. each tag consists of a key and an optional value. tag keys must be unique per resource. choose create jobs to run the hyperparameter tuning job. to use an algorithm to run a hyperparameter tuning job by using the amazon sagemaker api, specify either the name or the amazon resource name (arn) of the algorithm as the  field of the  object that you pass to . for information about hyperparameter tuning in amazon sagemaker, see . use an algorithm that you created or subscribed to on aws marketplace to create a hyperparameter tuning job, create an  object and specify either the amazon resource name (arn) or the name of the algorithm as the value of the  argument. then initialize a  object with the  you created as the value of the  argument. finally, call the  method of the . for example: 
to deploy a model in amazon sagemaker, hosting services, you can use either the  or the aws sdk for python (boto 3). this exercise provides code examples for both libraries.  the  abstracts several implementation details, and is easy to use. if you're a first-time amazon sagemaker user, we recommend that you use it. for more information, see . topics deploy the model that you trained in  by calling the  method of the  object. this is the same object that you used to train the model. when you call the  method, specify the number and type of ml instances that you want to use to host the endpoint. the  method creates the deployable model, configures the amazon sagemaker hosting services endpoint, and launches the endpoint to host the model. for more information, see . it also returns a  object, which you can use to get inferences from the model. for information, see . next step deploying a model using the aws sdk for python (boto 3) is a three-step process:  create a model in amazon sagemaker – send a  request to provide information such as the location of the s3 bucket that contains your model artifacts and the registry path of the image that contains inference code. create an endpoint configuration – send a  request to provide the resource configuration for hosting. this includes the type and number of ml compute instances to launch to deploy the model.  create an endpoint – send a  request to create an endpoint. amazon sagemaker launches the ml compute instances and deploys the model. amazon sagemaker returns an endpoint. applications can send requests for inference to this endpoint. to deploy the model (aws sdk for python (boto 3)) for each of the following steps, paste the code in a cell in the jupyter notebook you created in  and run the cell. create a deployable model by identifying the location of model artifacts and the docker image that contains the inference code.  create an amazon sagemaker endpoint configuration by specifying the ml compute instances that you want to deploy your model to. create an amazon sagemaker endpoint.  this code continuously calls the  command in a  loop until the endpoint either fails or is in service, and then prints the status of the endpoint. when the status changes to , the endpoint is ready to serve inference requests. next step 
the following topics provide information common to all of the algorithms provided by amazon sagemaker. topics 
autopilot get started tutorials demonstrate how to create a machine learning model automatically without having to write code. they show you how autopilot simplifies the machine learning experience by helping you explore your data and try different algorithms. autopilot builds the best machine learning model for the problem type using automl capabilities while allowing full control and visibility.  : you assume the role of a developer working at a bank in this tutorial. you have been asked to develop a machine learning model to predict whether or not a customer will enroll for a certificate of deposit (cd). this is a binary classification problem. the model is trained on the marketing dataset that contains information on customer demographics, responses to marketing events, and external factors.
automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many jobs that test a range of hyperparameters on your dataset. you choose the tunable hyperparameters, a range of values for each, and an objective metric. for the objective metric, you use one of the metrics that the algorithm computes. automatic model tuning searches the chosen hyperparameters to find the combination of values that result in the model that optimizes the objective metric. for more information about model tuning, see . the object2vec algorithm has both classification and regression metrics. the  type determines which metric you can use for automatic model tuning.  the algorithm reports a mean squared error regressor metric, which is computed during testing and validation. when tuning the model for regression tasks, choose this metric as the objective. the object2vec algorithm reports accuracy and cross-entropy classification metrics, which are computed during test and validation. when tuning the model for classification tasks, choose one of these as the objective. you can tune the following hyperparameters for the object2vec algorithm. 
with amazon sagemaker studio notebooks, you can change the notebook's sagemaker image and kernel from within the notebook. the following screenshot shows the menu from a studio notebook. the current sagemaker image and kernel are displayed as python 3 (data science), where  denotes the kernel and  denotes the sagemaker image that contains the kernel. the color of the circle to the right of the sagemaker image indicates the idle or busy status of the kernel.  to change a notebook's image choose the kernel name. from the drop-down list, choose an image. for a list of available sagemaker images, see .  
with aws marketplace, you can browse and search for hundreds of machine learning algorithms and models in a broad range of categories, such as computer vision, natural language processing, speech recognition, text, data, voice, image, video analysis, fraud detection, predictive analysis, and more.  to find algorithms on aws marketplace open the amazon sagemaker console at . choose algorithms, then choose find algorithms. this takes you to the aws marketplace algorithms page. for information about finding and subscribing to algorithms on aws marketplace, see  in the aws marketplace user guide for aws consumers. to find model packages on aws marketplace open the amazon sagemaker console at . choose model packages, then choose find model packages. this takes you to the aws marketplace model packages page. for information about finding and subscribing to model packages on aws marketplace, see  in the aws marketplace user guide for aws consumers. for information about using algorithms and model packages that you subscribe to in amazon sagemaker, see . notewhen you create a training job, inference endpoint, and batch transform job from an algorithm or model package that you subscribe to on aws marketplace, the training and inference containers do not have access to the internet. because the containers do not have access to the internet, the seller of the algorithm or model package does not have access to your data. 
topics to get you started, the task type starts with a drop-down menu listing a number of our more common task types, plus a custom type. choose one and the code editor area will be filled with a sample template for that task type. if you prefer not to start with a sample, choose custom html for a minimal template skeleton. if you've already created a template, upload the file directly using the upload file button in the upper right of the task setup area or paste your template code into the editor area. for a repository of demo templates for a variety of labeling job task types, see . while you need to be in the console to test how your template will process incoming data, you can test the look and feel of your template's html and custom elements in your browser by adding this code to the top of your html file. example   this loads the necessary code to render the custom html elements. use this if you want to develop your template's look and feel in your preferred editor rather than in the console. remember, though, this will not parse your variables. you may want to replace them with sample content while developing locally. amazon sagemaker ground truth custom templates allow external scripts and style sheets to be embedded. example   if you encounter errors, ensure that your originating server is sending the correct mime type and encoding headers with the assets. for example, the mime and encoding types for remote scripts: . the mime and encoding type for remote stylesheets: . in the process of building the sample below, there will be a step that adds variables to it to represent the pieces of data that may change from task to task, worker to worker. if you're starting with one of the sample templates, you will need to make sure you're aware of the variables it already uses. when you create your pre-annotation aws lambda script, its output will need to contain values for any of those variables you choose to keep. the values you use for the variables can come from your manifest file. all the key-value pairs in your data object are provided to your pre-annotation lambda. if it's a simple pass-through script, matching keys for values in your data object to variable names in your template is the easiest way to pass those values through to the tasks forms your workers see.  all tasks begin and end with the  elements. like standard html  elements, all of your form code should go between them.   for a simple tweet-analysis task, use the  element. it requires the following attributes:  name - the variable name to use for the result in the form output.categories - a json formatted array of the possible answers.header - a title for the annotation toolas children of the  element, you must have three regions. <classification-target> - the text the worker will classify based on the options specified in the  attribute above.<full-instructions> - instructions that are available from the "view full instructions" link in the tool. this can be left blank, but it is recommended that you give good instructions to get better results.<short-instructions> - a more brief description of the task that appears in the tool's sidebar. this can be left blank, but it is recommended that you give good instructions to get better results.a simple version of this tool would look like this. example of using    you can copy and paste the code into the editor in the ground truth labeling job creation workflow to preview the tool, or try out a     our custom template system uses  for automation. it is an open source inline markup language. for more information and documentation, visit the . the most common use of liquid will be to parse the data coming from your pre-annotation lambda and pull out the relevant variables to create the task. in liquid, the text between single curly braces and percent symbols is an instruction or "tag" that creates control flow. text between double curly braces is a variable or "object" which outputs its value. the  object returned by your  will be available as the  object in your templates. the properties in your manifest's data objects are passed into your  as the . a simple pass-through script simply returns that object as the  object. you would represent values from your manifest as variables as follows. example manifest data object   example sample html using variables   note the addition of "" to the  property above. that's a filter to turn the array into a json representation of the array. variable filters are explained next. in addition to the standard liquid filters and actions, ground truth offers a few additional filters. filters are applied by placing a pipe () character after the variable name, then specifying a filter name. filters can be chained in the form of: example   by default, inputs will be html escaped to prevent confusion between your variable text and html. you can explicitly add the  filter to make it more obvious to someone reading the source of your template that the escaping is being done.  ensures that if you've already escaped your code, it doesn't get re-escaped on top of that. for example, so that &amp; doesn't become &amp;amp;.  is useful when your content is meant to be used as html. for example, you might have a few paragraphs of text and some images in the full instructions for a bounding box. use  sparinglythe best practice in templates is to avoid passing in functional code or markup with  unless you are absolutely sure you have strict control over what's being passed. if you're passing user input, you could be opening your workers up to a cross site scripting attack.  will encode what you feed it to json (javascript object notation). if you feed it an object, it will serialize it.  takes an s3 uri and encodes it into an https url with a short-lived access token for that resource. this makes it possible to display to workers photo, audio, or video objects stored in s3 buckets that are not otherwise publicly accessible. example of the filtersinput   exampleoutput   example of an automated classification template.to automate the simple text classification sample, replace the tweet text with a variable.the text classification template is below with automation added. the changes/additions are highlighted in bold.   the tweet text that was in the prior sample is now replaced with an object. the  object uses  (or another name you specify in your pre-annotation lambda) as the property name for the text and it is inserted directly in the html by virtue of being between double curly braces. you can view the following end-to-end demos which include sample lambdas:  
a private workforce corresponds to a single amazon cognito user pool. private work teams correspond to amazon cognito user groups within that user pool. workers correspond to amazon cognito users within those groups.  after your workforce has been created, you can add work teams and individual workers through the amazon cognito console. you can also delete workers from your private workforce and/or remove them from individual teams in the amazon cognito console.  importantyou can't delete work teams from the amazon cognito console. deleting a amazon cognito user group that is associated with a amazon sagemaker work team will result in an error. to remove work teams, use the amazon sagemaker console.    you can create a new work team to complete a job by adding a amazon cognito user group to the user pool associated with your private workforce. to add a amazon cognito user group to an existing worker pool, see .   to create a work team using an existing amazon cognito user group open the amazon sagemaker console at .  in the navigation pane, choose workforces.  for private teams, choose create private team.  under team details, give the team a name. the name must be unique in your account in an aws region.  for add workers, choose import existing amazon cognito user groups, and choose one or more user groups that are part of the new team.  if you choose an sns topic, all workers added to the team are subscribed to the amazon simple notification service (amazon sns) topic and notified when new work items are available to the team. choose from a list of your existing sns topics related to amazon sagemaker ground truth or amazon augmented ai or choose create new topic to create one.  noteamazon sns notifications are supported by ground truth and are not supported by augmented ai. if you subscribe workers to receive sns notifications, they will only receive notifications about ground truth labeling jobs. they will not receive notifications about augmented ai tasks.  after you have created a work team, you can see more information about the team and change or set the sns topic to which its members are subscribed using the amazon cognito console. any members of the team who were added to the team prior to the team being subscribed to a topic need to be subscribed to that topic manually. for information, see .   when using the amazon cognito console to add workers to a work team, you must add a user to the user pool associated with the workforce before adding that user to a user group. users can be added to a user pool in various ways. for more information, see .  after a user has been added to a pool, the user can be associated with user groups inside of that pool. after a user has been added to a user group, that user becomes a worker on any work team created using that user group. to add a user to a user group open the amazon cognito console:   choose manage user pools  choose the user pool associated with your amazon sagemaker workforce.   under general settings, choose users and groups and do one of the following:  choose groups, choose the group that you want to add the user to, and choose add users. choose the users that you want to add by choosing the plus-icon to the right of the users’ name.  choose users, choose the user that you want to add to the user group, and choose add to group. from the drop down menu, choose the group and choose add to group.disabling a worker stops the worker from receiving a jobs. this action doesn't remove the worker from the workforce, or any work team the worker is associated with. to remove a user from a work team in amazon cognito, you remove the user from the user group associated with that team. to deactivate a worker  (amazon cognito console) open the amazon cognito console: .  choose manage user pools  choose the user pool associated with your amazon sagemaker workforce. under general settings, choose users and groups. choose the user that you want to disable. choose disable user  you can enable a disabled user by choosing enable user.   to remove a user from a user group (amazon cognito console) open the amazon cognito console: .  choose manage user pools  choose the user pool associated with your amazon sagemaker workforce.   under general settings, choose users and groups.  for user tab, choose the x-icon to the right of the group that you want to remove the user from.  
to categorize articles and text into multiple predefined categories, use the multi-label text classification task type. for example, you can use this task type to identify more than one emotion conveyed in text.  when working on a multi-label text classification task, workers should choose all applicable labels, but must choose at least one. when creating a job using this task type, you can provide up to 50 label categories.  amazon sagemaker ground truth doesn't provide a "none" category for when none of the labels applies. to provide this option to workers, include a label similar to "none" or "other" when you create a multi-label text classification job.  to restrict workers to choosing a single label for each document or text selection, use the  task type.  importantfor this task type, if you create your own manifest file, use  to identify the location of text files in amazon s3 that you want labeled. if you provide the text that you want labeled directly in the input manifest file, use . for more information, see . you can follow the instructions  to learn how to create a multi-label text classification labeling job in the amazon sagemaker console. in step 10, choose text from the task category drop down menu, and choose text classification (multi-label) as the task type.  ground truth provides a worker ui similar to the following for labeling tasks. when you create the labeling job with the console, you specify instructions to help workers complete the job and labels that workers can choose from.   to create a multi-label text classification labeling job, use the amazon sagemaker api operation . this api defines this operation for all aws sdks. to see a list of language-specific sdks supported for this operation, review the see also section of . follow the instructions on  and do the following while you configure your request:  pre-annotation lambda functions for this task type end with . to find the pre-annotation lambda arn for your region, see  . annotation-consolidation lambda functions for this task type end with . to find the annotation-consolidation lambda arn for your region, see . the following is an example of an  to create a labeling job in the us east (n. virginia) region. all parameters in red should be replaced with your specifications and resources.  if you create a labeling job using the api, you must supply a worker task template in . copy and modify the following template. only modify the , , and .  upload this template to s3, and provide the s3 uri for this file in . to learn how to create a custom template, see .  once you have created a multi-label text classification labeling job, your output data will be located in the amazon s3 bucket specified in the  parameter when using the api or in the output dataset location field of the job overview section of the console.  to learn more about the output manifest file generated by ground truth and the file structure the ground truth uses to store your output data, see .  to see an example of output manifest files for multi-label text classification labeling job, see . 
topics amazon sagemaker provides an apache spark library (in both python and scala) that you can use to integrate your apache spark applications with amazon sagemaker. for example, you might use apache spark for data preprocessing and amazon sagemaker for model training and hosting. for more information, see . this section provides example code that uses the apache spark scala library provided by amazon sagemaker to train a model in amazon sagemaker using s in your spark cluster. the example also hosts the resulting model artifacts using amazon sagemaker hosting services. specifically, this example does the following: uses the  to fit (or train) a model on data   because the example uses the k-means algorithm provided by amazon sagemaker to train a model, you use the . you train the model using images of handwritten single-digit numbers (from the mnist dataset). you provide the images as an input . for your convenience, amazon sagemaker provides this dataset in an s3 bucket.   in response, the estimator returns a  object.   obtains inferences using the trained    to get inferences from a model hosted in amazon sagemaker, you call the  method. you pass a  as input. the method transforms the input  to another  containing inferences obtained from the model.    for a given input image of a handwritten single-digit number, the inference identifies a cluster that the image belongs to. for more information, see . this is the example code: the code does the following: loads the mnist dataset from an s3 bucket provided by amazon sagemaker () into a spark  (): the  method displays the first 20 rows in the data frame: in each row: the  column identifies the image's label. for example, if the image of the handwritten number is the digit 5, the label value is 5. the  column stores a vector () of  values. these are the 784 features of the handwritten number. (each handwritten number is a 28 x 28-pixel image, making 784 features.)  creates an amazon sagemaker estimator ()  the  method of this estimator uses the k-means algorithm provided by amazon sagemaker to train models using an input . in response, it returns a  object that you can use to get inferences. notethe  extends the amazon sagemaker , which extends the apache spark .  the constructor parameters provide information that is used for training a model and deploying it on amazon sagemaker:  and —identify the type and number of ml compute instances to use for model training. —identifies the ml compute instance type to use when hosting the model in amazon sagemaker. by default, one ml compute instance is assumed. —identifies the number of ml compute instances initially backing the endpoint hosting the model in amazon sagemaker. —amazon sagemaker assumes this iam role to perform tasks on your behalf. for example, for model training, it reads data from s3 and writes training results (model artifacts) to s3.  notethis example implicitly creates an amazon sagemaker client. to create this client, you must provide your credentials. the api uses these credentials to authenticate requests to amazon sagemaker. for example, it uses the credentials to authenticate requests to create a training job and api calls for deploying the model using amazon sagemaker hosting services.after the  object has been created, you set the following parameters, are used in model training: the number of clusters that the k-means algorithm should create during model training. you specify 10 clusters, one for each digit, 0 through 9. identifies that each input image has 784 features (each handwritten number is a 28 x 28-pixel image, making 784 features). calls the estimator  method you pass the input  as a parameter. the model does all the work of training the model and deploying it to amazon sagemaker. for more information see, . in response, you get a  object, which you can use to get inferences from your model deployed in amazon sagemaker.    you provide only the input . you don't need to specify the registry path to the k-means algorithm used for model training because the  knows it.   calls the  method to get inferences from the model deployed in amazon sagemaker. the  method takes a  as input, transforms it, and returns another  containing inferences obtained from the model.  for simplicity, we use the same  as input to the  method that we used for model training in this example. the  method does the following: serializes the  column in the input  to protobuf and sends it to the amazon sagemaker endpoint for inference.deserializes the protobuf response into the two additional columns ( and ) in the transformed .the  method gets inferences to the first 20 rows in the input :  you can interpret the data, as follows: a handwritten number with the  5 belongs to cluster 4 ().a handwritten number with the  0 belongs to cluster 5.a handwritten number with the  4 belongs to cluster 9.a handwritten number with the  1 belongs to cluster 6.for more information on how to run these examples, see  on github. 
a widget for classifying non-image content, such as audio, video, or text. the following is an example of an html worker task template built using . this example uses the  to automate: label categories in the  parameter the objects that are being classified in the  parameter. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  the following attributes are supported by this element. a json formatted array of strings, each of which is a category that a worker can assign to the text. you should include "other" as a category, otherwise the worker my not be able to provide an answer. the text to display above the image. this is typically a question or simple instruction for the worker. the name of this widget. it is used as a key for the widget's input in the form output. this element has the following parent and child elements. parent elements: child elements: , , the following regions are supported by this element. the content to be classified by the worker. this can be plain text or html. examples of how the html can be used include but are not limited to embedding a video or audio player, embedding a pdf, or performing a comparison of two or more images. general instructions about how to do text classification. important task-specific instructions that are displayed in a prominent place. the output of this element is an object using the specified  value as a property name, and a string from the  as the property's value. example : sample element outputsthe following is a sample of output from this element.   for more information, see the following. 
you can use the amazon sagemaker studio launcher to create a notebook, launch a python interactive shell, or open a terminal. the following sections describe how to perform basic tasks from the launcher. topics you can open the studio launcher in the following ways. in the left sidebar, choose the file browser icon ( ). on the file browser, choose the plus (+) sign. choose file then new launcher.use the keyboard shortcut .the launcher opens in a new tab in studio. your screen should look similar to the following:  
over time, you might find that a model generates inference that are not as good as they were in the past. with incremental training, you can use the artifacts from an existing model and use an expanded dataset to train a new model. incremental training saves both time and resources. use incremental training to: train a new model using an expanded dataset that contains an underlying pattern that was not accounted for in the previous training and which resulted in poor model performance.use the model artifacts or a portion of the model artifacts from a popular publicly available model in a training job. you don't need to train a new model from scratch.resume a training job that was stopped.train several variants of a model, either with different hyperparameter settings or using different datasets.for more information about training jobs, see . you can train incrementally using the amazon sagemaker console or the . importantonly three built-in algorithms currently support incremental training: , , and . topics to complete this procedure, you need: the url of the amazon simple storage service (amazon s3) bucket where you've stored the training data.the url of the s3 bucket where you want to store the output of the job. the amazon elastic container registry path where the training code is stored. for more information, see .the url of the s3 bucket where you've stored the model artifacts that you want to use in incremental training. to find the url for the model artifacts, see the details page of the training job used to create the model. to find the details page, in the amazon sagemaker console, choose inference, choose models, and then choose the model.to restart a stopped training job, use the url to the model artifacts that are stored in the details page as you would with a model or a completed training job. to perform incremental training (console) open the amazon sagemaker console at . in the navigation pane, choose training, then choose training jobs.  choose create training job. provide a name for the training job. the name must be unique within an aws region in an aws account. the training job name must have 1 to 63 characters. valid characters: a-z, a-z, 0-9, and . : + = @ _ % - (hyphen). choose the algorithm that you want to use. for information about algorithms, see .  (optional) for resource configuration, either leave the default values or increase the resource consumption to reduce computation time. (optional) for instance type, choose the ml compute instance type that you want to use. in most cases, ml.m4.xlarge is sufficient.  for instance count, use the default, 1. (optional) for additional volume per instance (gb), choose the size of the ml storage volume that you want to provision. in most cases, you can use the default, 1. if you are using a large dataset, use a larger size. provide information about the input data for the training dataset. for channel name, either leave the default (train) or enter a more meaningful name for the training dataset, such as expanded-training-dataset. for inputmode, choose file. for incremental training, you need to use file input mode. for s3 data distribution type, choose fullyreplicated. this causes each ml compute instance to use a full replicate of the expanded dataset when training incrementally. if the expanded dataset is uncompressed, set the compression type to none. if the expanded dataset is compressed using gzip, set it to gzip. (optional) if you are using file input mode, leave content type empty. for pipe input mode, specify the appropriate mime type. content type is the multipurpose internet mail extension (mime) type of the data. for record wrapper, if the dataset is saved in recordio format, choose recordio. if your dataset is not saved as a recordio formatted file, choose none. for s3 data type, if the dataset us stored as a single file, choose s3prefix. if the dataset is stored as several files in a folder, choose manifest. for s3 location, provide the url to the path where you stored the expanded dataset. choose done. to use model artifacts in a training job, you need to add a new channel and provide the needed information about the model artifacts. for input data configuration, choose add channel. for channel name, enter model to identify this channel as the source of the model artifacts. for inputmode, choose file. model artifacts are stored as files. for s3 data distribution type, choose fullyreplicated. this indicates that each ml compute instance should use all of the model artifacts for training.  for compression type, choose none because we are using a model for the channel. leave content type empty. content type is the multipurpose internet mail extension (mime) type of the data. for model artifacts, we leave it empty. set record wrapper to none because model artifacts are not stored in recordio format. for s3 data type, if you are using a built-in algorithm or an algorithm that stores the model as a single file, choose s3prefix. if you are using an algorithm that stores the model as several files, choose manifest. for s3 location, provide the url to the path where you stored the model artifacts. typically, the model is stored with the name . to find the url for the model artifacts, in the navigation pane, choose inference, then choose models. from the list of models, choose a model to display its details page. the url for the model artifacts is listed under primary container . choose done. for output data configuration, provide the following information: for s3 location, type the path to the s3 bucket where you want to store the output data. (optional) for encryption key, you can add your aws key management service (aws kms) encryption key to encrypt the output data at rest. provide the key id or its amazon resource number (arn). for more information, see . (optional) for tags, add one or more tags to the training job. a tag is metadata that you can define and assign to aws resources. in this case, you can use tags to help you manage your training jobs. a tag consists of a key and a value, which you define. for example, you might want to create a tag with project as a key and a value referring to a project that is related to the training job, such as home value forecasts. choose create training job. amazon sagemaker creates and runs training job. after the training job has completed, the newly trained model artifacts are stored under the s3 output path that you provided in the output data configuration field. to deploy the model to get predictions, see . this example shows how to use amazon sagemaker apis to train a model using the amazon sagemaker image classification algorithm and the , then train a new model using the first one. it uses amazon s3 for input and output sources. please see the  for more details on using incremental training. notein this example we used the original datasets in the incremental training, however you can use different datasets, such as ones that contain newly added samples. upload the new datasets to s3 and make adjustments to the  variable used to train the new model. get an aws identity and access management (iam) role that grants required permissions and initialize environment variables: get the training image for the image classification algorithm: download the training and validation datasets, then upload them to amazon simple storage service (amazon s3): define the training hyperparameters: create an estimator object and train the first model using the training and validation datasets: to use the model to incrementally train another model, create a new estimator object and use the model artifacts (, in this example) for the  input argument: after the training job has completed, the newly trained model artifacts are stored under the  that you provided in . to deploy the model to get predictions, see . 
you can use amazon sagemaker to train and deploy a model using custom scikit-learn code. the amazon sagemaker python sdk scikit-learn estimators and models and the amazon sagemaker open-source scikit-learn container make writing a scikit-learn script and running it in amazon sagemaker easier. i want to use scikit-learn for data processing, feature engineering, or model evaluation in amazon sagemaker.for a sample jupyter notebook, see .for documentation, see  i want to train a custom scikit-learn model in amazon sagemaker.for a sample jupyter notebook, see .for documentation, see . i have a scikit-learn model that i trained in amazon sagemaker, and i want to deploy it to a hosted endpoint.. i have a scikit-learn model that i trained outside of amazon sagemaker, and i want to deploy it to an amazon sagemaker endpoint. i want to see the api documentation for  scikit-learn classes. i want to see information about amazon sagemaker scikit-learn containers..  for general information about writing scikit-learn training scripts and using scikit-learn estimators and models with amazon sagemaker, see .  scikit-learn versions supported by the amazon sagemaker scikit-learn container: , .  
in a random search, hyperparameter tuning chooses a random combination of values from within the ranges that you specify for hyperparameters for each training job it launches. because the choice of hyperparameter values doesn't depend on the results of previous training jobs, you can run the maximum number of concurrent training jobs without affecting the performance of the search. for an example notebook that uses random search, see . bayesian search treats hyperparameter tuning like a  problem. given a set of input features (the hyperparameters), hyperparameter tuning optimizes a model for the metric that you choose. to solve a regression problem, hyperparameter tuning makes guesses about which hyperparameter combinations are likely to get the best results, and runs training jobs to test these values. after testing the first set of hyperparameter values, hyperparameter tuning uses regression to choose the next set of hyperparameter values to test. hyperparameter tuning uses an amazon sagemaker implementation of bayesian optimization. when choosing the best hyperparameters for the next training job, hyperparameter tuning considers everything that it knows about this problem so far. sometimes it chooses a combination of hyperparameter values close to the combination that resulted in the best previous training job to incrementally improve performance. this allows hyperparameter tuning to exploit the best known results. other times, it chooses a set of hyperparameter values far removed from those it has tried. this allows it to explore the range of hyperparameter values to try to find new areas that are not well understood. the explore/exploit trade-off is common in many machine learning problems. for more information about bayesian optimization, see the following: basic topics on bayesian optimization speeding up bayesian optimization advanced modeling and transfer learning notehyperparameter tuning might not improve your model. it is an advanced tool for building machine solutions, and, as such, should be considered part of the scientific development process.when you build complex machine learning systems like deep learning neural networks, exploring all of the possible combinations is impractical. hyperparameter tuning can accelerate your productivity by trying many variations of a model, focusing on the most promising combinations of hyperparameter values within the ranges that you specify. to get good results, you need to choose the right ranges to explore. because the algorithm itself is stochastic, it’s possible that the hyperparameter tuning model will fail to converge on the best answer, even if the best possible combination of values is within the ranges that you choose.  
the best way to learn how to use amazon sagemaker is to create, train, and deploy a simple machine learning model. to do this, you need the following: a dataset. you use the mnist (modified national institute of standards and technology database) dataset of images of handwritten, single digit numbers. this dataset provides a training set of 50,000 example images of handwritten single-digit numbers, a validation set of 10,000 images, and a test dataset of 10,000 images. you provide this dataset to the algorithm for model training. for more information about the mnist dataset, see .an algorithm. you use the xgboost algorithm provided by amazon sagemaker to train the model using the mnist dataset. during model training, the algorithm assigns example data of handwritten numbers into 10 clusters: one for each number, 0 through 9. for more information about the algorithm, see .you also need a few resources for storing your data and running the code in this exercise: an amazon simple storage service (amazon s3) bucket to store the training data and the model artifacts that amazon sagemaker creates when it trains the model.an amazon sagemaker notebook instance to prepare and process data and to train and deploy a machine learning model.a jupyter notebook to use with the notebook instance to prepare your training data and train and deploy the model.in this exercise, you learn how to create all of the resources that you need to create, train, and deploy a model.  importantfor model training, deployment, and validation, you can use either of the following:the high-level  the aws sdk for python (boto 3) the  abstracts several implementation details, and is easy to use. this exercise provides code examples for both libraries. if you're a first-time amazon sagemaker user, we recommend that you use the . if you're new to amazon sagemaker, we recommend that you read  before starting this exercise. topics 
this rule detects whether a tensor is no longer changing across steps.  this rule runs the  method to check if the tensor isn't changing. this rule can be applied either to one of the supported deep learning frameworks (tensorflow, mxnet, and pytorch) or to the xgboost algorithm. you must specify either the  or  parameter. if both the parameters are specified, the rule inspects the union of tensors from both sets. for an example of how to configure and deploy a built-in rule, see . parameter descriptions for the unchangedtensor rule   
amazon sagemaker k-nearest neighbors (k-nn) algorithm is an index-based algorithm. it uses a non-parametric method for classification or regression. for classification problems, the algorithm queries the k points that are closest to the sample point and returns the most frequently used label of their class as the predicted label. for regression problems, the algorithm queries the k closest points to the sample point and returns the average of their feature values as the predicted value.  training with the k-nn algorithm has three steps: sampling, dimension reduction, and index building. sampling reduces the size of the initial dataset so that it fits into memory. for dimension reduction, the algorithm decreases the feature dimension of the data to reduce the footprint of the k-nn model in memory and inference latency. we provide two methods of dimension reduction methods: random projection and the fast johnson-lindenstrauss transform. typically, you use dimension reduction for high-dimensional (d >1000) datasets to avoid the “curse of dimensionality” that troubles the statistical analysis of data that becomes sparse as dimensionality increases. the main objective of k-nn's training is to construct the index. the index enables efficient lookups of distances between points whose values or class labels have not yet been determined and the k nearest points to use for inference. topics amazon sagemaker k-nn supports train and test data channels. use a train channel for data that you want to sample and construct into the k-nn index.use a test channel to emit scores in log files. scores are listed as one line per mini-batch: accuracy for , mean-squared error (mse) for  for score.for training inputs, k-nn supports  and  data formats. for input type , the first  columns are interpreted as the label vector for that row. you can use either file mode or pipe mode to train models on data that is formatted as  or as . for inference inputs, k-nn supports the , , and  data formats. the  format accepts a  and encoding parameter. it assumes a  of 0 and a utf-8 encoding. for inference outputs, k-nn supports the  and  data formats. these two data formats also support a verbose output mode. in verbose output mode, the api provides the search results with the distances vector sorted from smallest to largest, and corresponding elements in the labels vector. for batch transform, k-nn supports the  data format for both input and output. an example input is as follows: an example output is as follows: for more information on input and output file formats, see  for training,  for inference, and the . for a sample notebook that uses the amazon sagemaker k-nearest neighbor algorithm to predict wilderness cover types from geological and forest service data, see the .  use a jupyter notebook instance to run the example in amazon sagemaker. to learn how to create and open a jupyter notebook instance in amazon sagemaker, see . once you have created a notebook instance and opened it, select the sagemaker examples tab to see a list of all the amazon sagemaker example notebooks. find k-nearest neighbor notebooks in the introduction to amazon algorithms section. to open a notebook, click on its use tab and select create copy. to start, try running training on a cpu, using, for example, an ml.m5.2xlarge instance, or on a gpu using, for example, an ml.p2.xlarge instance. inference requests from cpus generally have a lower average latency than requests from gpus because there is a tax on cpu-to-gpu communication when you use gpu hardware. however, gpus generally have higher throughput for larger batches. 
to categorize articles and text into predefined categories, use text classification. for example, you can use text classification to identify the sentiment conveyed in a review or the emotion underlying a section of text. use amazon sagemaker ground truth text classification to have workers sort text into categories that you define.  you create a text classification labeling job using the ground truth section of the amazon sagemaker console or the  operation.  importantfor this task type, if you create your own manifest file, use  to identify the location of text files in amazon s3 that you want labeled. if you provide the text that you want labeled directly in the input manifest file, use . for more information, see . you can follow the instructions  to learn how to create a text classification labeling job in the amazon sagemaker console. in step 10, choose text from the task category drop down menu, and choose text classification (single label) as the task type.  ground truth provides a worker ui similar to the following for labeling tasks. when you create the labeling job with the console, you specify instructions to help workers complete the job and labels that workers can choose from.   to create a text classification labeling job, use the amazon sagemaker api operation . this api defines this operation for all aws sdks. to see a list of language-specific sdks supported for this operation, review the see also section of . follow the instructions on  and do the following while you configure your request:  pre-annotation lambda functions for this task type end with . to find the pre-annotation lambda arn for your region, see  . annotation-consolidation lambda functions for this task type end with . to find the annotation-consolidation lambda arn for your region, see . the following is an example of an  to create a labeling job in the us east (n. virginia) region. all parameters in red should be replaced with your specifications and resources.  if you create a labeling job using the api, you must supply a worker task template in . copy and modify the following template. only modify the , , and .  upload this template to s3, and provide the s3 uri for this file in . once you have created a text classification labeling job, your output data will be located in the amazon s3 bucket specified in the  parameter when using the api or in the output dataset location field of the job overview section of the console.  to learn more about the output manifest file generated by ground truth and the file structure the ground truth uses to store your output data, see .  to see an example of an output manifest files from a text classification labeling job, see . 
you can use amazon sagemaker to train and deploy a model using custom tensorflow code. the amazon sagemaker python sdk tensorflow estimators and models and the amazon sagemaker open-source tensorflow containers make writing a tensorflow script and running it in amazon sagemaker easier. for tensorflow versions 1.11 and later, the  supports script mode training scripts. i want to train a custom tensorflow model in amazon sagemaker.for a sample jupyter notebook, see .for documentation, see . i have a tensorflow model that i trained in amazon sagemaker, and i want to deploy it to a hosted endpoint.. i have a tensorflow model that i trained outside of amazon sagemaker, and i want to deploy it to an amazon sagemaker endpoint. i want to see the api documentation for  tensorflow classes. i want to see information about amazon sagemaker tensorflow containers..  for general information about writing tensorflow script mode training scripts and using tensorflow script mode estimators and models with amazon sagemaker, see . for information about tensorflow versions supported by the amazon sagemaker tensorflow container, see . the  provides a legacy mode that supports tensorflow versions 1.11 and earlier. use legacy mode tensorflow training scripts to run tensorflow jobs in amazon sagemaker if: you have existing legacy mode scripts that you do not want to convert to script mode.you want to use a tensorflow version earlier than 1.11.for information about writing legacy mode tensorflow scripts to use with the amazon sagemaker python sdk, see . 
when you start a training job with a  request, you specify a training algorithm. you can also specify algorithm-specific hyperparameters as string-to-string maps. the hyperparameters for the blazingtext algorithm depend on which mode you use: word2vec (unsupervised) and text classification (supervised). the following table lists the hyperparameters for the blazingtext word2vec training algorithm provided by amazon sagemaker. the following table lists the hyperparameters for the text classification training algorithm provided by amazon sagemaker. notealthough some of the parameters are common between the text classification and word2vec modes, they might have different meanings depending on the context. 
amazon sagemaker studio is an integrated machine learning environment where you can build, train, deploy, and analyze your models all in the same application. amazon sagemaker studio is available in the following aws regions: us east (ohio), us-east-2us east (n. virginia), us-east-1us west (n. oregon), us-west-2china (beijing), cn-north-1china (ningxia), cn-northwest-1eu (ireland), eu-west-1sagemaker studio includes the following features: topics amazon sagemaker studio notebooks is the next generation of amazon sagemaker notebooks. these notebooks include the following new features: aws single sign-on (aws sso) integrationfast start-up timesability to share notebooks with a few clicksfor more information, see . amazon sagemaker experiments provides experiment management and tracking. users can organize their experiments and artifacts in a centralized location using a structured organization scheme. an experiment is a collection of machine learning iterations called trials. a trial is a set of steps called trial components. a trial takes a combination of inputs such as a dataset, an algorithm, and parameters, and produces specific outputs such as a model, metrics, and checkpoints. experiment tracking enables both amazon sagemaker automated tracking of model training, tuning, and evaluation jobs, and api-enabled tracking of experiments done locally on amazon sagemaker notebooks. customers can use the tracked data to reconstruct an experiment, incrementally build on experiments conducted by peers, and trace model lineage for compliance and audit verifications. for more information, see . amazon sagemaker autopilot provides automatic machine learning that allows users without machine learning knowledge to quickly build classification and regression models. users only need to provide a tabular dataset and select the target column to predict. autopilot automatically explores machine learning solutions with different combinations of data preprocessors, algorithms, and algorithm parameters, to find the best model. when a user runs an autopilot job, sagemaker studio creates an experiment for the job and then creates a trial for each combination and stores all data and results. after the best model is determined, the user can drill down to view each trial and see which features had the most influence on the result. for more information, see . amazon sagemaker debugger provides full visibility into the model training process by enabling the inspection of all the training parameters and data throughout the training process. debugger provides a visual interface to analyze debug data and visual indicators about potential anomalies in the data. debugger automatically detects and alerts users to commonly occurring errors such as parameter values getting too large or small. users can extend debugger to detect new classes of errors that are specific to their model. for more information, see . amazon sagemaker model monitor is a tool for the monitoring and analysis of models in production (amazon sagemaker endpoints). model monitor offers a framework-agnostic analysis. machine learning models are typically trained and evaluated using historical data. after they are deployed in production, the quality of their predictions can degrade over time due to model drift. model drift is when the distribution of the data sent to the models for predictions varies from the distribution of data used during training. model monitor continuously monitors and analyzes the prediction requests. model monitor can store this data and use built-in statistical rules to detect common issues such as outliers in data and data drift. for more information, see . 
for a sample notebook that shows how to run scikit-learn scripts using a docker image provided and maintained by amazon sagemaker to preprocess data and evaluate models, see . to use this notebook, you need to install the amazon sagemaker python sdk for processing.  this notebook runs a processing job using  class from the the amazon sagemaker python sdk to execute a scikit-learn script that you provide. the script preprocesses data, trains a model using an amazon sagemaker training job, and then runs a processing job to evaluate the trained model. the processing job estimates how the model is expected to perform in production. to learn more about using the amazon sagemaker python sdk with processing containers, see . the following code example shows how the notebook uses  to run your own scikit-learn script using a docker image provided and maintained by amazon sagemaker, instead of your own docker image. to process data in parallel using scikit-learn on amazon sagemaker processing, you can shard input objects by s3 key by setting  inside a  so that each instance receives about the same number of input objects. 
this rule compares tensors gathered from a base trial with tensors from another trial.  this rule can be applied either to one of the supported deep learning frameworks (tensorflow, mxnet, and pytorch) or to the xgboost algorithm. for an example of how to configure and deploy a built-in rule, see . parameter descriptions for the similaracrossruns rule   
amazon sagemaker runs training jobs in an amazon virtual private cloud by default. however, training containers access aws resources—such as the amazon s3 buckets where you store training data and model artifacts—over the internet. to control access to your data and training containers, we recommend that you create a private vpc and configure it so that they aren't accessible over the internet. for information about creating and configuring a vpc, see  in the amazon vpc user guide. using a vpc helps to protect your training containers and data because you can configure your vpc so that it is not connected to the internet. using a vpc also allows you to monitor all network traffic in and out of your training containers by using vpc flow logs. for more information, see  in the amazon vpc user guide. you specify your private vpc configuration when you create training jobs by specifying subnets and security groups. when you specify the subnets and security groups, amazon sagemaker creates elastic network interfaces (enis) that are associated with your security groups in one of the subnets. enis allow your training containers to connect to resources in your vpc. for information about enis, see  in the amazon vpc user guide. notefor training jobs, you can configure only subnets with a default tenancy vpc in which your instance runs on shared hardware. for more information on the tenancy attribute for vpcs, see . to specify subnets and security groups in your private vpc, use the  request parameter of the  api, or provide this information when you create a training job in the amazon sagemaker console. amazon sagemaker uses this information to create enis and attach them to your training containers. the enis provide your training containers with a network connection within your vpc that is not connected to the internet. they also enable your training job to connect to resources in your private vpc. the following is an example of the  parameter that you include in your call to : when configuring the private vpc for your amazon sagemaker training jobs, use the following guidelines. for information about setting up a vpc, see  in the amazon vpc user guide. topics your vpc subnets should have at least two private ip addresses for each instance in a training job. for more information, see  in the amazon vpc user guide. if you configure your vpc so that training containers don't have access to the internet, they can't connect to the amazon s3 buckets that contain your training data unless you create a vpc endpoint that allows access. by creating a vpc endpoint, you allow your training containers to access the buckets where you store your data and model artifacts . we recommend that you also create a custom policy that allows only requests from your private vpc to access to your s3 buckets. for more information, see . to create an s3 vpc endpoint: open the amazon vpc console at . in the navigation pane, choose endpoints, then choose create endpoint for service name, choose com.amazonaws.region.s3, where region is the name of the region where your vpc resides. for vpc, choose the vpc you want to use for this endpoint. for configure route tables, select the route tables to be used by the endpoint. the vpc service automatically adds a route to each route table you select that points any s3 traffic to the new endpoint. for policy, choose full access to allow full access to the s3 service by any user or service within the vpc. choose custom to restrict access further. for information, see . the default endpoint policy allows full access to s3 for any user or service in your vpc. to further restrict access to s3, create a custom endpoint policy. for more information, see . you can also use a bucket policy to restrict access to your s3 buckets to only traffic that comes from your amazon vpc. for information, see . the default endpoint policy allows users to install packages from the amazon linux and amazon linux 2 repositories on the training container. if you don't want users to install packages from that repository, create a custom endpoint policy that explicitly denies access to the amazon linux and amazon linux 2 repositories. the following is an example of a policy that denies access to these repositories: use default dns settings for your endpoint route table, so that standard amazon s3 urls (for example, ) resolve. if you don't use default dns settings, ensure that the urls that you use to specify the locations of the data in your training jobs resolve by configuring the endpoint route tables. for information about vpc endpoint route tables, see  in the amazon vpc user guide. in distributed training, you must allow communication between the different containers in the same training job. to do that, configure a rule for your security group that allows inbound connections between members of the same security group for information, see . if you configure your vpc so that it doesn't have internet access, training jobs that use that vpc do not have access to resources outside your vpc. if your training job needs access to resources outside your vpc, provide access with one of the following options: if your training job needs access to an aws service that supports interface vpc endpoints, create an endpoint to connect to that service. for a list of services that support interface endpoints, see  in the amazon vpc user guide. for information about creating an interface vpc endpoint, see  in the amazon vpc user guide.if your training job needs access to an aws service that doesn't support interface vpc endpoints or to a resource outside of aws, create a nat gateway and configure your security groups to allow outbound connections. for information about setting up a nat gateway for your vpc, see  in the amazon virtual private cloud user guide.
here are the known limitations for amazon sagemaker debugger: horovod support: debugger does not support training on jobs that use distributed training with the tensorflow horovod container. (distributed training with horovod is supported for mxnet and pytorch.)distributed training: parameter server-based distributed training is not supported for mxnet and tensorflow.
amazon sagemaker lda is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of different categories. these categories are themselves a probability distribution over the features. lda is a generative probability model, which means it attempts to provide a model for the distribution of outputs and inputs based on latent variables. this is opposed to discriminative models, which attempt to learn how inputs map to outputs. you can use lda for a variety of tasks, from clustering customers based on product purchases to automatic harmonic analysis in music. however, it is most commonly associated with topic modeling in text corpuses. observations are referred to as documents. the feature set is referred to as vocabulary. a feature is referred to as a word. and the resulting categories are referred to as topics. notelemmatization significantly increases algorithm performance and accuracy. consider pre-processing any input text data. an lda model is defined by two parameters: α—a prior estimate on topic probability (in other words, the average frequency that each topic within a given document occurs). β—a collection of k topics where each topic is given a probability distribution over the vocabulary used in a document corpus, also called a "topic-word distribution."lda is a "bag-of-words" model, which means that the order of words does not matter. lda is a generative model where each document is generated word-by-word by choosing a topic mixture θ ∼ dirichlet(α).   for each word in the document:  choose a topic z ∼ multinomial(θ) choose the corresponding topic-word distribution β_z. draw a word w ∼ multinomial(β_z). when training the model, the goal is to find parameters α and β, which maximize the probability that the text corpus is generated by the model. the most popular methods for estimating the lda model use gibbs sampling or expectation maximization (em) techniques. the amazon sagemaker lda uses tensor spectral decomposition. this provides several advantages: theoretical guarantees on results. the standard em-method is guaranteed to converge only to local optima, which are often of poor quality. embarrassingly parallelizable. the work can be trivially divided over input documents in both training and inference. the em-method and gibbs sampling approaches can be parallelized, but not as easily. fast. although the em-method has low iteration cost it is prone to slow convergence rates. gibbs sampling is also subject to slow convergence rates and also requires a large number of samples. at a high-level, the tensor decomposition algorithm follows this process: the goal is to calculate the spectral decomposition of a v x v x v tensor, which summarizes the moments of the documents in our corpus. v is vocabulary size (in other words, the number of distinct words in all of the documents). the spectral components of this tensor are the lda parameters α and β, which maximize the overall likelihood of the document corpus. however, because vocabulary size tends to be large, this v x v x v tensor is prohibitively large to store in memory.  instead, it uses a v x v moment matrix, which is the two-dimensional analog of the tensor from step 1, to find a whitening matrix of dimension v x k. this matrix can be used to convert the v x v moment matrix into a k x k identity matrix. k is the number of topics in the model.  this same whitening matrix can then be used to find a smaller k x k x k tensor. when spectrally decomposed, this tensor has components that have a simple relationship with the components of the v x v x v tensor.  alternating least squares is used to decompose the smaller k x k x k tensor. this provides a substantial improvement in memory consumption and speed. the parameters α and β can be found by “unwhitening” these outputs in the spectral decomposition.  after the lda model’s parameters have been found, you can find the topic mixtures for each document. you use stochastic gradient descent to maximize the likelihood function of observing a given topic mixture corresponding to these data. topic quality can be improved by increasing the number of topics to look for in training and then filtering out poor quality ones. this is in fact done automatically in amazon sagemaker lda: 25% more topics are computed and only the ones with largest associated dirichlet priors are returned. to perform further topic filtering and analysis, you can increase the topic count and modify the resulting lda model as follows: for more information about algorithms for lda and the amazon sagemaker implementation, see the following: animashree anandkumar, rong ge, daniel hsu, sham m kakade, and matus telgarsky. tensor decompositions for learning latent variable models, journal of machine learning research, 15:2773–2832, 2014.david m blei, andrew y ng, and michael i jordan. latent dirichlet allocation. journal of machine learning research, 3(jan):993–1022, 2003.thomas l griffiths and mark steyvers. finding scientific topics. proceedings of the national academy of sciences, 101(suppl 1):5228–5235, 2004. tamara g kolda and brett w bader. tensor decompositions and applications. siam review, 51(3):455–500, 2009. 
you can create a neo endpoint in the . choose models, and then choose create models from the inference group. on the create model page, complete the model name, iam role, and, if needed, vpc fields. to add information about the container used to deploy your model, choose add container, then choose next. complete the container input options, location of inference code image, and location of model artifacts, and optionally, container host name, and environmental variables fields. to deploy neo-compiled models, choose the following: container input options: provide model artifacts and inference imagelocation of inference code image: choose one of the following images, depending the region and kind of application:amazon sagemaker image classificationamazon sagemaker xgboost    tensorflow : the tensorflow version used must be in  list.mxnet the mxnet version used must be in  list.pytorch the pytorch version used must be in  list.location of model artifact: the full s3 bucket path of the compiled model artifact generated by the neo compilation api.environmental variables:omit this field for sagemaker image classification and sagemaker xgboost.for tensorflow, pytorch, and mxnet, specify the environment variable sagemaker_submit_directory as the full s3 bucket path that contains the training script.the script must be packaged as a  file. the  file must contain the training script at the root level. the script must contain two additional functions for neo serving containers: : function that takes in the payload and  of each incoming request and returns a numpy array.: function that takes the prediction results produced by deep learning runtime and returns the response body.neither of these two functions use any functionalities of mxnet, pytorch, or tensorflow. for examples using these functions, see the . confirm that the information for the containers is accurate, and then choose create model.this takes you to the create model landing page. select the create endpoint button there. in create and configure endpoint diagram, specify the endpoint name. choose create a new endpoint configuration in attach endpoint configuration. in new endpoint configuration page, specify the endpoint configuration name. press edit next to the name of the model and specify the correct instance type on the edit production variant page. it is imperative that the instance type value match the one specified in your compilation job. when you’re done click save, then click create endpoint configuration on the new endpoint configuration page, and then click create endpoint. 
this walkthrough takes you on a tour of the main features of amazon sagemaker studio using the  sample notebook from the  repository. it is intended that you proceed through the walkthrough and run the notebook in studio at the same time. the code in the notebook trains multiple models and sets up the sagemaker debugger and sagemaker model monitor. the walkthrough shows you how to view the trials, compare the resulting models, show the debugger results, and deploy the best model using the sagemaker studio ui. you don't need to understand the code to follow this walkthrough. for a series of videos that shows how to use the main features of sagemaker studio, see  on youtube. prerequisites to run the notebook for this tour, you need: an aws sso or iam account to sign in to studio. for information, see .basic familiarity with the studio user interface and jupyter notebooks. for information, see .a copy of the  repository in your studio environment.to clone the repository sign in to sagemaker studio. for aws sso users, sign in using the url from your invitation email. for iam users, follow these steps. sign in to the . choose amazon sagemaker studio in the left navigation pane. choose open studio in the row next to your user name. on the top menu, choose file then new then terminal. at the command prompt, run the following command.  topics amazon sagemaker studio notebooks are collaborative jupyter notebooks that are built into sagemaker studio. you can launch studio notebooks without setting up compute instances and file storage, so you can get started fast. you can share notebooks with others in your organization, so that they can easily reproduce your results and collaborate while building models and exploring your data. for more information about sagemaker studio notebooks, see . to open the  notebook sign in to studio. for more information, see . choose the file browser icon (  ). navigate to . double-click xgboost_customer_churn_studio.ipynb to open the notebook. in the select kernel dialog, choose python 3 (data science), then choose select. your screen should resemble the following:  next, you use the notebook to create an experiment. amazon sagemaker experiments lets you organize, track, compare, and evaluate your machine learning experiments. an experiment is composed of multiple trials with the same objective. each trial is composed of multiple trial components such as a preprocessing job and a training job. first you create an experiment, then you create a trial which assigns it to the experiment. next, you create a training job as a trial component and associate the component with the trial. for more information, see . to create an experiment and a trial with a training job scroll down the notebook and choose the section titled amazon sagemaker experiments. in the studio main menu, choose run and then run all above selected cell. hold down the shift key and press enter to run the next code cell, which creates an experiment by calling the  method of the  class. in the left sidebar, choose the sagemaker experiment list icon (  ) to see the experiment (named ) in the experiments list. you might need to refresh the list. run the next two code cells. the first cell defines the hyperparameters to use in the training job. the second cell creates a trial that is assigned to the experiment that was created in the previous step. next, the cell creates a training job as a trial component, then runs the trial by calling the  method of the  class. it can take several minutes for the training job to complete. notethe output of the training job includes a long list of messages like . these aren't errors due to the training job but are the results from the model training process. in the experiments list, double-click the experiment name to see the trial (named ). double-click the trial name to see the associated trial component (named ). double-click the training trial component to open the describe trial component tab. you can follow the progress of the training job here. after the trial finishes, you can see details about the training job, such as metrics and hyperparameters, charts that visualize the training results. to see the billable time and instance type, choose the aws settings header.  next, the notebook creates and compares multiple trials that use different values for the  hyperparameter. to create and compare multiple trials scroll to the section of the notebook titled trying other hyperparameter values. run the following cell that creates and runs five trials, each with a different value of the  hyperparameter. notein the previous step of creating a single trial, the output of the training job is displayed. here, the output is suppressed as it would display about three thousand lines. to follow the progress and view the results in studio, choose the home icon above trial components. right-click the experiment name and choose open in trial component list. you can see details about the trials, compare trials to find the best performing model, and create charts to visualize training results.  after all the trials finish, sort the trials by choosing the validation:error header. in a later section, you will deploy the trial with the lowest . to visualize data after the training jobs run, you can create charts in amazon sagemaker studio. in this notebook, the training jobs run for a very short time, so they don't create much data. because of this, you create a scatter plot of the  metric (final validation error) for each of the  hyperparameter values that were specified in the training jobs. to create the scatter plot in the trial components list, multi-select the five trials from the previous step, then choose add chart. if the chart properties pane isn't open, choose the settings icon (  ) in the upper right corner to open it. choose the settings icon again when you want to close the pane. configure the chart properties as follows: for data type, choose summary statistics.for chart type, choose scatter plot.for x-axis, choose min_child_weight.for y-axis, choose validation:error_last.for color, choose trialcomponentname.studio displays the scatter plot.  next, the notebook sets up the sagemaker debugger. amazon sagemaker debugger helps you analyze your training jobs and find problems. it monitors, records, and analyzes tensor data from training jobs and checks the training tensors against a set of rules that you specify. you can choose from a list of built-in rules, or create your own custom rules. for more information, see . to debug a training job to specify the rules to use to analyze your training job, run the following cell in the section titled amazon sagemaker debugger. run the remaining cells in the section to create a new trial using the debug rules. note the  argument that is added to the  call. in the experiments list, double-click the experiment name to see the trials list. in the trials list, right-click the debug trial (named ) and choose open in trial component list. in the trial components list, right-click the trial and choose open in trial details. to see the results for each debug rule that you specified, choose the debugger heading.  notice that the training job passed all three of the rules that were configured for the job. if debugger had found issues, you could choose the rule in the list to see more information in the debugger details tab. next, you deploy the model. you can create an endpoint and deploy a model using the sdk or the amazon sagemaker studio ui. the notebook shows you how to deploy using the sdk. for this tour, we show you how to deploy using the studio ui. after you deploy the model, you can set up the sagemaker model monitor to monitor the endpoint. to deploy a model using the studio ui in the experiments list, right-click the experiment and choose open in trial component list. to sort the trials, choose the validation:error header. right-click the trial with the lowest  and choose deploy model. under deploy model, specify a name for the endpoint that will host the model. choose deploy model.  next, the notebook sets up the sagemaker model monitor. monitor the quality of your deployed models with amazon sagemaker model monitor. model monitor runs monitoring jobs on the endpoints where models are deployed. you can use its built-in monitoring capabilities, which don't require coding, or you can write code for custom analysis. for more information, see . the notebook first creates a processing job to generate baseline statistics. next, it creates a monitoring schedule, with a polling time of one hour, that compares the recent data captures to the baseline. to monitor the deployed model run all the code cells in the notebook sections titled host the model and amazon sagemaker model monitor. this can take some time. note that this creates a different endpoint than the endpoint you created in the previous section. in studio, choose the sagemaker endpoint list icon (  ). double-click the endpoint (named ) that was created by the notebook. in the monitoring job history list, you can see any issues that the monitoring jobs found. to see details about an issue, choose the issue. importantyou must shut down the kernel to stop monitoring. to shut down the kernel, from the top studio menu, choose kernel then shut down kernel. to stop incurring charges, you should clean up the resources that were created. to clean up the resources, follow the instructions in the notebook section titled clean up. 
to invoke a multi-model endpoint, use the  from the amazon sagemaker runtime just as you would invoke a single model endpoint, with one change. pass a new  parameter that specifies which of the models at the endpoint to target. the amazon sagemaker runtime  request supports  as a new header that takes the relative path of the model specified for invocation. the amazon sagemaker system constructs the absolute path of the model by combining the prefix that is provided as part of the  api call with the relative path of the model. the following example prediction request uses the  in the sample notebook. the multi-model endpoint dynamically loads target models as needed. you can observe this when running the  as it iterates through random invocations against multiple target models hosted behind a single endpoint. the first request against a given model takes longer because the model has to be downloaded from amazon simple storage service (amazon s3) and loaded into memory. (this is called a cold start.) subsequent calls finish faster because there's no additional overhead after the model has loaded. noteinvoking multi-model endpoints using the  isn't supported. 
the input data are the data objects that you send to your workforce to be labeled. each object in the input data is described in a manifest file. each line in the manifest is an entry containing an object to label. an entry can also contain labels from previous jobs. input data and the manifest file must be stored in amazon simple storage service (amazon s3). each has specific storage and access requirements, as follows: the s3 bucket that contains the input data must be in the same aws region in which you are running amazon sagemaker ground truth. you must give amazon sagemaker access to the data stored in the s3 bucket so that it can read it. for more information about s3 buckets, see . the manifest file must be in the same aws region as the data files, but it doesn't need to be in the same location as the data files. it can be stored in any s3 bucket that is accessible to the aws identity and access management (iam) role that you assigned to ground truth when you created the labeling job.the manifest is a utf-8 encoded file where each line is a complete and valid json object. each line is delimited by a standard line break, \n or \r\n. because each line must be a valid json object, you can't have unescaped line break characters. for more information about data format, see . each json object in the manifest file can be no larger than 100 k characters. no single attribute within an object can be larger than 20,000 characters. attribute names can't begin with  (dollar sign). importantfor 3d point cloud labeling job input data requirements, see .  each json object in the manifest file must contain one of the following keys:  or . the value of the keys are interpreted as follows:  – the source of the object is the amazon s3 object specified in the value. use this value when the object is a binary object, such as an image, or when you have text in individual files.  – the source of the object is the value. use this value when the object is a text value.the following is an example of a manifest file for files stored in an s3 bucket: use the  key for image files for bounding box, image classification (single and multi-label), and semantic segmentation labeling jobs.  use the  key for text-based labeling jobs (such as single and multi-label text classification and named entity recognition) if your dataset is stored in text files (for example, .txt or .csv files).  the following is an example of a manifest file with the input data stored in the manifest: use the  key for single and multi-label text classification and named entity recognition labeling jobs if the text you want labeled is listed directly in the input manifest file. you can include other key-value pairs in the manifest file. these pairs are passed to the output file unchanged. this is useful when you want to pass information between your applications. for more information, see . you can create a manifest file for your labeling jobs in the ground truth console using images, text (.txt) files, and comma-separated value (.csv) files. before using the following procedure, ensure that your input images or files are correctly formatted: image files – image files must comply with the size and resolution limits listed in the tables found in . text files – text data can be stored in one or more .txt files. each item that you want labeled must be separated by a standard line break. csv files – text data can be stored in one or more .csv files. each item that you want labeled must be in a separate row.to create a manifest file store the images or text files that you want to have labeled in an s3 bucket that is in the same region as your labeling job. you must give access to amazon sagemaker for the data to be read. for more information about amazon s3 buckets, see .  sign in to the ground truth console at . choose labeling job.  in the job overview section, under input dataset location, choose create manifest file. in input dataset location, enter the path to the s3 bucket where you data is stored (for example, s3://awsdoc-example-bucket/path-to-your-objects).  choose the data type, then choose create. you see a loading screen while ground truth examines the s3 bucket and creates a .manifest file in the s3 location specified above. if you would like to visually inspect the auto-generated manifest file, convert this file into a human-readable format by downloading a copy of the file and changing the file suffix from .manifest to .txt.  for images or text object, a green box indicates the number of objects detected. if you're satisfied with the results, choose use this manifest. if not, modify the image or text files in your s3 bucket and use this procedure to generate a new manifest file.  input datasets used in semantic segmentation labeling jobs have a quota of 20,000 items. for all other labeling job types, the dataset size quota is 100,000 items. to request an increase to the quota for labeling jobs other than semantic segmentation jobs, review the procedures in  to request a quota increase. input image data for active and non-active learning labeling jobs must not exceed size and resolution quotas. active learning refers to labeling job that use . non-active learning refers to labeling jobs that don't use automated data labeling. input files can't exceed the following size- quotas for both active and non-active learning labeling jobs. image file resolution refers to the number of pixels in an image, and determines the amount of detail an image holds. image resolution quotas differ depending on the labeling job type and the amazon sagemaker built-in algorithm used. the following table lists the resolution quotas for images used in active and non-active learning labeling jobs. noteactive learning isn't available for label verification jobs. you can use the amazon sagemaker console to select a portion of your dataset for labeling. the data must be stored in an amazon s3 bucket. you have three options: use the full dataset.choose a randomly selected sample of the dataset.specify a subset of the dataset using a query.the following options are available in the labeling jobs section of the  after selecting create labeling job. to learn how to create a labeling job in the console, see . to configure the dataset that you use for labeling, in the job overview section, choose additional configuration. when you choose to use full dataset, you must provide a manifest file for your data objects. you can provide the path of the s3 bucket that contains the manifest file or you can use the amazon sagemaker console to create the file. to learn how to create a manifest file using the console, see .  when you want to label a random subset of your data, select random sample. the dataset is stored in the s3 bucket specified in the  input dataset location  field.  after you have specified the percentage of data objects that you want to include in the sample, choose create subset. amazon sagemaker randomly picks the data objects for your labeling job. after the objects are selected, choose use this subset.  amazon sagemaker creates a manifest file for the selected data objects. it also modifies the value in the input dataset location field to point to the new manifest file. you can specify a subset of your data objects using an amazon s3  query on the object file names.  the  statement of the sql query is defined for you. you provide the  clause to specify which data objects should be returned. for more information about the amazon s3  statement, see . choose create subset to start the selection, and then choose use this subset to use the selected data.  amazon sagemaker creates a manifest file for the selected data objects. it also updates the value in the input dataset location field to point to the new manifest file. 
to create a multi-model endpoint (console) open the amazon sagemaker console at . choose model, and then from the inference group, choose create model.  for model name, enter a name.  for iam role. choose or create an iam role that has the amazonsagemakerfullaccess iam policy attached.  in the container definition section, for provide model artifacts and inference image optionschoose use multiple models. choose create model. deploy your multi-model endpoint as you would a single model endpoint. for instructions, see . 
this example shows how to create a new notebook for configuring and launching a hyperparameter tuning job. the tuning job uses the  to train a model to predict whether a customer will enroll for a term deposit at a bank after being contacted by phone. you use the low-level aws sdk for python (boto) to configure and launch the hyperparameter tuning job, and the aws management console to monitor the status of hyperparameter training jobs. you can also use the amazon sagemaker high-level  to configure, run, monitor, and analyze hyperparameter tuning jobs. for more information, see . to run the code in this example, you need topics 
you can use amazon sagemaker to interact with docker containers and run your own inference code in one of two ways: to use your own inference code with a persistent endpoint to get one prediction at a time, use amazon sagemaker hosting services.to use your own inference code to get predictions for an entire dataset, use amazon sagemaker batch transform.topics 
all amazon sagemaker built-in algorithms adhere to the common input inference format described in . note that amazon sagemaker random cut forest supports both dense and sparse json and recordio formats. this topic contains a list of the available output formats for the amazon sagemaker rcf algorithm. accept: application/json. accept: application/jsonlines. accept: application/x-recordio-protobuf. 
to build machine learning models in amazon sagemaker, you have the following options: use one of the built-in algorithms. amazon sagemaker provides several built-in machine learning algorithms that you can use for a variety of problem types. for more information, see .write a custom training script in a machine learning framework that amazon sagemaker supports, and use one of the pre-built framework containers to run it in amazon sagemaker. for information, see .bring your own algorithm or model to train or host in amazon sagemaker. for information, see .use an algorithm that you subscribe to from aws marketplace. for information, see .topics 
to prepare for training, you can preprocess your data using a variety of aws services, including aws glue, amazon emr, amazon redshift, amazon relational database service, and amazon athena. after preprocessing, publish the data to an amazon s3 bucket. for training, the data need to go through a series of conversions and transformations, including:  training data serialization (handled by you) training data deserialization (handled by the algorithm) training model serialization (handled by the algorithm) trained model deserialization (optional, handled by you) when using amazon sagemaker in the training portion of the algorithm, make sure to upload all data at once. if more data is added to that location, a new training call would need to be made to construct a brand new model. the following table lists supported  values: many amazon sagemaker algorithms support training with data in csv format. to use data in csv format for training, in the input data channel specification, specify text/csv as the . amazon sagemaker requires that a csv file doesn't have a header record and that the target variable is in the first column. to run unsupervised learning algorithms that don't have a target, specify the number of label columns in the content type. for example, in this case 'text/csv;label_size=0'.  most amazon sagemaker algorithms work best when you use the optimized protobuf  format for the training data. using this format allows you to take advantage of pipe mode when training the algorithms that support it. file mode loads all of your data from amazon simple storage service (amazon s3) to the training instance volumes. in pipe mode, your training job streams data directly from amazon s3. streaming can provide faster start times for training jobs and better throughput. with pipe mode, you also reduce the size of the amazon elastic block store volumes for your training instances. pipe mode needs only enough disk space to store your final model artifacts. file mode needs disk space to store both your final model artifacts and your full training dataset. see the  for additional details on the training input mode. for a summary of the data formats supported by each algorithm, see the documentation for the individual algorithms or this .  note for an example that shows how to convert the commonly used numpy array into the protobuf recordio format, see . .  in the protobuf recordio format, amazon sagemaker converts each observation in the dataset into a binary representation as a set of 4-byte floats, then loads it in the protobuf values field. if you are using python for your data preparation, we strongly recommend that you use these existing transformations. however, if you are using another language, the protobuf definition file below provides the schema that you use to convert your data into amazon sagemaker protobuf format. after creating the protocol buffer, store it in an amazon s3 location that amazon sagemaker can access and that can be passed as part of  in .  notefor all amazon sagemaker algorithms, the  in  must be set to . some algorithms also support a validation or test . these are typically used to evaluate the model's performance by using a hold-out dataset. hold-out datasets are not used in the initial training but can be used to further tune the model. amazon sagemaker models are stored as model.tar.gz in the s3 bucket specified in   parameter of the  call. you can specify most of these model artifacts when creating a hosting model. you can also open and review them in your notebook instance. when  is untarred, it contains , which is a serialized apache mxnet object. for example, you use the following to load the k-means model into memory and view it:  
 a private workforce is a group of workers that you choose. these can be employees of your company or a group of subject matter experts from your industry. for example, if the task is to label medical images, you could create a private workforce of people knowledgeable about the images in question.  each aws account has access to a single private workforce per region, and the owner has the ability to create multiple private work teams within that workforce. a single private work team is used to complete a labeling job or human review task, or a job.  you can assign each work team to a separate job or use a single team for multiple jobs. a single worker can be in more than one work team.  topics 
creating good instructions for your human review jobs improves your worker's accuracy in completing their task. you can modify the default instructions that are provided in the console when creating a human review workflow, or you can use the console to create a custom worker template and include your instructions in this template. the instructions are shown to the worker on the ui page where they complete their labeling task. there are three kinds of instructions in the amazon augmented ai console: task description – the description should provide a succinct explanation of the task.instructions – these instructions are shown on the same webpage where workers complete a task. these instructions should provide an easy reference to show the worker the correct way to complete the task.additional instructions – these instructions are shown in a dialog box that appears when a worker chooses view full instructions. we recommend that you provide detailed instructions for completing the task, and include several examples showing edge cases and other difficult situations for labeling objects.images provide useful examples for your workers. to add a publicly accessible image to your instructions, do the following: place the cursor where the image should go in the instructions editor. choose the image icon in the editor toolbar. enter the url of your image. if your instruction image is in an s3 bucket that isn't publicly accessible, do the following: for the image url, enter: .this renders the image url with a short-lived, one-time access code that's appended so the worker's browser can display it. a broken image icon is displayed in the instructions editor, but previewing the tool displays the image in the rendered preview. see  for more information about the  element.  
the amazon sagemaker object detection algorithm detects and classifies objects in images using a single deep neural network. it is a supervised learning algorithm that takes images as input and identifies all instances of objects within the image scene. the object is categorized into one of the classes in a specified collection with a confidence score that it belongs to the class. its location and scale in the image are indicated by a rectangular bounding box. it uses the  framework and supports two base networks:  and . the network can be trained from scratch, or trained with models that have been pre-trained on the  dataset. topics the amazon sagemaker object detection algorithm supports both recordio () and image (, , and ) content types for training in file mode and supports recordio () for training in pipe mode. however you can also train in pipe mode using the image files (, , and ), without creating recordio files, by using the augmented manifest format. the recommended input format for the amazon sagemaker object detection algorithms is . however, you can also use raw images in .jpg or .png format. the algorithm supports only  for inference. noteto maintain better interoperability with existing deep learning frameworks, this differs from the protobuf data formats commonly used by other amazon sagemaker algorithms. see the  for more details on data formats. if you use the recordio format for training, specify both train and validation channels as values for the  parameter of the  request. specify one recordio (.rec) file in the train channel and one recordio file in the validation channel. set the content type for both channels to . an example of how to generate recordio file can be found in the object detection sample notebook. you can also use tools from the  example to generate recordio files for popular datasets like the  and . if you use the image format for training, specify , , , and  channels as values for the  parameter of  request. specify the individual image data (.jpg or .png) files for the train and validation channels. for annotation data, you can use the json format. specify the corresponding .json files in the  and  channels. set the content type for all four channels to  or  based on the image type. you can also use the content type  when your dataset contains both .jpg and .png images. the following is an example of a .json file. each image needs a .json file for annotation, and the .json file should have the same name as the corresponding image. the name of above .json file should be "sample_image1.json". there are four properties in the annotation .json file. the property "file" specifies the relative path of the image file. for example, if your training images and corresponding .json files are stored in s3://your_bucket/train/sample_image and s3://your_bucket/train_annotation, specify the path for your train and train_annotation channels as s3://your_bucket/train and s3://your_bucket/train_annotation, respectively.  in the .json file, the relative path for an image named sample_image1.jpg should be sample_image/sample_image1.jpg. the  property specifies the overall image dimensions. the sagemaker object detection algorithm currently only supports 3-channel images. the  property specifies the categories and bounding boxes for objects within the image. each object is annotated by a  index and by four bounding box coordinates (, , , ). the  (x-coordinate) and  (y-coordinate) values represent the upper-left corner of the bounding box. the  (x-coordinate) and  (y-coordinate) values represent the dimensions of the bounding box. the origin (0, 0) is the upper-left corner of the entire image. if you have multiple objects within one image, all the annotations should be included in a single .json file. the  property stores the mapping between the class index and class name. the class indices should be numbered successively and the numbering should start with 0. the  property is optional for the annotation .json file the augmented manifest format enables you to do training in pipe mode using image files without needing to create recordio files. you need to specify both train and validation channels as values for the  parameter of the  request. while using the format, an s3 manifest file needs to be generated that contains the list of images and their corresponding annotations. the manifest file format should be in  format in which each line represents one sample. the images are specified using the  tag that points to the s3 location of the image. the annotations are provided under the  parameter value as specified in the  request. it can also contain additional metadata under the  tag, but these are ignored by the algorithm. in the following example, the  are contained in the list : the order of  in the input files matters when training the object detection algorithm. it accepts piped data in a specific order, with  first, followed by . so the "attributenames" in this example are provided with  first, followed by . when using object detection with augmented manifest, the value of parameter  must be set as . for more information on augmented manifest files, see . you can also seed the training of a new model with the artifacts from a model that you trained previously with amazon sagemaker. incremental training saves training time when you want to train a new model with the same or similar data. amazon sagemaker object detection models can be seeded only with another built-in object detection model trained in amazon sagemaker. to use a pretrained model, in the  request, specify the  as "model" in the  parameter. set the  for the model channel to . the input hyperparameters of both the new model and the pretrained model that you upload to the model channel must have the same settings for the  and  input parameters. these parameters define the network architecture. for the pretrained model file, use the compressed model artifacts (in .tar.gz format) output by amazon sagemaker. you can use either recordio or image formats for input data. for a sample notebook that shows how to use incremental training with the amazon sagemaker object detection algorithm, see  sample notebook. for more information on incremental training and for instructions on how to use it, see .  for object detection, we support the following gpu instances for training: , , , ,  and . we recommend using gpu instances with more memory for training with large batch sizes. you can also run the algorithm on multi-gpu and multi-machine settings for distributed training. however, both cpu (such as c5 and m5) and gpu (such as p2 and p3) instances can be used for the inference. all the supported instance types for inference are itemized on . for a sample notebook that shows how to use the amazon sagemaker object detection algorithm to train and host a model on the coco dataset using the single shot multibox detector algorithm, see . for instructions how to create and access jupyter notebook instances that you can use to run the example in amazon sagemaker, see . once you have created a notebook instance and opened it, select the sagemaker examples tab to see a list of all the amazon sagemaker samples. the object detection example notebook using the object detection algorithm is located in the introduction to amazon algorithms section. to open a notebook, click on its use tab and select create copy. 
use this task type when you want workers to classify objects in a 3d point cloud by drawing 3d cuboids around objects. for example, you can use this task type to ask workers to identify different types of objects in a point cloud, such as cars, bikes, and pedestrians.  for this task type, the data object that workers label is a single point cloud frame. ground truth renders a 3d point cloud using point cloud data you provide. you can also provide camera data to give workers more visual information about scenes in the frame, and to help workers draw 3d cuboids around objects.  ground truth providers workers with tools to annotate objects with 9 degrees of freedom (x,y,z,rx,ry,rz,l,w,h) in three dimensions in both 3d scene and projected side views (top, side, and back). if you provide sensor fusion information (like camera data), when a worker adds a cuboid to identify an object in the 3d point cloud, the cuboid shows up and can be modified in the 2d images. after a cuboid has been added, all edits made to that cuboid in the 2d or 3d scene are projected into the other view. you can create a job to adjust annotations created in a 3d point cloud object detection labeling job using the 3d point cloud object detection adjustment task type.  if you are a new user of the ground truth 3d point cloud labeling modality, we recommend you review . this labeling modality is different from other ground truth task types, and this page provides an overview of important details you should be aware of when creating a 3d point cloud labeling job. topics ground truth provides workers with a web portal and tools to complete your 3d point cloud object detection annotation tasks. when you create the labeling job, you provide the amazon resource name (arn) for a pre-built ground truth worker ui in the  parameter. when you create a labeling job using this task type in the console, this worker ui is automatically used. you can preview and interact with the worker ui when you create a labeling job in the console. if you are a new user, it is recommended that you create a labeling job using the console to ensure your label attributes, point cloud frames, and if applicable, images, appear as expected.  the following is a gif of the 3d point cloud object detection worker task interface. if you provide camera data for sensor fusion in the world coordinate system, images are matched up with scenes in the point cloud frame. these images appear in the worker portal as shown in the following gif.   worker can navigate in the 3d scene using their keyboard and mouse. they can: double click on specific objects in the point cloud to zoom into them.use a mouse-scroller or trackpad to zoom in and out of the point cloud.use both keyboard arrow keys and q, e, a, and d keys to move up, down, left, right. use keyboard keys w and s to zoom in and out. once a worker places a cuboid in the 3d scene, a side-view will appear with the three projected side views: top, side, and back. these side-views show points in and around the placed cuboid and help workers refine cuboid boundaries in that area. workers can zoom in and out of each of those side-views using their mouse.  the following video demonstrates movements around the 3d point cloud and in the side-view.   additional view options and features are available in the view menu in the worker ui. see the  for a comprehensive overview of the worker ui.  assistive labeling toolsground truth helps workers annotate 3d point clouds faster and more accurately using machine learning and computer vision powered assistive labeling tools for 3d point cloud object tracking tasks. the following assistive labeling tools are available for this task type: snapping – workers can add a cuboid around an object and use a keyboard shortcut or menu option to have ground truth's autofit tool snap the cuboid tightly around the object. set to ground – after a worker adds a cuboid to the 3d scene, the worker can automatically snap the cuboid to the ground. for example, the worker can use this feature to snap a cuboid to the road or sidewalk in the scene. multi-view labeling – after a worker adds a 3d cuboid to the 3d scene, a side panel displays front, side, and top perspectives to help the worker adjust the cuboid tightly around the object. in all of these views, the cuboid includes an arrow that indicates the orientation, or heading of the object. when the worker adjusts the cuboid, the adjustment will appear in real time on all of the views (that is, 3d, top, side, and front). sensor fusion – if you provide data for sensor fusion, workers can adjust annotations in the 3d scenes and in 2d images, and the annotations will be projected into the other view in real time. additionally, workers will have the option to view the direction the camera is facing and the camera frustum.view options – enables workers to easily hide or view cuboids, label text, a ground mesh, and additional point attributes like color or intensity. workers can also choose between perspective and orthogonal projections. you can create a 3d point cloud labeling job using the amazon sagemaker console or api operation, . to create a labeling job for this task type you need the following:  a single-frame input manifest file. to learn how to create this type of manifest file, see . if you are a new user of ground truth 3d point cloud labeling modalities, you may also want to review . a work team from a private or vendor workforce. you cannot use amazon mechanical turk for 3d point cloud labeling jobs. to learn how to create workforces and work teams, see .a label category configuration file. for more information, see . additionally, make sure that you have reviewed and satisfied the .  use one of the following sections to learn how to create a labeling job using the console or an api.  this section covers details you need to know when you create a labeling job using the amazon sagemaker api operation . this api defines this operation for all aws sdks. to see a list of language-specific sdks supported for this operation, review the see also section of .  , provides an overview of the  operation. follow these instructions and do the following while you configure your request:  you must enter an arn for . use . replace  with the aws region you are creating the labeling job in.  there should not be an entry for the  parameter.  your input manifest file must be a single-frame manifest file. for more information, see . you specify your labels and worker instructions in a label category configuration file. to learn how to create this file, see . you need to provide a pre-defined arns for the pre-annotation and post-annotation (acs) lambda functions. these arns are specific to the aws region you use to create your labeling job. to find the pre-annotation lambda arn, refer to . use the region you are creating your labeling job in to find the correct arn. for example, if you are creating your labeling job in us-east-1, the arn will be . to find the post-annotation lambda arn, refer to . use the region you are creating your labeling job in to find the correct arn. for example, if you are creating your labeling job in us-east-1, the arn will be . the number of workers specified in  must be . automated data labeling is not supported for 3d point cloud labeling jobs. you should not specify values for parameters in . 3d point cloud object detection labeling jobs can take multiple hours to complete. you can specify a longer time limit for these labeling jobs in  (up to 7 days, or 604800 seconds).  importantif you set your take time limit to be greater than 8 hours, you must set  for your iam execution role to at least 8 hours. to see how to update this value for your iam role, see  in the iam user guide, choose your preferred method to modify the role, and then follow the steps in . to create an adjustment labeling job, use the instructions in the previous section, with the following modifications. note that the  is the same for object detection and object detection adjustment labeling jobs.  in your label category configuration file, you must include . use this parameter to input the  used in the labeling job that generated the annotations you want your worker to adjust.  importantwhen you create a labeling job in the console, if you did not specify a label category attribute name, the name of your job is used as the labelattributename.  for example, if your label category attribute name was  in your first labeling job, add the following to your object detection adjustment labeling job label category configuration file. to learn how to create this file, see .  you need to provide a pre-defined arns for the pre-annotation and post-annotation (acs) lambda functions. these arns are specific to the aws region you use to create your labeling job. to find the pre-annotation lambda arn, refer to . use the region you are creating your labeling job in to find the correct arn that ends with .to find the post-annotation lambda arn, refer to . use the region you are creating your labeling job in to find the correct arn that ends with .the  parameter must contain the same label categories as the previous labeling job. adding new label categories or adjusting label categories is not supported.you can follow the instructions  in order to learn how to create a 3d point cloud object detection labeling job in the amazon sagemaker console. while you are creating your labeling job, be aware of the following:  your input manifest file must be a single-frame manifest file. for more information, see . optionally, you can provide label category attributes. workers can assign one or more of these attributes to annotations to provide more information about that object. for example, you might want to use the attribute occluded to have workers identify when an object is partially obstructed.automated data labeling and annotation consolidation are not supported for 3d point cloud labeling tasks. 3d point cloud object detection labeling jobs can take multiple hours to complete. you can specify a longer time limit for these labeling jobs when you select your work team (up to 7 days, or 604800 seconds).  importantif you set your take time limit to be greater than 8 hours, you must set  for your iam execution role to at least 8 hours. to see how to update this value for your iam role, see  in the iam user guide, choose your preferred method to modify the role, and then follow the steps in . you can create an adjustment labeling job in the console by chaining a successfully completed object tracking labeling job. to learn more, see . when you create a 3d point cloud object detection labeling job, tasks are sent to workers. when these workers complete their tasks, labels are written to the amazon s3 bucket you specified when you created the labeling job. the output data format determines what you see in your amazon s3 bucket when your labeling job status () is .  if you are a new user of ground truth, see  to learn more about the ground truth output data format. to learn about the 3d point cloud object detection output data format, see .  
a field for text input. the following is an example of a liquid template designed to transcribe audio clips that uses the  element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  the following attributes are supported by this element. a boolean switch that, if present, puts the cursor in this element on-load so that users can immediately begin typing without having to click inside the element. a boolean switch that, if present, turns on input validation. the behavior of the validator can be modified by the error-message and allowed-pattern attributes. a boolean switch that, if present, puts a small text field beneath the lower-right corner of the element, displaying the number of characters inside the element. a boolean switch that, if present, displays the input area as disabled. the text to be displayed below the input field, on the left side, if validation fails. a string that is displayed inside a text field. this text shrinks and rises up above a text field when the worker starts typing in the field or when the value attribute is set. an integer that specifies the maximum number of characters allowed by the element. characters typed or pasted beyond the maximum are ignored. an integer that specifies the maximum number of rows of text that are allowed within a crowd-text-area. normally the element expands to accommodate new rows. if this is set, after the number of rows exceeds it, content scrolls upward out of view and a scrollbar control appears. a string used to represent the element's data in the output. a string presented to the user as placeholder text. it disappears after the user puts something in the input area. an integer that specifies the height of the element in rows of text. a preset that becomes the default if the worker does not provide input. the preset appears in a text field. this element has the following parent and child elements. parent elements: child elements: nonethis element outputs the  as a property name and the element's text contents as the value. carriage returns in the text are represented as . example sample output for this element   for more information, see the following. 
the aws global infrastructure is built around aws regions and availability zones. aws regions provide multiple physically separated and isolated availability zones, which are connected with low-latency, high-throughput, and highly redundant networking. with availability zones, you can design and operate applications and databases that automatically fail over between availability zones without interruption. availability zones are more highly available, fault tolerant, and scalable than traditional single or multiple data center infrastructures.  for more information about aws regions and availability zones, see . in addition to the aws global infrastructure, amazon sagemaker offers several features to help support your data resiliency and backup needs. 
specify the name of the amazon s3 bucket where you want to store the output of the training jobs that the tuning job launches. the name of the bucket must contain sagemaker, and be globally unique. the bucket must be in the same aws region as the notebook instance that you use for this example. you can use the bucket that you created when you set up amazon sagemaker, or you can create a new bucket. for information, see . notethe name of the bucket doesn't need to contain sagemaker if the role that you use to run the hyperparameter tuning job has a policy that gives the sagemaker service principle  permission.  is the path within the bucket where amazon sagemaker stores the output from training jobs.  
amazon sagemaker provides apis, sdks, and a command line interface that you can use to create and manage notebook instances and train and deploy models.   — recommended!you can also get code examples from the amazon sagemaker example notebooks github repository. making api calls directly from code is cumbersome, and requires you to write code to authenticate your requests. amazon sagemaker provides the following alternatives: use the amazon sagemaker console–with the console, you don't write any code. you use the console ui to start model training or deploy a model. the console works well for simple jobs, where you use a built-in training algorithm and you don't need to preprocess training data.  modify the example jupyter notebooks–amazon sagemaker provides several jupyter notebooks that train and deploy models using specific algorithms and datasets. start with a notebook that has a suitable algorithm and modify it to accommodate your data source and specific needs. write model training and inference code from scratch–amazon sagemaker provides multiple aws sdk languages (listed in the overview) and the , a high-level python library that you can use in your code to start model training jobs and deploy the resulting models.   the amazon sagemaker python sdk–this python library simplifies model training and deployment. in addition to authenticating your requests, the library abstracts platform specifics by providing simple methods and default parameters. for example:   to deploy your model, you call only the  method. the method creates an amazon sagemaker model artifact, an endpoint configuration, then deploys the model on an endpoint. if you use a custom framework script for model training, you call the  method. the method creates a .gzip file of your script, uploads it to an amazon s3 location, and then runs it for model training, and other tasks. for more information, see . the aws sdks – the sdks provide methods that correspond to the amazon sagemaker api (see ). use the sdks to programmatically start a model training job and host the model in amazon sagemaker. sdk clients authenticate your requests by using your access keys, so you don't need to write authentication code. they are available in multiple languages and platforms. for more information, see the preceeding list in the overview.  in , you train and deploy a model using an algorithm provided by amazon sagemaker. that exercise shows how to use both of these libraries. for more information, see .   integrate amazon sagemaker into your apache spark workflow–amazon sagemaker provides a library for calling its apis from apache spark. with it, you can use amazon sagemaker-based estimators in an apache spark pipeline. for more information, see .
for training and hosting amazon sagemaker algorithms, we recommend using the following ec2 instance types: ml.m4.xlarge, ml.m4.4xlarge, and ml.m4.10xlargeml.c4.xlarge, ml.c4.2xlarge, and ml.c4.8xlarge ml.p2.xlarge, ml.p2.8xlarge, and ml.p2.16xlarge most amazon sagemaker algorithms have been engineered to take advantage of gpu computing for training. despite higher per-instance costs, gpus train more quickly, making them more cost effective. exceptions, such as xgboost, are noted in this guide. (xgboost implements an open-source algorithm that has been optimized for cpu computation.) the size and type of data can have a great effect on which hardware configuration is most effective. when the same model is trained on a recurring basis, initial testing across a spectrum of instance types can discover configurations that are more cost effective in the long run. additionally, algorithms that train most efficiently on gpus might not require gpus for efficient inference. experiment to determine the most cost effectiveness solution. for more information on amazon sagemaker hardware specifications, see . 
download the mnist dataset to your notebook instance, review the data, transform it, and upload it to your s3 bucket.  you transform the data by changing its format from  to comma-separated values (csv). the  expects input in either the libsvm or csv format. libsvm is an open source machine learning library. in this exercise , you use csv format because it's simpler. topics 
automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many jobs that test a range of hyperparameters on your dataset. you choose the tunable hyperparameters, a range of values for each, and an objective metric. you choose the objective metric from the metrics that the algorithm computes. automatic model tuning searches the hyperparameters chosen to find the combination of values that result in the model that optimizes the objective metric. for more information about model tuning, see . the deepar algorithm reports three metrics, which are computed during training. when tuning a model, choose one of these as the objective. for the objective, use either the forecast accuracy on a provided test channel (recommended) or the training loss. for recommendations for the training/test split for the deepar algorithm, see .  tune a deepar model with the following hyperparameters. the hyperparameters that have the greatest impact, listed in order from the most to least impactful, on deepar objective metrics are: , , , , and . 
the amazon sagemaker latent dirichlet allocation (lda) algorithm is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of distinct categories. lda is most commonly used to discover a user-specified number of topics shared by documents within a text corpus. here each observation is a document, the features are the presence (or occurrence count) of each word, and the categories are the topics. since the method is unsupervised, the topics are not specified up front, and are not guaranteed to align with how a human may naturally categorize documents. the topics are learned as a probability distribution over the words that occur in each document. each document, in turn, is described as a mixture of topics. the exact content of two documents with similar topic mixtures will not be the same. but overall, you would expect these documents to more frequently use a shared subset of words, than when compared with a document from a different topic mixture. this allows lda to discover these word groups and use them to form topics. as an extremely simple example, given a set of documents where the only words that occur within them are: eat, sleep, play, meow, and bark, lda might produce topics like the following: you can infer that documents that are more likely to fall into topic 1 are about cats (who are more likely to meow and sleep), and documents that fall into topic 2 are about dogs (who prefer to play and bark). these topics can be found even though the words dog and cat never appear in any of the texts.  topics lda expects data to be provided on the train channel, and optionally supports a test channel, which is scored by the final model. lda supports both  (dense and sparse) and  file formats. for , the data must be dense and have dimension equal to number of records \ vocabulary size*. lda can be trained in file or pipe mode when using recordio-wrapped protobuf, but only in file mode for the  format. for inference, , , and  content types are supported. sparse data can also be passed for  and . lda inference returns  or  predictions, which include the  vector for each observation. please see the  for more detail on training and inference formats. lda currently only supports single-instance cpu training. cpu instances are recommended for hosting/inference. for a sample notebook that shows how to train the amazon sagemaker latent dirichlet allocation algorithm on a dataset and then how to deploy the trained model to perform inferences about the topic mixtures in input documents, see the . for instructions how to create and access jupyter notebook instances that you can use to run the example in amazon sagemaker, see . once you have created a notebook instance and opened it, select the sagemaker examples tab to see a list of all the amazon sagemaker samples. the topic modeling example notebooks using the ntm algorithms are located in the introduction to amazon algorithms section. to open a notebook, click on its use tab and select create copy. 
when you create an endpoint, amazon sagemaker attaches an amazon elastic block store (amazon ebs) storage volume to each ml compute instance that hosts the endpoint. the size of the storage volume depends on the instance type.  for a full list of the host instance storage volumes, see   
the trial components list in amazon sagemaker studio is referred to as the leaderboard. the leaderboard is where you compare the trials in your experiments. the leaderboard contains multiple variable width columns that list properties of the trials, such as the trial status, debugger status, tags, metrics, hyperparameters, and input and output artifacts. the following image shows the sagemaker studio leaderboard. at the top left is the search box. at the right is the table properties pane. use the settings icon (  ) to open and closed the pane. you can hide or display properties by category or by individual columns.  topics in the experiments browser, experiments, trials, and trial components are accessed as a hierarchy. double-click an entity to drill down the hierarchy. use the breadcrumb above the list to go up the hierarchy. to search for an experiment, trial, or trial component by name in the left sidebar, choose the sagemaker experiment list icon (  ). in the experiments browser, choose the entity type you want to search for, experiment, trial, or trial component. in the search box, start typing a substring of at least three characters that's part of the entity's name. as you type, the list of entities will be continuously filtered to only those entities where the substring is part of the entity name. you can add searchable tags to experiments, trials, and trial components when the entities are created or afterwards using the  api. a tag consists of a unique case-sensitive key and an optional value. multiple tags can be added to an entity. the same tag can be added to multiple entities. to search by tag in the left sidebar, choose the sagemaker experiment list icon (  ). in the experiments browser, choose either the experiments or trials list. choose the experiments or trials that you want to compare. hold down the ctrl/cmd key and choose each entity individually, or to choose a continuous range of entities, hold down the shift key and choose the top entity and the bottom entity that define the range. right-click the selection and choose open in trial component list. a new tab opens that lists the associated trials and trial components. if the table properties pane isn't open, choose the settings icon (  ) in the upper right corner to open it. verify that the tags column is enabled, then choose the settings icon to close the pane. in the search column name box, choose the column to search by typing tags, then choose tags. in search tag key, start typing a substring of at least three characters that's part of the tag's key. a list of tag keys will be displayed and continuously filtered to only those keys where the substring is part of the tag's key. choose a tag key. in search tag value, type a substring of the tag's value then choose apply. the trial component list will displays only those components that have tags that match the key-value pair. add additional filters by following steps 6 through 9. a trial component's tag must match all filters for the component to be displayed. you can have up to 20 filters. you can view the details of a trial component in the describe trial component pane. this pane includes charts, metrics, hyperparameters, input and output artifacts, aws settings, debugger data, and trial mappings. aws settings contains links to amazon cloudwatch logs. to view the details of a trial component from the trial components pane, right-click a trial component and choose open in trial details. choose the various headings to view the associated properties. to view the links to amazon cloudwatch logs, choose the aws settings header and scroll to the bottom of the page. 
you can run a hyperparameter tuning job to optimize hyperparameters for amazon sagemaker rl. the roboschool example in the sample notebooks at  shows how you can do this with rl coach. the launcher script shows how you can abstract parameters from the coach preset file and optimize them. 
to install packages or sample notebooks on your notebook instance, configure networking and security for it, or otherwise use a shell script to customize it, use a lifecycle configuration. a lifecycle configuration provides shell scripts that run only when you create the notebook instance or whenever you start one. when you create a notebook instance, you can create a new lifecycle configuration and the scripts it uses or apply one that you already have. you can also use a lifecycle configuration script to access aws services from your notebook. for example, you can create a script that lets you use your notebook to control other aws resources, such as an amazon emr instance. we maintain a public repository of notebook lifecycle configuration scripts that address common use cases for customizing notebook instances at . noteeach script has a limit of 16384 characters.the value of the  environment variable that is available to both scripts is . the working directory, which is the value of the  environment variable, is .view cloudwatch logs for notebook instance lifecycle configurations in log group  in log stream .scripts cannot run for longer than 5 minutes. if a script runs for longer than 5 minutes, it fails and the notebook instance is not created or started. to help decrease the run time of scripts, try the following:cut down on necessary steps. for example, limit which conda environments in which to install large packages. run tasks in parallel processes. use the  command in your script. to create a lifecycle configuration for lifecycle configuration - optional, choose create a new lifecycle configuration. for name, type a name using alphanumeric characters and "-", but no spaces. the name can have a maximum of 63 characters. (optional) to create a script that runs when you create the notebook and every time you start it, choose start notebook. in the start notebook editor, type the script. (optional) to create a script that runs only once, when you create the notebook, choose create notebook. in the create notebook editor, type the script configure networking. choose create configuration. you can see a list of notebook instance lifecycle configurations you previously created by choosing lifecycle configuration in the amazon sagemaker console. from there, you can view, edit, delete existing lifecycle configurations. you can create a new notebook instance lifecycle configuration by choosing create configuration. these notebook instance lifecycle configurations are available when you create a new notebook instance. the following are best practices for using lifecycle configurations: lifecycle configurations run as the  user. if your script makes any changes within the  directory, (for example, installing a package with ), use the command  to run as the  user. this is the same user that amazon sagemaker runs as.amazon sagemaker notebook instances use  environments to implement different kernels for jupyter notebooks. if you want to install packages that are available to one or more notebook kernels, enclose the commands to install the packages with  environment commands that activate the conda environment that contains the kernel where you want to install the packages. for example, if you want to install a package only for the  environment, use the following code: if you want to install a package in all conda environments in the notebook instance, use the following code: you must store all conda environments in the default environments folder (/home/user/anaconda3/envs).importantwhen you create or change a script, we recommend that you use a text editor that provides unix-style line breaks, such as the text editor available in the console when you create a notebook. copying text from a non-linux operating system might introduce incompatible line breaks and result in an unexpected error. 
you can use trained models in an inference pipeline to make real-time predictions directly without performing external preprocessing. when you configure the pipeline, you can choose to use the built-in feature transformers already available in amazon sagemaker. or, you can implement your own transformation logic using just a few lines of scikit-learn or spark code.  , a serialization format and execution engine for machine learning pipelines, supports spark, scikit-learn, and tensorflow for training pipelines and exporting them to a serialized pipeline called an mleap bundle. you can deserialize bundles back into spark for batch-mode scoring or into the mleap runtime to power real-time api services. the containers in a pipeline listen on the port specified in the  environment variable (instead of 8080). when running in an inference pipeline, amazon sagemaker automatically provides this environment variable to containers. if this environment variable isn't present, containers default to using port 8080. to indicate that your container complies with this requirement, use the following command to add a label to your dockerfile: if your container needs to listen on a second port, choose a port in the range specified by the  environment variable. specify the value as an inclusive range in the format "xxxx-yyyy", where  and  are multi-digit integers. amazon sagemaker provides this value automatically when you run the container in a multicontainer pipeline. noteto use custom docker images in a pipeline that includes , you need an . your amazon ecr repository must grant amazon sagemaker permission to pull the image. for more information, see . the following code creates and deploys a real-time inference pipeline model with sparkml and xgboost models in series using the amazon sagemaker sdk. the following example shows how to make real-time predictions by calling an inference endpoint and passing a request payload in json format: the response you get from  is the model's inference result. you can run this  that shows how to deploy an endpoint, run an inference request, then deserialize the response. find this notebook and more examples in the . 
monitoring is an important part of maintaining the reliability, availability, and performance of amazon sagemaker and your other aws solutions. aws provides the following monitoring tools to watch amazon sagemaker, report when something is wrong, and take automatic actions when appropriate: amazon cloudwatch monitors your aws resources and the applications that you run on aws in real time. you can collect and track metrics, create customized dashboards, and set alarms that notify you or take actions when a specified metric reaches a threshold that you specify. for example, you can have cloudwatch track cpu usage or other metrics of your amazon ec2 instances and automatically launch new instances when needed. for more information, see the .amazon cloudwatch logs enables you to monitor, store, and access your log files from ec2 instances, aws cloudtrail, and other sources. cloudwatch logs can monitor information in the log files and notify you when certain thresholds are met. you can also archive your log data in highly durable storage. for more information, see the .aws cloudtrail captures api calls and related events made by or on behalf of your aws account and delivers the log files to an amazon s3 bucket that you specify. you can identify which users and accounts called aws, the source ip address from which the calls were made, and when the calls occurred. for more information, see the .cloudwatch events delivers a near real-time stream of system events that describe changes in aws resources. create cloudwatch events rules react to a status change in a amazon sagemaker training, hyperparameter tuning, or batch transform jobtopics 
you can delete a scaling policy with the aws management console, the aws cli, or the application auto scaling api. you must delete a scaling policy if you wish to update a model's endpoint. to delete an automatic scaling policy (console) open the amazon sagemaker console at . in the navigation pane, choose endpoints.  choose the endpoint for which you want to delete automatic scaling. for endpoint runtime settings, choose the variant that you want to configure. choose configure auto scaling. choose deregister auto scaling. you can use the aws cli or the application auto scaling api to delete a scaling policy from a variant. to delete a scaling policy from a variant, use the  aws cli command with the following parameters: —the name of the scaling policy.—the resource identifier for the variant. for this parameter, the resource type is  and the unique identifier is the name of the variant. for example, .—set this value to .—set this value to .examplethe following example deletes a target-tracking scaling policy named  from a variant named .   to delete a scaling policy from your variant, use the  application auto scaling api action with the following parameters: —the name of the scaling policy.—set this value to .—the resource identifier for the variant. for this parameter, the resource type is  and the unique identifier is the name of the variant,. for example, .—set this value to .examplethe following example uses the application auto scaling api to delete a target-tracking scaling policy named  from a variant named .   
notewhen you use one of the amazon sagemaker built-in algorithms, you don't need to define metrics. built-in algorithms automatically send metrics to hyperparameter tuning. you do need to choose one of the metrics that the built-in algorithm emits as the objective metric for the tuning job. for a list of metrics that a built-in algorithm emits, see the metrics table for the algorithm in . to optimize hyperparameters for a machine learning model, a tuning job evaluates the training jobs it launches by using a metric that the training algorithm writes to logs. amazon sagemaker hyperparameter tuning parses your algorithm’s  and  streams to find algorithm metrics, such as loss or validation-accuracy, that show how well the model is performing on the dataset  notethese are the same metrics that amazon sagemaker sends to cloudwatch logs. for more information, see . if you use your own algorithm for hyperparameter tuning, make sure that your algorithm emits at least one metric by writing evaluation data to  or . notehyperparameter tuning sends an additional hyperparameter,  to the training algorithm. this hyperparameter specifies the objective metric being used for the hyperparameter tuning job, so that your algorithm can use that information during training. you can define up to 20 metrics for your tuning job to monitor. you choose one of those metrics to be the objective metric, which hyperparameter tuning uses to evaluate the training jobs. the hyperparameter tuning job returns the training job that returned the best value for the objective metric as the best training job. you define metrics for a tuning job by specifying a name and a regular expression for each metric that your tuning job monitors. design the regular expressions to capture the values of metrics that your algorithm emits. you pass these metrics to the  operation in the  parameter as the  field of the  field. the following example defines 4 metrics: the following is an example of the log that the algorithm writes: use the regular expression (regex) to match the algorithm's log output and capture the numeric values of metrics. for example, in the regex for the  metric defined above, the first part of the regex finds the exact text "loss = ", and the expression  captures zero or more of any character until the first semicolon character. in this expression, the parenthesis tell the regex to capture what is inside them,  means any character,  means zero or more, and  means capture only until the first instance of the  character. choose one of the metrics that you define as the objective metric for the tuning job. if you are using the api, specify the value of the  key in the  field of the  parameter that you send to the  operation. 
hyperparameters are parameters that are set before a machine learning model begins learning. the following hyperparameters are supported by the amazon sagemaker built-in image classification algorithm. see  for information on image classification hyperparameter tuning.  
when you chose to use a custom template as your task type in the amazon sagemaker ground truth console, you reach the custom labeling task panel. there you can choose from multiple base templates. the templates represent some of the most common tasks and provide a sample to work from as you create your customized labeling task's template. if you are not using the console, or as an additional recourse, see  for a repository of demo templates for a variety of labeling job task types. this demonstration works with the boundingbox template. the demonstration also works with the aws lambda functions needed for processing your data before and after the task. in the github repository above, to find templates that work with aws lambdafunctions, look for  in the template. topics this is the starter bounding box template that is provided. the custom templates use the , and each of the items between double curly braces is a variable. the pre-annotation aws lambda function should provide an object named  and that object's properties can be accessed as  in your template. as an example, assume you have a large collection of animal photos in which you know the kind of animal in an image from a prior image-classification job. now you want to have a bounding box drawn around it. in the starter sample, there are three variables: , , and . each of these would be represented in different parts of the bounding box.  is an http(s) url or s3 uri for the photo to be annotated. the added  is a filter that will convert an s3 uri to an https url with short-lived access to that resource. if you're using an http(s) url, it's not needed. is the text above the photo to be labeled, something like "draw a box around the bird in the photo." is an array, represented as . these are labels that can be assigned by the worker to the different boxes they draw. you can have one or many.each of the variable names come from the json object in the response from your pre-annotation lambda, the names above are merely suggested, use whatever variable names make sense to you and will promote code readability among your team. only use variables when necessaryif a field will not change, you can remove that variable from the template and replace it with that text, otherwise you have to repeat that text as a value in each object in your manifest or code it into your pre-annotation lambda function. example : final customized bounding box templateto keep things simple, this template will have one variable, one label, and very basic instructions. assuming your manifest has an "animal" property in each data object, that value can be re-used in two parts of the template.   note the re-use of  throughout the template. if your manifest had all of the animal names beginning with a capital letter, you could use , incorporating one of liquid's built-in filters in sentences where it needed to be presented lowercase. your manifest file should provide the variable values you're using in your template. you can do some transformation of your manifest data in your pre-annotation lambda, but if you don't need to, you maintain a lower risk of errors and your lambda will run faster. here's a sample manifest file for the template. as part of the job set-up, provide the arn of an aws lambda function that can be called to process your manifest entries and pass them to the template engine. naming your lambda functionthe best practice in naming your function is to use one of the following four strings as part of the function name: , , , or . this applies to both your pre-annotation and post-annotation functions. when you're using the console, if you have aws lambda functions that are owned by your account, a drop-down list of functions meeting the naming requirements will be provided to choose one. in this very basic example, you're just passing through the information from the manifest without doing any additional processing on it. this sample pre-annotation function is written for python 3.7. the json object from your manifest will be provided as a child of the  object. the properties inside the  object will be available as variables to your template, so simply setting the value of  to  will pass all the values from your manifest object to your template without having to copy them individually. if you wish to send more values to the template, you can add them to the  object. as part of the job set-up, provide the arn of an aws lambda function that can be called to process the form data when a worker completes a task. this can be as simple or complex as you want. if you want to do answer consolidation and scoring as it comes in, you can apply the scoring and/or consolidation algorithms of your choice. if you want to store the raw data for offline processing, that is an option. provide permissions to your post-annotation lambdathe annotation data will be in a file designated by the  string in the  object. to process the annotations as they come in, even for a simple pass through function, you need to assign  access to your lambda so it can read the annotation files.in the console page for creating your lambda, scroll to the execution role panel. select create a new role from one or more templates. give the role a name. from the policy templates drop-down, choose amazon s3 object read-only permissions. save the lambda and the role will be saved and selected. the following sample is in python 2.7. the post-annotation lambda will often receive batches of task results in the event object. that batch will be the  object the lambda should iterate through. what you send back will be an object meeting the . you'll find the output of the job in a folder named after your labeling job in the target s3 bucket you specified. it will be in a subfolder named . for a bounding box task, the output you find in the output manifest will look a bit like the demo below. the example has been cleaned up for printing. the actual output will be a single line per record. example : json in your output manifest   note how the additional  attribute from your original manifest is passed to the output manifest on the same level as the  and labeling data. any properties from your input manifest, whether they were used in your template or not, will be passed to the output manifest.this should help you create your own custom template. 
by default, amazon sagemaker runs training jobs in an amazon virtual private cloud (amazon vpc) to help keep your data secure. you can add another level of security to protect your training containers and data by configuring a private vpc. distributed ml frameworks and algorithms usually transmit information that is directly related to the model such as weights, not the training dataset. when performing distributed training, you can further protect data that is transmitted between instances. this can help you to comply with regulatory requirements. to do this, use inter-container traffic encryption.  enabling inter-container traffic encryption can increase training time, especially if you are using distributed deep learning algorithms. enabling inter-container traffic encryption doesn't affect training jobs with a single compute instance. however, for training jobs with several compute instances, the effect on training time depends on the amount of communication between compute instances. for affected algorithms, adding this additional level of security also increases cost. the training time for most amazon sagemaker built-in algorithms, such as xgboost, deepar, and linear learner, typically aren't affected. you can enable inter-container traffic encryption for training jobs or hyperparameter tuning jobs. you can use amazon sagemaker apis or console to enable inter-container traffic encryption. for information about running training jobs in a private vpc, see . before enabling inter-container traffic encryption on training or hyperparameter tuning jobs with apis, you need to add inbound and outbound rules to your private vpc's security group. to enable inter-container traffic encryption (api) add the following inbound and outbound rules in the security group for your private vpc: when you send a request to the  or  api, specify  for the  parameter. notethe aws security group console might show display ports range as "all", however ec2 ignores the specified port range because it is not applicable for the esp 50 ip protocol. to enable inter-container traffic encryption in a training job open the amazon sagemaker console at  in the navigation pane, choose training, then choose training jobs. choose create training job.  under network, choose a vpc. you can use the default vpc or one that you have created.  choose enable inter-container traffic encryption.  after you enable inter-container traffic encryption, finish creating the training job. for more information, see . to enable inter-container traffic encryption in a hyperparameter tuning job open the amazon sagemaker console at . in the navigation pane, choose training, then choose hyperparameter tuning jobs. choose create hyperparameter tuning job.  under network, choose a vpc. you can use the default vpc or one that you created.  choose enable inter-container traffic encryption.  after enabling inter-container traffic encryption, finish creating the hyperparameter tuning job. for more information, see . 
a button that acts as an on/off switch, toggling a state. the following example shows different ways you can use to use the  html element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  the following attributes are supported by this element. a boolean switch that, if present, displays the button switched to the on position. a boolean switch that, if present, displays the button as disabled and prevents toggling. when in an off position, a button using this attribute, will display in an alert color. the standard is red, but may be changed in css. when toggled on, the button will display in the same color as other buttons in the on position. a string that is used to identify the answer submitted by the worker. this value matches a key in the json object that specifies the answer. a boolean switch that, if present, requires the worker to provide input. a value used in the output as the property name for the element's boolean state. defaults to "on" if not provided. this element has the following parent and child elements. parent elements: child elements: nonethis element outputs the  as the name of an object, containing the  as a property name and the element's state as boolean value for the property. if no value for the element is specified, the property name defaults to "on." example sample output for this element   for more information, see the following. 
automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many jobs that test a range of hyperparameters on your dataset. you choose the tunable hyperparameters, a range of values for each, and an objective metric. you choose the objective metric from the metrics that the algorithm computes. automatic model tuning searches the hyperparameters chosen to find the combination of values that result in the model that optimizes the objective metric.  the linear learner algorithm also has an internal mechanism for tuning hyperparameters separate from the automatic model tuning feature described here. by default, the linear learner algorithm tunes hyperparameters by training multiple models in parallel. when you use automatic model tuning, the linear learner internal tuning mechanism is turned off automatically. this sets the number of parallel models, , to 1. the algorithm ignores any value that you set for . for more information about model tuning, see . the linear learner algorithm reports the metrics in the following table, which are computed during training. choose one of them as the objective metric. to avoid overfitting, we recommend tuning the model against a validation metric instead of a training metric. you can tune a linear learner model with the following hyperparameters. 
as your algorithm runs in a container, it generates output including the status of the training job and model and output artifacts. your algorithm should write this information to the following files, which are located in the container's  directory. amazon sagemaker processes the information contained in this directory as follows: —if training fails, after all algorithm output (for example, logging) completes, your algorithm should write the failure description to this file. in a  response, amazon sagemaker returns the first 1024 characters from this file as .  —your algorithm should write all final model artifacts to this directory. amazon sagemaker copies this data as a single object in compressed tar format to the s3 location that you specified in the  request.  if multiple containers in a single training job write to this directory they should ensure no  names clash. amazon sagemaker aggregates the result in a tar file and uploads to s3. 
amazon sagemaker ground truth manages sending your data objects to workers to be labeled. labeling each data object is a task. workers complete each task until the entire labeling job is complete. ground truth divides the total number of tasks into smaller batches that are sent to workers. a new batch is sent to workers when the previous one is finished. ground truth provides two features that help improve the accuracy of your data labels and reduce the total cost of labeling your data: annotation consolidation helps to improve the accuracy of your data object's labels. it combines the results of multiple worker's annotation tasks into one high-fidelity label.automated data labeling uses machine learning to label portions of your data automatically without having to send them to human workers.topics 
you can connect directly to the amazon sagemaker api or to the amazon sagemaker runtime through an  in your virtual private cloud (vpc) instead of connecting over the internet. when you use a vpc interface endpoint, communication between your vpc and the amazon sagemaker api or runtime is conducted entirely and securely within the aws network.  the amazon sagemaker api and runtime support  (amazon vpc) interface endpoints that are powered by . each vpc endpoint is represented by one or more  (enis) with private ip addresses in your vpc subnets. the vpc interface endpoint connects your vpc directly to the amazon sagemaker api or runtime without an internet gateway, nat device, vpn connection, or aws direct connect connection. the instances in your vpc don't need public ip addresses to communicate with the amazon sagemaker api or runtime. you can create an interface endpoint to connect to amazon sagemaker or to amazon sagemaker runtime with either the aws console or aws command line interface (aws cli) commands. for instructions, see . after you have created a vpc endpoint, you can use the following example cli commands that use the  parameter to specify interface endpoints to the amazon sagemaker api or runtime: if you enable private dns hostnames for your vpc endpoint, you don't need to specify the endpoint url. the amazon sagemaker api dns hostname that the cli and amazon sagemaker sdk use by default () resolves to your vpc endpoint. similarly, the amazon sagemaker runtime dns hostname that the cli and amazon sagemaker runtime sdk use by default () resolves to your vpc endpoint. the amazon sagemaker api and runtime support vpc endpoints in all aws regions where both  and  are available. amazon sagemaker supports making calls to all of its  inside your vpc. the result  from the  is not supported by private link. for information about how to enable privatelink for the authorized url that users use to connect to a notebook instance, see . to learn more about aws privatelink, see the  . refer to  for the price of vpc endpoints. to learn more about vpc and endpoints, see . for information about how to use identity-based aws identity and access management policies to restrict access to the amazon sagemaker api and runtime, see . you can create a policy for amazon vpc endpoints for amazon sagemaker to specify the following: the principal that can perform actions.the actions that can be performed.the resources on which actions can be performed.for more information, see  in the amazon vpc user guide. notevpc endpoint policies aren't supported for federal information processing standard (fips) amazon sagemaker runtime endpoints for . the following example vpc endpoint policy specifies that all users who have access to the vpc interface endpoint are allowed to invoke the amazon sagemaker hosted endpoint named . in this example, the following are denied: other amazon sagemaker api actions, such as  and .invoking amazon sagemaker hosted endpoints other than .notein this example, users can still take other amazon sagemaker api actions from outside the vpc. for information about how to restrict api calls to those from within the vpc, see . to call the amazon sagemaker api and runtime through your vpc, you have to connect from an instance that is inside the vpc or connect your private network to your vpc by using an amazon virtual private network (vpn) or aws direct connect. for information about amazon vpn, see  in the amazon virtual private cloud user guide. for information about aws direct connect, see  in the aws direct connect user guide. 
in this section, you sign up for an aws account. if you already have an aws account, skip this step. when you sign up for amazon web services (aws), your aws account is automatically signed up for all aws services, including amazon sagemaker. you are charged only for the services that you use.  to create an aws account open . follow the online instructions. part of the sign-up procedure involves receiving a phone call and entering a verification code on the phone keypad. write down your aws account id because you'll need it for the next task. 
a container for tabbed information. the following is an example template that uses the  element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  this element has no attributes. this element has the following parent and child elements. parent elements:  child elements: for more information, see the following. 
this rule detects if the model is being overtrained.  this rule can be applied either to one of the supported deep learning frameworks (tensorflow, mxnet, and pytorch) or to the xgboost algorithm. for an example of how to configure and deploy a built-in rule, see . noteovertraining can be avoided by early stopping. for information on early stopping, see . for an example that shows how to use spot training with debugger, see .  parameter descriptions for the overtraining rule   the following python example shows how to implement this rule. 
in this section, you sign up for an aws account, create an iam admin user, and onboard to amazon sagemaker studio. if you're new to amazon sagemaker, we recommend that you read . topics 
to create an endpoint that can host multiple models, use multi-model endpoints. multi-model endpoints provide a scalable and cost-effective solution to deploying large numbers of models. they use a shared serving container that is enabled to host multiple models. this reduces hosting costs by improving endpoint utilization compared with using single-model endpoints. it also reduces deployment overhead because amazon sagemaker manages loading models in memory and scaling them based on the traffic patterns to them. multi-model endpoints also enable time-sharing of memory resources across your models. this works best when the models are fairly similar in size and invocation latency. when this is the case, multi-model endpoints can effectively use instances across all models. if you have models that have significantly higher transactions per second (tps) or latency requirements, we recommend hosting them on dedicated endpoints. multi-model endpoints are also well suited to scenarios that can tolerate occasional cold-start-related latency penalties that occur when invoking infrequently used models. multi-model endpoints support a/b testing. they work with auto scaling and aws privatelink. you can use multi-model-enabled containers with serial inference pipelines, but only one multi-model-enabled container can be included in an inference pipeline. you can't use multi-model-enabled containers with amazon elastic inference. you can use the aws sdk for python (boto) or the amazon sagemaker console to create a multi-model endpoint. you can use multi-model endpoints with custom-built containers by integrating the  library. topics for a sample notebook that uses amazon sagemaker to deploy multiple xgboost models to an endpoint, see the . for a sample notebook that shows how to set up and deploy a custom container that supports multi-model endpoints in amazon sagemaker, see the . for instructions how to create and access jupyter notebook instances that you can use to run the example in amazon sagemaker, see . after you've created a notebook instance and opened it, choose the sagemaker examples tab to see a list of all the amazon sagemaker samples. the multi-model endpoint notebook is located in the advanced functionality section. to open a notebook, choose its use tab and choose create copy. there are several items to consider when selecting a sagemaker ml instance type for a multi-model endpoint. provision sufficient  capacity for all of the models that need to be served. balance performance (minimize cold starts) and cost (don’t over-provision instance capacity). for information about the size of the storage volume that amazon sagemaker attaches for each instance type for an endpoint and for a multi-model endpoint, see . for a container configured to run in  mode, the storage volume provisioned for its instances has more memory. this allows more models to be cached on the instance storage volume.  when choosing an amazon sagemaker ml instance type, consider the following: multi-model endpoints are not supported on gpu instance types.the traffic distribution (access patterns) to the models that you want to host behind the multi-model endpoint, along with the model size (how many models could be loaded in memory on the instance):think of the amount of memory on an instance as the cache space for models to be loaded. think of the number of vcpus as the concurrency limit to perform inference on the loaded models (assuming that invoking a model is bound to cpu).a higher amount of instance memory allows you to have more models loaded and ready to serve inference requests. you don't need to waste time loading the model.a higher amount of vcpus allows you to invoke more unique models concurrently (again assuming that inference is bound to cpu).have some "slack" memory available so that unused models can be unloaded, and especially for multi-model endpoints with multiple instances. if an instance or an availability zone fails, the models on those instances will be rerouted to other instances behind the endpoint.tolerance to loading/downloading times:d instance type families (for example, m5d, c5d, or r5d) come with an nvme (non-volatile memory express) ssd, which offers high i/o performance and might reduce the time it takes to download models to the storage volume and for the container to load the model from the storage volume.because d instance types come with an nvme ssd storage, amazon sagemaker does not attach an amazon ebs storage volume to these ml compute instances that hosts the multi-model endpoint. auto scaling works best when the models are simarlarly sized and homogenous, that is when they have similar inference latency andresource requirements.in some cases, you might opt to reduce costs by choosing an instance type that can't hold all of the targeted models in memory at once. amazon sagemaker dynamically unloads models when it runs out of memory to make room for a newly targeted model. for infrequently requested models, you are going to pay a price with the dynamic load latency. in cases with more stringent latency needs, you might opt for larger instance types or more instances. investing time up front for proper performance testing and analysis will pay great dividends in successful production deployments. you can use the  statistic of the  metric to monitor the ratio of requests where the model is already loaded. you can use the  statistic for the  metric to monitor the number of unload requests sent to the container during a time period. if models are unloaded too frequently (an indicator of thrashing, where models are being unloaded and loaded again because there is insufficient cache space for the working set of models), consider using a larger instance type with more memory or increasing the number of instances behind the multi-model endpoint. for multi-model endpoints with multiple instances, be aware that a model might be loaded on more than 1 instance.  amazon sagemaker multi-model endpoints fully supports auto scaling, which manages replicas of models to ensure models scale based on traffic patterns. we recommend that you configure your multi-model endpoint and the size of your instances by considering all of the above and also set up auto scaling for your endpoint. the invocation rates used to trigger an auto-scale event is based on the aggregate set of predictions across the full set of models served by the endpoint.  
the  expects comma-separated values (csv) for its training input. the format of the training dataset is numpy.array. transform the dataset from numpy.array format to the csv format. then upload it to the amazon s3 bucket that you created in  to convert the dataset to csv format and upload it type the following code into a cell in your notebook and then run the cell. ``` %%time import os import boto3 import re import copy import time import io import struct from time import gmtime, strftime from sagemaker import get_execution_role role = get_execution_role() region = boto3.session().region_name bucket='mybucket' # replace with your s3 bucket name prefix = 'sagemaker/xgboost-mnist' # used as part of the path in the bucket where you store data def convert_data():   convert_data()   ```   after it converts the dataset to the csv format, the code uploads the csv file to the s3 bucket.  next step 
amazon sagemaker debugger rules analyze tensors emitted during the training of a model. they monitor conditions that are critical for the success of a training job. for example, they can detect whether gradients are getting too large or too small or if a model is being overfit. debugger comes pre-packaged with certain python-coded, built-in rules.  you can deploy a built-in rule to monitor your training job either by using the  api or by using the open source  with the . the smdebug programming model provides the context for understanding this task. for information on the programming model, see . noteyou are not charged for the instances when running sagemaker built-in rules. topics 
this rule detects if your model is being overfit to the training data by comparing the validation and training losses. this rule can be applied either to one of the supported deep learning frameworks (tensorflow, mxnet, and pytorch) or to the xgboost algorithm. for an example of how to configure and deploy a built-in rule, see . notea standard way to prevent overfitting is to regularize your model. parameter descriptions for the overfit rule   
in this step you use the console to establish which worker type to use and make the necessary sub-selections for the worker type. it assumes you have already completed the steps up to this point in the  section and have chosen the custom labeling task as the task type. to configure your workforce. first choose an option from the worker types. there are three types currently available: public uses an on-demand workforce of independent contractors, powered by amazon mechanical turk. they are paid on a per-task basis.private uses your employees or contractors for handling data that needs to stay within your organization.vendor uses third party vendors that specialize in providing data labeling services, available via the aws marketplace.if you choose the public option, you are asked to set the number of workers per dataset object. having more than one worker perform the same task on the same object can help increase the accuracy of your results. the default is three. you can raise or lower that depending on the accuracy you need. you are also asked to set a price per task by using a drop-down menu. the menu recommends price points based on how long it will take to complete the task. the recommended method to determine this is to first run a short test of your task with a private workforce. the test provides a realistic estimate of how long the task takes to complete. you can then select the range your estimate falls within on the price per task menu. if your average time is more than 5 minutes, consider breaking your task into smaller units.  
amazon sagemaker algorithms produce amazon cloudwatch logs, which provide detailed information on the training process. to see the logs, in the aws management console, choose cloudwatch, choose logs, and then choose the /aws/sagemaker/trainingjobs log group. each training job has one log stream per node that it was trained on. the log stream’s name begins with the value specified in the  parameter when the job was created. noteif a job fails and logs do not appear in cloudwatch, it's likely that an error occurred before the start of training. reasons include specifying the wrong training image or s3 location. the contents of logs vary by algorithms. however, you can typically find the following information: confirmation of arguments provided at the beginning of the logerrors that occurred during trainingmeasurement of an algorithms accuracy or numerical performancetimings for the algorithm, and any major stages within the algorithmif a training job fails, some details about the failure are provided by the  return value in the training job description, as follows: others are reported only in the cloudwatch logs. common errors include the following: misspecifying a hyperparameter or specifying a hyperparameter that is invalid for the algorithm. from the cloudwatch log: specifying an invalid value for a hyperparameter. failurereason: failurereason: inaccurate protobuf file format. from the cloudwatch log: 
the following sections list the available resources for amazon sagemaker studio notebooks. topics 
when making predictions on a large dataset, you can exclude attributes that aren't needed for prediction. after the predictions have been made, you can associate some of the excluded attributes with those predictions or with other input data in your report. by using batch transform to perform these data processing steps, you can often eliminate additional preprocessing or postprocessing. you can use input files in json and cvs format only.  topics the following diagram shows the workflow for associating inferences with input records.  to associate inferences with input data, there are three main steps: filter the input data that is not needed for inference before passing the input data to the batch transform job. use the  parameter to determine which attributes to use as input for the model. associate the input data with the inference results. use the  parameter to combine the input data with the inference. filter the joined data to retain the inputs that are needed to provide context for interpreting the predictions in the reports. use  to store the specified portion of the joined dataset in the output file. when creating a batch transform job with  to process data: specify the portion of the input to pass to the model with the  parameter in the  data structure.  join the raw input data with the transformed data with the  parameter. specify which portion of the joined input and transformed data from the batch transform job to include in the output file with the  parameter. choose either json- or csv-formatted files for input:  for json- or json lines-formatted input files, amazon sagemaker either adds the  attribute to the input file or creates a new json output file with the  and  attributes. for more information, see . for csv-formatted input files, the joined input data is followed by the transformed data and the output is a csv file.if you use an algorithm with the  structure, it must support your chosen format for both input and output files. for example, with the  field of the  api, you must set both the  and  parameters to one of the following values: , , or . the syntax for specifying columns in a csv file and specifying attributes in a json file are different. using the wrong syntax causes an error. for more information, see . for more information about input and output file formats for built-in algorithms, see . the record delimiters for the input and output must also be consistent with your chosen file input. the  parameter indicates how to split the records in the input dataset. the  parameter indicates how to reassemble the records for the output. if you set input and output formats to , you must also set the  and  parameters to . if you set the input and output formats to , you can set both  and  to .  for json files, the attribute name  is reserved for output. the json input file can't have an attribute with this name. if it does, the data in the input file might be overwritten.  to filter and join the input data and inference, use a jsonpath subexpression. amazon sagemaker supports only a subset of the defined jsonpath operators. the following table lists the supported jsonpath operators. for csv data, each row is taken as a json array, so only index based jsonpaths can be applied, e.g. , . csv data should also follow . when using the bracket-notation to specify multiple child elements of a given field, additional nesting of children within brackets is not supported. for example,  is supported while  is not.  for more information about jsonpath operators, see  on github. the following examples show some common ways to join input data with prediction results. topics by default, the  parameter doesn't join inference results with input. it outputs only the inference results. if you want to explicitly specify to not join results with input, use the  and specify the following settings in a transformer call. to output inferences using the aws sdk for python, add the following code to your createtransformjob request. the following code mimics the default behavior. if you're using the , to combine the input data with the inferences in the output file, specify  for the  parameter in a transformer call. if you're using the aws sdk for python (boto 3), join all input data with the inference by adding the following code to your  request. for json or json lines input files, the results are in the  key in the input json file. for example, if the input is a json file that contains the key-value pair , the data transform result might be . amazon sagemaker stores both in the input file in the  key. notethe joined result for json must be a key-value pair object. if the input isn't a key-value pair object, amazon sagemaker creates a new json file. in the new json file, the input data is stored in the  key and the results are stored as the  value. for a csv file, for example, if the record is , and the label result is , then the output file would contain . if you are using the , to include results or an id column in the output, specify indexes of the joined dataset in a transformer call. for example, if your data includes five columns and the first one is the id column, use the following transformer request. if you are using the aws sdk for python (boto 3), add the following code to your  request. to specify columns in amazon sagemaker, use the index of the array elements. the first column is index 0, the second column is index 1, and the sixth column is index 5. to exclude the first column from the input, set  to . the colon () tells amazon sagemaker to include all of the elements between two values, inclusive. for example,  specifies the second through fifth columns. if you omit the number after the colon, for example, , the subset includes all columns from the 6th column through the last column. if you omit the number before the colon, for example, , the subset includes all columns from the first column (index 0) through the sixth column. if you are using the , include results of an id attribute in the output by specifying it in a transformer call. for example, if you store data in the  attribute and the record id in the  attribute, you would use the following transformer request. if you are using the aws sdk for python (boto 3), join all input data with the inference by adding the following code to your  request. warningif you are using a json-formatted input file, the file can't contain the attribute name . this attribute name is reserved for the output file. if your json-formatted input file contains an attribute with this name, values in the input file might be overwritten with the inference. 
 this document will walk you through ways of leveraging amazon sagemaker features using r. this guide introduces amazon sagemaker's built-in r kernel, how to get started with r on amazon sagemaker, and finally several example notebooks. the examples are organized in three levels, beginner, intermediate, and advanced. they start from , continue to end-to-end machine learning with r on amazon sagemaker, and then finish with more advanced topics such as amazon sagemaker processing with r script, and bring-your-own (byo) r algorithm to amazon sagemaker.    amazon sagemaker notebook instances support r using a pre-installed r kernel. also, the r kernel has the reticulate library, an r to python interface, so you can use the features of amazon sagemaker python sdk from within an r script.  is an optional library that you can add to your r kernel to get further functionality.   : provides an r interface to the . the reticulate package translates between r and python objects. : provides an r interface to make api calls to aws services, similar to how  works.  enables python developers to create, configure, and manage aws services, such as ec2 and s3 using r.    using the t2.medium instance type and default storage size. you can pick a faster instance and more storage if you plan to continue using the instance for more advanced examples, or create a bigger instance later. wait until the status of the notebook is in service, and then click open jupyter.    create a new notebook with r kernel from the list of available environments.     when the new notebook is created, you should see an r logo in the upper right corner of the notebook environment, and also r as the kernel under that logo. this indicates that amazon sagemaker has successfully launched the r kernel for this notebook.    alternatively, when you are in a jupyter notebook, you can use kernel menu, and then select r from change kernel option.     prerequisites   : this sample notebook describes how you can develop r scripts using amazon amazon sagemaker‘s r kernel. in this notebook you set up your amazon sagemaker environment and permissions, download the  from the , do some basic processing and visualization on the data, then save the data as .csv format to s3.   beginner level    this sample notebook extends the previous prerequisite getting started notebook. you learn how to train a model on the abalone dataset that predicts abalone age as measured by the number of rings in the shell. after you train your model, you create an endpoint and deploy your model to the endpoint. with your endpoint in place, you can test the model and generate predictions. the reticulate package will be used as an r interface to the .    : this sample notebook describes how to conduct a batch transform job using amazon sagemaker’s transformer api and the . the notebook also uses the abalone dataset.   intermediate level    this sample notebook extends the previous beginner notebooks that use the abalone dataset and xgboost. it describes how to do model tuning with . you will also learn how to use batch transform for batching predictions, as well as how to create a model endpoint to make real-time predictions.    :  lets you preprocess, post-process and run model evaluation workloads. this example shows you how to create an r script to orchestrate a processing job.    advanced level    do you already have an r algorithm, and you want to bring it into amazon sagemaker to tune, train, or deploy it? this example walks you through how to customize amazon sagemaker containers with custom r packages, all the way to using a hosted endpoint for inference on your r-origin model.  
use the following values for the components of the registry urls for the images that provide built-in rules for amazon sagemaker debugger. for account ids, see the following table. ecr repository name: sagemaker-debugger-rules  tag: latest  example of a full registry url:   account ids for built-in rules container images by aws region   
you can use the aws sdk for python (boto) or the amazon sagemaker to create a multi-model endpoint. topics 
*copyright &copy; 2020 amazon web services, inc. and/or its affiliates. all rights reserved.* amazon's trademarks and trade dress may not be used in       connection with any product or service that is not amazon's,       in any manner that is likely to cause confusion among customers,       or in any manner that disparages or discredits amazon. all other       trademarks not owned by amazon are the property of their respective      owners, who may or may not be affiliated with, connected to, or       sponsored by amazon.  +   + 
this rule detects if the tanh and sigmoid activation layers are becoming saturated. an activation layer is saturated when the input of the layer is close to the maximum or minimum of the activation function. the minimum and maximum of the tanh and sigmoid activation functions are defined by their respective  and  values. if the activity of a node drops below the  percentage, it is considered saturated. if more than a  percent of the nodes are saturated, the rule returns . parameter descriptions for the saturatedactivation rule   for an example of how to configure and deploy a built-in rule, see . notethis rule can't be applied to the xgboost algorithm. 
the baseline calculations of statistics and constraints are needed as a standard against which data drift and other data quality issues can be detected. amazon sagemaker model monitor provides a built-in container that provides the ability to suggest the constraints automatically for csv and flat json input. this sagemaker-model-monitor-analyzer container also provides you with a range of model monitoring capabilities, including constraint validation against a baseline, and emitting amazon cloudwatch metrics. this container is based on spark and is built with .  the training dataset that you used to trained the model is usually a good baseline dataset. the training dataset data schema and the inference dataset schema should exactly match (the number and order of the features). note that the prediction/output column(s) are assumed to be the 1st column(s) in the training dataset. from the training dataset, you can ask amazon sagemaker to suggest a set of baseline constraints and generate descriptive statistics to explore the data. for this example, upload the training dataset that was used to train the pretrained model included in this example. if you already have it in amazon s3, you can point to it directly. create a baseline from a training dataset: when you have your training data ready and stored in amazon s3, start a baseline processing job with  using the . this uses an  that generates baseline statistics and suggests baseline constraints for the dataset and writes them to the  location that you specify. noteif you provide the feature/column names in the training dataset as the 1st row and set the  option as in the code sample above, amazon sagemaker uses the feature name in the constraints and statistics file. the baseline statistics for the dataset are contained in the statistics.json file and the suggested baseline constraints are contained in the constraints.json file in the location you specify with . table: output files for tabular dataset statistics and constraints   the  provides convenience functions described to generate the baseline statistics and constraints. but if you want to call processing job directly for this purpose instead, you need to set the  map as in the following example. 
you can use amazon sagemaker to train and deploy a model using custom mxnet code. the  mxnet estimators and models and the amazon sagemaker open-source mxnet container make writing a mxnet script and running it in amazon sagemaker easier. i want to train a custom mxnet model in amazon sagemaker.for a sample jupyter notebook, see .for documentation, see . i have an mxnet model that i trained in amazon sagemaker, and i want to deploy it to a hosted endpoint.. i have an mxnet model that i trained outside of amazon sagemaker, and i want to deploy it to an amazon sagemaker endpoint. i want to see the api documentation for  mxnet classes. i want to see information about amazon sagemaker mxnet containers..  for general information about writing mxnet script mode training scripts and using mxnet script mode estimators and models with amazon sagemaker, see . for information about mxnet versions supported by the amazon sagemaker mxnet container, see . 
to create a 3d point cloud labeling job, you must create an input manifest file. use this topic to learn the formatting requirements of the input manifest file for each task type. to learn about the raw input data formats ground truth accepts for 3d point cloud labeling jobs, see the section . use your  to choose a topics on  to learn about the formatting requirements for each line of your input manifest file. topics 
create a 3d point cloud labeling job to have workers label objects in 3d point clouds generated from 3d sensors like light detection and ranging (lidar) sensors and depth cameras, or generated from 3d reconstruction by stitching images captured by an agent like a drone.  point clouds are made up of three-dimensional (3d) visual data that consists of points. each point is described using three coordinates, typically , , and . to add color or variations in point intensity to the point cloud, points may be described with additional attributes, such as  for intensity or values for the red (), green (), and blue () 8-bit color channels. when you create a ground truth 3d point cloud labeling job, you can provide point cloud and, optionally, sensor fusion data.  the following image shows a single, 3d point cloud scene rendered by ground truth and displayed in the semantic segmentation worker ui.  a light detection and ranging (lidar) sensor is a common type of sensor used to collect measurements that are used to generate point cloud data. lidar is a remote sensing method that uses light in the form of a pulsed laser to measure the distances of objects from the sensor. you can provide 3d point cloud data generated from a lidar sensor for a ground truth 3d point cloud labeling job using the raw data formats described in . ground truth 3d point cloud labeling jobs include a sensor fusion feature that supports video camera sensor fusion for all task types. some sensors come with multiple lidar devices and video cameras that capture images and associate them with a lidar frame. to help annotators visually complete your tasks with high confidence, you can use the ground truth sensor fusion feature to project annotations (labels) from a 3d point cloud to 2d camera images and vice versa using 3d scanner (such as lidar) extrinsic matrix and camera extrinsic and intrinsic matrices. to learn more, see . ground truth provides a user interface (ui) and tools that workers use to label or annotate 3d point clouds. when you use the object detection or semantic segmentation task types, workers can annotate a single point cloud frame. when you use object tracking, workers annotate a sequence of frames. you can use object tracking to track object movement across all frames in a sequence.  the following demonstrates how a worker would use the ground truth worker portal and tools to annotate a 3d point cloud for an object detection task. for similar visual examples of other task types, see .  ground truth offers assistive labeling tools to help workers complete your point cloud annotation tasks faster and with more accuracy. for details about assistive labeling tools that are included in the worker ui for each task type,  and refer to the view the worker task interface section of that page. you can create six types of tasks when you use ground truth 3d point cloud labeling jobs. use the topics in  to learn more about these task types and to learn how to create a labeling job using the task type of your choice.  the 3d point cloud labeling job is different from other ground truth labeling modalities. before creating a labeling job, we recommend that you read .  for an end-to-end demo using the amazon sagemaker api and aws python sdk (boto 3) to create a 3d point cloud labeling job, see  in the . importantif you use a notebook instance created before june 5th, 2020 to run this notebook, you must stop and restart that notebook instance for the notebook to work.  topics 
the amazon sagemaker object2vec algorithm is a general-purpose neural embedding algorithm that is highly customizable. it can learn low-dimensional dense embeddings of high-dimensional objects. the embeddings are learned in a way that preserves the semantics of the relationship between pairs of objects in the original space in the embedding space. you can use the learned embeddings to efficiently compute nearest neighbors of objects and to visualize natural clusters of related objects in low-dimensional space, for example. you can also use the embeddings as features of the corresponding objects in downstream supervised tasks, such as classification or regression.  object2vec generalizes the well-known word2vec embedding technique for words that is optimized in the amazon sagemaker . for a blog post that discusses how to apply object2vec to some practical use cases, see .  topics you can use object2vec on many input data types, including the following examples. to transform the input data into the supported formats, you must preprocess it. currently, object2vec natively supports two types of input:  a discrete token, which is represented as a list of a single . for example, .a sequences of discrete tokens, which is represented as a list of . for example, .the object in each pair can be asymmetric. for example, the pairs can be (token, sequence) or (token, token) or (sequence, sequence). for token inputs, the algorithm supports simple embeddings as compatible encoders. for sequences of token vectors, the algorithm supports the following as encoders: average-pooled embeddingshierarchical convolutional neural networks (cnns),multi-layered bidirectional long short-term memory (bilstms) the input label for each pair can be one of the following: a categorical label that expresses the relationship between the objects in the pair a score that expresses the strength of the similarity between the two objects for categorical labels used in classification, the algorithm supports the cross-entropy loss function. for ratings/score-based labels used in regression, the algorithm supports the mean squared error (mse) loss function. specify these loss functions with the  hyperparameter when you create the model training job. the type of amazon elastic compute cloud (amazon ec2) instance that you use depends on whether you are training or running inferences.  when training a model using the object2vec algorithm on a cpu, start with an ml.m5.2xlarge instance. for training on a gpu, start with an ml.p2.xlarge instance. if the training takes too long on this instance, you can use a larger instance, such as an ml.m5.4xlarge or an ml.m5.12xlarge instance currently, the object2vec algorithm can train only on a single machine. however, it does offer support for multiple gpus. for inference with a trained object2vec model that has a deep neural network, we recommend using ml.p3.2xlarge gpu instance. due to gpu memory scarcity, the  environment variable can be specified to optimize on whether the  or  inference network is loaded into gpu. for a sample notebook that uses the amazon sagemaker object2vec algorithm to encode sequences into fixed-length embeddings, see . for a sample notebook that uses the object2vec algorithm in a multi-label prediction setting to predict the genre of a movie from its plot description, see . for a sample notebook that uses the object2vec algorithm to make movie recommendations, see . for a sample notebook that uses the object2vec algorithm to learn document embeddings, see . for instructions on how to create and access jupyter notebook instances that you can use to run the example in amazon sagemaker, see . after you have created a notebook instance and opened it, choose sagemaker examples to see a list of amazon sagemaker samples. to open a notebook, choose its use tab and choose create copy. 
in the  request, you specify the training algorithm. you can also specify algorithm-specific hyperparameters as string-to-string maps. the following table lists the hyperparameters for the object2vec training algorithm. 
this section shows how to create, describe, stop, and list compilation jobs. the following options are available in neo for managing the compilation jobs for machine learning models: the neo cli, the amazon sagemaker console, or the amazon sagemaker sdk.  topics 
this section explains how amazon sagemaker interacts with a docker container that runs your own inference code for batch transform. use this information to write inference code and create a docker image.  topics to configure a container to run as an executable, use an  instruction in a dockerfile. note the following:  for batch transforms, amazon sagemaker runs the container as: amazon sagemaker overrides default  statements in a container by specifying the  argument after the image name. the  argument overrides arguments that you provide with the  command in the dockerfile.   we recommend that you use the  form of the  instruction: for example:   amazon sagemaker sets environment variables specified in  and  on your container. additionally, the following environment variables will be populated: is always set to  when the container runs in batch transform. is set to the largest size payload that will be sent to the container via http. will be set to  when the container will be sent a single record per call to invocations and  when the container will get as many records as will fit in the payload. is set to the maximum number of  requests that can be opened simultaneously. notethe last three environment variables come from the api call made by the user. if the user doesn’t set values for them, they aren't passed. in that case, either the default values or the values requested by the algorithm (in response to the ) are used.if you plan to use gpu devices for model inferences (by specifying gpu-based ml compute instances in your  request), make sure that your containers are nvidia-docker compatible. don't bundle nvidia drivers with the image. for more information about nvidia-docker, see .  you can't use the  initializer as your entry point in amazon sagemaker containers because it gets confused by the train and serve arguments.in a  request, container definitions includes the  parameter, which identifies the location in amazon s3 where model artifacts are stored. when you use amazon sagemaker to run inferences, it uses this information to determine where to copy the model artifacts from. it copies the artifacts to the  directory in the docker container for use by your inference code. the  parameter must point to a tar.gz file. otherwise, amazon sagemaker can't download the file. if you train a model in amazon sagemaker, it saves the artifacts as a single compressed tar file in amazon s3. if you train a model in another framework, you need to store the model artifacts in amazon s3 as a compressed tar file. amazon sagemaker decompresses this tar file and saves it in the  directory in the container before the batch transform job starts.  containers must implement a web server that responds to invocations and ping requests on port 8080. for batch transforms, you have the option to set algorithms to implement execution-parameters requests to provide a dynamic runtime configuration to amazon sagemaker. amazon sagemaker uses the following endpoints:  —used to periodically check the health of the container. amazon sagemaker waits for an http  status code and an empty body for a successful ping request before sending an invocations request. you might use a ping request to load a model into memory to generate inference when invocations requests are sent.(optional) —allows the algorithm to provide the optimal tuning parameters for a job during runtime. based on the memory and cpus available for a container, the algorithm chooses the appropriate , , and  values for the job.before calling the invocations request, amazon sagemaker attempts to invoke the execution-parameters request. when you create a batch transform job, you can provide values for the , , and  parameters. amazon sagemaker determines the values for these parameters using this order of precedence: the parameter values that you provide when you create the  request, the values that the model container returns when amazon sagemaker invokes the execution-parameters endpoint the parameters default values, listed in the following table. the response for a  execution-parameters request is a json object with keys for , , and  parameters. this is an example of a valid response: to obtain inferences, amazon sagemaker sends a post request to the inference container. the post request body contains data from s3. amazon sagemaker passes the request to the container, and returns the inference result from the container, saving the data from the response to s3. to receive inference requests, the container must have a web server listening on port 8080 and must accept post requests to the  endpoint. your model containers must respond to requests within 600 seconds. the simplest requirement on the container is to respond with an http 200 status code and an empty body. this indicates to amazon sagemaker that the container is ready to accept inference requests at the  endpoint. while the minimum bar is for the container to return a static 200, a container developer can use this functionality to perform deeper checks. the request timeout on  attempts is 2 seconds. 
using multi-algorithm hpo to use multi-algorithm hpo you must add more than one training definition to your hyperparameter tuning job. each training definition holds the configuration options for each algorithm you want to try.    in the console, you add training definitions when you create the hpo tuning job by choosing add training definition, and then following the configuration steps for each algorithm that you want to use.   when you start the configuration steps, please note that the warm start and early stopping features are not available with multi-algorithm hpo. if you want to use these features, you can only tune a single algorithm at a time.   if you’re using an api request, instead of the single , you must provide a list of training definitions using . you must use one or the other, not both.  
a button with an image placed in the center. when the user touches the button, a ripple effect emanates from the center of the button. the following is an example of a liquid template designed for image classification that uses the  element. this template uses javascript to enable workers to report issues with the worker ui. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  the following attributes are supported by this element. a boolean switch that, if present, displays the button as disabled and prevents clicks. a string that specifies the icon to be displayed in the center of the button. the string must be either the name of an icon from the open-source  set, which is pre-loaded, or the url to a custom icon. the following is an example of the syntax that you can use to add an iron-icon to a  html element. replace  with the name of the icon you'd like to use from this .  this element has the following parent and child elements. parent elements: child elements: nonefor more information, see the following. 
the deployment of a neo-compiled model with the cli has three steps. topics for the full syntax of the  api, see . for neo-compiled models, use one of the following values for /, depending on your region and applications:  amazon sagemaker image classification             amazon sagemaker xgboost                 tensorflow : the tensorflow version used must be in  list.               mxnet the mxnet version used must be in  list.               pytorch the pytorch version used must be in  list.             also, if you are using tensorflow, pytorch, or mxnet, add the following key-value pair to /: the script must be packaged as a  file. the  file must contain the training script at the root level. the script must contain two additional functions for neo serving containers: : function that takes in the payload and  of each incoming request and returns a numpy array.: function that takes the prediction results produced by deep learning runtime and returns the response body.neither of these two functions use any functionalities of mxnet, pytorch, or tensorflow. for examples using these functions, see . for the full syntax of the  api, see . you must specify the correct instance type in /. it is imperative that this value matches the instance type specified in your compilation job. for the full syntax of the  api, see .  
you can monitor amazon sagemaker using amazon cloudwatch, which collects raw data and processes it into readable, near real-time metrics. these statistics are kept for 15 months, so that you can access historical information and gain a better perspective on how your web application or service is performing. you can also set alarms that watch for certain thresholds and send notifications or take actions when those thresholds are met. for more information, see . amazon cloudwatch logs enables you to monitor, store, and access your log files from amazon ec2 instances, aws cloudtrail, and other sources. you can collect and track metrics, create customized dashboards, and set alarms that notify you or take actions when a specified metric reaches a threshold that you specify. cloudwatch logs can monitor information in the log files and notify you when certain thresholds are met. you can also archive your log data in highly durable storage. for more information, see . aws cloudtrail provides a record of actions taken by a user, role, or an aws service in amazon sagemaker. using the information collected by cloudtrail, you can determine the request that was made to amazon sagemaker, the ip address from which the request was made, who made the request, when it was made, and additional details. for more information, . notecloudtrail does not monitor calls to . you can create rules in amazon cloudwatch events to react to status changes in status in an amazon sagemaker training, hyperperparameter tuning, or batch transform job. for more information, see . 
a training algorithm indicates whether it succeeded or failed using the exit code of its process.  a successful training execution should exit with an exit code of 0 and an unsuccessful training execution should exit with a non-zero exit code. these will be converted to "completed" and "failed" in the  returned by . this exit code convention is standard and is easily implemented in all languages. for example, in python, you can use  to signal a failure exit and simply running to the end of the main routine will cause python to exit with code 0. in the case of failure, the algorithm can write a description of the failure to the failure file. see next section for details. 
automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many jobs that test a range of hyperparameters on your dataset. you choose the tunable hyperparameters, a range of values for each, and an objective metric. you choose the objective metric from the metrics that the algorithm computes. automatic model tuning searches the hyperparameters chosen to find the combination of values that result in the model that optimizes the objective metric.  noteautomatic model tuning for xgboost 0.90 is only available from the sdks, not from the amazon sagemaker console. for more information about model tuning, see . the xgboost algorithm computes the following nine metrics during training. when tuning the model, choose one of these metrics as the objective to evaluate the model. tune the open-source xgboost model with the following hyperparameters. the hyperparameters that have the greatest effect on xgboost objective metrics are: , , , , and .  
you can use  estimators and  models, and  estimators and  models in  pipelines, as shown in the following example: the parameter  configures spark to serialize to protobuf the "projectedfeatures" column for model training. additionally, spark serializes to protobuf the "label" column by default. because we want to make inferences using the "projectedfeatures" column, we pass the column name into the . the following example shows a transformed : 
you can share your amazon sagemaker studio notebooks with your colleagues. the shared notebook is a copy. after you share your notebook, any changes you make to your original notebook aren't reflected in the shared notebook and any changes your colleague's make in their shared copies of the notebook aren't reflected in your original notebook. if you want to share your latest version, you must create a new snapshot and then share it. the following screenshot shows the menu from a studio notebook.  to share a notebook in the upper-right corner of the notebook, choose share. (optional) in create shareable snapshot, choose any of the following items: include git repo information – includes a link to the git repository that contains the notebook. this enables you and your colleague to collaborate and contribute to the same git repository.include output – includes all notebook output that has been saved. noteif you're an aws sso user and you don't see these options, your aws sso administrator probably disabled the feature. contact your administrator.choose create. after the snapshot is created, choose copy link and then choose close. share the link with your colleague. after selecting your sharing options, you are provided with a url. you can share this link with users that have access to amazon sagemaker studio. when the user opens the url, they're prompted to log in using aws sso or iam authentication. this shared notebook becomes a copy, so changes made by the recipient will not be reproduced in your original notebook. you use a shared notebook in the same way you would with a notebook that you created yourself. when you click a link to a shared notebook for the first time, a read-only version of the notebook opens. to edit the shared notebook, choose create a copy. this copies the shared notebook to your personal storage. the copied notebook launches on an instance of the instance type and sagemaker image that the notebook was using when the sender shared it. if you aren't currently running an instance of the instance type, a new instance is started. customization to the sagemaker image isn't shared. you can also inspect the notebook snapshot by choosing snapshot details. the following are some important considerations about sharing and authentication: if you have an active session, you see a read-only view of the notebook until you choose create a copy.if you don't have an active session, you need to log in.if you use iam to login, after you login, select your user profile then choose open sagemaker studio. then you need to choose the link you were sent.if you use sso to login, after you login the shared notebook is opened automatically in studio.
this section provides information for developers who want to use apache spark for preprocessing data and amazon sagemaker for model training and hosting. for information about supported versions of apache spark, see . amazon sagemaker provides an apache spark library, in both python and scala, that you can use to easily train models in amazon sagemaker using  data frames in your spark clusters. after model training, you can also host the model using amazon sagemaker hosting services.  the amazon sagemaker spark library, , provides the following classes, among others: —extends the  interface. you can use this estimator for model training in amazon sagemaker., , and —extend the  class. —extends the  class. you can use this  for model hosting and obtaining inferences in amazon sagemaker.you have the following options for downloading the spark library provided by amazon sagemaker: you can download the source code for both pyspark and scala libraries from github at .  for the python spark library, you have the following additional options: use pip install: in a notebook instance, create a new notebook that uses either the  or the  kernel and connect to a remote amazon emr cluster.  notethe emr cluster must be configured with an iam role that has the  policy attached. for information about configuring roles for an emr cluster, see  in the amazon emr management guide. you can get the scala library from maven. add the spark library to your project by adding the following dependency to your  file: the following is high-level summary of the steps for integrating your apache spark application with amazon sagemaker. continue data preprocessing using the apache spark library that you are familiar with. your dataset remains a  in your spark cluster. load your data into a  and preprocess it so that you have a  column with  of , and an optional  column with values of ​ type. use the estimator in the amazon sagemaker spark library to train your model. for example, if you choose the k-means algorithm provided by amazon sagemaker for model training, you call the  method.  provide your  as input. the estimator returns a  object.  note extends the . the  method does the following:  converts the input  to the protobuf format by selecting the  and  columns from the input  and uploading the protobuf data to an amazon s3 bucket. the protobuf format is efficient for model training in amazon sagemaker. starts model training in amazon sagemaker by sending an amazon sagemaker  request. after model training has completed, amazon sagemaker saves the model artifacts to an s3 bucket.  amazon sagemaker assumes the iam role that you specified for model training to perform tasks on your behalf. for example, it uses the role to read training data from an s3 bucket and to write model artifacts to a bucket.  creates and returns a  object. the constructor does the following tasks, which are related to deploying your model to amazon sagemaker.  sends a  request to amazon sagemaker.  sends a  request to amazon sagemaker. sends a  request to amazon sagemaker, which then launches the specified resources, and hosts the model on them.  you can get inferences from your model hosted in amazon sagemaker with the .  provide an input  with features as input. the  method transforms it to a  containing inferences. internally, the  method sends a request to the  amazon sagemaker api to get inferences. the  method appends the inferences to the input . 
the amazon sagemaker semantic segmentation algorithm provides a fine-grained, pixel-level approach to developing computer vision applications. it tags every pixel in an image with a class label from a predefined set of classes. tagging is fundamental for understanding scenes, which is critical to an increasing number of computer vision applications, such as self-driving vehicles, medical imaging diagnostics, and robot sensing.  for comparison, the amazon sagemaker  is a supervised learning algorithm that analyzes only whole images, classifying them into one of multiple output categories. the  is a supervised learning algorithm that detects and classifies all instances of an object in an image. it indicates the location and scale of each object in the image with a rectangular bounding box.  because the semantic segmentation algorithm classifies every pixel in an image, it also provides information about the shapes of the objects contained in the image. the segmentation output is represented as an rgb or grayscale image, called a segmentation mask. a segmentation mask is an rgb (or grayscale) image with the same shape as the input image. the amazon sagemaker semantic segmentation algorithm is built using the , and provides you with a choice of three build-in algorithms to train a deep neural network. you can use the , , or .  each of the three algorithms has two distinct components:  the backbone (or encoder)—a network that produces reliable activation maps of features.the decoder—a network that constructs the segmentation mask from the encoded activation maps.you also have a choice of backbones for the fcn, psp, and deeplabv3 algorithms: . these backbones include pretrained artifacts that were originally trained on the  classification task. you can fine-tune these backbones for segmentation using your own data. or, you can initialize and train these networks from scratch using only your own data. the decoders are never pretrained.  to deploy the trained model for inference, use the amazon sagemaker hosting service. during inference, you can request the segmentation mask either as a png image or as a set of probabilities for each class for each pixel. you can use these masks as part of a larger pipeline that includes additional downstream image processing or other applications. topics for a sample jupyter notebook that uses the amazon sagemaker semantic segmentation algorithm to train a model and deploy it to perform inferences, see the . for instructions on how to create and access jupyter notebook instances that you can use to run the example in amazon sagemaker, see .  to see a list of all of the amazon sagemaker samples, create and open a notebook instance, and choose the sagemaker examples tab. the example semantic segmentation notebooks are located under introduction to amazon algorithms. to open a notebook, choose its use tab, and choose create copy. amazon sagemaker semantic segmentation expects the customer's training dataset to be on . once trained, it produces the resulting model artifacts on amazon s3. the input interface format for the amazon sagemaker semantic segmentation is similar to that of most standardized semantic segmentation benchmarking datasets. the dataset in amazon s3 is expected to be presented in two channels, one for  and one for  using four directories, two for images and two for annotations. annotations are expected to be uncompressed png images. the dataset might also have a label map that describes how the annotation mappings are established. if not, the algorithm uses a default. it also supports the augmented manifest image format () for training in pipe input mode straight from amazon s3. for inference, an endpoint accepts images with an  content type.  the training data is split into four directories: , , , and . there is a channel for each of these directories. the dataset also expected to have one  file per channel for  and  respectively. if you don't provide these json files, amazon sagemaker provides the default set label map. the dataset specifying these files should look similar to the following example: every jpg image in the train and validation directories has a corresponding png label image with the same name in the  and  directories. this naming convention helps the algorithm to associate a label with its corresponding image during training. the , , , and  channels are mandatory. the annotations are single-channel png images. the format works as long as the metadata (modes) in the image helps the algorithm read the annotation images into a single-channel 8-bit unsigned integer. for more information on our support for modes, see the . we recommend using the 8-bit pixel, true color  mode.  the image that is encoded is a simple 8-bit integer when using modes. to get from this mapping to a map of a label, the algorithm uses one mapping file per channel, called the label map. the label map is used to map the values in the image with actual label indices. in the default label map, which is provided by default if you don’t provide one, the pixel value in an annotation matrix (image) directly index the label. these images can be grayscale png files or 8-bit indexed png files. the label map file for the unscaled default case is the following:  to provide some contrast for viewing, some annotation software scales the label images by a constant amount. to support this, the amazon sagemaker semantic segmentation algorithm provides a rescaling option to scale down the values to actual label values. when scaling down doesn’t convert the value to an appropriate integer, the algorithm defaults to the greatest integer less than or equal to the scale value. the following code shows how to set the scale value to rescale the label values: the following example shows how this  value is used to rescale the  values of the input annotation image when they are mapped to the  values to be used in training. the label values in the input annotation image are 0, 3, 6, with scale 3, so they are mapped to 0, 1, 2 for training: in some cases, you might need to specify a particular color mapping for each class. use the map option in the label mapping as shown in the following example of a  file: this label mapping for this example is: with label mappings, you can use different annotation systems and annotation software to obtain data without a lot of preprocessing. you can provide one label map per channel. the files for a label map in the  channel must follow the naming conventions for the four directory structure. if you don't provide a label map, the algorithm assumes a scale of 1 (the default). the augmented manifest format enables you to do training in pipe mode using image files without needing to create recordio files. the augmented manifest file contains data objects and should be in  format, as described in the  request. each line in the manifest is an entry containing the amazon s3 uri for the image and the uri for the annotation image. each json object in the manifest file must contain a  key. the  key should contain the value of the amazon s3 uri to the image. the labels are provided under the  parameter value as specified in the  request. it can also contain additional metadata under the metadata tag, but these are ignored by the algorithm. in the example below, the  are contained in the list of image and annotation references . these names must have  appended to them. when using the semantic segmentation algorithm with augmented manifest, the value of the  parameter must be  and value of the  parameter must be . for more information on augmented manifest files, see . you can also seed the training of a new model with a model that you trained previously using amazon sagemaker. this incremental training saves training time when you want to train a new model with the same or similar data. currently, incremental training is supported only for models trained with the built-in amazon sagemaker semantic segmentation. to use your own pre-trained model, specify the  as "model" in the  for the  request. set the  for the model channel to . the , , , and  input parameters that define the network architecture must be consistently specified in the input hyperparameters of the new model and the pre-trained model that you upload to the model channel. for the pretrained model file, you can use the compressed (.tar.gz) artifacts from amazon sagemaker outputs. you can use either recordio or image formats for input data. for more information on incremental training and for instructions on how to use it, see .  to query a trained model that is deployed to an endpoint, you need to provide an image and an  that denotes the type of output required. the endpoint takes jpeg images with an  content type. if you request an  of , the algorithm outputs a png file with a segmentation mask in the same format as the labels themselves. if you request an accept type of, the algorithm returns class probabilities encoded in recordio-protobuf format. the latter format outputs a 3d tensor where the third dimension is the same size as the number of classes. this component denotes the probability of each class label for each pixel. the amazon sagemaker semantic segmentation algorithm only supports gpu instances for training, and we recommend using gpu instances with more memory for training with large batch sizes. the algorithm can be trained using  instances in single machine configurations. it supports the following gpu instances for training: for inference, you can use either cpu instances (such as c5 and m5) and gpu instances (such as p2 and p3) or both. for information about the instance types that provide varying combinations of cpu, gpu, memory, and networking capacity for inference, see . 
stop the training jobs that a hyperparameter tuning job launches early when they are not improving significantly as measured by the objective metric. stopping training jobs early can help reduce compute time and helps you avoid overfitting your model. to configure a hyperparameter tuning job to stop training jobs early, do one of the following: if you are using the aws sdk for python (boto 3), set the  field of the  object that you use to configure the tuning job to .if you are using the , set the  parameter of the  object to .in the amazon sagemaker console, in the create hyperparameter tuning job workflow, under early stopping, choose auto.for a sample notebook that demonstrates how to use early stopping, see  or open the  notebook in the hyperparameter tuning section of the sagemaker examples in a notebook instance. for information about using sample notebooks in a notebook instance, see . when you enable early stopping for a hyperparameter tuning job, amazon sagemaker evaluates each training job the hyperparameter tuning job launches as follows: after each epoch of training, get the value of the objective metric.compute the running average of the objective metric for all previous training jobs up to the same epoch, and then compute the median of all of the running averages.if the value of the objective metric for the current training job is worse (higher when minimizing or lower when maximizing the objective metric) than the median value of running averages of the objective metric for previous training jobs up to the same epoch, amazon sagemaker stops the current training job.to support early stopping, an algorithm must emit objective metrics for each epoch. the following built-in amazon sagemaker algorithms support early stopping: —supported only if you use  as the objective metric.notethis list of built-in algorithms that support early stopping is current as of december 13, 2018. other built-in algorithms might support early stopping in the future. if an algorithm emits a metric that can be used as an objective metric for a hyperparameter tuning job (preferably a validation metric), then it supports early stopping. to use early stopping with your own algorithm, you must write your algorithms such that it emits the value of the objective metric after each epoch. the following list shows how you can do that in different frameworks: tensorflowuse the  class. for information, see . mxnetuse the . for information, see . chainerextend chainer by using the  class. for information, see . pytorch and sparkthere is no high-level support. you must explicitly write your training code so that it computes objective metrics and writes them to logs after each epoch. 
a machine learning algorithm uses example data to create a generalized solution (a "model") that addresses the business question you are trying to answer. after you create a model using example data, you can use it to answer the same business question for a new set of data. you have several choices when choosing an algorithm to use with amazon sagemaker. these choices include: amazon sagemaker provides several built-in machine learning algorithms that you can use for a variety of problem types. for more information on amazon sagemaker built-in algorithms, see .you can use your code to access the most popular machine learning and deep learning framework algorithms to build models in amazon sagemaker. for more information on the machine learning and deep learning frameworks supported by amazon sagemaker, see .you can use your own algorithm or model with amazon sagemaker. for more information on using your own algorithms with amazon sagemaker, see .
amazon sagemaker makes it easy to train machine learning models using managed amazon ec2 spot instances. managed spot training can optimize the cost of training models up to 90% over on-demand instances. amazon sagemaker manages the spot interruptions on your behalf.  managed spot training uses amazon ec2 spot instance to run training jobs instead of on-demand instances. you can specify which training jobs use spot instances and a stopping condition that specifies how long amazon sagemaker waits for a job to run using amazon ec2 spot instances. metrics and logs generated during training runs are available in cloudwatch.  spot instances can be interrupted, causing jobs to take longer to start or finish. you can configure your managed spot training job to use checkpoints. amazon sagemaker copies checkpoint data from a local path to amazon s3. when the job is restarted, amazon sagemaker copies the data from amazon s3 back into the local path. the training can then resume from the last checkpoint instead of restarting. for more information about checkpointing, see . noteunless your training job will complete quickly, we recommend you use checkpointing with managed spot training. sagemaker built-in algorithms and marketplace algorithms that do not checkpoint are currently limited to a  of 3600 seconds (60 minutes).  topics to use managed spot training, create a training job. set  to  and specify the .  must be larger than . for more information about creating a training job, see .  you can calculate the savings from using managed spot training using the formula . for example, if  is 100 and  is 500, the savings is 80%. you can monitor a training job using  and  returned by . the list below shows how  and  values change depending on the training scenario: spot instances acquired with no interruption during training : ↠  ↠  ↠ spot instances interrupted once. later, enough spot instances were acquired to finish the training job. :  ↠  ↠  ↠  ↠  ↠  ↠  ↠  spot instances interrupted twice and  exceeded. :  ↠  ↠  ↠  ↠  ↠  ↠  ↠  ↠  ↠   :   :   spot instances were never launched. :   :   :   
you now have a file in amazon s3 that contains inferences that you got by running a batch transform job in . to validate the model, check a subset of the inferences from the file to see whether they match the actual numbers from the test dataset. to validate the batch transform inferences download the test data from amazon s3. plot the first 10 images from the test dataset with their labels.  download the output from the batch transform job from amazon s3 to a local file. get the first 10 results from the batch transform job. to see if the batch transform job made accurate predictions, check the output from this step against the numbers that you plotted from the test data. you have now trained, deployed, and validated your first model in amazon sagemaker. next step 
to deploy a neo-compiled model to an https endpoint, you must configure and create the endpoint for the model using amazon sagemaker hosting services. currently developers can use amazon sagemaker apis to deploy modules on to ml.c5, ml.c4, ml.m5, ml.m4, ml.p3, ml.p2, and ml.inf1 instances. for , models need to be compiled specifically for ml.inf1 instances. models compiled for other instance types are not guaranteed to work with ml.inf1 instances. for more information on compiling your model, see . when you deploy a compiled model, you need to use the same instance for the target that you used for compilation. this creates an amazon sagemaker endpoint that you can use to perform inferences. there are three options available for deploying neo-compiled models: topics 
to configure autoscaling for a model using the console open the amazon sagemaker console at . in the navigation pane, choose endpoints.  choose the endpoint that you want to configure. for endpoint runtime settings, choose the model variant that you want to configure. for endpoint runtime settings, choose configure autoscaling. the configure variant automatic scaling page appears. for minimum capacity, type the minimum number of instances that you want the scaling policy to maintain. at least 1 instance is required. for maximum capacity, type the maximum number of instances that you want the scaling policy to maintain. for the target value, type the average number of invocations per instance per minute for the model. to determine this value, follow the guidelines in . application auto scaling adds or removes instances to keep the metric close to the value that you specify. for scale-in cool down (seconds) and scale-out cool down (seconds), type the number seconds for each cool down period. assuming that the order in the list is based on either most important to less important of first applied to last applied. select disable scale in to prevent the scaling policy from deleting variant instances if you want to ensure that your variant scales out to address increased traffic, but are not concerned with removing instances to reduce costs when traffic decreases, disable scale-in activities. scale-out activities are always enabled so that the scaling policy can create endpoint instances as needed. choose save. this procedure registers a model as a scalable target with application auto scaling. when you register a model, application auto scaling performs validation checks to ensure the following: the model existsthe permissions are sufficientyou aren't registering a variant with an instance that is a burstable performance instance such as t2 noteamazon sagemaker doesn't support autoscaling for burstable instances such as t2, because they already allow for increased capacity under increased workloads. for information about burstable performance instances, see .
query a trained model by using the model's endpoint. the endpoint takes the following json request format.  in the request, the  field corresponds to the time series that should be forecast by the model.  if the model was trained with categories, you must provide a  for each instance. if the model was trained without the  field, it should be omitted. if the model was trained with a custom feature time series (), you have to provide the same number of values for each instance. each of them should have a length given by , where the last  values correspond to the time points in the future that will be predicted. if the model was trained without custom feature time series, the field should not be included in the request. the  field is optional.  sets the number of sample paths that the model generates to estimate the mean and quantiles.  describes the information that will be returned in the request. valid values are   and . if you specify , each of the quantile values in  is returned as a time series. if you specify , the model also returns the raw samples used to calculate the other outputs. the following is the format of a response, where  are arrays of numbers: deepar has a response timeout of 60 seconds. when passing multiple time series in a single request, the forecasts are generated sequentially. because the forecast for each time series typically takes about 300 to 1000 milliseconds or longer, depending on the model size, passing too many time series in a single request can cause time outs. it's better to send fewer time series per request and send more requests. because the deepar algorithm uses multiple workers per instance, you can achieve much higher throughput by sending multiple requests in parallel. by default, deepar uses one worker per cpu for inference, if there is sufficient memory per cpu. if the model is large and there isn't enough memory to run a model on each cpu, the number of workers is reduced. the number of workers used for inference can be overwritten using the environment variable  for example, by setting ) when calling the amazon sagemaker  api. deepar forecasting supports getting inferences by using batch transform from data using the json lines format. in this format, each record is represented on a single line as a json object, and lines are separated by newline characters. the format is identical to the json lines format used for model training. for information, see . for example: notewhen creating the transformation job with , set the  value to  and set the  value in the  configuration to , as the default values currently cause runtime failures. similar to the hosted endpoint inference request format, the  and the  fields for each instance are required if both of the following are true: the model is trained on a dataset that contained both the  and the  fields.the corresponding  and  values used in the training job are not set to unlike hosted endpoint inference, the configuration field is set once for the entire batch inference job using an environment variable named . the value of  can be passed when the model is created by calling  api. if  is missing in the container environment, the inference container uses the following default: the output is also in json lines format, with one line per prediction, in an order identical to the instance order in the corresponding input file. predictions are encoded as objects identical to the ones returned by responses in online inference mode. for example: note that in the  configuration of the amazon sagemaker  request clients must explicitly set the  value to , as the default value  concatenates all json objects on the same line. for example, here is an amazon sagemaker  request for a deepar job with a custom : 
if you deployed a model to amazon sagemaker hosting services in , you now have an endpoint that you can invoke to get inferences in real time. to validate the model, invoke the endpoint with example images from the test dataset and check whether the inferences you get match the actual labels of the images. topics to validate the model by using the , use the  object that you created in . for information, see . to validate the model () download the test data from amazon s3. plot the first 10 images from the test dataset with their labels.  to get inferences for the first 10 examples in the test dataset, call the  method of the  object. to see if the model is making accurate predictions, check the output from this step against the numbers that you plotted in the previous step. you have now trained, deployed, and validated your first model in amazon sagemaker. next step to use the aws sdk for python (boto 3) to validate the model, call the  method. this method corresponds to the  api provided by the amazon sagemaker runtime. to validate the model (aws sdk for python (boto 3)) download the test data from amazon s3. plot the first 10 images from the test dataset with their labels.  get the amazon sagemaker runtime client, which provides the  method. get inferences from the first 10 examples in the test dataset by calling . to see if the model is making accurate predictions, check the output from this step against the numbers you plotted in the previous step. you have now trained, deployed, and validated your first model in amazon sagemaker. next step 

in the  request, you specify the training algorithm that you want to use. you can also specify algorithm-specific hyperparameters that are used to help estimate the parameters of the model from a training dataset. the following table lists the hyperparameters provided by amazon sagemaker for training the object detection algorithm. for more information about how object training works, see . 
built-in amazon sagemaker model monitor container for cloudwatch metrics: when the  option is  in the baseline constraints file, amazon sagemaker emits these metrics for each feature/column observed in the dataset in the  namespace with  and  dimensions. for numerical fields: metric : max → query for metric : min → query for metric : sum → query for metric : samplecount → query for metric : average → query for for both numerical and string fields: metric: completeness → query for metric: baseline drift → query for 
this rule detects whether the tensors emitted during training have non-finite values, either infinite or nan (not a number). if a non-finite value is detected, the rule returns . parameter descriptions for the explodingtensor rule   for an example of how to configure and deploy a built-in rule, see . notethis rule can't be applied to the xgboost algorithm. 
customers can specify aws kms keys, including bring your own keys (byok), to use for envelope encryption with amazon s3 input/output buckets and machine learning (ml) amazon ebs volumes. ml volumes for notebook instances and for processing, training, and hosted model docker containers can be optionally encrypted by using aws kms customer-owned keys. all instance os volumes are encrypted with an aws-managed aws kms key.  notecertain nitro-based instances include local storage, dependent on the instance type. local storage volumes are encrypted using a hardware module on the instance. you can't request a  when using an instance type with local storage.for a list of instance types that support local instance storage, see .for more information about local instance storage encryption, see .for more information about storage volumes on nitro-based instances, see . for information about aws kms keys see  in the aws key management service developer guide. 
you can monitor amazon sagemaker using amazon cloudwatch, which collects raw data and processes it into readable, near real-time metrics. these statistics are kept for 15 months, so that you can access historical information and gain a better perspective on how your web application or service is performing. however, the amazon cloudwatch console limits the search to metrics that were updated in the last 2 weeks. this limitation ensures that the most current jobs are shown in your namespace. to graph metrics without using a search, specify its exact name in the source view. you can also set alarms that watch for certain thresholds, and send notifications or take actions when those thresholds are met. for more information, see the . amazon sagemaker model training jobs and endpoints write cloudwatch metrics and logs. the following tables list the metrics and dimensions for amazon sagemaker. endpoint invocation metrics  the  namespace includes the following request metrics from calls to . metrics are available at a 1-minute frequency. for information about how long cloudwatch metrics are retained for, see  in the amazon cloudwatch api reference. dimensions for endpoint invocation metrics multi-model endpoint model loading metrics  the  namespace includes the following model loading metrics from calls to  . metrics are available at a 1-minute frequency. for information about how long cloudwatch metrics are retained for, see  in the amazon cloudwatch api reference. dimensions for multi-model endpoint model loading metrics multi-model endpoint model instance metrics  the  namespaces include the following instance metrics from calls to . metrics are available at a 1-minute frequency. for information about how long cloudwatch metrics are retained for, see  in the amazon cloudwatch api reference. dimensions for multi-model endpoint model loading metrics processing job, training job, batch transform job, and endpoint instance metrics the , ,  and  namespaces include the following metrics for the training jobs and endpoint instances. metrics are available at a 1-minute frequency. dimensions for processing job, training job and batch transform job instance metrics amazon sagemaker ground truth metrics dimensions for dataset object metrics 
this rule measures the depth of trees in an xgboost model. xgboost rejects splits if they don't improve loss. this regularizes the training. as a result, the tree might not grow as deep as defined in . this rule is valid only for the xgboost algorithm. for an example of how to configure and deploy a built-in rule, see . parameter descriptions for the treedepth rule   
by default, when you create a notebook instance, users that log into that notebook instance have root access. data science is an iterative process that might require the data scientist to test and use different software tools and packages, so many notebook instance users need to have root access to be able to install these tools and packages. because users with root access have administrator privileges, users can access and edit all files on a notebook instance with root access enabled. if you don't want users to have root access to a notebook instance, when you call  or  operations, set the  field to . you can also disable root access for users when you create or update a notebook instance in the amazon sagemaker console. for information, see . notelifecycle configurations need root access to be able to set up a notebook instance. because of this, lifecycle configurations associated with a notebook instance always run with root access even if you disable root access for users. 
amazon sagemaker notebook instances come with multiple environments already installed. these environments contain jupyter kernels and python packages including: scikit, pandas, numpy, tensorflow, and mxnet. these environments, along with all files in the  folder, are refreshed when you stop and start a notebook instance. you can also install your own environments that contain your choice of packages and kernels. this is typically done using  or . the different jupyter kernels in amazon sagemaker notebook instances are separate conda environments. for information about conda environments, see  in the conda documentation. for an example of creating a custom kernel in an amazon sagemaker studio notebook, see . if you want to use an external library in a specific kernel, install the library in the environment for that kernel. you can do this either in the terminal or in a notebook cell. the following procedures show how to install theano so that you can use it in a notebook with a conda_mxnet_p36`` kernel. to install theano from a terminal open a notebook instance. in the jupyter dashboard, choose new, and then choose terminal. in the terminal, type the following commands: to install theano from a jupyter notebook cell open a notebook instance. in the jupyter dashboard, choose new, and then choose conda_mxnet_p36. in a cell in the new notebook, type the following command: amazon sagemaker periodically updates the python and dependency versions in the environments installed on a notebook instance when it is stopped and restarted. for more information, see . to maintain an isolated python environment that does not change versions, create a lifecycle configuration that runs each time you start your notebook instance. for information about creating lifecycle configurations, see . the following example lifecycle configuration script installs miniconda on your notebook instance. this allows you to create environments in your notebook instance with specific versions of python and dependencies that amazon sagemaker does not update: you can also add a sandboxed python installation as a kernel that you can use in a jupyter notebook by including the following code to the above lifecycle configuration: 
in the  request, you specify the training algorithm. you can also specify algorithm-specific hyperparameters as string-to-string maps. the following table lists the hyperparameters for the amazon sagemaker ip insights algorithm. 
ground truth enables you to use your own private workforce to work on labeling jobs. a private workforce is an abstract concept and it refers to a set of people who work for you. each labeling job is created using a work team, composed of workers in your workforce. ground truth supports private workforce creation using amazon cognito.  a ground truth workforce maps to a cognito user pool. a ground truth work team maps to a coginto user group. coginito manages the worker authentication. cognito supports open id connection (oidc) and customers can set up cognito federation with their own identity provider (idp).  ground truth only allows one workforce per account per aws region. each workforce has a dedicated ground truth work portal login url.  you can also restrict workers to a classless inter-domain routing (cidr) block/ip address range. this means annotators must be on a specific network to access the annotation site. you can add up to four cidr blocks for one workforce. to learn more, see . to learn how you can create a private workforce, see . amazon sagemaker ground truth work teams fall into one of three : public (with amazon mechanical turk), private, and vendor. to restrict iam user access to a specific work team using one of these types or the work team arn, use the  and/or the  condition keys. for the  condition key, use . for the  condition key, use . if the user attempts to create a labeling job with a restricted work team, amazon sagemaker returns an access denied error.  the policies below demonstrate different ways to use the  and  condition keys with appropriate condition operators and valid condition values.  the following example uses the  condition key with the  condition operator to restrict access to a public work team. it accepts condition values in the following format: , where workforcetype can equal , , or . the following policies show how to restrict access to a public work team using the  condition key. the first shows how to use it with a valid iam regex-variant of the work team arn and the  condition operator. the second shows how to use it with the  condition operator and the work team arn. 
amazon sagemaker runs processing jobs in an amazon virtual private cloud by default. however, processing containers access aws resources—such as the amazon s3 buckets where you store data—over the internet. to control access to your data and processing containers, we recommend that you create a private vpc and configure it so that they aren't accessible over the internet. for information about creating and configuring a vpc, see  in the amazon vpc user guide. using a vpc helps to protect your processing containers and data because you can configure your vpc so that it is not connected to the internet. using a vpc also allows you to monitor all network traffic in and out of your processing containers by using vpc flow logs. for more information, see  in the amazon vpc user guide. you specify your private vpc configuration when you create processing jobs by specifying subnets and security groups. when you specify the subnets and security groups, amazon sagemaker creates elastic network interfaces (enis) that are associated with your security groups in one of the subnets. enis allow your processing containers to connect to resources in your vpc. for information about enis, see  in the amazon vpc user guide. to specify subnets and security groups in your private vpc, use the  request parameter of the  api, or provide this information when you create a processing job in the amazon sagemaker console. amazon sagemaker uses this information to create enis and attach them to your processing containers. the enis provide your processing containers with a network connection within your vpc that is not connected to the internet. they also enable your processing job to connect to resources in your private vpc. the following is an example of the  parameter that you include in your call to : when configuring the private vpc for your amazon sagemaker processing jobs, use the following guidelines. for information about setting up a vpc, see  in the amazon vpc user guide. topics your vpc subnets should have at least two private ip addresses for each instance in a processing job. for more information, see  in the amazon vpc user guide. if you configure your vpc so that processing containers don't have access to the internet, they can't connect to the amazon s3 buckets that contain your data unless you create a vpc endpoint that allows access. by creating a vpc endpoint, you allow your processing containers to access the buckets where you store your data. we recommend that you also create a custom policy that allows only requests from your private vpc to access to your s3 buckets. for more information, see . to create an s3 vpc endpoint: open the amazon vpc console at . in the navigation pane, choose endpoints, then choose create endpoint for service name, choose com.amazonaws.region.s3, where region is the name of the region where your vpc resides. for vpc, choose the vpc you want to use for this endpoint. for configure route tables, select the route tables to be used by the endpoint. the vpc service automatically adds a route to each route table you select that points any s3 traffic to the new endpoint. for policy, choose full access to allow full access to the s3 service by any user or service within the vpc. choose custom to restrict access further. for information, see . the default endpoint policy allows full access to s3 for any user or service in your vpc. to further restrict access to s3, create a custom endpoint policy. for more information, see . you can also use a bucket policy to restrict access to your s3 buckets to only traffic that comes from your amazon vpc. for information, see . the default endpoint policy allows users to install packages from the amazon linux and amazon linux 2 repositories on the processing container. if you don't want users to install packages from that repository, create a custom endpoint policy that explicitly denies access to the amazon linux and amazon linux 2 repositories. the following is an example of a policy that denies access to these repositories: use default dns settings for your endpoint route table, so that standard amazon s3 urls (for example, ) resolve. if you don't use default dns settings, ensure that the urls that you use to specify the locations of the data in your processing jobs resolve by configuring the endpoint route tables. for information about vpc endpoint route tables, see  in the amazon vpc user guide. in distributed processing, you must allow communication between the different containers in the same processing job. to do that, configure a rule for your security group that allows inbound connections between members of the same security group for information, see . processing jobs that use that vpc do not have access to resources outside your vpc. if your processing job needs access to resources outside your vpc, provide access with one of the following options: if your processing job needs access to an aws service that supports interface vpc endpoints, create an endpoint to connect to that service. for a list of services that support interface endpoints, see  in the amazon vpc user guide. for information about creating an interface vpc endpoint, see  in the amazon vpc user guide.if your processing job needs access to an aws service that doesn't support interface vpc endpoints or to a resource outside of aws, create a nat gateway and configure your security groups to allow outbound connections. for information about setting up a nat gateway for your vpc, see  in the amazon virtual private cloud user guide.
k-means is an algorithm that trains a model that groups similar objects together. the k-means algorithm accomplishes this by mapping each observation in the input dataset to a point in the n-dimensional space (where n is the number of attributes of the observation). for example, your dataset might contain observations of temperature and humidity in a particular location, which are mapped to points (t, h) in 2-dimensional space.  noteclustering algorithms are unsupervised. in unsupervised learning, labels that might be associated with the objects in the training dataset aren't used.  in k-means clustering, each cluster has a center. during model training, the k-means algorithm uses the distance of the point that corresponds to each observation in the dataset to the cluster centers as the basis for clustering. you choose the number of clusters (k) to create.  for example, suppose that you want to create a model to recognize handwritten digits and you choose the mnist dataset for training. the dataset provides thousands of images of handwritten digits (0 through 9). in this example, you might choose to create 10 clusters, one for each digit (0, 1, …, 9). as part of model training, the k-means algorithm groups the input images into 10 clusters. each image in the mnist dataset is a 28x28-pixel image, with a total of 784 pixels. each image corresponds to a point in a 784-dimensional space, similar to a point in a 2-dimensional space (x,y). to find a cluster to which a point belongs, the k-means algorithm finds the distance of that point from all of the cluster centers. it then chooses the cluster with the closest center as the cluster to which the image belongs.  noteamazon sagemaker uses a customized version of the algorithm where, instead of specifying that the algorithm create k clusters, you might choose to improve model accuracy by specifying extra cluster centers (k = k\x). however, the algorithm ultimately reduces these to k* clusters. in amazon sagemaker, you specify the number of clusters when creating a training job. for more information, see . in the request body, you add the  string map to specify the  and  strings. the following is a summary of how k-means works for model training in amazon sagemaker: it determines the initial k cluster centers.  notein the following topics, k clusters refer to k \ x, where you specify k and x* when creating a model training job.  it iterates over input training data and recalculates cluster centers. it reduces resulting clusters to k (if the data scientist specified the creation of k\x* clusters in the request).  the following sections also explain some of the parameters that a data scientist might specify to configure a model training job as part of the  string map.  topics when using k-means in amazon sagemaker, the initial cluster centers are chosen from the observations in a small, randomly sampled batch. choose one of the following strategies to determine how these initial cluster centers are selected: the random approach—randomly choose k observations in your input dataset as cluster centers. for example, you might choose a cluster center that points to the 784-dimensional space that corresponds to any 10 images in the mnist training dataset. the k-means++ approach, which works as follows:  start with one cluster and determine its center. you randomly select an observation from your training dataset and use the point corresponding to the observation as the cluster center. for example, in the mnist dataset, randomly choose a handwritten digit image. then choose the point in the 784-dimensional space that corresponds to the image as your cluster center. this is cluster center 1. determine the center for cluster 2. from the remaining observations in the training dataset, pick an observation at random. choose one that is different than the one you previously selected. this observation corresponds to a point that is far away from cluster center 1. using the mnist dataset as an example, you do the following: for each of the remaining images, find the distance of the corresponding point from cluster center 1. square the distance and assign a probability that is proportional to the square of the distance. that way, an image that is different from the one that you previously selected has a higher probability of getting selected as cluster center 2. choose one of the images randomly, based on probabilities assigned in the previous step. the point that corresponds to the image is cluster center 2.repeat step 2 to find cluster center 3. this time, find the distances of the remaining images from cluster center 2. repeat the process until you have the k cluster centers. to train a model in amazon sagemaker, you create a training job. in the request, you provide configuration information by specifying the following  string maps: to specify the number of clusters to create, add the  string.for greater accuracy, add the optional  string. to specify the strategy that you want to use to determine the initial cluster centers, add the  string and set its value to  or .for more information, see . for an example, see .  you now have an initial set of cluster centers.  the cluster centers that you created in the preceding step are mostly random, with some consideration for the training dataset. in this step, you use the training dataset to move these centers toward the true cluster centers. the algorithm iterates over the training dataset, and recalculates the k cluster centers. read a mini-batch of observations (a small, randomly chosen subset of all records) from the training dataset and do the following.  notewhen creating a model training job, you specify the batch size in the  string in the  string map.  assign all of the observations in the mini-batch to one of the clusters with the closest cluster center. calculate the number of observations assigned to each cluster. then, calculate the proportion of new points assigned per cluster. for example, consider the following clusters: cluster c1 = 100 previously assigned points. you added 25 points from the mini-batch in this step. cluster c2 = 150 previously assigned points. you added 40 points from the mini-batch in this step. cluster c3 = 450 previously assigned points. you added 5 points from the mini-batch in this step. calculate the proportion of new points assigned to each of clusters as follows: compute the center of the new points added to each cluster: compute the weighted average to find the updated cluster centers as follows: read the next mini-batch, and repeat step 1 to recalculate the cluster centers.  for more information about mini-batch k-means, see ). if the algorithm created k clusters—(k = k\x) where x is greater than 1—then it reduces the k clusters to k clusters. (for more information, see  in the preceding discussion.) it does this by applying lloyd's method with  initialization to the k* cluster centers. for more information about lloyd's method, see .  
for a sample notebook that shows you how to use a custom rule to monitor your training job with a tf.keras resnet example, see .  the following code sample shows how to configure a custom  rule using the open source with the . this example assumes that the custom rule you’ve written has path /rules/custom_rules.py. you do not need to specify a pre-built docker image when using the  as the  handle this task.  
create custom instructions for labeling jobs to improve your worker's accuracy in completing their task. you can modify the default instructions that are provided in the console or you can create your own. the instructions are shown to the worker on the page where they complete their labeling task. there are two kinds of instructions: short instructions—instructions that are shown on the same webpage where the worker completes their task. these instructions should provide an easy reference to show the worker the correct way to label an object.full instructions—instructions that are shown on a dialog box that overlays the page where the worker completes their task. we recommend that you provide detailed instructions for completing the task with multiple examples showing edge cases and other difficult situations for labeling objects.create instructions in the console when you are creating your labeling job. start with the existing instructions for the task and use the editor to modify them to suit your labeling job. noteonce you create your labeling job, it will automatically start and you will not be able to modify your worker instructions. if you need to change your worker instructions, stop the labeling job that you created, clone it, and modify your worker instructions before creating a new job.you can clone a labeling job in the console by selecting the labeling job and then selecting clone in the actions menu.to clone a labeling job using the amazon sagemaker api or your preferred amazon sagemaker sdk, make a new request to the  operation with the same specifications as your original job after modifying your worker instructions.  short instructions appear on the same web page that workers use to label your data object. for example, the following is the editing page for a bounding box task. the short instructions panel is on the left.  keep in mind that a worker will only spend seconds looking at the short instructions. workers must be able to scan and understand your information quickly. in all cases it should take less time to understand the instructions than it takes to complete the task. keep these points in mind: your instructions should be clear and simple.pictures are better than words. create a simple illustration of your task that your workers can immediately understand.if you must use words, use short, concise examples.your short instructions are more important than your full instructions.the amazon sagemaker ground truth console provides an editor so that you can create your short instructions. replace the placeholder text and images with instructions for your task. preview the worker's task page by choosing preview. the preview will open in a new window, be sure to turn off pop-up blocking so that the window will show. you can provide additional instructions for your workers in a dialog box that overlays the page where workers label your data objects. use full instructions to explain more complex tasks and to show workers the proper way to label edge cases or other difficult objects. you can create full instructions using an editor in the ground truth console. as with quick instructions, keep the following in mind: workers will want detailed instruction the first few times that the complete your task. any information that they must have should be in the quick instructions.pictures are more important than words.text should be concise.full instructions should supplement the short instructions. don't repeat information that appears in the short instructions.the ground truth console provides an editor so that you can create your full instructions. replace the placeholder text and images with instructions for your task. preview the full instruction page by choosing preview. the preview will open in a new window, be sure to turn off pop-up blocking so that the window will show. images provide useful examples for your workers. to add a publicly accessible image to your instructions: place the cursor where the image should go in the instructions editor.click the image icon in the editor toolbar.enter the url of your image.if your instruction image in amazon s3 is not publicly accessible: as the image url, enter: .this renders the image url with a short-lived, one-time access code appended so the worker's browser can display it. a broken image icon is displayed in the instructions editor, but previewing the tool displays the image in the rendered preview.
to use aws cloudformation to create a monitoring schedule, use an aws cloudformation custom resource. the custom resource is in python. to deploy it, see . start by adding a custom resource to your aws cloudformation template. this will point to a aws lambda function that you create next.  this resource allows you to customize the parameters for the monitoring schedule you can add or remove more parameters by modifying the aws cloudformation resource and the lambda function in the following example resource. this aws cloudformation custom resource uses the  aws library, which you can install with pip using .  this lambda function is invoked by aws cloudformation during the creation and deletion of the stack. this lambda function is responsible for creating and deleting the monitoring schedule and using the parameters defined in the custom resource described in the preceding section. 
the following diagram shows how you train and deploy a model with amazon sagemaker:   the area labeled amazon sagemaker highlights the two components of amazon sagemaker: model training and model deployment. to train a model in amazon sagemaker, you create a training job. the training job includes the following information: the url of the amazon simple storage service (amazon s3) bucket where you've stored the training data.the compute resources that you want amazon sagemaker to use for model training. compute resources are ml compute instances that are managed by amazon sagemaker.the url of the s3 bucket where you want to store the output of the job.the amazon elastic container registry path where the training code is stored. for more information, see .you have the following options for a training algorithm: use an algorithm provided by amazon sagemaker—amazon sagemaker provides training algorithms. if one of these meets your needs, it's a great out-of-the-box solution for quick model training. for a list of algorithms provided by amazon sagemaker, see . to try an exercise that uses an algorithm provided by amazon sagemaker, see .use amazon sagemaker debugger—to inspect training parameters and data throughout the training process when working with the tensorflow, pytorch, and apache mxnet learning frameworks or the xgboost algorithm. debugger automatically detects and alerts users to commonly occurring errors such as parameter values getting too large or small. for more information about using debugger, see . debugger sample notebooks are available at .use apache spark with amazon sagemaker—amazon sagemaker provides a library that you can use in apache spark to train models with amazon sagemaker. using the library provided by amazon sagemaker is similar to using apache spark mllib. for more information, see .submit custom code to train with deep learning frameworks—you can submit custom python code that uses tensorflow, pytorch, or apache mxnet for model training. for more information, see , , and .use your own custom algorithms—put your code together as a docker image and specify the registry path of the image in an amazon sagemaker  api call. for more information, see .use an algorithm that you subscribe to from aws marketplace—for information, see .after you create the training job, amazon sagemaker launches the ml compute instances and uses the training code and the training dataset to train the model. it saves the resulting model artifacts and other output in the s3 bucket you specified for that purpose.  you can create a training job with the amazon sagemaker console or the api. for information about creating a training job with the api, see the  api.  when you create a training job with the api, amazon sagemaker replicates the entire dataset on ml compute instances by default. to make amazon sagemaker replicate a subset of the data on each ml compute instance, you must set the  field to . you can set this field using the low-level sdk. for more information, see  in .  importantto prevent your algorithm container from contending for memory, you should reserve some memory for amazon sagemaker critical system processes on your ml compute instances. if the algorithm container is allowed to use memory needed for system processes, it can trigger a system failure.  
this tutorial demonstrates how to visually track and compare trials in a model training experiment using amazon sagemaker studio. the basis of the tutorial is the mnist handwritten digits classification experiment (mnist) example notebook. it is intended that this topic be viewed alongside studio with the mnist notebook open. as you run through the cells, the sections in this document highlight the relevant code and show you how to observe the results in studio. some of the code snippets have been edited for brevity. for a tutorial that showcases additional features of studio, see . prerequisites a local copy of the  example notebook and the companion  file. both are available from the  repository. to download the files, choose each link, right-click on the raw button, and then choose save as.an aws sso or iam account to sign-on to amazon sagemaker studio. for more information, see .topics to open the notebook sign-on to studio. in the left sidebar, choose the file browser icon (  ). at the top of the file browser pane, choose the up arrow icon and then a file upload dialog opens. browse to and choose your local versions of the mnist-handwritten-digits-classification-experiment.ipynb and mnist.py files, and then choose open. the two files are listed in the file browser. double-click the uploaded notebook file to open the notebook in a new tab. at the top right of the notebook, make sure the kernel is python 3 (data science). if not, choose the current kernel name to open the select kernel dropdown. choose python 3 (data science) and then choose select. the amazon sagemaker experiments sdk is separate from the , which comes preinstalled in amazon sagemaker studio. run the first few cells in the notebook to install the experiments sdk and import the experiments modules. the relevant sections of the notebook cells are displayed below. for more information on the experiments sdk, see . the next few cells create an amazon s3 bucket and a folder in the bucket named mnist. in studio, the file browser displays the mnist folder. the input data is downloaded to the mnist/mnist/raw folder, normalized, and then the transformed data is uploaded to the mnist/mnist/processed folder. you can drill down into the mnist folder to display, but not open, the data files. your screen should look similar to the following:  the last cell in the dataset section creates a  for the transform job. the tracker logs the normalization parameters and the uri of the amazon s3 bucket where the transformed dataset is stored. in a later section, we show how to find this information in studio. in the next section, the tracker is used to track the experiment and trial runs. the following procedure creates and tracks an experiment to determine the effect of the model's  hyperparameter. as part of the experiment, five trials are created inside a loop, one for each value of the  hyperparameter. later in the notebook, you'll compare the results of these five trials. in the left sidebar of studio, choose the sagemaker experiment list icon (  ) to display the experiments browser. run the following cell. output: after the code runs, the experiments list contains an entry for the experiment. it might take a moment to display and you might have to refresh the experiments list. your screen should look similar to the following: run the following cell. after the code runs, the experiments list contains an entry labeled unassigned trial components. the trial component entry is the data preprocessing step previously created. double-click the trial component for verification. the trial component isn't associated with an experiment at this time. your screen should look similar to the following: choose the  icon in the navigation breadcrumb at the top on the experiments browser. from there, double-click on your experiment to display a list of the trials in the experiment. the following code create trials for the experiment. each trial trains a model using a different number for the  hyperparameter. the preprocessing trial component is added to each trial for complete tracking (for example, for auditing purposes). the code also specifies definitions for the following metrics: train losstest losstest accuracythe definitions tell amazon sagemaker to capture those metrics from the algorithm's log output. the metrics are used later to evaluate and compare the models. the trial list automatically updates as each training job runs. it takes a few minutes for each trial to be displayed. your screen should look similar to the following: this section deviates from the notebook and shows you how to compare and analyze the trained models using the amazon sagemaker studio ui. to view the details of a trial double-click one of the trials to display a list of the trial components associated with the trial. there's a preprocessing job and training job for each trial. double-click one of the components to open a new tab that displays information about each component. under trial stages, choose preprocessing. on the describe trial component menu, choose parameters to display the normalization parameters that were previously logged. next, choose artifacts to display the uri of the amazon s3 bucket where the transformed dataset was stored. under trial stages, choose training. on the describe trial component menu, choose the following items to display information about the training job trial component. metrics – , , and parameters – hyperparameter values and instance informationartifacts – amazon s3 storage for the input dataset and the output modelaws settings – job name, arn, status, creation time, training time, billable time, instance information, and othersto view a list of trials ordered by  choose the experiment name on the navigation breadcrumb above trial components to display the trial list. choose all five trials. hold down the ctrl/cmd key and select each trial. right-click the selection and then choose open in trial component list. a new tab opens that displays each trial and trial component. if the table properties pane isn't open, choose the settings icon (  ) in the upper right corner to open it. deselect everything except trial, metrics, and training job. choose the settings icon to close the pane. choose the test:accuracy column header to sort the list by decreasing maximum test accuracy. your screen should look similar to the following: to view a chart of  versus  in the trial components pane, choose all five trials and then choose add chart. select inside the chart area to open the preferences pane for chart properties. in chart properties, choose the following: data type - summary statisticschart type - linex-axis - hidden-channelsy-axis - test:lost_lastcolor - noneyour screen should look similar to the following: to avoid incurring unnecessary charges, delete the resources you created after you're done with the tutorial. you can't delete the experiment resources through the amazon sagemaker management console or the studio ui. the following code shows how to clean up these resources using the experiments sdk. to delete the experiment, first you must delete all trials in the experiment. to delete a trial, first you must remove all trial components from the trial. to delete a trial component, first you must remove the component from all trials. notetrial components can exist independent of trials and experiments. you might want keep them if you plan on further exploration. if so, comment out  to cleanup  to cleanup any experiment for information on deleting your amazon s3 buckets, see . to delete the notebook choose the notebook tab. on the top menu, choose file and then choose close and shutdown notebook. right-click the notebook file and choose delete. 
to use aws sso authentication, you must belong to an aws sso organization. if you don't belong to an aws sso organization, you can create one with the following procedure. to create an aws sso organization on the amazon sagemaker studio control panel, under get started, choose standard setup. choose the set up an account link to open the aws single sign-on (aws sso) console. choose enable aws sso, and then choose create aws organization. when your organization has been created, the aws sso dashboard opens. aws also sends you email to verify the email address associated with the organization. to add a user to your aws sso organization, in the navigation pane, choose users. on the users page, choose add user. under user details, specify all required fields. for password, choose send an email to the user. choose next: groups and then add user. aws sends an email to the user inviting them to create a password and activate their aws sso account. to add more users, repeat steps 4 through 6. return to amazon sagemaker studio to continue to onboard using aws sso authentication. 
use the instructions in this topic only if one of the following applies to you: you want to use a customized role or permission policy.you want to use a vpc for your hosted model or notebook instance.noteif you already have an execution role that has the  managed policy attached (this is true for any iam role that you create when you create a notebook instance, training job, or model in the console) and you are not connecting to an ei model or notebook instance in a vpc, you do not need to make any of these changes to use ei in amazon sagemaker. topics to use ei in amazon sagemaker, the role that you use to open a notebook instance or create a deployable model must have a policy with the required permissions attached. you can attach the  managed policy, which contains the required permissions, to the role, or you can add a custom policy that has the required permissions. for information about creating an iam role, see  in the aws identity and access management user guide. for information about attaching a policy to a role, see  . add these permissions specifically for connecting ei in an iam policy. the following iam policy is the complete list of required permissions to use ei in amazon sagemaker. to use ei with amazon sagemaker in a vpc, you need to create and configure two security groups, and set up a privatelink vpc interface endpoint. ei uses vpc interface endpoint to communicate with amazon sagemaker endpoints in your vpc. the security groups you create are used to connect to the vpc interface endpoint. to use ei within a vpc, you need to create two security groups: a security group to control access to the vpc interface endpoint that you will set up for ei.a security group that allows amazon sagemaker to call into the first security group.to configure the two security groups create a security group with no outbound connections. you will attach this to the vpc endpoint interface you create in the next section. create a second security group with no inbound connections, but with an outbound connection to the first security group. edit the first security group to allow inbound connections only to the second security group an all outbound connections. for more information about vpc security groups, see  in the amazon virtual private cloud user guide. to use ei with amazon sagemaker in a custom vpc, you need to set up a vpc interface endpoint (privatelink) for the ei service. set up a vpc interface endpoint (privatelink) for the ei. follow the instructions at . in the list of services, choose com.amazonaws..elastic-inference.runtime. for security group, make sure you select the first security group you created in the previous section to the endpoint.when you set up the interface endpoint, choose all of the availability zones where ei is available. ei fails if you do not set up at least two availability zones. for information about vpc subnets, see .
as a managed service, amazon sagemaker is protected by the aws global network security procedures that are described in the  whitepaper. you use aws published api calls to access amazon sagemaker through the network. clients must support transport layer security (tls) 1.0 or later. we recommend tls 1.2 or later. clients must also support cipher suites with perfect forward secrecy (pfs) such as ephemeral diffie-hellman (dhe) or elliptic curve ephemeral diffie-hellman (ecdhe). most modern systems such as java 7 and later support these modes. additionally, requests must be signed by using an access key id and a secret access key that is associated with an iam principal. or you can use the  (aws sts) to generate temporary security credentials to sign requests. topics to meet our security requirements, algorithms and model packages listed in aws marketplace are scanned for common vulnerabilities and exposures (cve). cve is a list of publicly known information about security vulnerability and exposure. the national vulnerability database (nvd) provides cve details such as severity, impact rating, and fix information. both cve and nvd are available for public consumption and free for security tools and services to use. for more information, see .  
the amazon sagemaker k-nearest neighbors algorithm is a supervised algorithm. the algorithm consumes a test data set and emits a metric about the accuracy for a classification task or about the mean squared error for a regression task. these accuracy metrics compare the model predictions for their respective task to the ground truth provided by the empirical test data. to find the best model that reports the highest accuracy or lowest error on the test dataset, run a hyperparameter tuning job for k-nn.  automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many jobs that test a range of hyperparameters on your dataset. you choose the tunable hyperparameters, a range of values for each, and an objective metric. you choose the objective metric appropriate for the prediction task of the algorithm. automatic model tuning searches the hyperparameters chosen to find the combination of values that result in the model that optimizes the objective metric. the hyperparameters are used only to help estimate model parameters and are not used by the trained model to make predictions. for more information about model tuning, see . the k-nearest neighbors algorithm computes one of two metrics in the following table during training depending on the type of task specified by the  hyper-parameter.  classifier specifies a classification task and computes  regressor specifies a regression task and computes .choose the  value appropriate for the type of task undertaken to calculate the relevant objective metric when tuning a model. tune the amazon sagemaker k-nearest neighbor model with the following hyperparameters. 
amazon sagemaker studio extends the jupyterlab interface. previous users of jupyterlab will notice the similarity of the user interface, including the workspace. studio adds many additions to the interface. the most prominent additions are detailed in the following sections. for an overview of the basic jupyterlab interface, see . the following image shows sagemaker studio with the file browser open and the studio landing page displayed. the studio version is shown on the bottom-left of the landing page. to update to the latest version, see .  at the top of the screen is the menu bar. at the left of the screen is the left sidebar which contains icons to open file browsers, resource browsers, and tools. at the right of the screen is the right sidebar, represented by the settings icon (  ), which displays contextual property settings when open. above the settings icon, there's a button to provide feedback about your experiences with sagemaker studio. at the bottom of the screen is the status bar. the main work area is divided horizontally into two panes. the left pane is the file and resource browser. the right pane contains one or more tabs for resources such as notebooks, terminals, metrics, and graphs. topics the left sidebar includes the following icons. when you hover over an icon, a tooltip displays the icon name. when you choose an icon, the file and resource browser displays the described functionality. for hierarchical entries, a selectable breadcrumb at the top of the browser shows your location in the hierarchy. the file and resource browser displays lists of your notebooks, experiments, trials, trial components, and endpoints. on the menu at the top of the file browser, choose the plus (+) sign to open the studio launcher. the launcher allows you to create a notebook, launch a python interactive shell, or open a terminal. the main work area consists of multiple tabs that contain your open notebooks and terminals, and detailed information about your experiments and endpoints. the settings pane allows you to adjust table and chart properties. by default, the pane is hidden on the far right of the screen. to open the pane, choose the settings icon (  ) on the top right of the screen. 

use the built-in rules provided by amazon sagemaker debugger to analyze tensors emitted during the training of machine learning models. these rules monitor various common conditions that are critical for the success of a training job. there are four scopes of validity for the built-in rules. scopes of validity for build-in rules   topics 
all amazon sagemaker built-in algorithms adhere to the common input inference format described in . the following are the available output formats for the amazon sagemaker linear learner algorithm. binary classification multiclass classification regression binary classification multiclass classification regression binary classification multiclass classification regression 
automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many jobs that test a range of hyperparameters on your dataset. you choose the tunable hyperparameters, a range of values for each, and an objective metric. you choose the objective metric from the metrics that the algorithm computes. automatic model tuning searches the hyperparameters chosen to find the combination of values that result in the model that optimizes the objective metric. lda is an unsupervised topic modeling algorithm that attempts to describe a set of observations (documents) as a mixture of different categories (topics). the “per-word log-likelihood” (pwll) metric measures the likelihood that a learned set of topics (an lda model) accurately describes a test document dataset. larger values of pwll indicate that the test data is more likely to be described by the lda model. for more information about model tuning, see . the lda algorithm reports on a single metric during training: . when tuning a model, choose this metric as the objective metric. you can tune the following hyperparameters for the lda algorithm. both hyperparameters,  and , can affect the lda objective metric (). if you don't already know the optimal values for these hyperparameters, which maximize per-word log-likelihood and produce an accurate lda model, automatic model tuning can help find them. 
crowd html elements are web components that provide a number of task widgets and design elements that you can tailor to the question you want to ask. you can use these crowd elements to create a custom worker template and integrate it with an amazon augmented ai (amazon a2i) human review workflow to customize the worker console and instructions.  for a list of all html crowd elements available to amazon a2i users, see . to see examples of templates, see the , which contains over 60 sample custom task templates. when in the console to test how your template process incoming data, you can test the look and feel of your template's html and custom elements in your browser by adding the following code to the top of your html file. this loads the necessary code to render the custom html elements. use this code if you want to develop your template's look and feel in your preferred editor instead of in the console. this code won't parse your variables. you might want to replace them with sample content while developing locally. amazon augmented ai custom templates enable you to embed external scripts and style sheets. for example, the following header embeds a  style sheet name  located at  into the custom template. example   if you encounter errors, ensure that your originating server is sending the correct mime type and encoding headers with the assets. for example, the mime and encoding types for remote scripts is . the mime and encoding type for remote stylesheets is . when building a custom template, you must add variables to it to represent the pieces of data that might change from task to task, or worker to worker. if you're starting with one of the sample templates, you need to make sure you're aware of the variables it already uses.  for example, for a custom template that integrates an augmented ai human review loop with a amazon textract text review task,  is used for initial-value input data. for amazon augmented ai (amazon a2i) integration with amazon rekognition ,  is used. for a custom task type, you need to determine the input parameter for your task type. use  where you specify .  all custom templates begin and end with the  elements. like standard html  elements, all of your form code should go between these elements.   for a amazon textract document analysis task, use the  element. it uses the following attributes:   – specifies the url of the image file to be annotated. – sets initial values for attributes found in the worker ui. (required) – determines the kind of analysis that the workers can do. only  is currently supported.  (required) – specifies new keys and the associated text value that the worker can add. (required) – prevents the workers from editing the keys of annotations passed through . – prevents workers from editing the polygons of annotations passed through .for children of the  element, you must have two regions. you can use arbitrary html and css elements in these regions.   – instructions that are available from the view full instructions link in the tool. you can leave this blank, but we recommend that you provide complete instructions to get better results. – a brief description of the task that appears in the tool's sidebar. you can leave this blank, but we recommend that you provide complete instructions to get better results. an amazon textract template would look similar to the following. example   all custom templates begin and end with the  elements. like standard html  elements, all of your form code should go between these elements. for an amazon rekognition custom task template, use the  element. this element supports the following attributes:   – an array of strings or an array of objects where each object has a  field.if the categories come in as objects, the following applies:the displayed categories are the value of the  field. the returned answer contains the full objects of any selected categories.if the categories come in as strings, the following applies:the returned answer is an array of all the strings that were selected. – by setting this attribute, you create a button underneath the categories in the ui. when a user presses the button, all categories are deselected and disabled. if the worker presses the button again, you re-enable users to choose categories. if the worker submits the task by selecting the submit button after you pressing the button, that task will return an empty array.for children of the  element, you must have three regions.  – instructions that are available from the view full instructions link in the tool. you can leave this blank, but we recommend that you provide complete instructions to get better results. – brief description of the task that appears in the tool's sidebar. you can leave this blank, but we recommend that you provide complete instructions to get better results.a template using these elements would look similar to the following. the custom template system uses  for automation. liquid is an open-source inline markup language. for more information and documentation, see the . in liquid, the text between single curly braces and percent symbols is an instruction or tag that creates control flow. text between double curly braces is a variable or object that outputs its value. in addition to the standard liquid filters and actions, amazon augmented ai (amazon a2i) offers a few additional filters. you apply filters by placing a pipe () character after the variable name, and then specifying a filter name. to chain filters use the following format. example   by default, inputs are html-escaped to prevent confusion between your variable text and html. you can explicitly add the  filter to make it more obvious to someone reading the source of your template that escaping is being done.  ensures that if you've already escaped your code, it doesn't get re-escaped again. for example, to ensure that  doesn't become .  is useful when your content is meant to be used as html. for example, you might have a few paragraphs of text and some images in the full instructions for a bounding box. use  sparingly. as a best practice for templates, avoid passing in functional code or markup with  unless you are absolutely sure that you have strict control over what's being passed. if you're passing user input, you could be opening your workers up to a cross-site scripting attack.  encodes data that you provide to javascript object notation (json). if you provide an object, it serializes it.  takes an amazon simple storage service (amazon s3) uri and encodes it into an https url with a short-lived access token for that resource. this makes it possible to display photo, audio, or video objects stored in s3 buckets that are not otherwise publicly accessible to workers. example example of the to_json and grant_read_access filtersinput   exampleoutput   example example of an automated classification template.to automate this simple text classification sample, include the liquid tag . this example uses the  element.   to preview a custom worker task template, use the amazon sagemaker  operation. you can use the  operation with the aws cli or your preferred aws sdk. for documentation on the supported language specific sdk's for this api operation use the  section of the .  prerequisites to preview your worker task template, the aws identity and access management (iam) role amazon resource name (arn), or , that you use must have permission to access to the s3 objects that are used by the template. to learn how to configure your role or user see . to preview your worker task template using the  operation: provide a  of the role with required policies attached to preview your custom template.  in the  parameter of , provide a json object that contains values for the variables defined in the template. these are the variables that are substituted for the  variable. for example, if you define a variable task.input.text in your template, you can supply the variable in the json object as "text": "sample text". in the  parameter of , insert your template. once you've configured , use your prefered sdk or the aws cli to submit a request to render your template. if your request was successful, the response will include , a liquid template that renders the html for the worker ui. importantto preview your template, you need an iam role with permissions to read amazon s3 objects that get rendered on your user interface. for a sample policy that you can attach to your iam role to grant these permissions, see .  
during the analysis phase of the automl job, two notebooks are created that describe the plan that autopilot follows to generate candidate models. a candidate model consists of a (pipeline, algorithm) pair. first, there’s a data exploration notebook, that describes what autopilot learned about the data that you provided. second, there’s a candidate generation notebook, which uses the information about the data to generate candidates.  you can run both notebooks in sagemaker or locally if you have installed the sagemaker python sdk. you can share the notebooks just like any other sagemaker studio notebook. the notebooks are created for you to conduct experiment. for example, you could edit the following items in the notebooks: the preprocessors used on the data the number of hyperparameter optimization (hpo) runs and their parallelismthe algorithms to trythe instance types used for the hpo jobsthe hyperparameter rangesmodifications to the candidate generation notebook are encouraged to be used as a learning tool. this capability allows you to learn about how the decisions made during the machine learning process impact the your results.  notewhen you run the notebooks in your default instance you incur baseline costs, but when you execute hpo jobs from the candidate notebook, these jobs use additional compute resources that incur additional costs.  a notebook is produced during the analysis phase of the automl job that helps you identify problems in your dataset. it identifies specific areas for investigation in order to help you identify upstream problems with your data that may result in a suboptimal model.  the candidate generation notebook contains each suggested preprocessing step, algorithm, and hyperparameter ranges. if you chose just to produce the notebook and not to run the automl job, you can then decide which candidates are to be trained and tuned. they optimize automatically and a final, best candidate will be identified. if you ran the job directly without seeing the candidates first, then only best candidate is displayed when you open the notebook after the completion of the job. 
amazon sagemaker provides metrics for endpoints so you can monitor the cache hit rate, the number of models loaded, and the model wait times for loading, downloading, and uploading at a multi-model endpoint. for information, see multi-model endpoint model loading metrics and multi-model endpoint model instance metrics in . per-model metrics aren't supported.  
to get started using amazon sagemaker ground truth, follow the instructions in the following sections. the sections here explain how to use the console to create a labeling job, assign a public or private workforce, and send the labeling job to your workforce. you can also learn how to monitor the progress of a labeling job. if you want to create a custom labeling job, see  for instructions. before you create a labeling job, you must upload your dataset to an amazon s3 bucket. for more information, see . topics 
if the  value is  in the  map in the  file, the container code emits amazon cloudwatch metrics in this location: .  the schema for this file is closely based on the cloudwatch  api. the namespace is not specified here. it defaults to . however, you can specify dimensions. we recommend that you add the  and  dimensions at a minimum. 
in the point cloud sequence input manifest file, each line in the manifest contains a sequence of point cloud frames. the point cloud data for each frame in the sequence can either be stored in binary or ascii format. for more information, see . this is the manifest file formatting required for 3d point cloud object tracking. optionally, you can also provide point attribute and camera sensor fusion data for each point cloud frame. when you create a sequence input manifest file, you must provide lidar and video camera sensor fusion data in a .  the following example demonstrates the syntax used for an input manifest file when each line in the manifest is a sequence file.  the data for each sequence of point cloud frames needs to be stored in a json data object. the following is an example of the format you use for a sequence file. information about each frame is included as a json object and is listed in the  list. in this example, information is given for a single frame, and ... is used to indicated where you should include information for additional frames.  the following table provides details about the top-level parameters shown in the this code example. for detailed information about the parameters required for individual frames in the sequence file, see . the following table shows the parameters you can include in your input manifest file. use the ego-vehicle location to provide information about the pose of the vehicle used to capture point cloud data. ground truth use this information to compute lidar extrinsic matrices.  ground truth uses extrinsic matrices to project labels to and from the 3d scene and 2d images. for more information, see . the following table provides more information about the  and orientation () parameters that are required when you provide ego-vehicle information.  if you want to include color camera data with a frame, use the following parameters to provide information about each image. the required column in the following table applies when the  parameter is included in the input manifest file. you are not required to include images in your input manifest file.  if you include camera images, you must include information about the  and orientation () of the camera used the capture the images.  if your images are distorted, ground truth can automatically undistort them using information you provide about the image in your input manifest file, including distortion coefficients (, , , , , ), camera model and focal length (, ), and the principal point (, . to learn more about these coefficients and undistorting images, see . if distortion coefficients are not included, ground truth will not undistort an image.  you can include up to 100,000 point cloud frame sequences in your input manifest file. you can include up to 500 point cloud frames in each sequence file.  keep in mind that 3d point cloud labeling job have longer pre-processing times than other ground truth task types. for more information, see . 
by default, iam users and roles don't have permission to create or modify amazon sagemaker resources. they also can't perform tasks using the aws management console, aws cli, or aws api. an iam administrator must create iam policies that grant users and roles permission to perform specific api operations on the specified resources they need. the administrator must then attach those policies to the iam users or groups that require those permissions. to learn how to attach policies to an iam user or group, see  in the iam user guide. to learn how to create an iam identity-based policy using these example json policy documents, see  in the iam user guide. topics identity-based policies are very powerful. they determine whether someone can create, access, or delete amazon sagemaker resources in your account. these actions can incur costs for your aws account. when you create or edit identity-based policies, follow these guidelines and recommendations: get started using aws managed policies – to start using amazon sagemaker quickly, use aws managed policies to give your employees the permissions they need. these policies are already available in your account and are maintained and updated by aws. for more information, see  in the iam user guide.grant least privilege – when you create custom policies, grant only the permissions required to perform a task. start with a minimum set of permissions and grant additional permissions as necessary. doing so is more secure than starting with permissions that are too lenient and then trying to tighten them later. for more information, see  in the iam user guide.enable mfa for sensitive operations – for extra security, require iam users to use multi-factor authentication (mfa) to access sensitive resources or api operations. for more information, see  in the iam user guide.use policy conditions for extra security – to the extent that it's practical, define the conditions under which your identity-based policies allow access to a resource. for example, you can write conditions to specify a range of allowable ip addresses that a request must come from. you can also write conditions to allow requests only within a specified date or time range, or to require the use of ssl or mfa. for more information, see  in the iam user guide.to access the amazon sagemaker console, you must have a minimum set of permissions. these permissions must allow you to list and view details about the amazon sagemaker resources in your aws account. if you create an identity-based policy that is more restrictive than the minimum required permissions, the console won't function as intended for entities (iam users or roles) with that policy. to ensure that those entities can still use the amazon sagemaker console, also attach the following aws managed policy to the entities. for more information, see  in the iam user guide: you don't need to allow minimum console permissions for users that are making calls only to the aws cli or the aws api. instead, allow access to only the actions that match the api operation that you're trying to perform. topics the permissions reference table lists the amazon sagemaker api operations and shows the required permissions for each operation. for more information about amazon sagemaker api operations, see . to use the amazon sagemaker console, you need to grant permissions for additional actions. specifically, the console needs permissions that allow the  actions to display subnets, vpcs, and security groups. optionally, the console needs permission to create execution roles for tasks such as , , and . grant these permissions with the following permissions policy: to use the amazon sagemaker ground truth console, you need to grant permissions for additional resources. specifically, the console needs permissions for the aws marketplace to view subscriptions, amazon cognito operations to manage your private workforce, amazon s3 actions for access to your input and output files, and aws lambda actions to list and invoke functions. grant these permissions with the following permissions policy: to use the augmented ai console, you need to grant permissions for additional resources. grant these permissions with the following permissions policy: this example shows how you might create a policy that allows iam users to view the inline and managed policies that are attached to their user identity. this policy includes permissions to complete this action on the console or programmatically using the aws cli or aws api. control fine-grained access to allow the creation of amazon sagemaker resources by using amazon sagemaker-specific condition keys. for information about using condition keys in iam policies, see  in the iam user guide. the following table lists the amazon sagemaker condition keys. the condition keys, along with related api actions, and links to relevant documentation are also listed in  in the iam user guide. amazon sagemaker file system condition keys   the following examples show how to use the amazon sagemaker condition keys to control access. topics amazon sagemaker training provides a secure infrastructure for the training algorithm to run in, but for some cases you may want increased defense in depth. for example, you minimize the risk of running untrusted code in your algorithm, or you have specific security mandates in your organization. for these scenarios, you can use the service-specific condition keys in the condition element of an iam policy to scope down the user to specific file systems, directories, access modes (read-write, read-only) and security groups. topics the policy below restricts an iam user to the  and  directories of an efs file system to  (read-only) accessmode: notewhen a directory is allowed, all of its subdirectories are also accessible by the training algorithm. posix permissions are ignored. to prevent a malicious algorithm using a user space client from accessing any file system directly in your account, you can restrict networking traffic by allowing ingress from a specific security group. in the following example, the iam user can only use the specified security group to access the file system: although the above example can restrict an algorithm to a specific file system, it does not prevent an algorithm from accessing any directory within that file system using the user space client. to mitigate this, you can: ensure that the file system only contains data that you trust your iam users to accesscreate an iam role that restricts your iam users to launching training jobs with algorithms from approved ecr repositoriesfor more information on how to use roles with amazon sagemaker, see .  restrict an aws user to creating training jobs from within a amazon vpc. when a training job is created within a vpc, you can use vpc flow logs to monitor all traffic to and from the training cluster. for information about using vpc flow logs, see  in the amazon virtual private cloud user guide. the following policy enforces that a training job is created by an iam user calling  from within a vpc: amazon sagemaker ground truth and amazon augmented ai work teams fall into one of three : public (with amazon mechanical turk), private, and vendor. to restrict iam user access to a specific work team using one of these types or the work team arn, use the  and/or the  condition keys. for the  condition key, use . for the  condition key, use . if the user attempts to create a labeling job with a restricted work team, amazon sagemaker returns an access denied error.  the policies below demonstrate different ways to use the  and  condition keys with appropriate condition operators and valid condition values.  the following example uses the  condition key with the  condition operator to restrict access to a public work team. it accepts condition values in the following format: , where workforcetype can equal , , or . the following policies show how to restrict access to a public work team using the  condition key. the first shows how to use it with a valid iam regex-variant of the work team arn and the  condition operator. the second shows how to use it with the  condition operator and the work team arn. the following policy restricts an iam user to specify a aws kms key to encrypt input data when creating training, hyperparameter tuning, and labeling jobs by using the  condition key: the following policy restricts an iam user to specify a aws kms key to encrypt the attached storage volume when creating or updating a notebook instance by using the  condition key: the following policy restricts an iam user to enable network isolation when creating training jobs by using the  condition key: the following policy restricts an iam user to use a specific instance type when creating training jobs by using the  condition key: the following policy restricts an iam user to use a specific elastic inference (ei) accelerator, if an accelerator is provided, when creating or updating notebook instances and when creating endpoint configurations by using the  condition key: you can disable both internet access and root access to notebook instances to help make them more secure. for information about controlling root access to a notebook instance, see . for information about disabling internet access for a notebook instance, see . the following policy requires an iam user to disable network access when creating instance, and disable root access when creating or updating a notebook instance.  to control access to amazon sagemaker api calls and calls to amazon sagemaker hosted endpoints, use identity-based iam policies. topics if you set up an interface endpoint in your vpc, individuals outside the vpc can still connect to the amazon sagemaker api and runtime over the internet unless you attach an iam policy that restricts access to calls coming from within the vpc to all users and groups that have access to your amazon sagemaker resources. for information about creating a vpc interface endpoint for the amazon sagemaker api and runtime, see . importantif you apply an iam policy similar to one of the following, users can't access the specified amazon sagemaker apis through the console. to restrict access to only connections made from within your vpc, create an aws identity and access management policy that restricts access to only calls that come from within your vpc. then add that policy to every aws identity and access management user, group, or role used to access the amazon sagemaker api or runtime. notethis policy allows connections only to callers within a subnet where you created an interface endpoint. if you want to restrict access to the api to only calls made using the interface endpoint, use the  condition key instead of : to allow access to amazon sagemaker api calls and runtime invocations only from ip addresses in a list that you specify, attach an iam policy that denies access to the api unless the call comes from an ip address in the list to every aws identity and access management user, group, or role used to access the api or runtime. for information about creating iam policies, see  in the aws identity and access management user guide. to specify the list of ip addresses that you want to have access to the api call, use the  condition operator and the  condition context key. for information about iam condition operators, see  in the aws identity and access management user guide. for information about iam condition context keys, see . for example, the following policy allows access to the  only from ip addresses in the ranges - and -: to allow access to a notebook instance only from ip addresses in a list that you specify, attach an iam policy that denies access to  unless the call comes from an ip address in the list to every aws identity and access management user, group, or role used to access the notebook instance. for information about creating iam policies, see  in the aws identity and access management user guide. to specify the list of ip addresses that you want to have access to the notebook instance, use the  condition operator and the  condition context key. for information about iam condition operators, see  in the aws identity and access management user guide. for information about iam condition context keys, see . for example, the following policy allows access to a notebook instance only from ip addresses in the ranges - and -: the policy restricts access to both the call to  and to the url that the call returns. the policy also restricts access to opening a notebook instance in the console and is enforced for every http request and websocket frame that attempts to connect to the notebook instance. noteusing this method to filter by ip address is incompatible when . for information about restricting access to a notebook instance when connecting through a vpc interface endpoint, see . control access to groups of amazon sagemaker resources by attaching tags to the resources and specifying  conditions in iam policies. notetag-based policies don't work to restrict the following api calls:listalgorithms listcoderepositories listcompilationjobs listendpointconfigs listendpoints listflowdefinitions listhumantaskuis listhyperparametertuningjobs listlabelingjobs listlabelingjobsforworkteam listmodelpackages listmodels listnotebookinstancelifecycleconfigs listnotebookinstances listsubscribedworkteams listtags listprocessingjobs listtrainingjobs listtrainingjobsforhyperparametertuningjob listtransformjobs listworkteams search for example, suppose you've defined two different iam groups, named  and , in your aws account. suppose also that you've created 10 notebook instances, 5 of which are used for one project, and 5 of which are used for a second project. you want to allow members of  to make api calls on notebook instances used for the first project, and members of  to make api calls on notebook instances used for the second project. to control access to api calls (example) add a tag with the key  and value  to the notebook instances used for the first project. for information about adding tags to amazon sagemaker resources, see .  add a tag with the key  and value  to the notebook instances used for the second project. create an iam policy with a  condition that denies access to the notebook instances used for the second project, and attach that policy to . the following is an example of a policy that denies all api calls on any notebook instance that has a tag with a key of  and a value of : for information about creating iam policies and attaching them to identities, see  in the aws identity and access management user guide. create an iam policy with a  condition that denies access to the notebook instances used for the first project, and attach that policy to . the following is an example of a policy that denies all api calls on any notebook instance that has a tag with a key of  and a value of : require the presence or absence of specific tags or specific tag values by using  condition keys in an iam policy. for example, if you want to require that every endpoint created by any member of an iam group to be created with a tag with the key  and value , create a policy as follows: you can add tags to a hyperparameter tuning job when you create the tuning job by specifying the tags as the  parameter when you call . if you do this, the tags you specify for the hyperparameter tuning job are also added to all training jobs that the hyperparameter tuning job launches. if you add tags to a hyperparameter tuning job by calling , the tags you add are also added to any training jobs that the hyperparameter tuning job launches after you call , but are not added to training jobs the hyperparameter tuning jobs launched before you called . similarly, when you remove tags from a hyperparameter tuning job by calling , those tags are not removed from training jobs that the hyperparameter tuning job launched previously. because of this, the tags associated with training jobs can be out of sync with the tags associated with the hyperparameter tuning job that launched them. if you use tags to control access to a hyperparameter tuning job and the training jobs it launches, you might want to keep the tags in sync. to make sure the tags associated with training jobs stay sync with the tags associated with the hyperparameter tuning job that launched them, first call  for the hyperparameter tuning job to get a list of the training jobs that the hyperparameter tuning job launched. then, call  or  for the hyperparameter tuning job and for each of the training jobs in the list of training jobs to add or delete the same set of tags for all of the jobs. the following python example demonstrates this: 
when you have created your custom ui template (step 2) and processing lambda functions (step 3), you should place the template in an amazon s3 bucket with a file name format of: . use the  action to configure your task. you'll use the location of a custom template () stored in a  file on s3 as the value for the  field in the  object within the  object. for the aws lambda tasks described in , the post-annotation task's arn will be used as the value for the  field, and the pre-annotation task will be used as the value for the   
a floating button with an image in its center.  the following is an example of a liquid template designed for image classification that uses the  element. this template uses javascript to enable workers to report issues with the worker ui. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  the following attributes are supported by this element. a boolean switch that, if present, displays the floating button as disabled and prevents clicks. a string that specifies the icon to be displayed in the center of the button. the string must be either the name of an icon from the open-source  set, which is pre-loaded, or the url to a custom icon. the following is an example of the syntax that you can use to add an iron-icon to a  html element. replace  with the name of the icon you'd like to use from this .  a string consisting of a single character that can be used instead of an icon. emojis or multiple characters may result in the button displaying an ellipsis instead. a string that will display as a tool tip when the mouse hovers over the button. this element has the following parent and child elements. parent elements: child elements: nonefor more information, see the following. 
ground truth uses your 3d point cloud data to render a 3d scenes that workers annotate. this section describes the raw data formats that are accepted for point cloud data and sensor fusion data for a point cloud frame. for each frame, ground truth supports compact binary pack format (.bin) and ascii (.txt) files. these files contain information about the location (, , and  coordinates) of all points that make up that frame, and, optionally, information about the pixel color of each point for colored point clouds. when you create a 3d point cloud labeling job input manifest file, you can specify the format of your raw data in the  parameter.  the following table lists elements that ground truth supports in point cloud frame files to describe individual points.  ground truth assumes the following about your input data: all of the positional coordinates (x, y, z) are in meters. all the pose headings (qx, qy, qz, qw) are measured in spatial  .the compact binary pack format represents a point cloud as an ordered set of a stream of points. each point in the stream is an ordered binary pack of 4-byte float values in some variant of the form . the , , and  elements are required and additional information about that pixel can be included in a variety of ways using , , , and .  to use a binary file to input point cloud frame data to a ground truth 3d point cloud labeling job, enter  in the  parameter for your input manifest file and replace format` parameter.   – when you use this format, your point element stream would be in the following order:  – when you use this format, your point element stream would be in the following order:  – when you use this format, your point element stream would be in the following order: when you use a binary file for your point cloud frame data, if you do not enter a value for , the default pack format  is used.  the ascii format uses a text file to represent a point cloud, where each line in the ascii point cloud file represents a single point. each point is a line the text file and contains white space separated values, each of which is a 4-byte float ascii values. the , , and  elements are required for each point and additional information about that point can be included in a variety of ways using , , , and . to use a text file to input point cloud frame data to a ground truth 3d point cloud labeling job, enter  in the  parameter for your input manifest file and replace `` with the order of point elements on each line.  for example, if you enter  for , your text file for each point cloud frame should look similar to the following:  if you enter , your text file should look similar to the following:  when you use a text file for your point cloud frame data, if you do not enter a value for , the default format  will be used.  ground truth does not have a resolution limit for 3d point cloud frames. however, we recommend that you limit each point cloud frame to 500k points for optimal performance. when ground truth renders the 3d point cloud visualization, it must be viewable on your workers' computers, which depends on workers' computer hardware. point cloud frames that are larger than 1 million points may not render on standard machines, or may take too long to load.  
automatic model tuning, also called hyperparameter tuning, finds the best version of a model by running many jobs that test a range of hyperparameters on your dataset. you choose the tunable hyperparameters, a range of values for each, and an objective metric. you choose the objective metric from the metrics that the algorithm computes. automatic model tuning searches the hyperparameters chosen to find the combination of values that result in the model that optimizes the objective metric. for more information about model tuning, see . the amazon sagemaker ip insights algorithm is an unsupervised learning algorithm that learns associations between ip addresses and entities. the algorithm trains a discriminator model , which learns to separate observed data points (positive samples) from randomly generated data points (negative samples). automatic model tuning on ip insights helps you find the model that can most accurately distinguish between unlabeled validation data and automatically generated negative samples. the model accuracy on the validation dataset is measured by the area under the receiver operating characteristic curve. this  metric can take values between 0.0 and 1.0, where 1.0 indicates perfect accuracy. the ip insights algorithm computes a  metric during validation, the value of which is used as the objective function to optimize for hyperparameter tuning. you can tune the following hyperparameters for the amazon sagemaker ip insights algorithm.  
use batch transform when you need to do the following:  preprocess datasets to remove noise or bias that interferes with training or inference from your dataset.get inferences from large datasets.run inference when you don't need a persistent endpoint.associate input records with inferences to assist the interpretation of results.to filter input data before performing inferences or to associate input records with inferences about those records, see . for example, you can filter input data to provide context for creating and interpreting reports about the output data. for more information about batch transforms, see . topics batch transform automatically manages the processing of large datasets within the limits of specified parameters. for example, suppose that you have a dataset file, , stored in an s3 bucket. the content of the input file might look like the following.:  when a batch transform job starts, amazon sagemaker initializes compute instances and distributes the inference or preprocessing workload between them. batch transform partitions the amazon s3 objects in the input by key and maps amazon s3 objects to instances. when you have multiples files, one instance might process , and another instance might process the file named .  to keep large payloads below the  limit, you can split an input file into several mini-batches. for example, you might create a mini-batch from  by including only two of the records. noteamazon sagemaker processes each input file separately. it doesn't combine mini-batches from different input files to comply with the  limit. to split input files into mini-batches, when you create a batch transform job, set the  parameter value to . if  is set to  or if an input file can't be split into mini-batches, amazon sagemaker uses the entire input file in a single request.  if the batch transform job successfully processes all of the records in an input file, it creates an output file with the same name and the  file extension. for multiple input files, such as  and , the output files are named  and . the batch transform job stores the output files in the specified location in amazon s3, such as .  the predictions in an output file are listed in the same order as the corresponding records in the input file. the output file , based on the input file shown earlier, would look like the following. to combine the results of multiple output files into a single output file, set the  parameter to . when the input data is very large and is transmitted using http chunked encoding, to stream the data to the algorithm, set  to . amazon sagemaker built-in algorithms don't support this feature. for information about using the api to create a batch transform job, see the  api. for more information about the correlation between batch transform input and output objects, see . for an example of how to use batch transform, see . if you are using the  api, you can reduce the time it takes to complete batch transform jobs by using optimal values for parameters such as , , or . if you are using the amazon sagemaker console, you can specify these optimal parameter values in the additional configuration section of the batch transform job configuration page. amazon sagemaker automatically finds the optimal parameter settings for built-in algorithms. for custom algorithms, provide these values through an  endpoint. to test different models or various hyperparameter settings, create a separate transform job for each new model variant and use a validation dataset. for each transform job, specify a unique model name and location in amazon s3 for the output file. to analyze the results, use . amazon sagemaker uses the amazon s3  to upload results from a batch transform job to amazon s3. if an error occurs, the uploaded results are removed from amazon s3. in some cases, such as when a network outage occurs, an incomplete multipart upload might remain in amazon s3. to avoid incurring storage charges, we recommend that you add the  to the s3 bucket lifecycle rules. this policy deletes incomplete multipart uploads that might be stored in the s3 bucket. for more information, see . if a batch transform job fails to process an input file because of a problem with the dataset, amazon sagemaker marks the job as . if an input file contains a bad record, the transform job doesn't create an output file for that input file because doing so prevents it from maintaining the same order in the transformed data as in the input file. when your dataset has multiple input files, a transform job continues to process input files even if it fails to process one. the processed files still generate useable results. exceeding the  limit causes an error. this might happen with a large dataset if it can't be split, the  parameter is set to , or individual records within the dataset exceed the limit. if you are using your own algorithms, you can use placeholder text, such as , when the algorithm finds a bad record in an input file. for example, if the last record in a dataset is bad, the algorithm places the placeholder text for that record in the output file. for a sample notebook that uses batch transform with a principal component analysis (pca) model to reduce data in a user-item review matrix, followed by the application of a density-based spatial clustering of applications with noise (dbscan) algorithm to cluster movies, see . for instructions on creating and accessing jupyter notebook instances that you can use to run the example in amazon sagemaker, see . after creating and opening a notebook instance, choose the sagemaker examples tab to see a list of all the amazon sagemaker examples. the topic modeling example notebooks that use the ntm algorithms are located in the advanced functionality section. to open a notebook, choose its use tab, then choose create copy. 
the following sample notebooks show how to use your own algorithms or pretrained models from an amazon sagemaker notebook instance. . after you have created a notebook instance and opened it, choose the sagemaker examples tab for a list of all amazon sagemaker example notebooks. you can open the sample notebooks from the advanced functionality section in your notebook instance or in github at the provided links. to open a notebook, choose its use tab, then choose create copy. for instructions on how to create and access jupyter notebook instances, see  to learn how to host models trained in scikit-learn for making predictions in amazon sagemaker by injecting them first-party k-means and xgboost containers, see the following sample notebooks. kmeans_bring_your_own_model - xgboost_bring_your_own_model - to learn how to package algorithms that you have developed in tensorflow and scikit-learn frameworks for training and deployment in the amazon sagemaker environment, see the following notebooks. they show you how to build, register, and deploy you own docker containers using dockerfiles. tensorflow_bring_your_own - scikit_bring_your_own - to learn how to train a neural network locally using mxnet or tensorflow, and then create an endpoint from the trained model and deploy it on amazon sagemaker, see the following notebooks. the mxnet model is trained to recognize handwritten numbers from the mnist dataset. the tensorflow model is trained to classify irises. mxnet_mnist_byom - tensorflow_iris_byom - to learn how to use a dockerfile to build a container that calls the  and uses pipe mode to custom train an algorithm, see the following notebook. in pipe mode, the input data is transferred to the algorithm while it is training. this can decrease training time compared to using file-mode.  pipe_bring_your_own - to learn how to use an r container to train and host a model with the r kernel installed in a notebook , see the following notebook. to take advantage of the aws sdk for python (boto 3), we use python within the notebook. you can achieve the same results completely in r by invoking command line arguments. r_bring_your_own - to learn how to extend a prebuilt amazon sagemaker pytorch container image when you have additional functional requirements for your algorithm or model that the pre-built docker image doesn't support, see the following notebook. pytorch_extending_our_containers - for links to the github repositories with the prebuilt dockerfiles for the tensorflow, mxnet, chainer, and pytorch frameworks and instructions on use the aws sdk for python (boto 3) estimators to run your own training algorithms on amazon sagemaker learner and your own models on amazon sagemaker hosting, see  
amazon sagemaker runs batch transform jobs in an amazon virtual private cloud by default. however, model containers access aws resources—such as the amazon s3 buckets where you store your data and model artifacts—over the internet. to control access to your model containers and data, we recommend that you create a private vpc and configure it so that they aren't accessible over the internet. for information about creating and configuring a vpc, see  in the amazon vpc user guide. using a vpc helps to protect your model containers and data because you can configure your vpc so that it is not connected to the internet. using a vpc also allows you to monitor all network traffic in and out of your model containers by using vpc flow logs. for more information, see  in the amazon vpc user guide. you specify your private vpc configuration when you create a model by specifying subnets and security groups. you then specify the same model when you create a batch transform job. when you specify the subnets and security groups, amazon sagemaker creates elastic network interfaces (enis) that are associated with your security groups in one of the subnets. enis allow your model containers to connect to resources in your vpc. for information about enis, see  in the amazon vpc user guide. to specify subnets and security groups in your private vpc, use the  request parameter of the  api, or provide this information when you create a model in the amazon sagemaker console. then specify the same model in the  request parameter of the  api, or in the model name field when you create a transform job in the amazon sagemaker console. amazon sagemaker uses this information to create enis and attach them to your model containers. the enis provide your model containers with a network connection within your vpc that is not connected to the internet. they also enable your transform job to connect to resources in your private vpc. the following is an example of the  parameter that you include in your call to : if you are creating a model using the  api operation, the iam execution role that you use to create your model must include the permissions described in , including the following permissions required for a private vpc.  when creating a model in the console, if you select create a new role in the model settings section, the  policy used to create the role will already contain these permissions. if you select enter a custom iam role arn or use existing role, the role arn that you specify must have an execution policy attached with the following permissions.  when configuring the private vpc for your amazon sagemaker batch transform jobs, use the following guidelines. for information about setting up a vpc, see  in the amazon vpc user guide. topics your vpc subnets should have at least two private ip addresses for each instance in a transform job. for more information, see  in the amazon vpc user guide. if you configure your vpc so that model containers don't have access to the internet, they can't connect to the amazon s3 buckets that contain your data unless you create a vpc endpoint that allows access. by creating a vpc endpoint, you allow your model containers to access the buckets where you store your data and model artifacts . we recommend that you also create a custom policy that allows only requests from your private vpc to access to your s3 buckets. for more information, see . to create an s3 vpc endpoint: open the amazon vpc console at . in the navigation pane, choose endpoints, then choose create endpoint for service name, choose com.amazonaws.region.s3, where region is the name of the region where your vpc resides. for vpc, choose the vpc you want to use for this endpoint. for configure route tables, select the route tables to be used by the endpoint. the vpc service automatically adds a route to each route table you select that points any s3 traffic to the new endpoint. for policy, choose full access to allow full access to the s3 service by any user or service within the vpc. choose custom to restrict access further. for information, see . the default endpoint policy allows full access to s3 for any user or service in your vpc. to further restrict access to s3, create a custom endpoint policy. for more information, see . you can also use a bucket policy to restrict access to your s3 buckets to only traffic that comes from your amazon vpc. for information, see . the default endpoint policy allows users to install packages from the amazon linux and amazon linux 2 repositories on the training container. if you don't want users to install packages from that repository, create a custom endpoint policy that explicitly denies access to the amazon linux and amazon linux 2 repositories. the following is an example of a policy that denies access to these repositories: use default dns settings for your endpoint route table, so that standard amazon s3 urls (for example, ) resolve. if you don't use default dns settings, ensure that the urls that you use to specify the locations of the data in your batch transform jobs resolve by configuring the endpoint route tables. for information about vpc endpoint route tables, see  in the amazon vpc user guide. in distributed batch transform, you must allow communication between the different containers in the same batch transform job. to do that, configure a rule for your security group that allows inbound connections between members of the same security group for information, see . if you configure your vpc so that it doesn't have internet access, batch transform jobs that use that vpc do not have access to resources outside your vpc. if your batch transform job needs access to resources outside your vpc, provide access with one of the following options: if your batch transform job needs access to an aws service that supports interface vpc endpoints, create an endpoint to connect to that service. for a list of services that support interface endpoints, see  in the amazon vpc user guide. for information about creating an interface vpc endpoint, see  in the amazon vpc user guide.if your batch transform job needs access to an aws service that doesn't support interface vpc endpoints or to a resource outside of aws, create a nat gateway and configure your security groups to allow outbound connections. for information about setting up a nat gateway for your vpc, see  in the amazon virtual private cloud user guide.
the object detection algorithm identifies and locates all instances of objects in an image from a known collection of object categories. the algorithm takes an image as input and outputs the category that the object belongs to, along with a confidence score that it belongs to the category. the algorithm also predicts the object's location and scale with a rectangular bounding box. amazon sagemaker object detection uses the  algorithm that takes a convolutional neural network (cnn) pretrained for classification task as the base network. ssd uses the output of intermediate layers as features for detection.  various cnns such as  and  have achieved great performance on the image classification task. object detection in amazon sagemaker supports both vgg-16 and resnet-50 as a base network for ssd. the algorithm can be trained in full training mode or in transfer learning mode. in full training mode, the base network is initialized with random weights and then trained on user data. in transfer learning mode, the base network weights are loaded from pretrained models. the object detection algorithm uses standard data augmentation operations, such as flip, rescale, and jitter, on the fly internally to help avoid overfitting. 
a machine learning algorithm uses example data to create a generalized solution (a model) that addresses the business question you are trying to answer. after you create a model using example data, you can use it to answer the same business question for a new set of data. this is also referred to as obtaining inferences. amazon sagemaker provides several built-in machine learning algorithms that you can use for a variety of problem types.  because you create a model to address a business question, your first step is to understand the problem that you want to solve. specifically, the format of the answer that you are looking for influences the algorithm that you choose. for example, suppose that you are a bank marketing manager, and that you want to conduct a direct mail campaign to attract new customers. consider the potential types of answers that you're looking for: answers that fit into discrete categories—for example, answers to these questions:   "based on past customer responses, should i mail this particular customer?" answers to this question fall into two categories, "yes" or "no." in this case, you use the answer to narrow the recipients of the mail campaign. "based on past customer segmentation, which segment does this customer fall into?" answers might fall into categories such as "empty nester," "suburban family," or "urban professional." you could use these segments to decide who should receive the mailing. for this type of discrete classification problem, amazon sagemaker provides two algorithms:  and the . you set the following hyperparameters to direct these algorithms to produce discrete results:   for the linear learner algorithm, set the  hyperparameter to .  for the xgboost algorithm, set the  hyperparameter to .  answers that are quantitative—consider this question: "based on the return on investment (roi) from past mailings, what is the roi for mailing this customer?” in this case, you use the roi to target customers for the mail campaign. for these quantitative analysis problems, you can also use the  or the  algorithms. you set the following hyperparameters to direct these algorithms to produce quantitative results:   for the linear learner algorithm, set the  hyperparameter to .  for the xgboost algorithm, set the  hyperparameter to .  answers in the form of discrete recommendations—consider this question: "based on past responses to mailings, what is the recommended content for each customer?" in this case, you are looking for a recommendation on what to mail, not whether to mail, the customer. for this problem, amazon sagemaker provides the  algorithm. all of the questions in the preceding examples rely on having example data that includes answers. there are times that you don't need, or can't get, example data with answers. this is true for problems whose answers identify groups. for example: "i want to group current and prospective customers into 10 groups based on their attributes. how should i group them? " you might choose to send the mailing to customers in the group that has the highest percentage of current customers. that is, prospective customers that most resemble current customers based on the same set of attributes. for this type of question, amazon sagemaker provides the . "what are the attributes that differentiate these customers, and what are the values for each customer along those dimensions." you use these answers to simplify the view of current and prospective customers, and, maybe, to better understand these customer attributes. for this type of question, amazon sagemaker provides the  algorithm.in addition to these general-purpose algorithms, amazon sagemaker provides algorithms that are tailored to specific use cases. these include: —use this algorithm to classify images. it uses example data with answers (referred to as supervised algorithm). —this supervised algorithm is commonly used for neural machine translation.  —this algorithm is suitable for determining topics in a set of documents. it is an unsupervised algorithm, which means that it doesn't use example data with answers during training. —another unsupervised technique for determining topics in a set of documents, using a neural network approach.topics 
when you are setting up access control and writing a permissions policy that you can attach to an iam identity (an identity-based policy), use the following table as a reference. the table lists each amazon sagemaker api operation, the corresponding actions for which you can grant permissions to perform the action, and the aws resource for which you can grant the permissions. you specify the actions in the policy's  field, and you specify the resource value in the policy's  field.  noteexcept for the  api, resource-level restrictions are not available on  calls . any user calling a  api will see all resources of that type in the account. to express conditions in your amazon sagemaker policies, you can use aws-wide condition keys. for a complete list of aws-wide keys, see  in the iam user guide.  use the scroll bars to see the rest of the table. amazon sagemaker api operations and required permissions for actions   
get the execution role for the notebook instance. this is the iam role that you created when you created your notebook instance. you pass the role to the tuning job.   
the following sections contain more advanced tasks that explain how to customize monitoring using preprocessing and postprocessing scripts, how to build your own container, and how to use cloudformation to create a monitoring schedule. topics 
in addition to using the built-in monitoring mechanisms, you can create your own custom monitoring schedules and procedures using preprocessing and postprocessing scripts or by using or building your own container. topics 
amazon sagemaker provides two sets of docker images for rules: one set for evaluating rules provided by amazon sagemaker (built-in rules) and one set for evaluating custom rules provided in python source files.  if you’re using the , you can use amazon sagemaker rules with an estimator, without having to retrieve a docker image. the estimator manages the container in which it runs for you. if you're not using the sdk and one of its estimators, you have to retrieve the relevant pre-built container. the pre-built docker images for rules are stored in amazon elastic container registry (amazon ecr). to pull an image from an amazon ecr repository (or to push an image to one), use the full name registry url of the image. amazon sagemaker uses the following url patterns for rules container image registry addresses.  for the account id in each aws region, ecr repository name, and tag value, see the following topics. topics 
if you have followed instructions in , you should have an amazon sagemaker endpoint set up and running. you can now submit inference requests using boto3 client. here is an example of sending an image for inference: for xgboost application, you should submit a csv text instead: note that byom allows for a custom content type. for more information, see . 
when you create a 3d point cloud labeling job using the amazon sagemaker api operation , you use a label category configuration file to specify your labels and worker instructions. you must save this file in an amazon s3 bucket and specify an s3 uri for .  optionally, you can provide label category attributes for object tracking and object detection task types. workers can assign one or more attributes to annotations to give more information about that object. for example, you may want to use the attribute occluded to have workers identify when an object is partially obstructed. you can either specify a label category attribute for a single label using the  parameter, or for all labels using the  parameter.  workers can select multiple attributes for each label, up to the total number of attributes you provide for that label.  the following is an example of the required format of a 3d point cloud label category configuration file. see the quotas for these parameters in . label category attributes are not supported for 3d point cloud semantic segmentation task types. if you provide label category attributes for a semantic segmentation labeling job, they will be ignored. your label category configure file for a 3d point cloud semantic segmentation labeling job may look similar to the following: importantyou should only provide a label attribute name in  if you are running an audit job to verify or adjust 3d cuboids. use this parameter to input the  used in the labeling job that generated the annotations you want your worker to adjust. when you create a labeling job in the console, if you did not specify a label attribute name, the name of your job is used as the labelattributename. the following table lists elements you can and must include in your label category configuration file. the following table describes the parameters that you can and must use to describe a specific label category attribute in the  and  parameters. note that both of these parameters are optional in the previous table.  you can specify up to 10 label category attributes per class. this 10-attribute quotas includes global label category attributes. for example, if you create four global label category attributes, and then assign three label category attributes to label , that label will have 4+3=7 label category attributes in total. for all label category and label category attribute limits, refer to the following table.  create custom instructions for labeling jobs to improve your worker's accuracy in completing their task. your instructions are accessible when workers select the instructions menu option in the worker ui. short instructions must be under 255 characters and long instruction must be under 2,048 characters.  there are two kinds of instructions: short instructions – these instructions are shown to works when they select instructions in the worker ui menu. they should provide an easy reference to show the worker the correct way to label an object.full instructions – these instructions are shown when workers select more instructions in instructions the pop-up window. we recommend that you provide detailed instructions for completing the task with multiple examples showing edge cases and other difficult situations for labeling objects.for 3d point cloud labeling jobs, you can add worker instructions to your label category configuration file. you can use a single string to create instructions or you can add html mark up to customize the appearance of your instructions and add images. make sure that any images you include in your instructions are publicly available, or if your instructions are in amazon s3, that your workers have read-access so that they can view them. 
the input data that you provide to amazon sagemaker ground truth is sent to your workers for labeling. you choose the data to send to your workers by creating a manifest file that defines the data that requires labeling. the output data is the result of your labeling job. the output data file contains one entry for each object in the input dataset that specifies the label. topics after you create an augmented manifest file, you can use it in a training job. for a demonstration of how to use an augmented manifest to train an object detection machine learning model with amazon sagemaker, see . for more information, see . 
building a highly accurate training dataset for your machine learning (ml) algorithm is an iterative process. typically, you review and continuously adjust your labels until you are satisfied that they accurately represent the ground truth, or what is directly observable in the real world.  you can use an amazon sagemaker ground truth label verification task to direct workers to review a dataset's labels and improve label accuracy. workers can indicate if the existing labels are correct or rate label quality. they can also add comments to explain their reasoning. amazon sagemaker ground truth supports label verification for  and  labels.  you create a label verification labeling job using the ground truth section of the amazon sagemaker console or the  operation. ground truth provides a worker console similar to the following for labeling tasks. when you create the labeling job with the console, you can modify the images and content that are shown. if you create a labeling job using the api, you must supply a custom-built template. to learn how to create a custom template, see . to see examples of custom templates that can be used for label verification labeling job types, see this .   you can create a label verification labeling job using the amazon sagemaker console or api. to learn how to start a label verification job on the console, see . to use the api, see . 
an inference pipeline is an amazon sagemaker model that is composed of a linear sequence of two to five containers that process requests for inferences on data. you use an inference pipeline to define and deploy any combination of pretrained amazon sagemaker built-in algorithms and your own custom algorithms packaged in docker containers. you can use an inference pipeline to combine preprocessing, predictions, and post-processing data science tasks. inference pipelines are fully managed. you can add amazon sagemaker spark ml serving and scikit-learn containers that reuse the data transformers developed for training models. the entire assembled inference pipeline can be considered as an amazon sagemaker model that you can use to make either real-time predictions or to process batch transforms directly without any external preprocessing.  within an inference pipeline model, amazon sagemaker handles invocations as a sequence of http requests. the first container in the pipeline handles the initial request, then the intermediate response is sent as a request to the second container, and so on, for each container in the pipeline. amazon sagemaker returns the final response to the client.  when you deploy the pipeline model, amazon sagemaker installs and runs all of the containers on each amazon elastic compute cloud (amazon ec2) instance in the endpoint or transform job. feature processing and inferences run with low latency because the containers are co-located on the same ec2 instances. you define the containers for a pipeline model using the  operation or from the console. instead of setting one , you use the   parameter. to set the containers that make up the pipeline you also specify the order in which the containers are executed.  a pipeline model is immutable, but you can update an inference pipeline by deploying a new one using the  operation. this modularity supports greater flexibility during experimentation.  there are no additional costs for using this feature. you pay only for the instances running on an endpoint. topics for a sample notebook that uploads and processes a dataset, trains a model, and builds a pipeline model, see the  notebook. this notebook shows how you can build your machine learning pipeline by using spark feature transformers and the amazon sagemaker xgboost algorithm. after training the model, the sample shows how to deploy the pipeline (feature transformer and xgboost) for real-time predictions and also performs a batch transform job using the same pipeline.  for more examples that show how to create and deploy inference pipelines, see the  and  sample notebooks. for instructions on creating and accessing jupyter notebook instances that you can use to run the example in amazon sagemaker, see .  to see a list of all the amazon sagemaker samples, after creating and opening a notebook instance, choose the sagemaker examples tab. there are three inference pipeline notebooks. the first two inference pipeline notebooks just described are located in the  folder and the third notebook is in the  folder. to open a notebook, choose its use tab, then choose create copy. 
amazon sagemaker training and deployed inference containers are internet-enabled by default. this allows containers to access external services and resources on the public internet as part of your training and inference workloads. however, this offers an avenue for unauthorized access to your data. for example, a malicious user or code that you accidentally install on the container (in the form of a publicly available source code library) could access your data and transfer it to a remote host. if you use an amazon vpc by specifying a value for the  parameter when you call , , or , you can protect your data and resources by managing security groups and restricting internet access from your vpc. however, this comes at the cost of additional network configuration, and has the risk of configuring your network incorrectly. if you do not want amazon sagemaker to provide external network access to your training or inference containers, you can enable network isolation when you create your training job or model by setting the value of the  parameter to  when you call , , or . if you enable network isolation, the containers are not able to make any outbound network calls, even to other aws services such as amazon s3. additionally, no aws credentials are made available to the container runtime environment. in the case of a training job with multiple instances, network inbound and outbound traffic is limited to the peers of each training container. amazon sagemaker still performs download and upload operations against amazon s3 using your amazon sagemaker execution role in isolation from the training or inference container. network isolation is required for training jobs and models run using resources from aws marketplace. network isolation can be used in conjunction with a vpc. in this scenario, download and upload of customer data and model artifacts are routed via your vpc subnet. however, the training and inference containers themselves continue to be isolated from the network, and do not have access to any resource within your vpc or on the internet.  network isolation is not supported by the following managed amazon sagemaker containers as they require access to amazon s3:  chainerpytorchscikit-learnamazon sagemaker reinforcement learning
docker containers provide isolation, portability, and security. they simplify the creation of highly distributed systems and save money by improving resource utilization. docker relies on linux kernel functionality to provide a lightweight virtualization to package applications into an image that is totally self-contained. docker uses a file, called a dockerfile, to specify how the image is assembled. when you have an image, you use docker to build and run a container based on that image.  you can build your docker images from scratch or base them on other docker images that you or others have built. images are stored in repositories that are indexed and maintained by registries. an image can be pushed into or pulled out of a repository using its registry address, which is similar to a url. docker hub is a registry hosted by docker, inc. that provides publicly available repositories. aws provides the , a highly scalable, fast container management service. with amazon ecs, you can deploy any kind of code in amazon sagemaker. you can also create a logical division of labor by creating a deployment team that handles devops and infrastructure, and that maintains the container, and a data science team that creates the algorithms and models that are later added to a container. docker builds images by reading the instructions from a dockerfile text file that contains all of the commands, in order, that are needed to build the image. a dockerfile adheres to a specific format and set of instructions. for more information, see . dockerfiles used in amazon sagemaker must also satisfy additional requirements regarding the environmental variables, directory structure, timeouts, and other common functionality. for information, see  and .  for general information about docker containers managed by amazon ecs, see  in the amazon elastic container service developer guide. for more information about writing dockerfiles to build images, see . for general information about docker, see the following:  
the  is an input parameter of the  api. this parameter is a json-formatted string. the json models the conditions under which a human loop is created, when those conditions are evaluated against the response from an integrating ai service api (such as  or ). this response is referred to as an inference. for example, amazon rekognition sends an inference of a moderation label with an associated confidence score. in this example, the inference is the model's best estimate of the appropriate label for an image. for amazon textract, inference is made on the association between blocks of text (key-value pairs), such as the association between  and  in a form as well as content within a block of text, or word block, such as 'name'. the following is the schema for the json. at the top level, the  has a json array, . each member of this array is an independent condition that, if evaluated to true, will result in amazon a2i creating a human loop. each such independent condition can be a primitive condition or a complex condition. a simple condition has the following attributes: : this attribute identifies the type of condition. each aws ai service api that integrates with amazon a2i defines its own set of allowed . rekognition  – this api supports the  and   values.textract  – this api supports the ,  and   values. – this is a json object that parameterizes the condition. the set of allowed attributes of this object is dependent on the value of the . each  defines its own set of . a member of the  array can model a complex condition. this is accomplished by logically connecting primitive conditions using the  and  logical operators and nesting the underlying primitive conditions. up to two levels of nesting are supported.  notehuman loop activation conditions aren't available for human review workflows that are integrated with custom task types. the  parameter is disabled for custom task types.  topics 
amazon augmented ai (amazon a2i) helps you integrate human judgment into ai/ml workflows. with amazon a2i, you can let ai handle straight-forward data and invoke human reviewers only when their skills are needed.  the the ai/ml workflow that you integrate amazon a2i into defines an amazon a2i task type. amazon a2i supports two built-in task types: amazon textract and amazon rekognition, and a custom task type. the built-in task types integrate amazon a2i with 's  api operation and 's  api operation to incorporate a human review workflow when inference confidence is low for a given object (for example, for an image or text in a document). the custom task type allows you to use amazon a2i in custom machine learning applications. for more information about task types, see . to incorporate amazon a2i into your data labeling workflow for all task types, you need these resources:  a work team to complete your human review tasks. a work team can be created from the amazon mechanical turk workforce, vendor-managed workforce, or your own private workforce. for each type of workforce, you can create multiple work teams, and have each team work on multiple human review tasks. to learn to create a workforce and work teams, see .a worker task template to create a worker ui. the worker ui displays your input data, such as documents or images, and instructions to workers. it also provides interactive tools that the worker uses to complete your tasks. for more information, see . a human review workflow, also referred to as a flow definition. you use the flow definition to configure your human work team and provide information about how to accomplish the review task. you can use a flow definition to create multiple human loops. once you've created a human loop, the flow definition will be used to identify the resources used to create human review tasks each time a new data-object is sent to humans for review. for example, a human loop may get created each time an activation condition is met or your custom ml application calls .  for built-in task types, you also use the flow definition to identify the conditions under which a review human loop is triggered. for example, amazon rekognition can perform image content moderation using machine learning. you can use the flow definition to specify that an image will be sent to a human for content moderation review if amazon rekognition's confidence is too low.  you can create a flow definition in the amazon sagemaker console or with the amazon sagemaker api. to learn more about both of these options, see . a human loop to start your human review workflow. when you use one of the built-in task types, the corresponding aws service creates and starts a human loop on your behalf when the conditions specified in your flow definition are met or for each object if no conditions were specified. when a human loop is triggered, human review tasks are sent to the workers as specified in the flow definition.  when using a custom task type, you start a human loop using the . when you call  in your custom application, a task is sent to human reviewers.  to learn how to create and start a human loop, see . to generate these resources and create a human review workflow, amazon a2i integrates multiple apis including the amazon augmented ai runtime model, the amazon sagemaker apis, and apis associated with your task type. to learn more, see . noteaws region availability may differ when you use augmented ai with other aws services, such as amazon textract. create augmented ai resources in the same region that you use to interact with those aws services. for aws region availability for all services, see the . you can create a amazon a2i human review workflow using both the amazon sagemaker console and an api. to create a human review workflow, you need the following:  one or more amazon s3 buckets in the same aws region as the workflow for your input and output data. to create a bucket, follow the instructions in  in the amazon simple storage service console user guide. an iam role with required permissions to create a human review workflow and an iam user or role with permission to access augmented ai. for more information, see .you're prompted to choose a public, private, or vendor workforce for your human review workflows. if you plan to use a private workforce, you need to set one up ahead of time in the same aws region as your amazon a2i workflow. to learn more about these workforce types, see . importantamazon maintains a list of aws services in scope for our various compliance efforts . amazon augmented ai is not listed as in scope for compliance program assessments such as hipaa, pci, soc, iso, etc. at this time. if you use the amazon augmented ai service in conjunction with other aws services such as amazon rekognition and amazon textract, please note that amazon augmented ai is not in scope with various compliance programs even if the other services are. your use of the amazon augmented ai service is part of the shared responsibility for your organization to determine the nature of the data. you should determine if the service will process or store customer data and how it will or will not impact the compliance of your customer data environment. we encourage you to discuss your workload objectives and goals with your aws account team; they will be able to assist you in evaluating your proposed use case and architecture, and how our security and compliance processes overlay that architecture.if you're new to amazon a2i and are integrating a human review workflow with an amazon rekognition or amazon textract task, we recommend that you start by creating a human review workflow using the console, and then following next steps. for more information, see .  if you know which task type you want to use — amazon textract, amazon rekognition, or custom – you can set up an end-to-end walkthrough in a jupyter notebook to learn how to integrate augmented ai into a machine learning workflow for that task type. to get started, see  and select your task type.  
this section shows how to manage compilation jobs for machine learning models. you can create, describe, stop, and list compilation jobs.  create a compilation job as shown in the following json file, you specify the data input format, the s3 bucket where you stored your model, the s3 bucket where you want to write the compiled model, and the target hardware: describe a compilation job stop a compilation job list a compilation job 
to troubleshoot inference pipeline issues, use cloudwatch logs and error messages. if you are using custom docker images in a pipeline that includes amazon sagemaker built-in algorithms, you might also encounter permissions problems. to grant the required permissions, create an amazon elastic container registry (amazon ecr) policy. topics when you use custom docker images in a pipeline that includes , you need an . the policy allows your amazon ecr repository to grant permission for amazon sagemaker to pull the image. the policy must add the following permissions: amazon sagemaker publishes the container logs for endpoints that deploy an inference pipeline to amazon cloudwatch at the following path for each container. for example, logs for this endpoint are published to the following log groups and streams: a log stream is a sequence of log events that share the same source. each separate source of logs into cloudwatch makes up a separate log stream. a log group is a group of log streams that share the same retention, monitoring, and access control settings. to see the log groups and streams open the cloudwatch console at . in the navigation page, choose logs. in log groups. filter on myinferencepipelinesendpoint: to see the log streams, on the cloudwatch log groups page, choose myinferencepipelinesendpoint, and then search log group. for a list of the logs that amazon sagemaker publishes, see . the inference pipeline error messages indicate which containers failed.  if an error occurs while amazon sagemaker is invoking an endpoint, the service returns a  (error code 424), which indicates which container failed. if the request payload (the response from the previous container) exceeds the limit of 5 mb, amazon sagemaker provides a detailed error message, such as:  received response from mycontainername1 with status code 200. however, the request payload from mycontainername1 to mycontainername2 is 6000000 bytes, which has exceeded the maximum limit of 5 mb. see  in account 123456789012 for more information. `` if a container fails the ping health check while amazon sagemaker is creating an endpoint, it returns a  and indicates all of the containers that failed the ping check in the last health check. 
to identify the contents of an image at the pixel level, use an amazon sagemaker ground truth semantic segmentation labeling task. when assigned a semantic segmentation labeling job, workers classify pixels in the image into a set of predefined labels or classes. ground truth supports single and multi-class semantic segmentation labeling jobs. images that contain large numbers of objects that need to be segmented require more time. to help workers (from a private or vendor workforce) label these objects in less time and with greater accuracy, ground truth provides an ai-assisted auto-segmentation tool. for information, see . you create a semantic segmentation labeling job using the ground truth section of the amazon sagemaker console or the  operation.  importantfor this task type, if you create your own manifest file, use  to identify the location of each image file in amazon s3 that you want labeled. for more information, see . you can follow the instructions  to learn how to create a semantic segmentation labeling job in the amazon sagemaker console. in step 10, choose image from the task category drop down menu, and choose semantic segmentation as the task type.  ground truth provides a worker ui similar to the following for labeling tasks. when you create the labeling job with the console, you specify instructions to help workers complete the job and labels that workers can choose from.   to create a semantic segmentation labeling job, use the amazon sagemaker api operation . this api defines this operation for all aws sdks. to see a list of language-specific sdks supported for this operation, review the see also section of . follow the instructions on  and do the following while you configure your request:  pre-annotation lambda functions for this task type end with . to find the pre-annotation lambda arn for your region, see  . annotation-consolidation lambda functions for this task type end with . to find the annotation-consolidation lambda arn for your region, see . the following is an example of an  to create a labeling job in the us east (n. virginia) region. all parameters in red should be replaced with your specifications and resources.  if you create a labeling job using the api, you must supply a worker task template in . copy and modify the following template. only modify the , , and .  upload this template to s3, and provide the s3 uri for this file in . once you have created a semantic segmentation labeling job, your output data will be located in the amazon s3 bucket specified in the  parameter when using the api or in the output dataset location field of the job overview section of the console.  to learn more about the output manifest file generated by ground truth and the file structure the ground truth uses to store your output data, see .  to see an example of an output manifest file for a semantic segmentation labeling job, see . 
by using amazon elastic inference (ei), you can speed up the throughput and decrease the latency of getting real-time inferences from your deep learning models that are deployed as , but at a fraction of the cost of using a gpu instance for your endpoint. ei allows you to add inference acceleration to a hosted endpoint for a fraction of the cost of using a full gpu instance. add an ei accelerator in one of the available sizes to a deployable model in addition to a cpu instance type, and then add that model as a production variant to an endpoint configuration that you use to deploy a hosted endpoint. you can also add an ei accelerator to a amazon sagemaker  so that you can test and evaluate inference performance when you are building your models.  elastic inference is supported in ei-enabled versions of tensorflow, apache mxnet, and pytorch. to use any other deep learning framework, export your model by using onnx, and then import your model into mxnet. you can then use your model with ei as an mxnet model. for information about importing an onnx model into mxnet, see . topics amazon elastic inference accelerators are network attached devices that work along with amazon sagemaker instances in your endpoint to accelerate your inference calls. elastic inference accelerates inference by allowing you to attach fractional gpus to any amazon sagemaker instance. you can select the client instance to run your application and attach an elastic inference accelerator to use the right amount of gpu acceleration for your inference needs. elastic inference helps you lower your cost when not fully utilizing your gpu instance for inference. we recommend trying elastic inference with your model using different cpu instances and accelerator sizes. the following ei accelerator types are available. you can configure your endpoints or notebook instances with any ei accelerator type. in the table, the throughput in teraflops (tflops) is listed for both single-precision floating-point (f32) and half-precision floating-point (f16) operations. the memory in gb is also listed. consider the following factors when choosing an accelerator type for a hosted model: models, input tensors and batch sizes influence the amount of accelerator memory you need. start with an accelerator type that provides at least as much memory as the file size of your trained model. factor in that a model might use significantly more memory than the file size at runtime.demands on cpu compute resources, main system memory, and gpu-based acceleration and accelerator memory vary significantly between different kinds of deep learning models. the latency and throughput requirements of the application also determine the amount of compute and acceleration you need. thoroughly test different configurations of instance types and ei accelerator sizes to make sure you choose the configuration that best fits the performance needs of your application.for more information on selecting an ei accelerator, see: typically, you build and test machine learning models in a amazon sagemaker notebook before you deploy them for production. you can attach ei to your notebook instance when you create the notebook instance. you can set up an endpoint that is hosted locally on the notebook instance by using the local mode supported by tensorflow, mxnet, and pytorch estimators and models in the  to test inference performance. elastic inference enabled pytorch is not currently supported on notebook instances. for instructions on how to attach ei to a notebook instance and set up a local endpoint for inference, see . there are also elastic inference-enabled amazon sagemaker notebook jupyter kernels for elastic inference-enabled versions of tensorflow and apache mxnet. for information about using amazon sagemaker notebook instances, see  when you are ready to deploy your model for production to provide inferences, you create a amazon sagemaker hosted endpoint. you can attach ei to the instance where your endpoint is hosted to increase its performance at providing inferences. for instructions on how to attach ei to a hosted endpoint instance, see . amazon elastic inference is designed to be used with aws enhanced versions of tensorflow, apache mxnet, or pytorch machine learning frameworks. these enhanced versions of the frameworks are automatically built into containers when you use the amazon sagemaker python sdk, or you can download them as binary files and import them in your own docker containers.  you can download the ei-enabled tensorflow binary files from the public  amazon s3 bucket to the tensorflow serving containers. for more information about building a container that uses the ei-enabled version of tensorflow, see . you can download the ei-enabled mxnet binary files from the public  amazon s3 bucket to the mxnet serving containers. for more information about building a container that uses the ei-enabled version of mxnet, see . you can download the ei-enabled pytorch binary files from the public  amazon s3 bucket to the pytorch serving containers. for more information about building a container that uses the ei-enabled version of pytorch, see . to use elastic inference in a hosted endpoint, you can choose any of the following frameworks depending on your needs. if you need to create a custom container for deploying your model that is complex and requires extensions to a framework that the amazon sagemaker pre-built containers do not support, use . currently, the  and  built-in algorithms support ei. for an example that uses the image classification algorithm with ei, see . the following sample notebooks provide examples of using ei in amazon sagemaker:  
hyperparameter optimization is not a fully-automated process. to improve optimization, use the following guidelines when you create hyperparameters. topics the difficulty of a hyperparameter tuning job depends primarily on the number of hyperparameters that amazon sagemaker has to search. although you can simultaneously use up to 20 variables in a hyperparameter tuning job, limiting your search to a much smaller number is likely to give better results. the range of values for hyperparameters that you choose to search can significantly affect the success of hyperparameter optimization. although you might want to specify a very large range that covers every possible value for a hyperparameter, you will get better results by limiting your search to a small range of values. if you get the best metric values within a part of a range, consider limiting the range to that part. during hyperparameter tuning, amazon sagemaker attempts to ﬁgure out if your hyperparameters are log-scaled or linear-scaled. initially, it assumes that hyperparameters are linear-scaled. if they should be log-scaled, it might take some time for amazon sagemaker to discover that. if you know that a hyperparameter should be log-scaled and can convert it yourself, doing so could improve hyperparameter optimization. running more hyperparameter tuning jobs concurrently gets more work done quickly, but a tuning job improves only through successive rounds of experiments. typically, running one training job at a time achieves the best results with the least amount of compute time. when a training job runs on multiple instances, hyperparameter tuning uses the last-reported objective metric value from all instances of that training job as the value of the objective metric for that training job. design distributed training jobs so that the objective metric reported is the one that you want. 
to monitor the status of your labeling jobs, you can set up an  (cloudwatch events) rule for amazon sagemaker ground truth (ground truth) to send an event to cloudwatch events when a labeling job status changes to , , or .  once you create a rule, you can add a target to it. cloudwatch events uses this target to invoke another aws service to process the event. for example, you can create a target using a amazon simple notification service (amazon sns) topic to send a notification to your email when a labeling job status changes. prerequisites: to create a cloudwatch events rule, you will need an aws identity and access management (iam) role with an events.amazonaws.com trust policy attached. the following is an example of an events.amazonaws.com trust policy. topics to configure a cloudwatch events rule to get status updates, or events, for your ground truth labeling jobs, use the aws command line interface (aws cli)  command. you can filter events that are sent to your rule by status change. for example, you can create a rule that notifies you only if a labeling job status changes to . when using the  command, specify the following to receive labeling job statuses:  to configure a cloudwatch events rule to watch for all status changes, use the following command and replace the placeholder text. for example, replace  with a unique cloudwatch events rule name and  with the amazon resource number (arn) of an iam role with an events.amazonaws.com trust policy attached.  to filter by job status, use the  syntax. valid values for  are , , and .  the following example creates a cloudwatch events rule that notifies you when a labeling job in us-west-2 (oregon) changes to . the following example creates a cloudwatch events rule that notifies you when a labeling job in us-east-1 (virginia) changes to  or .  to learn more about the  request, see  in the amazon cloudwatch events user guide. after you have created a rule, events similar to the following are sent to cloudwatch events. in this example, the labeling job 's status changed to . to process events, you need to set up a target. for example, if you want to receive an email when your labeling job status changes, use a procedure in  in the amazon cloudwatch user guide to set up an amazon sns topic and subscribe your email to it. once you have create a topic, you can use it to create a target.  to add a target to your cloudwatch events rule open the cloudwatch console:  in the navigation pane, choose rules. choose the rule that you want to add a target to.  choose actions, and then choose edit. under targets, choose add target and choose the aws service you want to act when a labeling job status change event is detected.  configure your target. for instructions, see the topic for configuring a target in the . choose configure details. for name, enter a name and, optionally, provide details about the purpose of the rule in description.  make sure that the check box next to state is selected so that your rule is listed as enabled.  choose update rule. if your labeling job is not completed after 30 days, it will expire. if your labeling job expires, you can chain the job to create a new labeling job that will only send unlabeled data to workers. for more information, and to learn how to create a labeling job using chaining, see . 
aws identity and access management (iam) is an aws service that helps an administrator securely control access to aws resources. iam administrators control who can be authenticated (signed in) and authorized (have permissions) to use amazon sagemaker resources. iam is an aws service that you can use with no additional charge. topics how you use aws identity and access management (iam) differs, depending on the work you do in amazon sagemaker. service user – if you use the amazon sagemaker service to do your job, then your administrator provides you with the credentials and permissions that you need. as you use more amazon sagemaker features to do your work, you might need additional permissions. understanding how access is managed can help you request the right permissions from your administrator. if you cannot access a feature in amazon sagemaker, see . service administrator – if you're in charge of amazon sagemaker resources at your company, you probably have full access to amazon sagemaker. it's your job to determine which amazon sagemaker features and resources your employees should access. you must then submit requests to your iam administrator to change the permissions of your service users. review the information on this page to understand the basic concepts of iam. to learn more about how your company can use iam with amazon sagemaker, see . iam administrator – if you're an iam administrator, you might want to learn details about how you can write policies to manage access to amazon sagemaker. to view example amazon sagemaker identity-based policies that you can use in iam, see . authentication is how you sign in to aws using your identity credentials. for more information about signing in using the aws management console, see  in the iam user guide. you must be authenticated (signed in to aws) as the aws account root user, an iam user, or by assuming an iam role. you can also use your company's single sign-on authentication, or even sign in using google or facebook. in these cases, your administrator previously set up identity federation using iam roles. when you access aws using credentials from another company, you are assuming a role indirectly.  to sign in directly to the , use your password with your root user email or your iam user name. you can access aws programmatically using your root user or iam user access keys. aws provides sdk and command line tools to cryptographically sign your request using your credentials. if you don’t use aws tools, you must sign the request yourself. do this using signature version 4, a protocol for authenticating inbound api requests. for more information about authenticating requests, see  in the aws general reference. regardless of the authentication method that you use, you might also be required to provide additional security information. for example, aws recommends that you use multi-factor authentication (mfa) to increase the security of your account. to learn more, see  in the iam user guide.   when you first create an aws account, you begin with a single sign-in identity that has complete access to all aws services and resources in the account. this identity is called the aws account root user and is accessed by signing in with the email address and password that you used to create the account. we strongly recommend that you do not use the root user for your everyday tasks, even the administrative ones. instead, adhere to the . then securely lock away the root user credentials and use them to perform only a few account and service management tasks.  an  is an identity within your aws account that has specific permissions for a single person or application. an iam user can have long-term credentials such as a user name and password or a set of access keys. to learn how to generate access keys, see  in the iam user guide. when you generate access keys for an iam user, make sure you view and securely save the key pair. you cannot recover the secret access key in the future. instead, you must generate a new access key pair. an  is an identity that specifies a collection of iam users. you can't sign in as a group. you can use groups to specify permissions for multiple users at a time. groups make permissions easier to manage for large sets of users. for example, you could have a group named iamadmins and give that group permissions to administer iam resources. users are different from roles. a user is uniquely associated with one person or application, but a role is intended to be assumable by anyone who needs it. users have permanent long-term credentials, but roles provide temporary credentials. to learn more, see  in the iam user guide. an  is an identity within your aws account that has specific permissions. it is similar to an iam user, but is not associated with a specific person. you can temporarily assume an iam role in the aws management console by . you can assume a role by calling an aws cli or aws api operation or by using a custom url. for more information about methods for using roles, see  in the iam user guide. iam roles with temporary credentials are useful in the following situations: temporary iam user permissions – an iam user can assume an iam role to temporarily take on different permissions for a specific task. federated user access –  instead of creating an iam user, you can use existing identities from aws directory service, your enterprise user directory, or a web identity provider. these are known as federated users. aws assigns a role to a federated user when access is requested through an . for more information about federated users, see  in the iam user guide. cross-account access – you can use an iam role to allow someone (a trusted principal) in a different account to access resources in your account. roles are the primary way to grant cross-account access. however, with some aws services, you can attach a policy directly to a resource (instead of using a role as a proxy). to learn the difference between roles and resource-based policies for cross-account access, see  in the iam user guide.aws service access –  a service role is an iam role that a service assumes to perform actions in your account on your behalf. when you set up some aws service environments, you must define a role for the service to assume. this service role must include all the permissions that are required for the service to access the aws resources that it needs. service roles vary from service to service, but many allow you to choose your permissions as long as you meet the documented requirements for that service. service roles provide access only within your account and cannot be used to grant access to services in other accounts. you can create, modify, and delete a service role from within iam. for example, you can create a role that allows amazon redshift to access an amazon s3 bucket on your behalf and then load data from that bucket into an amazon redshift cluster. for more information, see  in the iam user guide. applications running on amazon ec2 –  you can use an iam role to manage temporary credentials for applications that are running on an ec2 instance and making aws cli or aws api requests. this is preferable to storing access keys within the ec2 instance. to assign an aws role to an ec2 instance and make it available to all of its applications, you create an instance profile that is attached to the instance. an instance profile contains the role and enables programs that are running on the ec2 instance to get temporary credentials. for more information, see  in the iam user guide. to learn whether to use iam roles, see  in the iam user guide. you control access in aws by creating policies and attaching them to iam identities or aws resources. a policy is an object in aws that, when associated with an identity or resource, defines their permissions. aws evaluates these policies when an entity (root user, iam user, or iam role) makes a request. permissions in the policies determine whether the request is allowed or denied. most policies are stored in aws as json documents. for more information about the structure and contents of json policy documents, see  in the iam user guide. an iam administrator can use policies to specify who has access to aws resources, and what actions they can perform on those resources. every iam entity (user or role) starts with no permissions. in other words, by default, users can do nothing, not even change their own password. to give a user permission to do something, an administrator must attach a permissions policy to a user. or the administrator can add the user to a group that has the intended permissions. when an administrator gives permissions to a group, all users in that group are granted those permissions. iam policies define permissions for an action regardless of the method that you use to perform the operation. for example, suppose that you have a policy that allows the  action. a user with that policy can get role information from the aws management console, the aws cli, or the aws api. identity-based policies are json permissions policy documents that you can attach to an identity, such as an iam user, role, or group. these policies control what actions that identity can perform, on which resources, and under what conditions. to learn how to create an identity-based policy, see  in the iam user guide. identity-based policies can be further categorized as inline policies or managed policies. inline policies are embedded directly into a single user, group, or role. managed policies are standalone policies that you can attach to multiple users, groups, and roles in your aws account. managed policies include aws managed policies and customer managed policies. to learn how to choose between a managed policy or an inline policy, see  in the iam user guide. resource-based policies are json policy documents that you attach to a resource such as an amazon s3 bucket. service administrators can use these policies to define what actions a specified principal (account member, user, or role) can perform on that resource and under what conditions. resource-based policies are inline policies. there are no managed resource-based policies. access control lists (acls) are a type of policy that controls which principals (account members, users, or roles) have permissions to access a resource. acls are similar to resource-based policies, although they do not use the json policy document format. amazon s3, aws waf, and amazon vpc are examples of services that support acls. to learn more about acls, see  in the amazon simple storage service developer guide. aws supports additional, less-common policy types. these policy types can set the maximum permissions granted to you by the more common policy types.  permissions boundaries – a permissions boundary is an advanced feature in which you set the maximum permissions that an identity-based policy can grant to an iam entity (iam user or role). you can set a permissions boundary for an entity. the resulting permissions are the intersection of entity's identity-based policies and its permissions boundaries. resource-based policies that specify the user or role in the  field are not limited by the permissions boundary. an explicit deny in any of these policies overrides the allow. for more information about permissions boundaries, see  in the iam user guide.service control policies (scps) – scps are json policies that specify the maximum permissions for an organization or organizational unit (ou) in aws organizations. aws organizations is a service for grouping and centrally managing multiple aws accounts that your business owns. if you enable all features in an organization, then you can apply service control policies (scps) to any or all of your accounts. the scp limits permissions for entities in member accounts, including each aws account root user. for more information about organizations and scps, see  in the aws organizations user guide.session policies – session policies are advanced policies that you pass as a parameter when you programmatically create a temporary session for a role or federated user. the resulting session's permissions are the intersection of the user or role's identity-based policies and the session policies. permissions can also come from a resource-based policy. an explicit deny in any of these policies overrides the allow. for more information, see  in the iam user guide. when multiple types of policies apply to a request, the resulting permissions are more complicated to understand. to learn how aws determines whether to allow a request when multiple policy types are involved, see  in the iam user guide. 
to get inferences for an entire dataset, use batch transform. with batch transform, you create a batch transform job using a trained model and the dataset, which must be stored in amazon s3. amazon sagemaker saves the inferences in an s3 bucket that you specify when you create the batch transform job. batch transform manages all of the compute resources required to get inferences. this includes launching instances and deleting them after the batch transform job has completed. batch transform manages interactions between the data and the model with an object within the instance node called an agent. use batch transform when you: want to get inferences for an entire dataset and index them to serve inferences in real timedon't need a persistent endpoint that applications (for example, web or mobile apps) can call to get inferencesdon't need the subsecond latency that amazon sagemaker hosted endpoints provideyou can also use batch transform to preprocess your data before using it to train a new model or generate inferences. the following diagram shows the workflow of a batch transform job:  to perform a batch transform, create a batch transform job using either the amazon sagemaker console or the api. provide the following: the path to the s3 bucket where you've stored the data that you want to transform.the compute resources that you want amazon sagemaker to use for the transform job. compute resources are machine learning (ml) compute instances that are managed by amazon sagemaker.the path to the s3 bucket where you want to store the output of the job.the name of the amazon sagemaker model that you want to use to create inferences. you must use a model that you have already created either with the  operation or the console.the following is an example of what a dataset file might look like.  a record is a single input data unit. for information about how to delimit records for batch transform jobs, see . for an example of how to use batch transform, see . 
the following tables list the hyperparameters supported by the amazon sagemaker semantic segmentation algorithm for network architecture, data inputs, and training. you specify semantic segmentation for training in the  of the  request. network architecture hyperparameters data hyperparameters training hyperparameters 
a widget to enable human review of an amazon rekognition image moderation result. the following attributes are supported by this element. this is the text that is displayed as the header. this is a link to the image to be analyzed by the worker.  this supports  as an array of strings or an array of objects where each object has a  field. if the categories come in as objects, the following applies: the displayed categories are the value of the  field.the returned answer contains the full objects of any selected categories.if the categories come in as strings, the following applies: the returned answer is an array of all the strings that were selected.by setting this attribute you create a button underneath the categories in the ui.  when a user chooses the button, all categories are deselected and disabled.choosing the button again re-enables the categories so that users can choose them.if you submit after choosing the button, it returns an empty array.this element has the following parent and child elements. parent elements – crowd-formchild elements – ,  the following aws regions are supported by this element. you can use custom html and css code within these regions to format your instructions to workers. for example, use the  section to provide good and bad examples of how to complete a task.  general instructions about how to work with the widget.  important task-specific instructions that are displayed in a prominent place.  an example of a worker template using the crowd element would look like the following. the following is a sample of the output from this element. for details about this output, see amazon rekognition  api documentation.  
selling amazon sagemaker algorithms and model packages is a three-step process: develop your algorithm or model, and package it in a docker container. for information, see . create an algorithm or model package resource in amazon sagemaker. for information, see . register as a seller on aws marketplace and list your algorithm or model package on aws marketplace. for information about registering as a seller, see  in the user guide for aws marketplace providers. for information about listing and monetizing your algorithms and model packages, see  in the user guide for aws marketplace providers.  
you can edit an autoscaling policy with the aws management console, the aws cli, or the application auto scaling api.  if a model’s traffic becomes zero, amazon sagemaker automatic scaling doesn't scale-in. this is because amazon sagemaker doesn't emit metrics with a value of zero, and without a metric, the scaling policy will never be triggered. as a workaround, do either of the following: send requests to the model variant until autoscaling scales-in to the minimum capacitychange the policy to reduce the maximum provisioned capacity to match the minimum provisioned capacityyou can prevent the target-tracking scaling policy configuration from scaling in your variant by disabling scale-in activity. disabling scale-in activity prevents the scaling policy from deleting instances, while still allowing it to create them as needed. to enable or disable scale-in activity for your model, specify a boolean value for . for more information about , see  in the application auto scaling api reference.  examplethe following is an example of a target-tracking configuration for a scaling policy where it will scale-out, but not scale-in. in this configuration, the  predefined metric will scale-out based on an average of 70 invocations (inference requests) across all instances the model is on. the configuration also disables scale-in activity for the scaling policy.   to manually scale-out, adjust the minimum capacity. you can use the console to update this value. alternatively, use the aws cli with the  argument, or use the application auto scaling api's  parameter. to prevent scale-out, adjust the maximum capacity. you can use the console to update this value. alternatively, use the aws cli with the  argument, or use the application auto scaling api's  parameter.  to edit a scaling policy with the aws management console, use the same procedure that you used to . you can use the aws cli or the application auto scaling api to edit a scaling policy in the same way that you apply a scaling policy: with the aws cli, specify the name of the policy that you want to edit in the  parameter. specify new values for the parameters that you want to change.with the application auto scaling api, specify the name of the policy that you want to edit in the  parameter. specify new values for the parameters that you want to change.for more information, see . 
amazon sagemaker rl supports multi-core and multi-instance distributed training. depending on your use case, training and/or environment rollout can be distributed. for example, amazon sagemaker rl works for the following distributed scenarios: single training instance and multiple rollout instances of the same instance type. for an example, see the neural network compression example in the amazon sagemaker examples repository at .single trainer instance and multiple rollout instances, where different instance types for training and rollouts. for an example, see the aws deepracer / aws robomaker example in the amazon sagemaker examples repository at .single trainer instance that uses multiple cores for rollout. for an example, see the roboschool example in the amazon sagemaker examples repository at . this is useful if the simulation environment is light-weight and can run on a single thread. multiple instances for training and rollouts. for an example, see the roboschool example in the amazon sagemaker examples repository at .
amazon sagemaker model monitor pre-built container computes per column/feature statistics. the statistics are calculated for the baseline dataset and also for the current dataset that is being analyzed. note the following: the prebuilt containers compute  which is a compact quantiles sketch.by default, we materialize the distribution in 10 buckets. this is not currently configurable.
amazon sagemaker makes extensive use of docker containers for build and runtime tasks. before using your own algorithm or model with amazon sagemaker, you need to understand how amazon sagemaker manages and runs them. amazon sagemaker provides pre-built docker images for its built-in algorithms and the supported deep learning frameworks used for training and inference. by using containers, you can train machine learning algorithms and deploy models quickly and reliably at any scale. docker is a program that performs operating-system-level virtualization for installing, distributing, and managing software. it packages applications and their dependencies into virtual containers that provide isolation, portability, and security. you can put scripts, algorithms, and inference code for your machine learning models into containers. the container includes the runtime, system tools, system libraries, and other code required to train your algorithms or deploy your models. this gives you the flexibility to use almost any script or algorithm code with amazon sagemaker, regardless of runtime or implementation language. the code that runs in containers is effectively isolated from its surroundings, ensuring a consistent runtime, regardless of where the container is deployed. after packaging your training code, inference code, or both into docker containers, you can create algorithm resources and model package resources for use in amazon sagemaker or to publish on aws marketplace. with docker, you can ship code faster, standardize application operations, seamlessly move code, and economize by improving resource utilization. you create docker containers from images that are saved in a repository. you build the images from scripted instructions provided in a dockerfile. to use docker containers in amazon sagemaker, the scripts that you use must satisfy certain requirements. for information about the requirements, see  and .  amazon sagemaker always uses docker containers when running scripts, training algorithms or deploying models. however, your level of engagement with containers varies depending on whether you are using a built-in algorithm provided by amazon sagemaker or a script or model that you have developed yourself. if you're using your own code, it also depends on the language and framework or environment used to develop it, and any other of the dependencies it requires to run. in particular, it depends on whether you use the  or aws sdk for python (boto3) or some other sdk. amazon sagemaker provides containers for its built-in algorithms and pre-built docker images for some of the most common machine learning frameworks. you can use the containers and images as provided or extend them to cover more complicated use cases. you can also create your own container images to manage more advanced use cases not addressed by the containers provided by amazon sagemaker.  there are four main scenarios for running scripts, algorithms, and models in the amazon sagemaker environment. the last three describe the scenarios covered here: the ways you can use containers to bring your own script, algorithm or model. use a built-in algorithm. containers are used behind the scenes when you use one of the amazon sagemaker built-in algorithms, but you do not deal with them directly. you can train and deploy these algorithms from the amazon sagemaker console, the aws command line interface (aws cli), a python notebook, or the . the built-in algorithms available are itemized and described in the  topic. for an example of how to train and deploy a built-in algorithm using jupyter notebook running in an amazon sagemaker notebook instance, see the  topic. use pre-built container images.amazon sagemaker provides pre-built containers to supports deep learning frameworks such as apache mxnet, tensorflow, pytorch, and chainer. it also supports machine learning libraries such a scikit-learn and sparkml by providing pre-built docker images. if you use the , they are deployed using their respective amazon sagemaker sdk  class. in this case, you can supply the python code that implements your algorithm and configure the pre-built image to access your code as an entry point. for a list of deep learning frameworks currently supported by amazon sagemaker and samples that show how to use their pre-build container images, see . for information on the scikit-learn and sparkml pre-built container images, see . for more information about using frameworks with the , see their respective topics in .extend a pre-built container image. if you have additional functional requirements for an algorithm or model that you developed in a framework that a pre-built amazon sagemaker docker image doesn't support, you can modify an amazon sagemaker image to satisfy your needs. for an example, see . build your own custom container image: if there is no pre-built amazon sagemaker container image that you can use or modify for an advanced scenario, you can package your own script or algorithm to use with amazon sagemaker.you can use any programming language or framework to develop your container. for an example that shows how to build your own containers to train and host an algorithm, see .the next topic provides a brief introduction to docker containers. amazon sagemaker has certain contractual requirements that a container must satisfy to be used with it. the following topic describes the amazon sagemaker containers library that can be used to create amazon sagemaker-compatible containers, including a list of the environmental variables it defines and may require. then a tutorial that shows how to get started by using amazon sagemaker containers to train a python script. after the tutorial, topics:  describe the pre-built docker containers provided by amazon sagemaker for deep learning frameworks and other libraries.provide examples of how to deploy containers for the various scenarios.subsequent sections describe in more detail the contractual requirements to use docker with amazon sagemaker to train your custom algorithms and to deploy your inference code to make predictions. there are two ways to make predictions when deploying a model. first, to get individual, real-time predictions, you can make inferences with a hosting service. second, to get predictions for an entire dataset, you can use a batch transform. the final sections describe how to create algorithm and model package resources for use in your amazon sagemaker account or to publish on aws marketplace. topics 
all amazon sagemaker built-in algorithms adhere to the common input inference format described in . this topic contains a list of the available output formats for the amazon sagemaker k-nearest-neighbor algorithm. content-type: text/csv this accepts a  or encoding parameter. it assumes a  of 0 and a utf-8 encoding. content-type: application/json content-type: application/jsonlines content-type: application/x-recordio-protobuf accept: application/json accept: application/jsonlines in verbose mode, the api provides the search results with the distances vector sorted from smallest to largest, with corresponding elements in the labels vector. in this example, k is set to 3. accept: application/json; verbose=true content-type: application/x-recordio-protobuf in verbose mode, the api provides the search results with the distances vector sorted from smallest to largest, with corresponding elements in the labels vector. in this example, k is set to 3. accept: application/x-recordio-protobuf; verbose=true for regressor tasks: for classifier tasks: 

you can create a task ui for your workers by creating a worker task template with detailed instructions on how to complete your task. depending on your task type, this can be done directly in the amazon sagemaker console or by using a custom template.  if you are creating a human review workflow for amazon textract or amazon rekognition tasks, follow the instructions in  to create your worker template using the default templates provided by amazon a2i.if you are adding a human review workflow to a custom task type, or want to create a custom worker ui using html and css elements: create a custom worker template using the instructions found in .  generate an amazon resource name (arn) to use this template in your flow definition. generate a worker task template arn using the instructions found in  or by using the  api operation.  topics 
amazon sagemaker is integrated with aws cloudtrail, a service that provides a record of actions taken by a user, role, or an aws service in amazon sagemaker. cloudtrail captures all api calls for amazon sagemaker, with the exception of , as events. the calls captured include calls from the amazon sagemaker console and code calls to the amazon sagemaker api operations. if you create a trail, you can enable continuous delivery of cloudtrail events to an amazon s3 bucket, including events for amazon sagemaker. if you don't configure a trail, you can still view the most recent events in the cloudtrail console in event history. using the information collected by cloudtrail, you can determine the request that was made to amazon sagemaker, the ip address from which the request was made, who made the request, when it was made, and additional details.  to learn more about cloudtrail, see the . by default, log data is stored in cloudwatch logs indefinitely. however, you can configure how long to store log data in a log group. for information, see  in the amazon cloudwatch logs user guide. cloudtrail is enabled on your aws account when you create the account. when activity occurs in amazon sagemaker, that activity is recorded in a cloudtrail event along with other aws service events in event history. you can view, search, and download recent events in your aws account. for more information, see .  for an ongoing record of events in your aws account, including events for amazon sagemaker, create a trail. a trail enables cloudtrail to deliver log files to an amazon s3 bucket. by default, when you create a trail in the console, the trail applies to all aws regions. the trail logs events from all regions in the aws partition and delivers the log files to the amazon s3 bucket that you specify. additionally, you can configure other aws services to further analyze and act upon the event data collected in cloudtrail logs. for more information, see the following:   and all amazon sagemaker actions, with the exception of , are logged by cloudtrail and are documented in the . for example, calls to the ,  and  actions generate entries in the cloudtrail log files.  every event or log entry contains information about who generated the request. the identity information helps you determine the following:  whether the request was made with root or aws identity and access management (iam) user credentials.whether the request was made with temporary security credentials for a role or federated user.whether the request was made by another aws service.for more information, see the . amazon sagemaker supports logging non-api service events to your cloudtrail log files for automatic model tuning jobs. these events are related to your tuning jobs but, are not the direct result of a customer request to the public aws api. for example, when you create a hyperparameter tuning job by calling , amazon sagemaker creates training jobs to evaluate various combinations of hyperparameters to find the best result. similarly, when you call  to stop a hyperparameter tuning job, amazon sagemaker might stop any of the associated running training jobs. non-api events for your tuning jobs are logged to cloudtrail to help you improve governance, compliance, and operational and risk auditing of your aws account. log entries that result from non-api service events have an  of  instead of . a trail is a configuration that enables delivery of events as log files to an s3 bucket that you specify. cloudtrail log files contain one or more log entries. an event represents a single request from any source and includes information about the requested action, the date and time of the action, request parameters, and so on. cloudtrail log files are not an ordered stack trace of the public api calls, so they do not appear in any specific order.  the following examples a log entry for the  action, which creates an endpoint to deploy a trained model. the following example is a log entry for the  action, which creates one or more containers to host a previously trained model. 
you can change the instance type, and sagemaker image and kernel from within an amazon sagemaker studio notebook. you can create a custom kernel to use with your notebooks. topics 
 your tuning job settings are applied across all of the algorithms in the hpo tuning job. warm start and early stopping are available only when tuning a single algorithm. after you define the job settings you will create individual training definitions for each algorithm or variation you want to tune.  warm start if you cloned this job, you can choose to use the results from a previous tuning job to improve the performance of this tuning job. this is the warm start feature and it is only available when tuning a single algorithm. when you choose this option, you can choose up to five previous hyperparameter tuning jobs to use. alternatively, you can use transfer learning to add additional data to the parent tuning job. when you select this option, you choose one previous tuning job as the parent.   warm start is compatible with tuning jobs created after october 1, 2018. for more information, see .  early stopping  when they are unlikely to improve the current best objective metric of the hyperparameter tuning job. like warm start, this feature is only available when tuning a single algorithm. this is an automatic feature without configuration options, and it’s disabled by default.   tuning strategy tuning strategy can be either random or bayesian. it specifies how the automatic tuning searches over specified hyperparameter ranges. you specify the ranges in a later step. for more information, see .   you must provide at least one training definition for each training job. each training definition specifies the configuration for an algorithm. to create several definitions for your training job you can clone a definition.  name provide a unique name for the training definition.  permissions amazon sagemaker requires permissions to call other services on your behalf. choose an iam role or let aws create a role that has the  iam policy attached.  optional security settings the network isolation setting prevents the container from making any outbound network calls. this is required for aws marketplace machine learning offerings.   you can also choose to use a private vpc.  note inter-container encryption is only available when creating job definitions from the api.  algorithm options you can choose one of the built-in algorithms, your own algorithm, your own container with an algorithm, or you can subscribe to an algorithm from aws marketplace.   if you choose a built-in algorithm, it has the ecr image information prepopulated. if you choose your own container, you must specify the ecr image information. you can select the input mode for the algorithm as file or pipe. if you plan to supply your data using a .csv file from amazon s3, you should select the file.  metrics when you choose a built-in algorithm, metrics are provided for you. if you choose your own algorithm, you need to define your metrics.  objective metric to find the best training job, set an objective metric and tuning type. after the training job is complete, you can view the tuning job detail page for a summary of the best training job found using this objective metric.  hyperparameter configuration when you choose a built-in algorithm, hyperparameters’ default values are set for you, using ranges that are optimized for the particular algorithm. you can change these values as you see fit. instead of a range, you can set a fixed value for a hyperparameter by setting the parameter’s type to static. each algorithm has different required and optional parameters. for more information, see  and .  input data configuration input data is defined by channels, each with their own source location (amazon s3 or amazon elastic file system), compression, and format options. you can define up to 20 channels of input sources. if the algorithm you chose supports multiple input channels, you can specify those too.   for example, when using the , you could add two channels: train and validation.  checkpoint configuration checkpoints are periodically generated during training. you must choose an amazon s3 location for the checkpoints to be saved. checkpoints are used in metrics reporting, and are also used to resume managed spot training jobs.  output data configuration you must define an amazon s3 location for the artifacts of the training job to be stored. you have the option of adding encryption to the output using an aws key management service (aws kms) key.  resource limits and configuration each training definition can have a different resource configuration. you choose the instance type and number of nodes.  finalizing the job settings you can run parallel jobs and limit the total number of jobs. the number of parallel jobs should not exceed the number of nodes you have requested across all of your training definitions. the total number of jobs can’t exceed the number of jobs that your definitions are expected to run.  
amazon sagemaker random cut forest (rcf) is an unsupervised algorithm for detecting anomalous data points within a dataset. these are observations which diverge from otherwise well-structured or patterned data. anomalies can manifest as unexpected spikes in time series data, breaks in periodicity, or unclassifiable data points. they are easy to describe in that, when viewed in a plot, they are often easily distinguishable from the "regular" data. including these anomalies in a dataset can drastically increase the complexity of a machine learning task since the "regular" data can often be described with a simple model. the main idea behind the rcf algorithm is to create a forest of trees where each tree is obtained using a partition of a sample of the training data. for example, a random sample of the input data is first determined. the random sample is then partitioned according to the number of trees in the forest. each tree is given such a partition and organizes that subset of points into a k-d tree. the anomaly score assigned to a data point by the tree is defined as the expected change in complexity of the tree as a result adding that point to the tree; which, in approximation, is inversely proportional to the resulting depth of the point in the tree. the random cut forest assigns an anomaly score by computing the average score from each constituent tree and scaling the result with respect to the sample size. the rcf algorithm is based on the one described in reference [1]. the first step in the rcf algorithm is to obtain a random sample of the training data. in particular, suppose we want a sample of size  from  total data points. if the training data is small enough, the entire dataset can be used, and we could randomly draw  elements from this set. however, frequently the training data is too large to fit all at once, and this approach isn't feasible. instead, we use a technique called reservoir sampling.  is an algorithm for efficiently drawing random samples from a dataset  where the elements in the dataset can only be observed one at a time or in batches. in fact, reservoir sampling works even when  is not known a priori. if only one sample is requested, such as when , the algorithm is like this: algorithm: reservoir sampling input: dataset or data stream  initialize the random sample  for each observed sample :pick a uniform random number  if  set  return  this algorithm selects a random sample such that  for all . when  the algorithm is more complicated. additionally, a distinction must be made between random sampling that is with and without replacement. rcf performs an augmented reservoir sampling without replacement on the training data based on the algorithms described in [2]. the next step in rcf is to construct a random cut forest using the random sample of data. first, the sample is partitioned into a number of equal-sized partitions equal to the number of trees in the forest. then, each partition is sent to an individual tree. the tree recursively organizes its partition into a binary tree by partitioning the data domain into bounding boxes. this procedure is best illustrated with an example. suppose a tree is given the following two-dimensional dataset. the corresponding tree is initialized to the root node:  the rcf algorithm organizes these data in a tree by first computing a bounding box of the data, selecting a random dimension (giving more weight to dimensions with higher "variance"), and then randomly determining the position of a hyperplane "cut" through that dimension. the two resulting subspaces define their own sub tree. in this example, the cut happens to separate a lone point from the remainder of the sample. the first level of the resulting binary tree consists of two nodes, one which will consist of the subtree of points to the left of the initial cut and the other representing the single point on the right.  bounding boxes are then computed for the left and right halves of the data and the process is repeated until every leaf of the tree represents a single data point from the sample. note that if the lone point is sufficiently far away then it is more likely that a random cut would result in point isolation. this observation provides the intuition that tree depth is, loosely speaking, inversely proportional to the anomaly score. when performing inference using a trained rcf model the final anomaly score is reported as the average across scores reported by each tree. note that it is often the case that the new data point does not already reside in the tree. to determine the score associated with the new point the data point is inserted into the given tree and the tree is efficiently (and temporarily) reassembled in a manner equivalent to the training process described above. that is, the resulting tree is as if the input data point were a member of the sample used to construct the tree in the first place. the reported score is inversely proportional to the depth of the input point within the tree. the primary hyperparameters used to tune the rcf model are  and . increasing  has the effect of reducing the noise observed in anomaly scores since the final score is the average of the scores reported by each tree. while the optimal value is application-dependent we recommend using 100 trees to begin with as a balance between score noise and model complexity. note that inference time is proportional to the number of trees. although training time is also affected it is dominated by the reservoir sampling algorithm describe above. the parameter  is related to the expected density of anomalies in the dataset. in particular,  should be chosen such that  approximates the ratio of anomalous data to normal data. for example, if 256 samples are used in each tree then we expect our data to contain anomalies 1/256 or approximately 0.4% of the time. again, an optimal value for this hyperparameter is dependent on the application. sudipto guha, nina mishra, gourav roy, and okke schrijvers. "robust random cut forest based anomaly detection on streams." in international conference on machine learning, pp. 2712-2721. 2016. byung-hoon park, george ostrouchov, nagiza f. samatova, and al geist. "reservoir-based random sampling with replacement from data stream." in proceedings of the 2004 siam international conference on data mining, pp. 492-496. society for industrial and applied mathematics, 2004. 
in the single-frame input manifest file, each line in the manifest contains data for a single point cloud frame. the point cloud frame data can either be stored in binary or ascii format (see ). this is the manifest file formatting required for 3d point cloud object detection and semantic segmentation. optionally, you can also provide camera sensor fusion data for each point cloud frame.  ground truth supports point cloud and video camera sensor fusion in the  for all modalities. if you can obtain your 3d sensor extrinsic (like a lidar extrinsic), we recommend that you transform 3d point cloud frames into the world coordinate system using the extrinsic. for more information, see .  however, if you cannot obtain a point cloud in world coordinate system, you can provide coordinates in the original coordinate system that the data was captured in. if you are providing camera data for sensor fusion, it is recommended that you provide lidar sensor and camera pose in the world coordinate system.  to create a single-frame input manifest file, you will identify the location of each point cloud frame that you want workers to label using the  key. additionally, you must use the  key to identify the format of your dataset, a timestamp for that frame, and, optionally, sensor fusion data and video camera images. the following example demonstrates the syntax used for an input manifest file for a single-frame point cloud labeling job. the example includes two point cloud frames. for details about each parameter, see the table following this example.  the following table shows the parameters you can include in your input manifest file: use the ego-vehicle location to provide information about the location of the vehicle used to capture point cloud data. ground truth use this information to compute lidar extrinsic matrix.  ground truth uses extrinsic matrices to project labels to and from the 3d scene and 2d images. for more information, see . the following table provides more information about the  and orientation () parameters that are required when you provide ego-vehicle information.  if you want to include video camera data with a frame, use the following parameters to provide information about each image. the required column below applies when the  parameter is included in the input manifest file under . you are not required to include images in your input manifest file.  if you include camera images, you must include information about the camera  and  used the capture the images in the world coordinate system. if your images are distorted, ground truth can automatically undistort them using information you provide about the image in your input manifest file, including distortion coefficients (, , , , , ), the camera model and the camera intrinsic matrix. the intrinsic matrix is made up of focal length (, ), and the principal point (, . see  to learn how ground truth uses the camera intrinsic. if distortion coefficients are not included, ground truth will not undistort an image.  you can include up to 100,000 point cloud frames in your input manifest file. 3d point cloud labeling job have longer pre-processing times than other ground truth task types. for more information, see . 
automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many jobs that test a range of hyperparameters on your dataset. you choose the tunable hyperparameters, a range of values for each, and an objective metric. you choose the objective metric from the metrics that the algorithm computes. automatic model tuning searches the hyperparameters chosen to find the combination of values that result in the model that optimizes the objective metric. amazon sagemaker ntm is an unsupervised learning algorithm that learns latent representations of large collections of discrete data, such as a corpus of documents. latent representations use inferred variables that are not directly measured to model the observations in a dataset. automatic model tuning on ntm helps you find the model that minimizes loss over the training or validation data. training loss measures how well the model fits the training data. validation loss measures how well the model can generalize to data that it is not trained on. low training loss indicates that a model is a good fit to the training data. low validation loss indicates that a model has not overfit the training data and so should be able to model documents on which is has not been trained successfully. usually, it's preferable to have both losses be small. however, minimizing training loss too much might result in overfitting and increase validation loss, which would reduce the generality of the model.  for more information about model tuning, see . the ntm algorithm reports a single metric that is computed during training: . the total loss is the sum of the reconstruction loss and kullback-leibler divergence. when tuning hyperparameter values, choose this metric as the objective. you can tune the following hyperparameters for the ntm algorithm. usually setting low  and small  values results in lower validation losses, although it might take longer to train. low validation losses don't necessarily produce more coherent topics as interpreted by humans. the effect of other hyperparameters on training and validation loss can vary from dataset to dataset. to see which values are compatible, see . 
to create an algorithm resource that you can use to run training jobs in amazon sagemaker and publish on aws marketplace specify the following information: the docker containers that contains the training and, optionally, inference code.the configuration of the input data that your algorithm expects for training.the hyperparameters that your algorithm supports.metrics that your algorithm sends to amazon cloudwatch during training jobs.the instance types that your algorithm supports for training and inference, and whether it supports distributed training across multiple instances.validation profiles, which are training jobs that amazon sagemaker uses to test your algorithm's training code and batch transform jobs that amazon sagemaker runs to test your algorithm's inference code. to ensure that buyers and sellers can be confident that products work in amazon sagemaker, we require that you validate your algorithms before listing them on aws marketplace. you can list products in the aws marketplace only if validation succeeds. to validate your algorithms, amazon sagemaker uses your validation profile and sample data to run the following validations tasks: create a training job in your account to verify that your training image works with amazon sagemaker. if you included inference code in your algorithm, create a model in your account using the algorithm's inference image and the model artifacts produced by the training job. if you included inference code in your algorithm, create a transform job in your account using the model to verify that your inference image works with amazon sagemaker. when you list your product on aws marketplace, the inputs and outputs of this validation process persist as part of your product and are made available to your buyers. this helps buyers understand and evaluate the product before they buy it. for example, buyers can inspect the input data that you used, the outputs generated, and the logs and metrics emitted by your code. the more comprehensive your validation specification, the easier it is for customers to evaluate your product. notein your validation profile, provide only data that you want to expose publicly. validation can take up to a few hours. to see the status of the jobs in your account, in the amazon sagemaker console, see the training jobs and transform jobs pages. if validation fails, you can access the scan and validation reports from the amazon sagemaker console. if any issues are found, you will have to create the algorithm again. noteto publish your algorithm on aws marketplace, at least one validation profile is required. you can create an algorithm by using either the amazon sagemaker console or the amazon sagemaker api. topics to create an algorithm resource (console) open the amazon sagemaker console at . choose algorithms, then choose create algorithm. on the training specifications page, provide the following information: for algorithm name, type a name for your algorithm. the algorithm name must be unique in your account and in the aws region. the name must have 1 to 64 characters. valid characters are a-z, a-z, 0-9, and - (hyphen). type a description for your algorithm. this description appears in the amazon sagemaker console and in the aws marketplace. for training image, type the path in amazon ecr where your training container is stored. for support distributed training, choose yes if your algorithm supports training on multiple instances. otherwise, choose no. for support instance types for training, choose the instance types that your algorithm supports. for channel specification, specify up to 8 channels of input data for your algorithm. for example, you might specify 3 input channels named , , and . for each channel, specify the following information: for channel name, type a name for the channel. the name must have 1 to 64 characters. valid characters are a-z, a-z, 0-9, and - (hyphen). to require the channel for your algorithm, choose channel required. type a description for the channel. for supported input modes, choose pipe mode if your algorithm supports streaming the input data, and file mode if your algorithm supports downloading the input data as a file. you can choose both. for supported content types, type the mime type that your algorithm expects for input data. for supported compression type, choose gzip if your algorithm supports gzip compression. otherwise, choose none. choose add channel to add another data input channel, or choose next if you are done adding channels. on the tuning specifications page, provide the following information: for hyperparameter specification, specify the hyperparameters that your algorithm supports by editing the json object. for each hyperparameter that your algorithm supports, construct a json block similar to the following: in the json, supply the following: for , specify a default value for the hyperparameter, if there is one. for , specify a description for the hyperparameter. for , specify whether the hyperparameter is required. for , specify  if this hyperparameter can be tuned when a user runs a hyperparameter tuning job that uses this algorithm. for information, see . for , specify a name for the hyperparameter. for , specify one of the following:  - the values of the hyperparameter are integers. specify minimum and maximum values for the hyperparameter. - the values of the hyperparameter are floating-point values. specify minimum and maximum values for the hyperparameter. - the values of the hyperparameter are categorical values. specify a list of all of the possible values.for , specify , , or . the value must correspond to the type of  that you specified. for metric definitions, specify any training metrics that you want your algorithm to emit. amazon sagemaker uses the regular expression that you specify to find the metrics by parsing the logs from your training container during training. users can view these metrics when they run training jobs with your algorithm, and they can monitor and plot the metrics in amazon cloudwatch. for information, see . for each metric, provide the following information: for metric name, type a name for the metric. for , type the regular expression that amazon sagemaker uses to parse training logs so that it can find the metric value. for objective metric support choose yes if this metric can be used as the objective metric for a hyperparameter tuning job. for information, see . choose add metric to add another metric, or choose next if you are done adding metrics. on the inference specifications page, provide the following information if your algorithm supports inference: for container definition, type path in amazon ecr where your inference container is stored. for container dns host name, type the name of a dns host for your image. for supported instance types for real-time inference, choose the instance types that your algorithm supports for models deployed as hosted endpoints in amazon sagemaker. for information, see . for supported instance types for batch transform jobs, choose the instance types that your algorithm supports for batch transform jobs. for information, see . for supported content types, type the type of input data that your algorithm expects for inference requests. for supported response mime types, type the mime types that your algorithm supports for inference responses. choose next. on the validation specifications page, provide the following information: for publish this algorithm on aws marketplace, choose yes to publish the algorithm on aws marketplace. for validate this algorithm, choose yes if you want amazon sagemaker to run training jobs and/or batch transform jobs that you specify to test the training and/or inference code of your algorithm. noteto publish your algorithm on aws marketplace, your algorithm must be validated. for iam role, choose an iam role that has the required permissions to run training jobs and batch transform jobs in amazon sagemaker, or choose create a new role to allow amazon sagemaker to create a role that has the  managed policy attached. for information, see . for validation profile, specify the following: a name for the validation profile.a training job definition. this is a json block that describes a training job. this is in the same format as the  input parameter of the  api.a transform job definition. this is a json block that describes a batch transform job. this is in the same format as the  input parameter of the  api.choose create algorithm. to create an algorithm resource by using the amazon sagemaker api, call the  api.  
open amazon sagemaker studio and login.  choose the experiments tab (it looks like a conical flask).  choose create experiment.  enter the experiment’s details in the job settings form.  name of the experiment - must be unique to this experiment in the current aws region. input dataset s3 location - an s3:// formatted url where sagemaker has read permissions. notethe input data must be in csv format and contain at least 1000 rows.target attribute - this is the column of your data you want the model to target. output s3 location - an s3:// formatted url where sagemaker has write permissions. (optional) tell autopilot whether you want a regression (e.g., house prices), binary classification (e.g., hotdog or not hotdog), or multi-class classification (e.g., cat, dog or bird) model. if you don’t specify this, autopilot infers it from the values of the attribute you want to predict. in some cases, autopilot is unable to infer accurately, in which case you must provide the value for the job to succeed. (optional) tell autopilot the metric you want it to use to evaluate models. if you don’t specify a metric, autopilot makes the decision on your behalf based on the best metrics for the situation. (optional) configure other job parameters. for example, security configuration or job completion criteria. in the latter case, if you are looking to limit the total wall clock time of the automl job, you can specify to limit the number of seconds/minutes the job is allowed to run. (optional) specify . in this case, instead of executing the entire automl workflow on autopilot, autopilot stops execution after generating the notebooks for data exploration and candidate generation. this way, you can use the notebooks as a starting point to guide your own process of data exploration and model training/tuning. both notebooks have highlighted sections that explain what kinds of changes are typical, such as changing instance type, cluster size, and so on. choose to run the training immediately, or only produce data exploration and candidate generation notebooks.  this is all you need to run a autopilot experiment. the process will generate a model as well as statistics that you can view in real time while the experiment is running. after the experiment is completed, you can view the trials, sort by objective metric, and right-click to deploy the model for use in other environments.  
to use elastic inference (ei) in amazon sagemaker with a hosted endpoint for real-time inference, specify an ei accelerator when you create the deployable model to be hosted at that endpoint. you can do this in one of the following ways: use the  versions of either the tensorflow, mxnet, or pytorch and the amazon sagemaker pre-built containers for tensorflow, mxnet, and pytorchbuild your own container, and use the low-level amazon sagemaker api (boto 3). you will need to import the ei-enabled version of either tensorflow, mxnet, or pytorch from the provided amazon s3 locations into your container, and use one of those versions to write your training script.use either the  or  build-in algorithms, and use the aws sdk for python (boto 3) to run your training job and create your deployable model and hosted endpoint.topics to use tensorflow with ei in amazon sagemaker, you need to call the  method of either the  or  objects. you then specify an accelerator type using the accelerator_type input argument. for information on using tensorflow in the amazon sagemaker python sdk, see: . amazon sagemaker provides default model training and inference code for your convenience. for custom file formats, you might need to implement custom model training and inference code. to use an estimator object with ei, when you use the deploy method, include the  input argument. the estimator returns a predictor object, which we call its deploy method, as shown in the example code. to use a model object with ei, when you use the deploy method, include the  input argument. the estimator returns a predictor object, which we call its deploy method, as shown in the example code. to use mxnet with ei in amazon sagemaker, you need to call the  method of either the  or  objects. you then specify an accelerator type using the  input argument. for information about using mxnet in the , see  for your convenience, amazon sagemaker provides default model training and inference code. for custom file formats, you might need to write custom model training and inference code. to use an estimator object with ei, when you use the deploy method, include the  input argument. the estimator returns a predictor object, which we call its deploy method, as shown in the example code.  to use a model object with ei, when you use the deploy method, include the  input argument. the estimator returns a predictor object, which we call its deploy method, as shown in the example code.  for a complete example of using ei with mxnet in amazon sagemaker, see the sample notebook at  to use pytorch with ei in amazon sagemaker, you need to call the  method of either the  or  objects. you then specify an accelerator type using the  input argument. for information about using pytorch in the , see . for your convenience, amazon sagemaker provides default model training and inference code. for custom file formats, you might need to write custom model training and inference code. to use an estimator object with ei, when you use the deploy method, include the  input argument. the estimator returns a predictor object, which we call its deploy method, as shown in this example code. to use a model object with ei, when you use the deploy method, include the  input argument. the model returns a predictor object, which we call its deploy method, as shown in this example code. to use ei with a model in a custom container that you build, use the low-level aws sdk for python (boto 3). download and import the aws ei-enabled versions of tensorflow, apache mxnet, or pytorch machine learning frameworks, and write your training script using those frameworks. to use ei with your own container, you need to import either the amazon ei tensorflow serving library, the amazon ei apache mxnet library, or the elastic inference enabled pytorch library into your container. the ei-enabled versions of tensorflow and mxnet are currently available as binary files stored in amazon s3 locations. you can download the ei-enabled binary for tensorflow from the amazon s3 bucket at . for information about building a container that uses the ei-enabled version of tensorflow, see . you can download the ei-enabled binary for apache mxnet from the public amazon s3 bucket at . for information about building a container that uses the ei-enabled version of mxnet, see . you can download the elastic inference enabled binary for pytorch from the public amazon s3 bucket at . for information about building a container that uses the elastic inference enabled version of pytorch, see .  to create an endpoint by using aws sdk for python (boto 3), you first create an endpoint configuration. the endpoint configuration specifies one or more models (called production variants) that you want to host at the endpoint. to attach ei to one or more of the production variants hosted at the endpoint, you specify one of the ei instance types as the  field for that . you then pass that endpoint configuration when you create the endpoint. to use ei, you need to specify an accelerator type in the endpoint configuration. after you create an endpoint configuration with an accelerator type, you can create an endpoint. after creating the endpoint, you can invoke it using the  method in a boto3 runtime object, as you would any other endpoint. 
amazon sagemaker random cut forest (rcf) is an unsupervised algorithm for detecting anomalous data points within a data set. these are observations which diverge from otherwise well-structured or patterned data. anomalies can manifest as unexpected spikes in time series data, breaks in periodicity, or unclassifiable data points. they are easy to describe in that, when viewed in a plot, they are often easily distinguishable from the "regular" data. including these anomalies in a data set can drastically increase the complexity of a machine learning task since the "regular" data can often be described with a simple model. with each data point, rcf associates an anomaly score. low score values indicate that the data point is considered "normal." high values indicate the presence of an anomaly in the data. the definitions of "low" and "high" depend on the application but common practice suggests that scores beyond three standard deviations from the mean score are considered anomalous. while there are many applications of anomaly detection algorithms to one-dimensional time series data such as traffic volume analysis or sound volume spike detection, rcf is designed to work with arbitrary-dimensional input. amazon sagemaker rcf scales well with respect to number of features, data set size, and number of instances. topics amazon sagemaker random cut forest supports the  and  data channels. the optional test channel is used to compute accuracy, precision, recall, and f1-score metrics on labeled data. train and test data content types can be either  or  formats. for the test data, when using text/csv format, the content must be specified as text/csv;label_size=1 where the first column of each row represents the anomaly label: "1" for an anomalous data point and "0" for a normal data point. you can use either file mode or pipe mode to train rcf models on data that is formatted as  or as  also note that the train channel only supports  and the test channel only supports . the s3 distribution type can be specified using the  as follows: see the  for more information on customizing the s3 data source attributes. finally, in order to take advantage of multi-instance training the training data must be partitioned into at least as many files as instances. for inference, rcf supports ,  and  input data content types. see the  documentation for more information. rcf inference returns  or  formatted output. each record in these output data contains the corresponding anomaly scores for each input data point. see  for more information. for more information on input and output file formats, see  for inference and the . for training, we recommend the , , and  instance families. for inference we recommend using a  instance type in particular, for maximum performance as well as minimized cost per hour of usage. although the algorithm could technically run on gpu instance types it does not take advantage of gpu hardware. for an example of how to train an rcf model and perform inferences with it, see the  notebook. for a sample notebook that uses the amazon sagemaker random cut forest algorithm for anomaly detection, see . for instructions how to create and access jupyter notebook instances that you can use to run the example in amazon sagemaker, see . once you have created a notebook instance and opened it, select the sagemaker examples tab to see a list of all the amazon sagemaker samples. to open a notebook, click on its use tab and select create copy. 

you create a multi-model endpoint using the amazon sagemaker , , and  apis just as you would create a single model endpoint, but with two changes. when defining the container, you need to pass a new  parameter value, . you also need to pass the  field that specifies the prefix in amazon s3 where the model artifacts are located, instead of the path to a single model artifact, as you would when deploying a single model. for a sample notebook that uses amazon sagemaker to deploy multiple xgboost models to an endpoint, see .  the following procedure outlines the key steps used in that sample to create a multi-model endpoint. to deploy the model (aws sdk for python (boto 3)) get a container whose image supports deploying models. currently, only the mxnet and pytorch pre-build containers support multi-model endpoints. to use any other framework or algorithm, use the amazon sagemaker inference toolkit to build a container that supports multi-model endpoints. for information, see . create the model that uses this container. (optional) if you are using a serial inference pipeline, get the additional container(s) to include in the pipeline, and include it in the  argument of : configure the multi-model endpoint for the model. we recommend configuring your endpoints with at least two instances. this allows amazon sagemaker to provide a highly available set of predictions across multiple availability zones for the models. noteyou can use only one multi-model-enabled endpoint in a serial inference pipeline. create the multi-model endpoint using the  and  parameters. 
the images used to train a machine learning model often contain more than one object. to classify and localize one or more objects within images, use the amazon sagemaker ground truth bounding box labeling job task type. in this context, localization means the pixel-location of the bounding box.  you create a bounding box labeling job using the ground truth section of the amazon sagemaker console or the  operation. importantfor this task type, if you create your own manifest file, use  to identify the location of each image file in amazon s3 that you want labeled. for more information, see . you can follow the instructions  to learn how to create a bounding box labeling job in the amazon sagemaker console. in step 10, choose image from the task category drop down menu, and choose bounding box as the task type.  ground truth provides a worker ui similar to the following for labeling tasks. when you create the labeling job with the console, you specify instructions to help workers complete the job and labels that workers can choose from.   to create a bounding box labeling job, use the amazon sagemaker api operation . this api defines this operation for all aws sdks. to see a list of language-specific sdks supported for this operation, review the see also section of . follow the instructions on  and do the following while you configure your request:  pre-annotation lambda functions for this task type end with . to find the pre-annotation lambda arn for your region, see  . annotation-consolidation lambda functions for this task type end with . to find the annotation-consolidation lambda arn for your region, see . the following is an example of an  to create a labeling job in the us east (n. virginia) region. all parameters in red should be replaced with your specifications and resources.  if you create a labeling job using the api, you must supply a worker task template in . copy and modify the following template. only modify the , , and . upload this template to s3, and provide the s3 uri for this file in . once you have created a bounding box labeling job, your output data will be located in the amazon s3 bucket specified in the  parameter when using the api or in the output dataset location field of the job overview section of the console.  for example, the output manifest file of a successfully completed single-class bounding box task will contain the following:  the  parameter identifies the location of the bounding box drawn around an object identified as a "bird" relative to the top-left corner of the image which is taken to be the (0,0) pixel-coordinate. in the previous example,  and  identify the location of the pixel in the top-left corner of the bounding box relative to the top-left corner of the image. the dimensions of the bounding box are identified with  and . the  parameter gives the pixel-dimensions of the original input image. when you use the bounding box task type, you can create single- and multi-class bounding box labeling jobs. the output manifest file of a successfully completed multi-class bounding box will contain the following:  to learn more about the output manifest file that results from a bounding box labeling job, see . to learn more about the output manifest file generated by ground truth and the file structure the ground truth uses to store your output data, see .  
in production ml workflows, data scientists and engineers frequently try to improve their models in various ways, such as by performing , training on additional or more-recent data, and improving feature selection. performing a/b testing between a new model and an old model with production traffic can be an effective final step in the validation process for a new model. in a/b testing, you test different variants of your models and compare how each variant performs. if the newer version of the model delivers better performance than the previously-existing version, replace the old version of the model with the new version in production. amazon sagemaker enables you to test multiple models or model versions behind the same endpoint using production variants. each production variant identifies a machine learning (ml) model and the resources deployed for hosting the model. by using production variants, you can test ml models that have been trained using different datasets, trained using different algorithms and ml frameworks, or are deployed to different instance type, or any combination of all of these. you can distribute endpoint invocation requests across multiple production variants by providing the traffic distribution for each variant, or you can invoke a specific variant directly for each request. in this topic, we look at both methods for testing ml models. topics to test multiple models by distributing traffic between them, specify the percentage of the traffic that gets routed to each model by specifying the weight for each production variant in the endpoint configuration. for information, see . the following diagram shows how this works in more detail.  to test multiple models by invoking specific models for each request, specify the specific version of the model you want to invoke by providing a value for the  parameter when you call . amazon sagemaker ensures that the request is processed by the production variant you specify. if you have already provided traffic distribution and specify a value for the  parameter, the targeted routing overrides the random traffic distribution. the following diagram shows how this works in more detail.  the following example shows how to perform a/b model testing. for a sample notebook that implements this example, see . first, we define where our models are located in amazon s3. these locations are used when we deploy our models in subsequent steps: next, we create the model objects with the image and model data. these model objects are used to deploy production variants on an endpoint. the models are developed by training ml models on different data sets, different algorithms or ml frameworks, and different hyperparameters: we now create two production variants, each with its own different model and resource requirements (instance type and counts). this enables you to also test models on different instance types. we set an initial_weight of 1 for both variants. this means that 50% of requests go to , and the remaining 50% of requests to . the sum of weights across both variants is 2 and each variant has weight assignment of 1. this means that each variant receives 1/2, or 50%, of the total traffic. finally we’re ready to deploy these production variants on an amazon sagemaker endpoint. now we send requests to this endpoint to get inferences in real time. we use both traffic distribution and direct targeting. first, we use traffic distribution that we configured in the previous step. each inference response contains the name of the production variant that processes the request, so we can see that traffic to the two production variants is roughly equal. amazon sagemaker emits metrics such as  and  for each variant in amazon cloudwatch. for a complete list of metrics that amazon sagemaker emits, see . let’s query cloudwatch to get the number of invocations per variant, to show how invocations are split across variants by default:  now let's invoke a specific version of the model by specifying  as the  in the call to . to confirm that all new invocations were processed by , we can query cloudwatch to get the number of invocations per variant. we see that for the most recent invocations (latest timestamp), all requests were processed by , as we had specified. there were no invocations made for .  to see which model version performs better, let's evaluate the accuracy, precision, recall, f1 score, and receiver operating charactersistic/area under the curve for each variant. first, let's look at these metrics for :  now let's look at the metrics for :  for most of our defined metrics,  is performing better, so this is the one that we want to use in production. now that we have determined that  performs better than , we shift more traffic to it. we can continue to use  to invoke a specific model variant, but a simpler approach is to update the weights assigned to each variant by calling . this changes the traffic distribution to your production variants without requiring updates to your endpoint. recall from the setup section that we set variant weights to split traffic 50/50. the cloudwatch metrics for the total invocations for each variant below show us the invocation patterns for each variant:  now we shift 75% of the traffic to  by assigning new weights to each variant using . amazon sagemaker now sends 75% of the inference requests to  and remaining 25% of requests to . the cloudwatch metrics for total invocations for each variant shows us higher invocations for  than for :  we can continue to monitor our metrics, and when we're satisfied with a variant's performance, we can route 100% of the traffic to that variant. we use  to update the traffic assignments for the variants. the weight for  is set to 0 and the weight for  is set to 1. amazon sagemaker now sends 100% of all inference requests to . the cloudwatch metrics for the total invocations for each variant show that all inference requests are being processed by  and there are no inference requests processed by .   you can now safely update your endpoint and delete  from your endpoint. you can also continue testing new models in production by adding new variants to your endpoint and following steps 2 - 4. 
in this step you use the console to create a labeling job. you tell amazon sagemaker ground truth the amazon s3 bucket where the manifest file is stored and configure the parameters for the job. for more information about storing data in an amazon s3 bucket, see . to create a labeling job open the amazon sagemaker console at . from the left navigation, choose labeling jobs. choose create labeling job to start the job creation process. in the job overview section, provide the following information: job name – give the labeling job a name that describes the job. this name is shown in your job list. the name must be unique in your account in an aws region.label attribute name – leave this unchecked as the default value is the best option for this introductory job.input dataset location – enter the s3 location of the manifest file that you created in step 1.output dataset location – the location where your output data is written.iam role – create or choose an iam role with the sagemakerfullaccess iam policy attached.in the task type section, for the dataset type field, choose bounding box as the task type. choose next to move on to configuring your labeling job.  
 this rule detects if your model parameters have been poorly initialized.  good initialization breaks the symmetry of the weights and gradients in a neural network and maintains commensurate activation variances across layers. otherwise, the neural network doesn't learn effectively. initializers like xavier aim to keep variance constant across activations, which is especially relevant for training very deep neural nets. too small an initialization can lead to vanishing gradients. too large an initialization can lead to exploding gradients. this rule checks the variance of activation inputs across layers, the distribution of gradients, and the loss convergence for the initial steps to determine if a neural network has been poorly initialized. parameter descriptions for the poorweightinitialization rule   for an example of how to configure and deploy a built-in rule, see . notethis rule can't be applied to the xgboost algorithm. 
 extends cloud capabilities to local devices. it enables devices to collect and analyze data closer to the source of information, react autonomously to local events, and communicate securely with each other on local networks. with aws iot greengrass, you can perform machine learning inference at the edge on locally generated data using cloud-trained models. currently, you can deploy models on to all aws iot greengrass devices based on arm cortex-a, intel atom, and nvidia jetson series processors. for more information on deploying a lambda inference application to perform machine learning inferences with aws iot greengrass, see . 
amazon sagemaker studio notebooks extends the jupyterlab interface. for an overview of the basic jupyterlab interface, see . the following image shows the menu bar and an empty cell from a sagemaker studio notebook.  the menu bar includes the following icons. when you hover over an icon, a tooltip displays the icon function. additional notebook commands are in the studio main menu. for a list of available notebook commands and shortcuts, in the left sidebar of studio, choose the commands icon ( ) then scroll to the notebook cell operations and notebook operations sections. to select multiple cells, click in the left margin outside of a cell. hold down the  key and use  or the  key to select previous cells, or use  or the  key to select following cells. 
you can use augmented ai to incorporate a human review into your workflow for built-in task types, amazon textract and amazon rekognition, or your own custom tasks.  when you create a flow definition using one of the built-in task types, you will be able to specify conditions, such as confidence thresholds, that will trigger a human review. for example, you can specify confidence thresholds for form keys in a document, or inference confidence for an image. these services create a human loop on your behalf using amazon augmented ai runtime api when these conditions are met, and supply your input data directly to amazon a2i to send to human reviewers.  when you use a custom task type, you create and start a human loop using the amazon augmented ai runtime api. for more details, see . use the custom task type to incorporate a human review workflow with other aws service, or your own custom ml application. the topics in this section provide an overview of these three task types and examples of worker task templates that amazon a2i provides for amazon textract and amazon rekognition task types. you specify your task type when creating a flow definition. topics 
this topic provides an overview of the unique features of a ground truth 3d point cloud labeling job. you can use the 3d point cloud labeling jobs to have workers label objects in a 3d point cloud generated from a 3d sensors like lidar and depth cameras or generated from 3d reconstruction by stitching images captured by an agent like a drone.  when you create a 3d point cloud labeling job, you need to provide an . the input manifest file can be: a frame input manifest file that has a single point cloud frame on each line. a sequence input manifest file that has a single sequence on each line. a sequence is defined as a temporal series of point cloud frames. for both types of manifest files, job pre-processing time (that is, the time before ground truth starts sending tasks to your workers) depends on the total number and size of point cloud frames you provide in your input manifest file. for frame input manifest files, this is the number of lines in your manifest file. for sequence manifest files, this is the number of frames in each sequence multiplied by the total number of sequences, or lines, in your manifest file.  additionally, the number of points per point cloud and the number of fused sensor data objects (like images) factor into job pre-processing times. on average, ground truth can pre-process 200 point cloud frames in approximately 5 minutes. if you create a 3d point cloud labeling job with a large number of point cloud frames, you might experience longer job pre-processing times. for example, if you create a sequence input manifest file with 4 point cloud sequences, and each sequence contains 200 point clouds, ground truth pre-processes 800 point clouds and so your job pre-processing time might be around 20 minutes. during this time, your labeling job status is .  while your 3d point cloud labeling job is pre-processing, you receive cloudwatch messages notifying you of the status of your job. to identify these messages, search for  in your labeling job logs.  for frame input manifest files, your cloudwatch logs will have a message similar to the following: the event log message,  identifies the number of frames from your input manifest that have been processed. you receive a new message every time a frame has been processed. for example, after a single frame has processed, you receive another message that says .  for sequence input manifest files, your cloudwatch logs will have a message similar to the following: the event log message,  identifies the number of sequences from your input manifest that have been processed. you receive a new message every time a sequence has been processed. for example, after a single sequence has processed, you receive a message that says  as the next sequence begins processing.  3d point cloud labeling jobs can take workers hours to complete. you can set the total amount of time that workers can work on each task when you create a labeling job. the maximum time you can set for workers to work on tasks is 7 days. the default value is 3 days.  it is strongly recommended that you create tasks that workers can complete within 12 hours. workers must keep the worker ui open while working on a task. they can save work as they go and ground truth will save their work every 15 minutes. when using the amazon sagemaker  api operation, set the total time a task is available to workers in the  parameter of .  when you create a labeling job in the console, you can specify this time limit when you select your workforce type and your work team. importantif you set your task time limit to be greater than 8 hours, you must set  for your iam execution role to at least 8 hours. see  for more information.  when you create a 3d point cloud labeling job, you need to specify a work team that will complete your point cloud annotation tasks. you can choose a work team from a private workforce of your own workers, or from a vendor workforce that you select in the aws marketplace. you cannot use the amazon mechanical turk workforce for 3d point cloud labeling jobs.  to learn more about vendor workforce, see . to learn how to create and manage a private workforce, see . ground truth provides a worker user interface (ui), tools, and assistive labeling features to help workers complete your 3d point cloud labeling tasks.  you can preview the worker ui when you create a labeling job in the console. when you create a labeling job using the api operation , you must provide an arn provided by ground truth in the parameter  to specify the worker ui for your task type. you can use  with the amazon sagemaker  api operation to preview the worker ui.  you provide worker instructions, labels, and optionally, label category attributes that are displayed in the worker ui. when you create a 3d point cloud object detection or object tracking labeling job, you can add one or more label category attributes to each label (class) that you specify. use label category attributes to have workers provide more information about the objects they annotate.  for example, you may create the label category car because you want workers to identify cars in a 3d scene. you might also want to capture additional data about your labeled cars, such as if they are occluded or the size of the car. you can capture this metadata using label category attributes.  for each attribute you assigned to a label, you can add multiple options that workers select from. workers can select a single value from those options. in the previous example, if you added the attribute occluded to the car label category, you might assign partial, completely, no to the occluded attribute and workers can select one of these options. to learn how to add label category attributes, use the create labeling job section on the  of your choice. you can provide worker instructions to help your workers complete your point cloud labeling tasks. you might want to use these instructions to do the following: best practices and things to avoid when annotating objects.explanation of the label category attributes provided (for object detection and object tracking tasks), and how to use them.advice on how to save time while labeling by using keyboard shortcuts. you can add your worker instructions using the amazon sagemaker console while creating a labeling job. if you create a labeling job using the api operation , you specify worker instructions in your label category configuration file.  in addition to your instructions, ground truth provides a link to help workers navigate and use the worker portal. view these instructions by selecting the task type on .  when you create a 3d point cloud labeling job, in addition to the permission requirements found in , you must add a cors policy to your s3 bucket that contains your input manifest file.  additionally if you choose to allow workers to work on tasks for more than 8 hours, you must increase the  of the iam execution role you use to create the labeling job.  when you create a 3d point cloud labeling job, you specify buckets in s3 where your input data and manifest file are located and where your output data will be stored. these buckets may be the same. you must attach the following cross-origin resource sharing (cors) policy to your input and output buckets:  to learn how to add a cors policy to an s3 bucket, see  in the amazon simple storage service console user guide. 3d point cloud labeling tasks can take workers more time to complete than other task types. you can set the total amount of time that workers can work on each task by doing one of the following:  when you create a labeling job in the console, set task timeout when you select your work team.using the  parameter when creating a labeling job using the amazon sagemaker api. the maximum time you can set for workers to work on tasks is 7 days. the default value is 3 days. if you set your task time limit to be greater than 8 hours, you must set  for your iam execution role to at least 8 hours.  to see how to update this value for your iam role, see  in the iam user guide, choose your preferred method to modify the role, and then follow the steps in . to learn more about permission requirements for the ground truth execution role, see . 
a checkpoint is a snapshot of the state of the model. they can be used with managed spot training. if a training job is interrupted, a snapshot can be used to resume from a previously saved point. this can save training time.  snapshots are saved to an amazon s3 location you specify. you can configure the local path to use for snapshots or use the default. when a training job is interrupted, amazon sagemaker copies the training data to amazon s3. when the training job is restarted, the checkpoint data is copied to the local path. it can be used to resume at the checkpoint.  to enable checkpoints, provide an amazon s3 location. you can optionally provide a local path and choose to use a shared folder. the default local path is . for more information, see . 
amazon sagemaker automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many training jobs on your dataset using the algorithm and ranges of hyperparameters that you specify. it then chooses the hyperparameter values that result in a model that performs the best, as measured by a metric that you choose. for example, suppose that you want to solve a  problem on a marketing dataset. your goal is to maximize the  metric of the algorithm by training an  model. you don't know which values of the , , , and  hyperparameters to use to train the best model. to find the best values for these hyperparameters, you can specify ranges of values that amazon sagemaker hyperparameter tuning searches to find the combination of values that results in the training job that performs the best as measured by the objective metric that you chose. hyperparameter tuning launches training jobs that use hyperparameter values in the ranges that you specified, and returns the training job with highest auc. you can use amazon sagemaker automatic model tuning with built-in algorithms, custom algorithms, and amazon sagemaker pre-built containers for machine learning frameworks. before you start using hyperparameter tuning, you should have a well-defined machine learning problem, including the following: a datasetan understanding of the type of algorithm you need to traina clear understanding of how you measure successyou should also prepare your dataset and algorithm so that they work in amazon sagemaker and successfully run a training job at least once. for information about setting up and running a training job, see . topics amazon sagemaker sets default limits for the following resources: number of concurrent hyperparameter tuning jobs - 100number of hyperparameters that can be searched - 20 noteevery possible value in a categorical hyperparameter counts against this limit.number of metrics defined per hyperparameter tuning job - 20number of concurrent training jobs per hyperparameter tuning job - 10number of training jobs per hyperparameter tuning job - 500maximum run time for a hyperparameter tuning job - 30 days when you plan hyperparameter tuning jobs, you also have to take the limits on training resources into account. for information about the default resource limits for amazon sagemaker training jobs, see . every concurrent training instance that all of your hyperparameter tuning jobs run on count against the total number of training instances allowed. for example, suppose you run 10 concurrent hyperparameter tuning jobs. each of those hyperparameter tuning jobs runs 100 total training jobs, and runs 20 concurrent training jobs. each of those training jobs runs on one ml.m4.xlarge instance. the following limits apply:  number of concurrent hyperparameter tuning jobs - you don't need to increase the limit, because 10 tuning jobs is below the limit of 100.number of training jobs per hyperparameter tuning job - you don't need to increase the limit, because 100 training jobs is below the limit of 500.number of concurrent training jobs per hyperparameter tuning job - you need to request a limit increase to 20, because the default limit is 10.amazon sagemaker training ml.m4.xlarge instances - you need to request limit increase to 200, because you have 10 hyperparameter tuning jobs, with each of them running 20 concurrent training jobs. the default limit is 20 instances.amazon sagemaker training total instance count - you need to request a limit increase to 200, because you have 10 hyperparameter tuning jobs, with each of them running 20 concurrent training jobs. the default limit is 20 instances.for information about requesting limit increases for aws resources, see . 
you can deploy a custom rule to monitor your training job either by using the  api or by using the open source  with the . the smdebug programming model provides the context for understanding this task. for information on the programming model, see . to run a custom rule, you have to provide a few additional parameters for the interface. key parameters are the python file that has the implementation of your  class, the name of the  class, the type of instance on which to run the  job, the size of the volume on that instance, and the docker image to use for running this job.  topics 
now that you have trained and deployed a model in amazon sagemaker, validate it to ensure that it generates accurate predictions on new data. that is, on data that is different from the data that the model was trained on. for this, use the test dataset that you created in . topics 
a component styled to look like a tab with information below. the following is an example template that uses the  element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  the following attributes are supported by this element. the text appearing on the tab. this is usually some short descriptive name indicative of the information contained below the tab. this element has the following parent and child elements. parent elements: child elements: none for more information, see the following. 
an icon that floats over the top right corner of another element to which it is attached. the following is an example of a template that uses the  element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  the following attributes are supported by this element. a string that specifies the id of the element to which the badge is attached. a string that specifies the icon to be displayed in the badge. the string must be either the name of an icon from the open-source  set, which is pre-loaded, or the url to a custom icon. this attribute overrides the label attribute.  the following is an example of the syntax that you can use to add an iron-icon to a  html element. replace  with the name of the icon you'd like to use from this .  the text to display in the badge. three characters or less is recommended because text that is too large will overflow the badge area. an icon can be displayed instead of text by setting the icon attribute. this element has the following parent and child elements. parent elements: child elements: nonefor more information, see the following. 
an amazon sagemaker notebook instance is a fully managed ml compute instance running the jupyter notebook app. amazon sagemaker manages creating the instance and related resources. use jupyter notebooks in your notebook instance to prepare and process data, write code to train models, deploy models to amazon sagemaker hosting, and test or validate your models. to create a notebook instance, use either the amazon sagemaker console or the  api. the notebook instance type you choose depends on how you use your notebook instance. you want to ensure that your notebook instance is not bound by memory, cpu, or io. if you plan to load a dataset into memory on the notebook instance for exploration or preprocessing, we recommend that you choose an instance type with enough ram memory for your dataset. this would require an instance with at least 16 gb of memory (.xlarge or larger). if you plan to use the notebook for compute intensive preprocessing, we recommend you choose a compute-optimized instance such as a c4 or c5. a best practice when using an amazon sagemaker notebook is to use the notebook instance to orchestrate other aws services. for example, you can use the notebook instance to manage large dataset processing by making calls to aws glue for etl (extract, transform, and load) services or amazon emr for mapping and data reduction using hadoop. you can use aws services as temporary forms of computation or storage for your data. you can store and retrieve your training and test data using an amazon s3 bucket. you can then use amazon sagemaker to train and build your model, so the instance type of your notebook would have no bearing on the speed of your model training and testing. after receiving the request, amazon sagemaker does the following: creates a network interface—if you choose the optional vpc configuration, amazon sagemaker creates the network interface in your vpc. it uses the subnet id that you provide in the request to determine which availability zone to create the subnet in. amazon sagemaker associates the security group that you provide in the request with the subnet. for more information, see . launches an ml compute instance—amazon sagemaker launches an ml compute instance in an amazon sagemaker vpc. amazon sagemaker performs the configuration tasks that allow it to manage your notebook instance, and if you specified your vpc, it enables traffic between your vpc and the notebook instance.installs anaconda packages and libraries for common deep learning platforms—amazon sagemaker installs all of the anaconda packages that are included in the installer. for more information, see . in addition, amazon sagemaker installs the tensorflow and apache mxnet deep learning libraries. attaches an ml storage volume—amazon sagemaker attaches an ml storage volume to the ml compute instance. you can use the volume as a working area to clean up the training dataset or to temporarily store validation, test, or other data. choose any size between 5 gb and 16384 gb, in 1 gb increments, for the volume. the default is 5 gb. ml storage volumes are encrypted, so amazon sagemaker can't determine the amount of available free space on the volume. because of this, you can increase the volume size when you update a notebook instance, but you can't decrease the volume size. if you want to decrease the size of the ml storage volume in use, create a new notebook instance with the desired size. only files and data saved within the  folder persist between notebook instance sessions. files and data that are saved outside this directory are overwritten when the notebook instance stops and restarts. each notebook instance's /tmp directory provides a minimum of 10 gb of storage in an instant store. an instance store is temporary, block-level storage that isn't persistent. when the instance is stopped or restarted, amazon sagemaker deletes the directory's contents. this temporary storage is part of the root volume of the notebook instance. copies example jupyter notebooks— these python code examples illustrate model training and hosting exercises using various algorithms and training datasets.to create a amazon sagemaker notebook instance: open the amazon sagemaker console at .  choose notebook instances, then choose create notebook instance. on the create notebook instance page, provide the following information:  for notebook instance name, type a name for your notebook instance. for notebook instance type, choose an instance type for your notebook instance. for a list of supported instance types, see .  for elastic inference, choose an inference accelerator type to associate with the notebook instance if you plan to conduct inferences from the notebook instance, or choose none. for information about elastic inference, see . (optional) additional configuration lets advanced users create a shell script that can run when you create or start the instance. this script, called a lifecycle configuration script, can be used to set the environment for the notebook or to perform other functions. for information, see . (optional) additional configuration also lets you specify the size, in gb, of the ml storage volume that is attached to the notebook instance. you can choose a size between 5 gb and 16,384 gb, in 1 gb increments. you can use the volume to clean up the training dataset or to temporarily store validation or other data. for iam role, choose either an existing iam role in your account that has the necessary permissions to access amazon sagemaker resources or choose create a new role. if you choose create a new role, amazon sagemaker creates an iam role named . the aws managed policy  is attached to the role. the role provides permissions that allow the notebook instance to call amazon sagemaker and amazon s3. for root access, to enable root access for all notebook instance users, choose enable. to disable root access for users, choose disable.if you enable root access, all notebook instance users have administrator privileges and can access and edit all files on it.  (optional) encryption key lets you encrypt data on the ml storage volume attached to the notebook instance using an aws key management service (aws kms) key. if you plan to store sensitive information on the ml storage volume, consider encrypting the information.  (optional) network lets you put your notebook instance inside a virtual private cloud (vpc). a vpc provides additional security and restricts access to resources in the vpc from sources outside the vpc. for more information on vpcs, see . to add your notebook instance to a vpc: choose the vpc and a subnetid. for security group, choose your vpc's default security group.  if you need your notebook instance to have internet access, enable direct internet access. for direct internet access, choose enable. internet access can make your notebook instance less secure. for more information, see .  (optional) to associate git repositories with the notebook instance, choose a default repository and up to three additional repositories. for more information, see . choose create notebook instance.  in a few minutes, amazon sagemaker launches an ml compute instance—in this case, a notebook instance—and attaches an ml storage volume to it. the notebook instance has a preconfigured jupyter notebook server and a set of anaconda libraries. for more information, see the  api.  when the status of the notebook instance is , in the console, the notebook instance is ready to use. choose open jupyter next to the notebook name to open the classic jupyter dashboard.  you can choose open jupyterlab to open the jupyterlab dashboard. the dashboard provides access to your notebook instance and sample amazon sagemaker notebooks that contain complete code walkthroughs. these walkthroughs show how to use amazon sagemaker to perform common machine learning tasks. for more information, see . for more information, see . for more information about jupyter notebooks, see . 
the following table contains the subset of hyperparameters that are required or most commonly used for the amazon sagemaker xgboost algorithm. these are parameters that are set by users to facilitate the estimation of model parameters from data. the required hyperparameters that must be set are listed first, in alphabetical order. the optional hyperparameters that can be set are listed next, also in alphabetical order. the amazon sagemaker xgboost algorithm is an implementation of the open-source dmlc xgboost package. currently amazon sagemaker supports version 0.90. for details about full set of hyperparameter that can be configured for this version of xgboost, see . 
 is a popular and efficient open-source implementation of the gradient boosted trees algorithm. gradient boosting is a supervised learning algorithm, which attempts to accurately predict a target variable by combining the estimates of a set of simpler, weaker models. when using  for regression, the weak learners are regression trees, and each regression tree maps an input data point to one of its leafs that contains a continuous score. xgboost minimizes a regularized (l1 and l2) objective function that combines a convex loss function (based on the difference between the predicted and target outputs) and a penalty term for model complexity (in other words, the regression tree functions). the training proceeds iteratively, adding new trees that predict the residuals or errors of prior trees that are then combined with previous trees to make the final prediction. it's called gradient boosting because it uses a gradient descent algorithm to minimize the loss when adding new models. for more detail on xgboost, see: 
after you have created a private workforce, you can do the following using either the amazon sagemaker or amazon cognito console. note, if you add workers to a workforce using the amazon cognito console, you must use the same console to remove the worker from the workforce. add work teams to your workforceadd workers to your workforce and one or more work teamsdisable or remove workers from your workforce and one or more workteams you can restrict access to tasks to workers at specific ip addresses using the amazon sagemaker api. for more information, see . noteyour private workforce is shared between amazon sagemaker ground truth and amazon augmented ai. to manage private work teams and workers used by amazon augmented ai, use the ground truth section of the amazon sagemaker console or the amazon cognito user pool that you used to create your shared private workforce.  topics 
when setting a problem type, such as binary classification or regression, with the automl api, you have the option of specifying it or of letting amazon sagemaker autopilot detect it on your behalf. you set the type of problem with the  parameter. this limits the kind of preprocessing and algorithms that autopilot tries. when the job is finished, if you had set the , then the  will match the  you set. if you leave it blank (or ), the  will be whatever autopilot decides on your behalf.  notein some cases, autopilot is unable to infer the  with high enough confidence, in which case you must provide the value for the job to succeed. your problem type options are as follows:  topics regression estimates the values of a dependent target variable based on one or more other variables or attributes that are correlated with it. an example is the prediction of house prices using features like the number of bathrooms and bedrooms, square footage of the house and garden. regression analysis can create a model that takes one or more of these features as an input and predicts the price of a house. binary classification is a type of supervised learning that assigns an individual to one of two predefined and mutually exclusive classes based on their attributes. it is supervised because the models are trained using examples where the attributes are provided with correctly labelled objects. a medical diagnosis for whether an individual has a disease or not based on the results of diagnostic tests is an example of binary classification. multiclass classification is a type of supervised learning that assigns an individual to one of several classes based on their attributes. it is supervised because the models are trained using examples where the attributes are provided with correctly labelled objects. an example is the prediction of the topic most relevant to a text document. a document may be classified as being about, say, religion or politics or finance, or about one of several other predefined topic classes. 
a human loop starts your human review workflow and sends data review tasks to human workers. when you use one of the amazon a2i built-in task types, the corresponding aws service creates and starts a human loop on your behalf when the conditions specified in your flow definition are met. if no conditions were specified in your flow definition, a human loop is created for each object. when using amazon a2i for a custom task, a human loops starts when  is called in your application.  use the following instructions to configure a human loop with amazon rekognition or amazon textract built-in task types and custom task types.  prerequisites to create and start a human loop, the amazonaugmentedaifullaccess policy must be attached to the aws identity and access management (iam) user or role that configures or starts the human loop. this will be the identity that you use to configure the human loop using  for built-in task types. for custom task types, this will be the identity that you use to call . additionally, when using a built-in task type, your iam user or role must have permission to invoke api operations of the aws service associated with your task type. for example, if using amazon rekognition with augmented ai, you must attach permissions required to call . for examples of identity-based policies you can use to grant these permissions, see  and . you can also use the more general policy amazonaugmentedaiintegratedapiaccess to grant these permissions. for more information, see .  to create and start a human loop, you will need a flow definition arn. to learn how to create a flow definition (or human review workflow), see . to start a human loop for a built-in task type jobs use the corresponding service's api to provide your input data and to configure the human loop. for amazon textract, you use the  api operation. for amazon rekognition, you use the  api operation. you can use the aws cli, or a language-specific sdk to create requests using these api operations.  after you start your ml job using your built-in task type's aws service api, amazon a2i monitors the inference-results of that service. for example, when running a job with amazon rekognition, amazon a2i checks the inference confidence score for each image and compares it to the confidence-thresholds specified in your flow definition. if the conditions to start a human review task are satisfied, or if you didn't specify conditions in your flow definition, a human review task is sent to workers.  amazon a2i integrates with amazon textract so that you can configure and start a human loop using the amazon textract api. to send a document file to amazon textract for text analysis, you use the amazon textract . to configure a human loop, set the  parameter when you configure . to learn how, see step 3 in  in the amazon textract developer guide.  after you run the  with a human loop configured, amazon a2i monitors the results from  and checks it against the flow definition's activation conditions. if amazon textract inference confidence score for one or more key value pairs meets the conditions for review, amazon a2i starts a human review loop and includes the  object in the  response. amazon a2i integrates with amazon rekognition so that you can configure and start a human loop using the amazon rekognition api. to send images to amazon rekognition for content moderation, you use the amazon rekognition . to configure a human loop, set the  parameter when you configure . to learn how, see step 3 in  in the amazon rekognition developer guide.  after you run the  with a human loop configured, amazon a2i monitors the results from  and checks it against the flow definition's activation conditions. if the amazon rekognition inference confidence score for an image meets the conditions for review, amazon a2i starts a human review loop and includes the response element  response object in the  response. to configure a human loop for a custom human review task, use the  operation within your application. this section provides an example of a human loop request using the aws sdk for python (boto 3) and the aws command line interface (aws cli). for documentation on other language specific sdk's that support , use the see also section of  in the amazon augmented ai runtime api documentation.  prerequisites to complete this procedure, you need: input data formatted as a string representation of a json-formatted file.the amazon resource name (arn) of your flow definitiona flow definition arn. to configure the human loop for , specify a set of  related to the input provided to the  operation. use content classifiers to decalre that your content is free of personally identifiable information or adult content.  to use amazon mechanical turk, ensure your data is free of personally identifiable information and include the  content classifier. if you data is free of adult content, also include the  classifier. if you do not use these content classifiers, amazon sagemaker may restrict the mechanical turk workers that can view your task. for , enter the amazon resource name (arn) of your flow definition. for , enter your input data as a string representation of a json-formatted file. structure your input data and custom worker task template so that your input data is properly displayed to human workers when you start your human loop. see to learn how to preview your custom worker task template.  for , enter a name for the human loop. the name must be unique within the region in your account, and can have up to 63 characters. valid characters: a-z, 0-9, and - (hyphen). to start a human loop to start a human loop, submit a request similar to the following examples using your preferred language specific sdk. the following request example uses the sdk for python (boto 3). for more information, see  in the aws sdk for python (boto) api reference. the following request example uses the aws cli. for more information, see  in the .  when you successfully start a human loop by invoking  directly, the response will include a  and a  object which will be set to . you can use this the human loop name to monitor and manage your human loop. after starting a human loop, you can manage and monitor it with the amazon augmented ai runtime api and amazon cloudwatch events. to learn more, see . 
the schema defined in the  file specifies the statistical parameters to be calculated for the baseline and data that is captured. it also configures the bucket to be used by , a very compact quantiles sketch with lazy compaction scheme. note the following: the specified metrics are recognized by amazon sagemaker in later visualization changes. the container can emit more metrics if required. is the recognized sketch. custom containers can write their own representation, but it won’t be recognized by amazon sagemaker in visualizations.by default, the distribution is materialized in 10 buckets. you can't change this.
you can provide amazon sagemaker processing with a docker image that has your own code and dependencies to run your data processing, feature engineering, and model evaluation workloads.  the following example of a dockerfile builds a container with the python libraries scikit-learn and pandas, which you can run as a processing job.  build and push this docker image to an amazon elastic container registry (amazon ecr) repository and ensure that your amazon sagemaker iam role can pull the image from amazon ecr. then you can run this image on amazon sagemaker processing. amazon sagemaker processing runs your processing container image in a similar way as the following command, where  is the amazon ecr image uri that you specify in a  operation.  this command runs the  command configured in your docker image.  you can also override the entrypoint command in the image or give command-line arguments to your entrypoint command using the  and  parameters in your  request. specifying these parameters configures amazon sagemaker processing to run the container similar to the way that the following command does.  for example, if you specify the  to be  in your request, and  to be , amazon sagemaker processing runs your container with the following command.   when building your processing container, consider the following details:  amazon sagemaker processing decides whether the job completes or fails depending on the exit code of the command run. a processing job completes if all of the processing containers exit successfully with an exit code of 0, and fails if any of the containers exits with a non-zero exit code.amazon sagemaker processing lets you override the processing container's entrypoint and set command-line arguments just like you can with the docker api. docker images can also configure the entrypoint and command-line arguments using the  and cmd instructions. the way 's  and  parameters configure a docker image's entrypoint and arguments mirrors how docker overrides the entrypoint and arguments through the docker api:if neither  nor  are provided, processing uses the default  or cmd in the image.if  is provided, but not , processing runs the image with the given entrypoint, and ignores the  and cmd in the image.if  is provided, but not , processing runs the image with the default  in the image and with the provided arguments.if both  and  are provided, processing runs the image with the given entrypoint and arguments, and ignores the  and cmd in the image.you must use the exec form of the  instruction in your dockerfile (  instead of the shell form (). this lets your processing container receive  and  signals, which processing uses to stop processing jobs with the  api. and all its subdirectories are reserved by amazon sagemaker. when building your processing docker image, don't place any data required by your processing container in these directories.if you plan to use gpu devices, make sure that your containers are nvidia-docker compatible. include only the cuda toolkit in containers. don't bundle nvidia drivers with the image. for more information about nvidia-docker, see .when you create a processing job using the  operation, you can specify multiple  and . values.  you use the  parameter to specify an amazon simple storage service (amazon s3) uri to download data from, and a path in your processing container to download the data to. the  parameter configures a path in your processing container from which to upload data, and where in amazon s3 to upload that data to. for both  and , the path in the processing container must begin with . for example, you might create a processing job with one  parameter that downloads data from  into  in your processing container, and a  parameter that uploads data from  to . your processing job would read the input data, and write output data to . then it uploads the data written to this path to the specified amazon s3 output location.  when your processing container writes to  or , amazon sagemaker processing saves the output from each processing container and puts it in amazon cloudwatch logs. for information about logging, see . amazon sagemaker processing also provides cloudwatch metrics for each instance running your processing container. for information about metrics, see .  amazon sagemaker processing provides configuration information to your processing container through environment variables and two json files— and — at predefined locations in the container.  when a processing job starts, it uses the environment variables that you specified with the  map in the  request. the  file contains information about the hostnames of your processing containers, and is also specified in the  request.  the following example shows the format of the  file. the  file contains information about the hostnames of your processing containers. use the following hostnames when creating or running distributed processing code. don't use the information about hostnames contained in  or  because it might be inaccurate. hostname information might not be immediately available to the processing container. we recommend adding a retry policy on hostname resolution operations as nodes become available in the cluster. to save metadata from the processing container after exiting it, containers can write utf-8 encoded text to the  file. after the processing job enters any terminal status ("", "", or ""), the "" field in  contains the first 1 kb of this file. access that initial part of file with a call to , which returns it through the  parameter. for failed processing jobs, you can use this field to communicate information about why the processing container failed. importantdon't write sensitive data to the  file.  if the data in this file isn't utf-8 encoded, the job fails and returns a . if multiple containers exit with an  the content of the  from each processing container is concatenated, then truncated to 1 kb. you can use the amazon sagemaker python sdk to run your own processing image by using the  class. the following example shows how to run your own processing container with one input from amazon simple storage service (amazon s3) and one output to amazon s3. instead of building your processing code into your processing image, you can provide a  with your image and the command that you want to run, along with the code that you want to run inside that container. for an example, see . you can also use the scikit-learn image that amazon sagemaker processing provides through  to run scikit-learn scripts. for an example, see .  
you can also visualize the results of monitoring in amazon sagemaker studio. for information about the onboarding process for using studio, see .  you can view monitoring results at your endpoints.  you can view the jobs being monitoring.  you can take a deep dive into each monitoring results for each job.  
your notebook instance contains example notebooks provided by amazon sagemaker. the example notebooks contain code that shows how to apply machine learning solutions by using amazon sagemaker. notebook instances use the  jupyter extension, which enables you to view a read-only version of an example notebook or create a copy of it so that you can modify and run it. for more information about the  extension, see . for information about example notebooks for amazon sagemaker studio, see . noteexample notebooks typically download datasets from the internet. if you disable amazon sagemaker-provided internet access when you create you notebook instance, example notebooks might not work. for more information, see . to view or use the example notebooks in the classic jupyter view, choose the sagemaker examples tab.  to view a read-only version of an example notebook in the jupyter classic view, on the sagemaker examples tab, choose preview for that notebook. to create a copy of an example notebook in the home directory of your notebook instance, choose use. in the dialog box, you can change the notebook's name before saving it.  to view or use the example notebooks in the jupyterlab view, choose the examples icon in the left navigation panel.  to view a read-only version of an example notebook, choose the name of the notebook. this opens the notebook as a tab in the main area. to create a copy of an example notebook in the home directory of your notebook instance, choose create a copy in the top banner. in the dialog box, type a name for the notebook and then choose create copy. for more information about the example notebooks, see the . 
all amazon sagemaker built-in algorithms adhere to the common input inference format described in . this topic contains a list of the available output formats for the amazon sagemaker ntm algorithm. 
 is a library that implements the functionality that you need to create containers to run scripts, train algorithms, and deploy models on amazon sagemaker. when installed, the library defines the following for users: the locations for storing code and other resources. the entry point that contains the code to run when the container is started. your dockerfile must copy the code that needs to be run into the location expected by a container that is compatible with amazon sagemaker. other information that a container needs to manage deployments for training and inference. to install the amazon sagemaker containers library, use the  command in your dockerfile.  after you build a docker image, you push it to the amazon elastic container registry (amazon ecr). to create a container, you pull the image from amazon ecr and then build the container using the  command. when amazon sagemaker trains a model, it creates the following file folder structure in the container's  directory. when you run a model training job, the amazon sagemaker container uses the  directory, which contains the json files that configure the hyperparameters for the algorithm and the network layout used for distributed training. the  directory also contains files that specify the channels through which amazon sagemaker accesses the data, which is stored in amazon simple storage service (amazon s3). the amazon sagemaker containers library places the scripts that the container will run in the  directory. your script should write the model generated by your algorithm to the  directory. for more information, see . when you host a trained model on amazon sagemaker to make inferences, you deploy the model to an http endpoint. the model makes real-time predictions in response to inference requests. the container must contain a serving stack to process these requests. in a hosting or batch transform container, the model files are located in the same folder that they were written to during training. for more information, see . you can provide separate docker images for the training algorithm and for the inference code. or you can use a single docker image for both. when creating docker images for use with amazon sagemaker, consider the following: providing two docker images can increase storage requirements and cost because common libraries might be duplicated.in general, smaller containers start faster for both training and hosting. models train faster and the hosting service can react to increases in traffic by automatically scaling more quickly.you might be able to write an inference container that is significantly smaller than the training container. this is especially common when you use gpus for training, but your inference code is optimized for cpus.amazon sagemaker requires that docker containers run without privileged access.both docker containers that you build and those provided by amazon sagemaker can send messages to the  and  files. amazon sagemaker sends these messages to amazon cloudwatch logs in your aws account.for more information about how to create amazon sagemaker containers and how scripts are executed inside them, see the  repository on github. it also provides lists of important environmental variables and the environmental variables provided by amazon sagemaker containers. 
the costs incurred for running amazon sagemaker studio notebooks, interactive shells, consoles, and terminals are based on amazon elastic compute cloud (amazon ec2) instance usage. when you launch the following resources, you must choose a sagemaker image and kernel: from the studio launcher notebookinteractive shellimage terminalfrom the file menu notebookconsolewhen launched, the resource is run on an amazon ec2 instance of an instance type based on the chosen sagemaker image and kernel. if an instance of that type was previously launched and is available, the resource is run on that instance. if an instance of that type is not available, the resource is run on a new default instance of that type. for cpu based images, the default instance type is . for gpu based images, the default instance type is . the costs incurred are based on the instance type and the number of instances of each instance type. you are billed separately for each instance. metering starts when an instance is created. metering ends when the sagemaker image running on the instance is shut down. for information on how to shut down a sagemaker image, see . importantyou must shut down the sagemaker image on the instance to stop incurring charges. if you shut down the notebook running on the instance but don't shut down the sagemaker image, you will still incur charges. when you open multiple notebooks on the same instance type, the notebooks run on the same instance even if they're using different kernels. you are billed only for the time that one instance is running. you can change the instance type from within the notebook after you open it. for more information, see . metering example in this example, the customer performed the following actions: opened notebook #1. studio opens this on a  instance type.after an hour, opened notebook #2. this notebook runs on the same instance as notebook #1, a  instance type.after another hour, opened notebook #3 and immediately switched to a  instance type.after another hour, shut down the kernels on all instance types.the notebooks ran for the following amount of time: notebook #1: 3 hoursnotebook #2: 2 hoursnotebook #3: 1 hoursthe customer is billed for the following usage: during the second and third hours, notebook #1 and notebook #2 ran on the same  instance. importantthis topic discusses costs only as it applies to the amount of time an instance is running. additional costs can be incurred, for example, from running the code in a notebook, such as compute time due to a training job. for more information about billing along with pricing examples, see . on the pricing page, under one of the aws regions supported by amazon sagemaker studio, review the building section where two types of notebooks are listed: on-demand ml notebook instances and amazon sagemaker studio notebook instances. in this example, we're discussing amazon sagemaker studio notebook instances. 
you can use a notebook instance created with a custom lifecycle configuration script to access aws services from your notebook. for example, you can create a script that lets you use your notebook with sparkmagic to control other aws resources, such as an amazon emr instance. you can then use the amazon emr instance to process your data instead of running the data analysis on your notebook. this allows you to create a smaller notebook instance because you won't use the instance to process data. this is helpful when you have large datasets that would require a large notebook instance to process the data. the process requires three procedures using the amazon sagemaker console: create the amazon emr spark instancecreate the jupyter notebooktest the notbook-to-amazon emr connectionto create an amazon emr spark instance that can be controlled from a notebook using sparkmagic open the amazon emr console at . in the navigation pane, choose create cluster. on the create cluster - quick options page, under software configuration, choose spark: spark 2.4.4 on hadoop 2.8.5 yarn with ganglia 3.7.2 and zeppelin 0.8.2. set additional parameters on the page and then choose create cluster. on the cluster page, choose the cluster name that you created. note the master public dns, the emr master's security group, and the vpc name and subnet id where the emr cluster was created. you will use these values when you create a notebook. to create a notebook that uses sparkmagic to control an amazon emr spark instance open the amazon sagemaker console at . in the navigation pane, under notebook instances, choose create notebook. enter the notebook instance name and choose the instance type. choose additional configuration, then, under lifecycle configuration, choose create a new lifecycle configuration. add the following code to the lifecycle configuration script: ``` # overview # this script connects an amazon emr cluster to an amazon sagemaker notebook instance that uses sparkmagic. #  # note that this script will fail if the amazon emr cluster's master node ip address is not reachable. #   1. ensure that the emr master node ip is resolvable from the notebook instance. #      one way to accomplish this is to have the notebook instance and the amazon emr cluster in the same subnet. #   2. ensure the emr master node security group provides inbound access from the notebook instance security group. #       type        - protocol - port - source #       custom tcp  - tcp      - 8998 - $notebook_security_group #   3. ensure the notebook instance has internet connectivity to fetch the sparkmagic example config.# #  # parameters emr_master_ip=your.emr.master.ip    cd /home/ec2-user/.sparkmagic    echo "fetching sparkmagic example config from github..."    wget     echo "replacing emr master node ip in sparkmagic config..."    sed -i -- "s/localhost/$emr_master_ip/g" example_config.json    mv example_config.json config.json    echo "sending a sample request to livy.."    curl "$emr_master_ip:8998/sessions"    current session configs: {'drivermemory': '1000m', 'executorcores': 2, 'kind': 'pyspark'}                        no active sessions.    ``` 
use this page to become familiarize with the user interface and tools available to complete your 3d point cloud semantic segmentation task. topics when you work on a 3d point cloud semantic segmentation task, you need to select a category from the annotations menu on the right side of your worker portal using the drop down menu label categories. after you've selected a category, use the paint brush and polygon tools to paint each object in the 3d point cloud that this category applies to. for example, if you select the category car, you would use these tools to paint all of the cars in the point cloud. the following video demonstrates how to use the paint brush tool to paint an object.  if you see one or more images in your worker portal, you can paint in the images or paint in the 3d point cloud and the paint will show up in the other medium.  importantif you see that objects have already been painted when you open the task, adjust those annotations. the following video includes an image that can be annotated. you may not see an image in your task.   after you've painted one or more objects using a lable category, you can select that category from the the label category menu on the right to only view points painted for that category.   you can navigate in the 3d scene using their keyboard and mouse. you can: double click on specific objects in the point cloud to zoom into them.use a mouse-scroller or trackpad to zoom in and out of the point cloud.use both keyboard arrow keys and q, e, a, and d keys to move up, down, left, right. use keyboard keys w and s to zoom in and out. the following video demonstrates movements around the 3d point cloud and in the side-view. you can hide and re-expand all side views using the full screen icon. in this gif, the side-views and menus have been collapsed.  when you are in the worker ui, you see the following menus: instructions – review these instructions before starting your task.shortcuts – use this menu to view keyboard shortcuts that you can use to navigate the point cloud and use the annotation tools provided. view – use this menu to toggle different view options on and off. for example, you can use this menu to add a ground mesh to the point cloud, and to choose the projection of the point cloud. 3d point cloud – use this menu to add additional attributes to the points in the point cloud, such as color, and pixel intensity. note that some or all of these options may not be available.paint – use this menu to modify the functionality of the paint brush. when you open a task, the move scene icon is on, and you can move around the point cloud using your mouse and the navigation buttons in the point cloud area of the screen. to return to the original view you see when you first opened the task, choose the reset scene icon.  after you select the paint icon, you can add paint to the point cloud and images (if included). you must select the move scene icon again to move to another area in the 3d point cloud or image.  to collapse all panels on the right and make the 3d point cloud full screen, select the full screen icon.  for the camera images and side-panels, you have the following view options: c – view the camera angle on point cloud view.f – view the frustum, or field of view, of the camera used to capture that image on point cloud view. p – view the point cloud overlaid on the image. use this table to learn about the icons available in your worker task portal.  the shortcuts listed in the shortcuts menu can help you navigate the 3d point cloud and use the paint tool. before you start your task, we recommend that you review the shortcuts menu and become acquainted with these commands.  you should periodically save your work. ground truth will automatically save your work ever 15 minutes.  when you open a task, you must complete your work on it before pressing submit. if you select stop working you will loose that task, and other workers will be able to start working on it.  
you can use ground truth 3d point cloud labeling modality for a variety of use cases. the following list briefly describes each 3d point cloud task type. for additional details and instructions on how to create a labeling job using a specific task type, select the task type name to see its task type page.   – use this task type when you want workers to locate and classify objects in a 3d point cloud by adding and fitting 3d cuboids around objects.  – use this task type when you want workers to add and fit 3d cuboids around objects to track their movement across a sequence of 3d point cloud frames. for example, you can use this task type to ask workers to track the movement of vehicles across multiple point cloud frames. use this task type when you want workers to create a point-level semantic segmentation mask by painting objects in a 3d point cloud using different colors where each color is assigned to one of the classes you specify.  – each of the task types above has an associated adjustment task type that you can use to audit and adjust annotations generated from a 3d point cloud labeling job. refer to the task type page of the associated type to learn how to create an adjustment labeling job for that task. 
you can deploy additional models to a multi-model endpoint and invoke them through that endpoint immediately. when adding a new model, you don't need to update or bring down the endpoint, so you avoid the cost of creating and running a separate endpoint for each new model.   amazon sagemaker unloads unused models from the container when the instance is reaching memory capacity and more models need to be downloaded into the container. amazon sagemaker also deletes unused model artifacts from the instance storage volume when the volume is reaching capacity and new models need to be downloaded. the first invocation to a newly added model takes longer because the endpoint takes time to download the model from s3 to the container's memory in instance hosting the endpoint with the endpoint already running, copy a new set of model artifacts to the amazon s3 location there you store your models. importantto update a model, proceed as you would when adding a new model. use a new and unique name. don't overwrite model artifacts in amazon s3 because the old version of the model might still be loaded in the containers or on the storage volume of the instances on the endpoint. invocations to the new model could then invoke the old version of the model.  client applications can request predictions from the additional target model as soon as it is stored in s3. to delete a model from a multi-model endpoint, stop invoking the model from the clients and remove it from the s3 location where model artifacts are stored. 
after you have created an endpoint, configure the permissions and paths to amazon s3 locations for storing data, report, and processing code. upload the pre-trained model to amazon s3:  enable data capture: you specify the capture option called . you can capture the request payload, the response payload, or both with this configuration. the capture configuration applies to all variants. invoke the deployed model: you can now send data to this endpoint to get inferences in real time. because you enabled the data capture in the previous steps, the request and response payload, along with some additional metadata, is saved in the amazon s3 location that you specified in . view captured data: list the data capture files stored in amazon s3. expect to see different files from different time periods, organized based on the hour when the invocation occurred.  the format of the amazon s3 path is: 
amazon sagemaker provides api reference documentation that describes all of the rest operations and data types used by autopilot and a higher level amazon sagemaker python sdk that you can use to create and manage automl jobs. it also provides a command line interface (cli), an aws sdk for python (boto) for low-level clients of amazon sagemaker services, and sdks for .net, c++, go, java, javascript, php v3, and ruby v3. the following sections describe these autopilot programming interfaces. topics this api provides http service apis for creating and managing amazon sagemaker autopilot resources. actions data types for more information on the entire amazon sagemaker rest api, see . this python library provides several high-level abstractions for working with amazon sagemaker. the following classes can be used to manage automl jobs. for more information how this python sdk simplifies model training and deployment, see . the  provides apis for creating and managing amazon sagemaker resources. here are the  autopilot commands. boto is the amazon web services (aws) sdk for python. it enables python developers to create, configure, and manage aws services such as amazon sagemaker. boto provides a low-level  api that maps to the underlying amazon sagemaker service api. here is a list of the methods used to manage automl jobs with the client class. the .net sdk enables developers to create, configure, and manage aws services such as amazon sagemaker. the api maps to the underlying amazon sagemaker service api. here is a list of the methods used to manage automl jobs with the client class. the c++ sdk enables developers to create, configure, and manage aws services such as amazon sagemaker. the api maps to the underlying amazon sagemaker service api. for information on the methods used to manage automl jobs with the client class, see . the go sdk enables developers to create, configure, and manage aws services such as amazon sagemaker. the api maps to the underlying amazon sagemaker service api. here is a list of the methods used to manage automl jobs with the client class. the java sdk enables developers to create, configure, and manage aws services such as amazon sagemaker. the api maps to the underlying amazon sagemaker service api. here is a list of the methods used to manage automl jobs with the client class. the javascript sdk enables developers to create, configure, and manage aws services such as amazon sagemaker. the api maps to the underlying amazon sagemaker service api. here is a list of the methods used to manage automl jobs with the client class. the php v3 sdk enables developers to create, configure, and manage aws services such as amazon sagemaker. the api maps to the underlying amazon sagemaker service api. here is a list of the methods used to manage automl jobs with the client class. the ruby v3 sdk enables developers to create, configure, and manage aws services such as amazon sagemaker. the api maps to the underlying amazon sagemaker service api. here is a list of the methods used to manage automl jobs with the client class. 
the following are the available data input formats for the ip insights algorithm. amazon sagemaker built-in algorithms adhere to the common input training format described in . however, the amazon sagemaker ip insights algorithm currently supports only the csv data input format. the csv file must have two columns. the first column is an opaque string that corresponds to an entity's unique identifier. the second column is the ipv4 address of the entity's access event in decimal-dot notation.  content-type: text/csv 
a built-in rule can be configured for a training job using the  and  objects in the  api. these rules are run on a pre-built docker image listed in the  topic. you specify the url registry address for the pre-built docker image in the  parameter. the following code sample shows how to configure a built-in  rule using this amazon sagemaker api. with a configuration like the one in this sample, amazon sagemaker debugger starts a rule evaluation job for your training job using the amazon sagemaker  rule. 
noteamazon sagemaker studio is available only in specific aws regions. to view the list of supported regions, see . this topic describes how to onboard to amazon sagemaker using the quick start procedure, which uses aws identity and access management (iam) authentication. for information on how to onboard using the standard iam procedure, see . for information on how to onboard using aws single sign-on (aws sso), see . to onboard to studio using quick start** open the . choose amazon sagemaker studio at the top left of the page. on the amazon sagemaker studio control panel, under get started, choose quick start. for user name, keep the default name or create a new name. the name can be up to 63 characters. valid characters: a-z, a-z, 0-9, and - (hyphen).  for execution role, choose an option from the role selector. if you choose enter a custom iam role arn, the role must have the amazonsagemakerfullaccess policy attached. if you choose create a new role, the create an iam role dialog opens: for s3 buckets you specify, specify additional s3 buckets that users of your notebooks can access. if you don't want to add access to more buckets, choose none.choose create role. amazon sagemaker creates a new iam role with the  policy attached.choose submit. on the amazon sagemaker studio control panel, under studio summary, wait for status to change to ready. when status is ready, the user name that you specified is enabled and chosen. the add user and delete user buttons, and the open studio link are also enabled. choose open studio. the amazon sagemaker studio loading page displays. when studio opens you can start using it. now that you've onboarded to amazon sagemaker studio, use the following steps to access studio later. to access studio after you onboard open the . choose amazon sagemaker studio at the top left of the page. on the amazon sagemaker studio control panel, choose your user name and then choose open studio. to add more users on the amazon sagemaker studio control panel, choose add user. repeat steps 4 and 5 from the first procedure, "to onboard to studio using quick start." choose submit. for information about using amazon sagemaker studio, see . 
to handle multiple models, your container must support a set of apis that enable the amazon sagemaker platform to communicate with the container for loading, listing, getting, and unloading models as required. the  is used in the new set of apis as the key input parameter. the customer container is expected to keep track of the loaded models using  as the mapping key. also, the  is an opaque identifier and is not necessarily the value of the  parameter passed into the  api. the original  value in the  request is passed to container in the apis as a  header that can be used for logging purposes. topics instructs the container to load a particular model present in the  field of the body into the memory of the customer container and to keep track of it with the assigned . after a model is loaded, the container should be ready to serve inference requests using this . noteif  is already loaded, this api should return 409. any time a model cannot be loaded due to lack of memory or to any other resource, this api should return a 507 http status code to amazon sagemaker, which then initiates unloading unused models to reclaim. returns the list of models loaded into the memory of the customer container. this api also supports pagination. amazon sagemaker can initially call the list models api without providing a value for . if a  field is returned as part of the response, it will be provided as the value for  in a subsequent list models call. if a  is not returned, it means that there are no more models to return. this is a simple read api on the  entity. noteif  is not loaded, this api should return 404. instructs the amazon sagemaker platform to instruct the customer container to unload a model from memory. this initiates the eviction of a candidate model as determined by the platform when starting the process of loading a new model. the resources provisioned to  should be reclaimed by the container when this api returns a response. noteif  is not loaded, this api should return 404. makes a prediction request from the particular  supplied. the amazon sagemaker runtime  request supports  as a new header that takes the relative path of the model specified for invocation. the amazon sagemaker system constructs the absolute path of the model by combining the prefix that is provided as part of the  api call with the relative path of the model. noteif  is not loaded, this api should return 404. 
an amazon sagemaker training job is an iterative process that teaches a model to make predictions by presenting examples from a training dataset. typically, a training algorithm computes several metrics, such as training error and prediction accuracy. these metrics help diagnose whether the model is learning well and will generalize well for making predictions on unseen data. the training algorithm writes the values of these metrics to logs, which amazon sagemaker monitors and sends to amazon cloudwatch in real time. to analyze the performance of your training job, you can view graphs of these metrics in cloudwatch. when a training job has completed, you can also get a list of the metric values that it computes in its final iteration by calling the  operation. topics the following sample notebooks show how to view and plot training metrics: for instructions how to create and access jupyter notebook instances that you can use to run the examples in amazon sagemaker, see . to see a list of all the amazon sagemaker samples, after creating and opening a notebook instance, choose the sagemaker examples tab. to access the example notebooks that show how to use training metrics,  and ., from the introduction to amazon algorithms section. to open a notebook, choose its use tab, then choose create copy. amazon sagemaker automatically parses the logs for metrics that built-in algorithms emit and sends those metrics to cloudwatch. if you want amazon sagemaker to parse logs from a custom algorithm and send metrics that the algorithm emits to cloudwatch, you have to specify the metrics that you want amazon sagemaker to send to cloudwatch when you configure the training job. you specify the name of the metrics that you want to send and the regular expressions that amazon sagemaker uses to parse the logs that your algorithm emits to find those metrics. you can specify the metrics that you want to track with the amazon sagemaker console;, the amazon sagemaker python sdk (), or the low-level amazon sagemaker api. topics to find a metric, amazon sagemaker searches the logs that your algorithm emits and finds logs that match the regular expression that you specify for that metric. if you are using your own algorithm, do the following: make sure that the algorithm writes the metrics that you want to capture to logsdefine a regular expression that accurately searches the logs to capture the values of the metrics that you want to send to cloudwatch metrics.for example, suppose your algorithm emits metrics for training error and validation error by writing logs similar to the following to  or : if you want to monitor both of those metrics in cloudwatch, your  would look like the following: in the regex for the  metric defined above, the first part of the regex finds the exact text "train_error=", and the expression  captures zero or more of any character until the first semicolon character. in this expression, the parenthesis tell the regex to capture what is inside them,  means any character,  means zero or more, and  means capture only until the first instance of the  character. define the metrics that you want to send to cloudwatch by specifying a list of metric names and regular expressions in the  field of the  input parameter that you pass to the  operation. for example, if you want to monitor both the  and  metrics in cloudwatch, your  would look like the following: for more information about defining and running a training job by using the low-level amazon sagemaker api, see . define the metrics that you want to send to cloudwatch by specifying a list of metric names and regular expressions as the  argument when you initialize an  object. for example, if you want to monitor both the  and  metrics in cloudwatch, your  initialization would look like the following: for more information about training by using  estimators, see . you can define metrics for a custom algorithm in the console when you create a training job by providing the name and regular expression (regex) for metrics. for example, if you want to monitor both the  and  metrics in cloudwatch, your metric definitions would look like the following: you can monitor the metrics that a training job emits in real time in the cloudwatch console. to monitor training job metrics (cloudwatch console) open the cloudwatch console at . choose metrics, then choose /aws/sagemaker/trainingjobs. choose trainingjobname. on the all metrics tab, choose the names of the training metrics that you want to monitor. on the graphed metrics tab, configure the graph options. for more information about using cloudwatch graphs, see  in the amazon cloudwatch user guide. you can monitor the metrics that a training job emits in real time by using the amazon sagemaker console. to monitor training job metrics (amazon sagemaker console) open the amazon sagemaker console at . choose training jobs, then choose the training job whose metrics you want to see. choose trainingjobname. in the monitor section, you can review the graphs of instance utilization and algorithm metrics. typically, you split the data that you train your model on into training and validation datasets. you use the training set to train the model parameters that are used to make predictions on the training dataset. then you test how well the model makes predictions by calculating predictions for the validation set. to analyze the performance of a training job, you commonly plot a training curve against a validation curve.  viewing a graph that shows the accuracy for both the training and validation sets over time can help you to improve the performance of your model. for example, if training accuracy continues to increase over time, but, at some point, validation accuracy starts to decrease, you are likely overfitting your model. to address this, you can make adjustments to your model, such as increasing . for this example, you can use the image-classification-full-training example that is in the example notebooks section of your amazon sagemaker notebook instance. if you don't have an amazon sagemaker notebook instance, create one by following the instructions at . if you prefer, you can follow along with the  in the example notebook on github. you also need an amazon s3 bucket to store the training data and for the model output. if you haven't created a bucket to use with amazon sagemaker, create one by following the instructions at . to view training and validation error curves open the amazon sagemaker console at . choose notebooks, and then choose notebook instances. choose the notebook instance that you want to use, and then choose open. on the dashboard for your notebook instance, choose sagemaker examples. expand the introduction to amazon algorithms section, and then choose use next to image-classification-full-training.ipynb. choose create copy. amazon sagemaker creates an editable copy of the image-classification-full-training.ipynb notebook in your notebook instance. in the first code cell of the notebook, replace <<bucket-name>> with the name of your s3 bucket. run all of the cells in the notebook up to the deploy section. you don't need to deploy an endpoint or get inference for this example. after the training job starts, open the cloudwatch console at . choose metrics, then choose /aws/sagemaker/trainingjobs. choose trainingjobname. on the all metrics tab, choose the train:accuracy and validation:accuracy metrics for the training job that you created in the notebook. on the graph, choose an area that the metric's values to zoom in. you should see something like the following: 
this topic provides one table for each aws region supported by amazon sagemaker. each table has a column for each availability zone in the region. for each availability zone, the amazon sagemaker components that support each instance type are shown. the components are listed in the tables as follows. notebook – notebook instancestraining – training jobsbatch – batch transform jobsendpoint – hosted endpointsif no components support the instance type in an availability zone, the cell contains "none". if all components support the instance type in an availability zone, the cell contains "all". not all components are supported on each instance type in each availability zone. to create a component on a specific instance type, you must specify the availability zone id, which is listed in the header row of each table, for example, . noteavailability zone names, for example, , don't map directly to availability zone ids. for different aws accounts, the same availability zone name might refer to a different availability zone id. for more information, see  in the amazon ec2 user guide and  in the aws ram user guide.  region tables 
when you use ai applications such as amazon rekognition, amazon textract, or your custom machine learning (ml) models you can use amazon augmented ai to get human review of low confidence or a random sample of predictions.  what is amazon augmented ai?amazon augmented ai (amazon a2i) makes it easy to build the workflows required for human review of ml predictions. amazon a2i brings human review to all developers, removing the undifferentiated heavy lifting associated with building human review systems or managing large numbers of human reviewers. many machine learning applications require humans to review low-confidence predictions to ensure the results are correct. for example, extracting information from scanned mortgage application forms can require human review in some cases due to low-quality scans or poor handwriting. but building human review systems can be time-consuming and expensive because it involves implementing complex processes or workflows, writing custom software to manage review tasks and results, and in many cases, managing large groups of reviewers. amazon a2i makes it easy to build and manage human reviews for machine learning applications. amazon a2i provides built-in human review workflows for common machine learning use cases, such as content moderation and text extraction from documents, which allows predictions from amazon rekognition and amazon textract to be reviewed easily. you can also create your own workflows for ml models built on amazon sagemaker or any other tools. using amazon a2i, you can allow human reviewers to step in when a model is unable to make a high-confidence prediction or to audit its predictions on an ongoing basis.  topics 
in this step you choose a workforce for labeling your dataset. you can create your own private workforce or you can use the amazon mechanical turk workforce. if you create a private workforce in this step you won't be able to import your amazon cognito user pool later. for more information, see . use the amazon mechanical turk workforce for this exercise instead. you can create a private workforce to test amazon sagemaker ground truth. use email addresses to invite the members of your workforce. to create a private workforce in the workers section, choose private. if this is your first time using a private workforce, in the email addresses field, enter up to 100 email addresses. the addresses must be separated by a comma. you should include your own email address so that you are part of the workforce and can see data object labeling tasks. in the organization name field, enter the name of your organization. this information is used to customize the email sent to invite a person to your private workforce. in the contact email field enter an email address that members of the workforce use to report problems with the task. if you choose to use the amazon mechanical turk workforce to label the dataset, you are charged for labeling tasks completed on the dataset. to use the amazon mechanical turk workforce in the workers section, choose public. choose the dataset does not contain pii to acknowledge that the dataset does not contain any personally identifiable information. choose the dataset does not contain adult content. to acknowledge that the sample dataset has no adult content. review and accept the statement that the dataset will be viewed by the public workforce.  
amazon sagemaker rl uses environments to mimic real-world scenarios. given the current state of the environment and an action taken by the agent or agents, the simulator processes the impact of the action, and returns the next state and a reward. simulators are useful in cases where it is not safe to train an agent in the real world (for example, flying a drone) or if the rl algorithm takes a long time to converge (for example, when playing chess). the following diagram shows an example of the interactions with a simulator for a car racing game.  the simulation environment consists of an agent and a simulator. here, a convolutional neural network (cnn) consumes images from the simulator and generates actions to control the game controller. with multiple simulations, this environment generates training data of the form , , , and . defining the reward is not trivial and impacts the rl model quality. we want to provide a few examples of reward functions, but would like to make it user-configurable.  topics to use openai gym environments in amazon sagemaker rl, use the following api elements. for more information about openai gym, see . —defines the actions the agent can take, specifies whether each action is continuous or discrete, and specifies the minimum and maximum if the action is continuous.—defines the observations the agent receives from the environment, as well as minimum and maximum for continuous observations.—initializes a training episode. the  function returns the initial state of the environment, and the agent uses the initial state to take its first action. the action is then sent to the  repeatedly until the episode reaches a terminal state. when  returns , the episode ends. the rl toolkit re-initializes the environment by calling .—takes the agent action as input and outputs the next state of the environment, the reward, whether the episode has terminated, and an  dictionary to communicate debugging information. it is the responsibility of the environment to validate the inputs.used for environments that have visualization. the rl toolkit calls this function to capture visualizations of the environment after each call to the  function.you can use open source environments, such as energyplus and roboschool, in amazon sagemaker rl by building your own container. for more information about energyplus, see . for more information about roboschool, see . the hvac and roboschool examples in the samples repository at  show how to build a custom container to use with amazon sagemaker rl: you can use commercial environments, such as matlab and simulink, in amazon sagemaker rl by building your own container. you need to manage your own licenses. 
once you have run a baseline processing job and obtained statistics and constraint for your dataset, you can execute monitoring jobs that calculate statistics and list any violations encountered relative to the baseline constraints. amazon cloudwatch metrics are also reported in your account by default. for information on viewing the results of monitoring in amazon sagemaker studio, see . list executions: the schedule starts monitoring jobs at the specified intervals. the following code lists the latest five executions. if you are running this code after creating the hourly schedule, the executions might be empty, and you might have to wait until you cross the hour boundary (in utc) to see the executions start. the following code includes the logic for waiting. inspect a specific execution: in the previous cell, you picked up the latest completed or failed scheduled execution. you can explore what went right or wrong. the terminal states are: : the monitoring execution completed and no issues were found in the violations report.: the execution completed, but constraint violations were detected.: the monitoring execution failed, possibly due to client error (for example, a role issues) or infrastructure issues. to identify the cause, see the  and .list the generated reports: violations report: if there are violations compared to the baseline, they are generated in the violations report. list the violations. this applies only to datasets that contain tabular data. the following schema files specify the statistics calculated and the violations monitored for. table: output files for tabular datasets   the  saves a set of amazon cloudwatch metrics for each feature by default.  the container code can emit cloudwatch metrics in this location: . the schema for these files is outlined in the following topics. topics 
use the following information to help you diagnose and fix common issues that you might encounter when working with amazon sagemaker and iam. topics if the aws management console tells you that you're not authorized to perform an action, then you must contact your administrator for assistance. your administrator is the person that provided you with your user name and password. the following example error occurs when the  iam user tries to use the console to view details about a training job but does not have  permissions. in this case, mateo asks his administrator to update his policies to allow him to access the  resource using the  action. if you receive an error that you're not authorized to perform the  action, then you must contact your administrator for assistance. your administrator is the person that provided you with your user name and password. ask that person to update your policies to allow you to pass a role to amazon sagemaker. some aws services allow you to pass an existing role to that service, instead of creating a new service role or service-linked role. to do this, you must have permissions to pass the role to the service. the following example error occurs when an iam user named  tries to use the console to perform an action in amazon sagemaker. however, the action requires the service to have permissions granted by a service role. mary does not have permissions to pass the role to the service. in this case, mary asks her administrator to update her policies to allow her to perform the  action. after you create your iam user access keys, you can view your access key id at any time. however, you can't view your secret access key again. if you lose your secret key, you must create a new access key pair.  access keys consist of two parts: an access key id (for example, ) and a secret access key (for example, ). like a user name and password, you must use both the access key id and secret access key together to authenticate your requests. manage your access keys as securely as you do your user name and password. important do not provide your access keys to a third party, even to help . by doing this, you might give someone permanent access to your account.  when you create an access key pair, you are prompted to save the access key id and secret access key in a secure location. the secret access key is available only at the time you create it. if you lose your secret access key, you must add new access keys to your iam user. you can have a maximum of two access keys. if you already have two, you must delete one key pair before creating a new one. to view instructions, see  in the iam user guide. to allow others to access amazon sagemaker, you must create an iam entity (user or role) for the person or application that needs access. they will use the credentials for that entity to access aws. you must then attach a policy to the entity that grants them the correct permissions in amazon sagemaker. to get started right away, see  in the iam user guide. you can create a role that users in other accounts or people outside of your organization can use to access your resources. you can specify who is trusted to assume the role. for services that support resource-based policies or access control lists (acls), you can use those policies to grant people access to your resources. to learn more, consult the following: to learn whether amazon sagemaker supports these features, see .to learn how to provide access to your resources across aws accounts that you own, see  in the iam user guide.to learn how to provide access to your resources to third-party aws accounts, see  in the iam user guide.to learn how to provide access through identity federation, see  in the iam user guide.to learn the difference between using roles and resource-based policies for cross-account access, see  in the iam user guide.
you can create use an algorithm resource to create a training job by using the amazon sagemaker console, the low-level amazon sagemaker api, or the . topics to use an algorithm to run a training job (console) open the amazon sagemaker console at . choose algorithms. choose an algorithm that you created from the list on the my algorithms tab or choose an algorithm that you subscribed to on the aws marketplace subscriptions tab. choose create training job. the algorithm you chose will automatically be selected. on the create training job page, provide the following information: for job name, type a name for the training job. for iam role, choose an iam role that has the required permissions to run training jobs in amazon sagemaker, or choose create a new role to allow amazon sagemaker to create a role that has the  managed policy attached. for information, see . for resource configuration, provide the following information: for instance type, choose the instance type to use for training. for instance count, type the number of ml instances to use for the training job. for additional volume per instance (gb), type the size of the ml storage volume that you want to provision. ml storage volumes store model artifacts and incremental states. for encryption key, if you want amazon sagemaker to use an aws key management service key to encrypt data in the ml storage volume attached to the training instance, specify the key. for stopping condition, specify the maximum amount of time in seconds, minutes, hours, or days, that you want the training job to run. for vpc, choose a amazon vpc that you want to allow your training container to access. for more information, see . for hyperparameters, specify the values of the hyperparameters to use for the training job. for input data configuration, specify the following values for each channel of input data to use for the training job. you can see what channels the algorithm you're using for training sports, and the content type, supported compression type, and supported input modes for each channel, under channel specification section of the algorithm summary page for the algorithm. for channel name, type the name of the input channel. for content type, type the content type of the data that the algorithm expects for the channel. for compression type, choose the data compression type to use, if any. for record wrapper, choose  if the algorithm expects data in the  format. for s3 data type, s3 data distribution type, and s3 location, specify the appropriate values. for information about what these values mean, see . for input mode, choose file to download the data from to the provisioned ml storage volume, and mount the directory to a docker volume. choose pipeto stream data directly from amazon s3 to the container. to add another input channel, choose add channel. if you are finished adding input channels, choose done. for output location, specify the following values: for s3 output path, choose the s3 location where the training job stores output, such as model artifacts. noteyou use the model artifacts stored at this location to create a model or model package from this training job. for encryption key, if you want amazon sagemaker to use a aws kms key to encrypt output data at rest in the s3 location. for tags, specify one or more tags to manage the training job. each tag consists of a key and an optional value. tag keys must be unique per resource. choose create training job to run the training job. to use an algorithm to run a training job by using the amazon sagemaker api, specify either the name or the amazon resource name (arn) as the  field of the  object that you pass to . for information about training models in amazon sagemaker, see . use an algorithm that you created or subscribed to on aws marketplace to create a training job, create an  object and specify either the amazon resource name (arn) or the name of the algorithm as the value of the  argument. then call the  method of the estimator. for example: 
this rule calculates the ratio of specific tokens given the rest of the input sequence that is useful for optimizing performance. for example, you can calculate the percentage of padding end-of-sentence (eos) tokens in your input sequence. if the number of eos tokens is too high, an alternate bucketing strategy should be performed. you also can calculate the percentage of unknown tokens in your input sequence. if the number of unknown words is too high, an alternate vocabulary could be used. this rule is applicable to deep learning applications. for an example of how to configure and deploy a built-in rule, see . parameter descriptions for the nlpsequenceratio rule   
to analyze data and evaluate machine learning models on the amazon sagemaker, use amazon sagemaker processing. with processing, you can use a simplified, managed experience on amazon sagemaker to run your data processing workloads, such as feature engineering, data validation, model evaluation, and model interpretation. you can also use the amazon sagemaker processing apis during the experimentation phase and after the code is deployed in production to evaluate performance.  to process and evaluate data, you run an amazon sagemaker processing job. your data must be stored in amazon simple storage service (amazon s3) processing saves the processed data in amazon s3.  topics we provide two sample jupyter notebooks that show how to perform data preprocessing, model evaluation, or both. for a sample notebook that shows how to run scikit-learn scripts to perform data preprocessing and model training and evaluation with the amazon sagemaker python sdk for processing, see . this notebook also shows how to use your own custom container to run processing workloads with your python libraries and other specific dependencies. for a sample notebook that shows how to use amazon sagemaker processing to perform distributed data preprocessing with spark, see . this notebook also shows how to train a regression model using xgboost on the preprocessed dataset. for instructions on how to create and access jupyter notebook instances that you can use to run these samples in amazon sagemaker, see . after you have created a notebook instance and opened it, choose the sagemaker examples tab to see a list of all the amazon sagemaker samples. to open a notebook, choose its use tab and choose create copy. amazon sagemaker processing provides amazon cloudwatch logs and metrics to monitor processing jobs. cloudwatch provides cpu, gpu, memory, gpu memory, and disk metrics, and event logging. for more information, see  and . 
amazon sagemaker algorithms accept and produce several different mime types for the http payloads used in retrieving online and mini-batch predictions. you can use various aws services to transform or preprocess records prior to running inference. at a minimum, you need to convert the data for the following: inference request serialization (handled by you) inference request deserialization (handled by the algorithm) inference response serialization (handled by the algorithm) inference response deserialization (handled by you) content type options for amazon sagemaker algorithm inference requests include: , , and . algorithms that don't support all of these types can support other types. xgboost, for example, only supports  from this list, but also supports . for  the value for the body argument to  should be a string with commas separating the values for each feature. for example, a record for a model with four features might look like: . any transformations performed on the training data should also be performed on the data before obtaining inference. the order of the features matters, and must remain unchanged.   is significantly more flexible and provides multiple possible formats for developers to use in their applications. at a high level, in javascript, the payload might look like:  you have the following options for specifying the :  protocol buffers equivalent: simple numeric vector:  and, for multiple records: amazon sagemaker algorithms return json in several layouts. at a high level, the structure is: the fields that are included in predictions differ across algorithms. the following are examples of output for the k-means algorithm. single-record inference:  multi-record inference:  multi-record inference with protobuf input:  amazon sagemaker algorithms also support jsonlines format, where the per-record response content is same as that in json format. the multi-record structure is a concatenation of per-record response objects separated by newline characters. the response content for the built-in kmeans algorithm for 2 input data points is: while running batch transform, it is recommended to use  response type by setting the  field in the  to . most algorithms use several of the following inference request formats. content-type: application/json dense format sparse format content-type: application/jsonlines dense format a single record in dense format can be represented as either: or: sparse format a single record in sparse format is represented as: multiple records are represented as a concatenation of the above single-record representations, separated by newline characters: content-type: text/csv;label_size=0 notecsv support is not available for factorization machines. content-type: application/x-recordio-protobuf while running batch transform, it's recommended to use jsonlines response type instead of json, if supported by the algorithm. this is accomplished by setting the  field in the  to . when you create a transform job, the  must be set according to the  of the input data. similarly, depending on the  field in the ,  must be set accordingly. please use the following table to help appropriately set these fields: for more information on response formats for specific algorithms, see the following: 
after registering your model and defining a scaling policy, apply the scaling policy to the registered model. to apply a scaling policy, you can use the aws cli or the application auto scaling api.  to apply a scaling policy to your model, use the  aws cli command with the following parameters: —the name of the scaling policy.—set this value to .—the resource identifier for the variant. for this parameter, the resource type is  and the unique identifier is the name of the variant. for example .—set this value to .—set this value to .—the target-tracking scaling policy configuration to use for the model.examplethe following example uses with application auto scaling to apply a target-tracking scaling policy named  to a model (variant) named . the policy configuration is saved in a file named .   to apply a scaling policy to a variant with the application auto scaling api, use the  application auto scaling api action with the following parameters: —the name of the scaling policy.—set this value to .—the resource identifier for the variant. for this parameter, the resource type is  and the unique identifier is the name of the variant. for example, .—set this value to .—set this value to .—the target-tracking scaling policy configuration to use for the variant.examplethe following example uses application auto scaling to apply a target-tracking scaling policy named  to a variant named . it uses a policy configuration based on the  predefined metric.   
amazon sagemaker manages the lifecycle of models hosted on multi-model endpoints in the container's memory. instead of downloading all of the models from an amazon s3 bucket to the container when you create the endpoint, amazon sagemaker dynamically loads them when you invoke them. when amazon sagemaker receives an invocation request for a particular model, it does the following: routes the request to an instance behind the endpoint. downloads the model from the s3 bucket to that instance's storage volume. loads the model to the container's memory on that instance. if the model is already loaded in the container's memory, invocation is faster because amazon sagemaker doesn't need to download and load it. amazon sagemaker continues to route requests for a model to the instance where the model is already loaded. however, if the model receives many invocation requests, and there are additional instances for the multi-model endpoint, amazon sagemaker routes some requests to another instance to accommodate the traffic. if the model isn't already loaded on the second instance, the model is downloaded to that instance's storage volume and loaded into the container's memory. when an instance's memory utilization is high and amazon sagemaker needs to load another model into memory, it unloads unused models from that instance's container to ensure that there is enough memory to load the model. models that are unloaded remain on the instance's storage volume and can be loaded into the container's memory later without being downloaded again from the s3 bucket. if the instance's storage volume reaches its capacity, amazon sagemaker deletes any unused models from the storage volume. to delete a model, stop sending requests and delete it from the s3 bucket. amazon sagemaker provides multi-model endpoint capability in a serving container. adding models to, and deleting them from, a multi-model endpoint doesn't require updating the endpoint itself. to add a model, you upload it to the s3 bucket and invoke it. you don’t need code changes to use it. when you update a multi-model endpoint, invocation requests on the endpoint might experience higher latencies as traffic is directed to the instances in the updated endpoint. 
amazon sagemaker ntm is an unsupervised learning algorithm that is used to organize a corpus of documents into topics that contain word groupings based on their statistical distribution. documents that contain frequent occurrences of words such as "bike", "car", "train", "mileage", and "speed" are likely to share a topic on "transportation" for example. topic modeling can be used to classify or summarize documents based on the topics detected or to retrieve information or recommend content based on topic similarities. the topics from documents that ntm learns are characterized as a latent representation because the topics are inferred from the observed word distributions in the corpus. the semantics of topics are usually inferred by examining the top ranking words they contain. because the method is unsupervised, only the number of topics, not the topics themselves, are prespecified. in addition, the topics are not guaranteed to align with how a human might naturally categorize documents. topic modeling provides a way to visualize the contents of a large document corpus in terms of the learned topics. documents relevant to each topic might be indexed or searched for based on their soft topic labels. the latent representations of documents might also be used to find similar documents in the topic space. you can also use the latent representations of documents that the topic model learns for input to another supervised algorithm such as a document classifier. because the latent representations of documents are expected to capture the semantics of the underlying documents, algorithms based in part on these representations are expected to perform better than those based on lexical features alone. although you can use both the amazon sagemaker ntm and lda algorithms for topic modeling, they are distinct algorithms and can be expected to produce different results on the same input data. for more information on the mathematics behind ntm, see . topics amazon sagemaker neural topic model supports four data channels: train, validation, test, and auxiliary. the validation, test, and auxiliary data channels are optional. if you specify any of these optional channels, set the value of the  parameter for them to . if you provide validation data, the loss on this data is logged at every epoch, and the model stops training as soon as it detects that the validation loss is not improving. if you don't provide validation data, the algorithm stops early based on the training data, but this can be less efficient. if you provide test data, the algorithm reports the test loss from the final model.  the train, validation, and test data channels for ntm support both  (dense and sparse) and  file formats. for  format, each row must be represented densely with zero counts for words not present in the corresponding document, and have dimension equal to: (number of records) * (vocabulary size). you can use either file mode or pipe mode to train models on data that is formatted as  or as . the auxiliary channel is used to supply a text file that contains vocabulary. by supplying the vocabulary file, users are able to see the top words for each of the topics printed in the log instead of their integer ids. having the vocabulary file also allows ntm to compute the word embedding topic coherence (wetc) scores, a new metric displayed in the log that captures similarity among the top words in each topic effectively. the  for the auxiliary channel is , with each line containing a single word, in the order corresponding to the integer ids provided in the data. the vocabulary file must be named  and currently only utf-8 encoding is supported.  for inference, , , , and  content types are supported. sparse data can also be passed for  and . ntm inference returns  or  predictions, which include the  vector for each observation. see the  and the companion  for more details on using the auxiliary channel and the wetc scores. for more information on how to compute the wetc score, see . we used the pairwise wetc described in this paper for the amazon sagemaker neural topic model. for more information on input and output file formats, see  for inference and the . ntm training supports both gpu and cpu instance types. we recommend gpu instances, but for certain workloads, cpu instances may result in lower training costs. cpu instances should be sufficient for inference. for a sample notebook that uses the amazon sagemaker ntm algorithm to uncover topics in documents from a synthetic data source where the topic distributions are known, see the . for instructions how to create and access jupyter notebook instances that you can use to run the example in amazon sagemaker, see . once you have created a notebook instance and opened it, select the sagemaker examples tab to see a list of all the amazon sagemaker samples. the topic modeling example notebooks using the ntm algorithms are located in the introduction to amazon algorithms section. to open a notebook, click on its use tab and select create copy. 
noteamazon sagemaker studio is available only in specific aws regions. to view the list of supported regions, see . this topic describes how to onboard to amazon sagemaker using the standard setup procedure for aws identity and access management (iam) authentication. to onboard faster using iam, see . for information on how to onboard using aws single sign-on (aws sso), see . to onboard to studio using iam open the . choose amazon sagemaker studio at the top left of the page. on the amazon sagemaker studio control panel, under get started, choose standard setup. for authentication method, choose aws identity and access management (iam). under permission, for execution role for all users, choose an option from the role selector. if you choose create a new role, the create an iam role dialog opens: for s3 buckets you specify, specify additional s3 buckets that users of your notebooks can access. if you don't want to add access to more buckets, choose none.choose create role. amazon sagemaker creates a new iam role with the  policy attached.choose submit. on the amazon sagemaker studio control panel, under studio summary, wait for status to change to ready and the add user button to be enabled. choose add user. on the add user page, keep the default name or create a new name. a name can be up to 63 characters. valid characters: a-z, a-z, 0-9, and - (hyphen).  for execution role, choose an option from the role selector. choose submit. the amazon sagemaker studio control panel opens with the new user listed. the delete user button and the open studio link are both enabled. to add more users, repeat steps 7 through 9. when multiple users are listed, none of the users in the user list is chosen. choose a user and the delete user button and the open studio link for that user are enabled. choose open studio. the amazon sagemaker studio loading page displays. when amazon sagemaker studio opens, you can start using studio. now that you've onboarded to amazon sagemaker studio, use the following steps to subsequently access studio. access studio after you onboard open the . choose amazon sagemaker studio at the top left of the page. on the amazon sagemaker studio control panel, choose your user name and then choose open studio. for information about using amazon sagemaker studio, see . 
a widget for identifying individual instances of specific objects within an image and creating a colored overlay for each labeled instance. the following is an example of a liquid template that uses the  element and allows annotators to add additional labels. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  the following attributes are supported by this element. the text to display above the image. this is typically a question or simple instruction for the worker. a json formatted array of strings, each of which is a label that a worker can assign to an instance of an object in the image. workers can generate different overlay colors for each relevant instance by selecting "add instance" under the label in the tool. the name of this widget. it is used as a key for the labeling data in the form output. the url of the image that is to be labeled. this element has the following parent and child elements. parent elements: child elements: , the following regions are supported by this element. general instructions about how to do image segmentation. important task-specific instructions that are displayed in a prominent place. the following output is supported by this element. a json object containing a base64 encoded png of the labels. a json array containing objects with the instance labels and colors. color – the hexadecimal value of the label's rgb color in the  png.label – the label given to overlay(s) using that color. this value may repeat, because the different instances of the label are identified by their unique color.a json object that specifies the dimensions of the image that is being annotated by the worker. this object contains the following properties. height – the height, in pixels, of the image.width – the width, in pixels, of the image.example : sample element outputsthe following is an example of output from this element.   for more information, see the following. 
typically, you explore training data to determine what you need to clean up and which transformations to apply to improve model training. for this exercise, you don't need to clean up the mnist dataset.  to explore the dataset type the following code in a cell in your notebook and run the cell to display the first 10 images in :.  contains the following structures:  – contains images.  – contains labels. the code uses the  library to get and display the first 10 images from the training dataset. next step 
this section contains information about how to understand and prevent common errors, the error messages they generate, and guidance on how to resolve these errors. it also contains lists of the frameworks and the operations in each of those frameworks that neo supports.  topics some of the most common errors are due to invalid inputs. this section contains information arranged in question and answer form to help you avoid these errors. what input data shapes does neo expect? neo expects the name and shape of the expected data inputs for your trained model with a json dictionary form or list form. the data inputs are framework specific.  : you must specify the name and shape (nhwc format) of the expected data inputs using a dictionary format for your trained model. the dictionary formats required for the console and cli are different.examples for one input:if using the console, if using the cli, examples for two inputs:if using the console, if using the cli, : you must specify the name and shape (nchw format) of expected data inputs using a dictionary format for your trained model. note that while keras model artifacts should be uploaded in nhwc (channel-last) format,  should be specified in nchw (channel-first) format. the dictionary formats required for the console and cli are different.examples for one input:if using the console, if using the cli, examples for two inputs:if using the console, if using the cli, : you must specify the name and shape (nchw format) of the expected data inputs in order using a dictionary format for your trained model. the dictionary formats required for the console and cli are different.examples for one input:if using the console, if using the cli, examples for two inputs:if using the console, if using the cli, : you can either specify the name and shape (nchw format) of expected data inputs in order using a dictionary format for your trained model or you can specify the shape only using a list format. the dictionary formats required for the console and cli are different. the list formats for the console and cli are the same.examples for one input in dictionary format:if using the console, if using the cli, example for one input in list format: examples for two inputs in dictionary format:if using the console, if using the cli, example for two inputs in list format: : input data name and shape are not needed.this section lists and classifies neo errors and error messages. this list catalogs the user and system error messages you can receive from neo deployments. user error messages client permission error: neo passes the errors for these straight through from the dependent service. access denied when calling sts:assumerole any 400 error when calling s3 to download or upload a client model. passrole error load error: keywords in error messages, 'inputconfiguration','modelsizetoobig'. load error: inputconfiguration: exactly one {.xxx} file is allowed for {yyy} model. load error: modelsizetoobig: number of nodes in a tree can't exceed 2^31 compilation error: keywords in error messages, 'operatornotimplemented',' operatorattributenotimplemented', 'operatorattributerequired', 'operatorattributevaluenotvalid'. operatornotimplemented: {xxx} is not supported. operatorattributenotimplemented: {xxx} is not supported in {yyy}. operatorattributerequired: required attribute {xxx} not found in {yyy}. operatorattributevaluenotvalid: the value of attribute {xxx} in operator {yyy} cannot be negative. any malformed input errors system error messagesfor system errors, neo shows only one error message similar to the following: there was an unexpected error during compilation, check your inputs and try again in a few minutes.this covers all unexpected errors and errors that are not user errors.this list classifies the user errors you can receive from neo. these include access and permission errors and load errors for each of the supported frameworks. all other errors are system errors. client permission error: neo passes the errors for these straight through from the dependent service. access denied when calling sts:assumerole any 400 error when calling amazon s3 to download or upload a client model. passrole error load error: assuming that the neo compiler successfully loaded .tar.gz from amazon s3, check whether the tarball contains the necessary files for compilation. the checking criteria is framework-specific:tensorflow: expects only protobuf file (*.pb or *.pbtxt). for saved models, expects one  folder.pytorch: expect only one pytorch file (*.pth).mxnet: expect only one symbol file (*.json) and one parameter file (*.params).xgboost: expect only one xgboost model file (*.model). the input model has size limitation.compilation error: assuming that the neo compiler successfully loaded .tar.gz from amazon s3, and that the tarball contains necessary files for compilation. the checking criteria is:operatornotimplemented: an operator has not been implemented.operatorattributenotimplemented: the attribute in the specified operator has not been implemented.operatorattributerequired: an attribute is required for an internal symbol graph, but it is not listed in the user input model graph.operatorattributevaluenotvalid: the value of the attribute in the specific operator is not valid.this section provides guidance on troubleshooting common issues with neo. these include permission, load, compilation, and system errors and errors involving invalid inputs and unsupported operations. catalog of known issues:if you see client permission error, review the set up documentation and make sure that you have correctly granted the permissions that are failing.if you see load error, check the model format files that neo expects for different frameworks.if you see compilation error, check and address the details error message in your input model graph.if you see system error, try again in a few minutes. if that fails, file a ticket.lack of roles and permissions: review the set up documentation and make sure that you have correctly granted the permissions that are failing.invalid api and console inputs: fix your input as described in the validation error.unsupported operators: check the failure reason where neo has listed all unsupported operators with the keyword ‘operatornotimplemented’.for example: compilation error: operatornotimplemented: the following operators are not implemented: {'_sample_multinomial', 'rnn' }remove the unsupported operators from your input model graph and test it again.
the following amazon elastic compute cloud (amazon ec2) instance types are available. for more information, see  in the amazon ec2 user guide for linux instances. fast launch instances types are optimized to start in under two minutes. general purpose ml.t3.medium a fast launch instance type. the default instance type for cpu-based sagemaker images. available as part of the . ml.t3.largeml.t3.xlargeml.t3.2xlargeml.m5.large a fast launch instance type. ml.m5.2xlargeml.m5.4xlargeml.m5.8xlargeml.m5.12xlargeml.m5.16xlargeml.m5.24xlargecompute optimized ml.c5.large a fast launch instance type. ml.c5.xlargeml.c5.2xlargeml.c5.4xlargeml.c5.9xlargeml.c5.12xlargeml.c5.18xlargeml.c5.24xlargeaccelerated computing ml.p3.2xlargeml.p3.8xlargeml.p3.16xlargeml.g4dn.xlarge a fast launch instance type. the default instance type for gpu-based sagemaker images. ml.g4dn.2xlargeml.g4dn.4xlargeml.g4dn.8xlargeml.g4dn.12xlargeml.g4dn.16xlarge
in the  request, you specify the training algorithm that you want to use. you can also specify algorithm-specific hyperparameters as string-to-string maps. the following table lists the hyperparameters for the k-means training algorithm provided by amazon sagemaker. for more information about how k-means clustering works, see . 
you can display the difference between the current notebook and the last checkpoint or the last git commit using the amazon sagemaker ui. the following screenshot shows the menu from a studio notebook.  topics when you create a notebook, a hidden checkpoint file that matches the notebook is created. you can view changes between the notebook and the checkpoint file or revert the notebook to match the checkpoint file. by default, a notebook is auto-saved every 120 seconds and also when you close the notebook. however, the checkpoint file isn't updated to match the notebook. to save the notebook and update the checkpoint file to match, you must choose the save notebook and create checkpoint icon ( ) on the left of the notebook menu or use the  keyboard shortcut. to view the changes between the notebook and the checkpoint file, choose the checkpoint diff icon ( ) in the center of the notebook menu. to revert the notebook to the checkpoint file, from the main studio menu, choose file then revert notebook to checkpoint. if a notebook is opened from a git repository, you can view the difference between the notebook and the last git commit. to view the changes in the notebook from the last git commit, choose the git diff icon ( ) in the center of the notebook menu. 
you can deploy the compact module to performance-critical cloud services with amazon sagemaker hosting services or to resource-constrained edge devices with aws iot greengrass.  topics 
crowd html elements are web components, a web standard that abstracts html markup, css, and javascript functionality into an html tag or set of tags. amazon sagemaker provides customers with the ability to design their own custom task templates in html.  as a starting point, you can use a template built using crowd html elements from one of the following github repositories:  these repositories include templates designed for audio, image, text, video, and other types of data labeling and annotation tasks.  for more information about how to implement custom templates in amazon sagemaker ground truth, see . to learn more about custom templates in amazon augmented ai, see . following is a list of crowd html elements that make building a custom template easier and provide a familiar ui for workers. these elements are supported in ground truth, augmented ai, and mechanical turk. topics the following crowd html elements are only available for amazon augmented ai human workflow tasks. topics 
the violations file is generated as the output of a , which lists the results of evaluating the constraints (specified in the constraints.json file) against the current dataset that was analyzed. the amazon sagemaker model monitor pre-built container provides the following violation checks. table: types of violations monitored   
 you can clone a job, add or edit tags, or create a new hyperparameter tuning job from the console. you can also use the search feature to find jobs by their name, creation time, and status.  creating a hyperparameter tuning jobto create a new job, open the amazon sagemaker console, choose training, choose hyperparameter tuning jobs, and then choose create hyperparameter tuning job.   for instructions on using the api to create a tuning job, see .  cloning an existing training job you can save time by cloning a training job, which copies all of the job’s settings, including data channels, s3 bucket locations, algorithms, and the hyperparameter options.   to clone a training job  on the training jobs page or on the hyperparameter tuning jobs page. choose actions and then choose clone.  editing tags you enter tags as key-value pairs. values are not required. you can use just the key. to see the keys associated with a job, choose the tags tab on the tuning job’s details page.  
before training a model with either amazon sagemaker built-in algorithms or custom algorithms, you can use spark and scikit-learn preprocessors to transform your data and engineer features.  you can run spark ml jobs with , a serverless etl (extract, transform, load) service, from your amazon sagemaker notebook. you can also connect to existing emr clusters to run spark ml jobs with . to do this, you need an aws identity and access management (iam) role that grants permission for making calls from your amazon sagemaker notebook to aws glue.  noteto see which python and spark versions aws glue supports, refer to . after engineering features, you package and serialize spark ml jobs with mleap into mleap containers that you can add to an inference pipeline. you don't need to use externally managed spark clusters. with this approach, you can seamlessly scale from a sample of rows to terabytes of data. the same transformers work for both training and inference, so you don't need to duplicate preprocessing and feature engineering logic or develop a one-time solution to make the models persist. with inference pipelines, you don't need to maintain outside infrastructure, and you can make predictions directly from data inputs. when you run a spark ml job on aws glue, a spark ml pipeline is serialized into  format. then, you can use the job with the  in an amazon sagemaker inference pipeline.mleap is a serialization format and execution engine for machine learning pipelines. it supports spark, scikit-learn, and tensorflow for training pipelines and exporting them to a serialized pipeline called an mleap bundle. you can deserialize bundles back into spark for batch-mode scoring or into the mleap runtime to power real-time api services. you can run and package scikit-learn jobs into containers directly in amazon sagemaker.  for an example of python code for building a scikit-learn featurizer model that trains on  and predicts the species of iris based on morphological measurements, see .  
all amazon sagemaker built-in algorithms adhere to the common input training formats described in . this topic contains a list of the available input formats for the amazon sagemaker k-nearest-neighbor algorithm. content-type: text/csv; label_size=1 the first  columns are interpreted as the label vector for that row. content-type: application/x-recordio-protobuf 
a widget for labeling words, phrases, or character strings within a longer text. workers select a label, and highlight the text that the label applies to.  important: self-contained widgetdo not use  element with the  element. it contains its own form submission logic and submit button. the following is an example of a template that uses the  element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  the following attributes are supported by this element. the text to display above the image. this is typically a question or simple instruction for the worker. a json formatted array of objects, each of which defines an annotation to apply to the text at initialization. objects contain a  value that matches one in the  attribute, an integer  value for labeled span's starting unicode offset, and an integer  value for the ending unicode offset. example   a json formatted array of objects, each of which contains:  (required): the name used to identify entities. (optional): used for the label list in the task widget. defaults to the label value if not specified. (optional): an abbreviation of 3-4 letters to display above selected entities. defaults to the label value if not specified.  is highly recommendedvalues displayed above the selections can overlap and create difficulty managing labeled entities in the workspace. providing a 3-4 character  for each label is highly recommended to prevent overlap and keep the workspace manageable for your workers.example   serves as the widget's name in the dom. it is also used as the label attribute name in form output and the output manifest. the text to be annotated. the templating system escapes quotes and html strings by default. if your code is already escaped or partially escaped, see  for more ways to control escaping. this element has the following parent and child elements. child elements: , the following regions are supported by this element. general instructions about how to work with the widget. important task-specific instructions that are displayed in a prominent place. the following output is supported by this element. a json object that specifies the start, end, and label of an annotation. this object contains the following properties. label – the assigned label.startoffset – the unicode offset of the beginning of the selected text.endoffset – the unicode offset of the first character after the selection.example : sample element outputsthe following is a sample of the output from this element.   for more information, see the following. 
if you encounter an issue when deploying machine learning models in amazon sagemaker, see the following guidance. topics if you deploy an amazon sagemaker model with a linux java virtual machine (jvm), you might encounter detection errors that prevent using available cpu resources. this issue affects some jvms that support java 8 and java 9, and most that support java 10 and java 11. these jvms implement a mechanism that detects and handles the cpu count and the maximum memory available when running a model in a docker container, and, more generally, within linux  commands or control groups (cgroups). amazon sagemaker deployments take advantage of some of the settings that the jvm uses for managing these resources. currently, this causes the container to incorrectly detect the number of available cpus.  amazon sagemaker doesn't limit access to cpus on an instance. however, the jvm might detect the cpu count as  when more cpus are available for the container. as a result, the jvm adjusts all of its internal settings to run as if only  cpu core is available. these settings affect garbage collection, locks, compiler threads, and other jvm internals that negatively affect the concurrency, throughput, and latency of the container. for an example of the misdetection, in a container configured for amazon sagemaker that is deployed with a jvm that is based on java8_191 and that has four available cpus on the instance, run the following command to start your jvm: this generates the following output: many of the jvms affected by this issue have an option to disable this behavior and reestablish full access to all of the cpus on the instance. disable the unwanted behavior and establish full access to all instance cpus by including the  parameter when starting java applications. for example, run the  command to start your jvm as follows: this generates the following output: check whether the jvm used in your container supports the  parameter. if it does, always pass the parameter when you start your jvm. this provides access to all of the cpus in your instances.  you might also encounter this issue when indirectly using a jvm in amazon sagemaker containers. for example, when using a jvm to support sparkml scala. the  parameter also affects the output returned by the java  api ``.  
this rule keeps track of the ratio of updates to weights during training and detects if that ratio gets too large or too small. if the ratio of updates to weights is larger than the  or if this ratio is smaller than , the rule returns . conditions for training are best when the updates are commensurate to the gradients. excessively large updates can push the weights away from optimal values, and very small updates result in very slow convergence. this rule requires weights to be available for two consecutive steps, so  needs to be set to 1. parameter descriptions for the weightupdateratio rule   noteif tensors have been saved with  mode on during training, the rule runs only on  mode steps. otherwise, it runs by default on  mode steps. for an example of how to configure and deploy a built-in rule, see . notethis rule can't be applied to the xgboost algorithm. 
when using the amazon sagemaker object2vec algorithm, you follow the standard workflow: process the data, train the model, and produce inferences.  topics during preprocessing, convert the data to the  text file format specified in  . to get the highest accuracy during training, also randomly shuffle the data before feeding it into the model. how you generate random permutations depends on the language. for python, you could use ; for unix, . the amazon sagemaker object2vec algorithm has the following main components: two input channels – the input channels take a pair of objects of the same or different types as inputs, and pass them to independent and customizable encoders.two encoders – the two encoders, enc0 and enc1, convert each object into a fixed-length embedding vector. the encoded embeddings of the objects in the pair are then passed into a comparator.a comparator – the comparator compares the embeddings in different ways and outputs scores that indicate the strength of the relationship between the paired objects. in the output score for a sentence pair. for example, 1 indicates a strong relationship between a sentence pair, and 0 represents a weak relationship. during training, the algorithm accepts pairs of objects and their relationship labels or scores as inputs. the objects in each pair can be of different types, as described earlier. if the inputs to both encoders are composed of the same token-level units, you can use a shared token embedding layer by setting the  hyperparameter to  when you create the training job. this is possible, for example, when comparing sentences that both have word token-level units. to generate negative samples at a specified rate, set the  hyperparameter to the desired ratio of negative to positive samples. this hyperparameter expedites learning how to discriminate between the positive samples observed in the training data and the negative samples that are not likely to be observed.  pairs of objects are passed through independent, customizable encoders that are compatible with the input types of corresponding objects. the encoders convert each object in a pair into a fixed-length embedding vector of equal length. the pair of vectors are passed to a comparator operator, which assembles the vectors into a single vector using the value specified in the he  hyperparameter. the assembled vector then passes through a multilayer perceptron (mlp) layer, which produces an output that the loss function compares with the labels that you provided. this comparison evaluates the strength of the relationship between the objects in the pair as predicted by the model. the following figure shows this workflow.  after the model is trained, you can use the trained encoder to preprocess input objects or to perform two types of inference: to convert singleton input objects into fixed-length embeddings using the corresponding encoderto predict the relationship label or score between a pair of input objectsthe inference server automatically figures out which of the types is requested based on the input data. to get the embeddings as output, provide only one input. to predict the relationship label or score, provide both inputs in the pair. 
this section describes a typical machine learning workflow and summarizes how you accomplish those tasks with amazon sagemaker.  in machine learning, you "teach" a computer to make predictions, or inferences. first, you use an algorithm and example data to train a model. then you integrate your model into your application to generate inferences in real time and at scale. in a production environment, a model typically learns from millions of example data items and produces inferences in hundreds to less than 20 milliseconds.  the following diagram illustrates the typical workflow for creating a machine learning model:   as the diagram illustrates, you typically perform the following activities: generate example data—to train a model, you need example data. the type of data that you need depends on the business problem that you want the model to solve (the inferences that you want the model to generate). for example, suppose that you want to create a model to predict a number given an input image of a handwritten digit. to train such a model, you need example images of handwritten numbers.  data scientists often spend a lot of time exploring and preprocessing, or "wrangling," example data before using it for model training. to preprocess data, you typically do the following:  fetch the data— you might have in-house example data repositories, or you might use datasets that are publicly available. typically, you pull the dataset or datasets into a single repository.  clean the data—to improve model training, inspect the data and clean it as needed. for example, if your data has a  attribute with values  and , you might want to edit the data to be consistent.  prepare or transform the data—to improve performance, you might perform additional data transformations. for example, you might choose to combine attributes. if your model predicts the conditions that require de-icing an aircraft, instead of using temperature and humidity attributes separately, you might combine those attributes into a new attribute to get a better model.  in amazon sagemaker, you preprocess example data in a jupyter notebook on your notebook instance. you use your notebook to fetch your dataset, explore it, and prepare it for model training. for more information, see . for more information about preparing data in aws marketplace, see .  train a model—model training includes both training and evaluating the model, as follows:  training the model— to train a model, you need an algorithm. the algorithm you choose depends on a number of factors. for a quick, out-of-the-box solution, you might be able to use one of the algorithms that amazon sagemaker provides. for a list of algorithms provided by amazon sagemaker and related considerations, see .   you also need compute resources for training. depending on the size of your training dataset and how quickly you need the results, you can use resources ranging from a single general-purpose instance to a distributed cluster of gpu instances. for more information, see .   evaluating the model—after you've trained your model, you evaluate it to determine whether the accuracy of the inferences is acceptable. in amazon sagemaker, you use either the aws sdk for python (boto) or the high-level python library that amazon sagemaker provides to send requests to the model for inferences.  you use a jupyter notebook in your amazon sagemaker notebook instance to train and evaluate your model.  deploy the model— you traditionally re-engineer a model before you integrate it with your application and deploy it. with amazon sagemaker hosting services, you can deploy your model independently, decoupling it from your application code. for more information, see . machine learning is a continuous cycle. after deploying a model, you monitor the inferences, collect "ground truth," and evaluate the model to identify drift. you then increase the accuracy of your inferences by updating your training data to include the newly collected ground truth. you do this by retraining the model with the new dataset. as more and more example data becomes available, you continue retraining your model to increase accuracy. 
amazon sagemaker integrates with aws marketplace, enabling developers to charge other amazon sagemaker users for the use of their algorithms and model packages. aws marketplace is a curated digital catalog that makes it easy for customers to find, buy, deploy, and manage third-party software and services that customers need to build solutions and run their businesses. aws marketplace includes thousands of software listings in popular categories, such as security, networking, storage, machine learning, business intelligence, database, and devops. it simplifies software licensing and procurement with flexible pricing options and multiple deployment methods.  for information, see . an algorithm enables you to perform end-to-end machine learning. it has two logical components: training and inference. buyers can use the training component to create training jobs in amazon sagemaker and build a machine learning model. amazon sagemaker saves the model artifacts generated by the algorithm during training to an amazon s3 bucket. for more information, see . buyers use the inference component with the model artifacts generated during a training job to create a deployable model in their amazon sagemaker account. they can use the deployable model for real-time inference by using amazon sagemaker hosting services. or, they can get inferences for an entire dataset by running batch transform jobs. for more information, see . buyers use a model package to build a deployable model in amazon sagemaker. they can use the deployable model for real-time inference by using amazon sagemaker hosting services. or, they can get inferences for an entire dataset by running batch transform jobs. for more information, see . as a seller, you can build your model artifacts by training in amazon sagemaker, or you can use your own model artifacts from a model that you trained outside of amazon sagemaker. you can charge buyers for inference. topics 
to classify data into different groupings, you train a model by using a dataset and metadata that act as labels. to include metadata with your dataset in a training job, use an augmented manifest file. when using an augmented manifest file, your dataset must be stored in amazon simple storage service (amazon s3) and you must configure your training job to use dataset stored there. you specify the location and format of this dataset for one or more . augmented manifests can only support pipe input mode. see the section, inputmode in  to learn more about pipe input mode.  when specifying a channel's parameters, you specify a path to the file, called a . amazon sagemaker interprets this uri based on the specified  in . the  option defines a manifest format that includes metadata with the input data. using an augmented manifest file is an alternative to preprocessing when you have labeled data. for training jobs using labeled data, you typically need to preprocess the dataset to combine input data with metadata before training. if your training dataset is large, preprocessing can be time consuming and expensive. an augmented manifest file must be formatted in  format. in json lines format, each line in the file is a complete json object followed by a newline separator. during training, amazon sagemaker parses each json line and sends some or all of its attributes on to the training algorithm. you specify which attribute contents to pass and the order in which to pass them with the  parameter of the  api. the  parameter is an ordered list of attribute names that amazon sagemaker looks for in the json object to use as training input. for example, if you list  for , the input data must include the attribute names of  and  in the specified order. for this example, the following augmented manifest file content is valid: amazon sagemaker ignores unlisted attribute names even if they precede, follow, or are in between listed attributes. when using augmented manifest files, observe the following guidelines: the order of the attributes listed in the  parameter determines the order of the attributes passed to the algorithm in the training job.the listed  can be a subset of all of the attributes in the json line. amazon sagemaker ignores unlisted attributes in the file.you can specify any type of data allowed by the json format in , including text, numerical, data arrays, or objects.to include an s3 uri as an attribute name, add the suffix  to it.if an attribute name contains the suffix , the attribute's value must be an s3 uri to a data file that is accessible to the training job. for example, if  contains , a valid augmented manifest file might contain these lines: for the first line of this manifest, amazon sagemaker retrieves the contents of the s3 object  and streams it to the algorithm for training. the second line is the string representation of the  attribute , which is followed by the contents of the second line. to create an augmented manifest file, use amazon sagemaker ground truth to create a labeling job. for more information, see . augmented manifest files are supported only for channels using pipe input mode. for each channel, the data is extracted from its augmented manifest file and streamed (in order) to the algorithm through the channel's named pipe. pipe mode uses the first in first out (fifo) method, so records are processed in the order in which they are queued. for information about pipe input mode, see . attribute names with a  suffix point to preformatted binary data. in some cases, the algorithm knows how to parse the data. in other cases, you might need to wrap the data so that records are delimited for the algorithm. if the algorithm is compatible with , specifying  for  solves this issue. if the algorithm is not compatible with  format, specify  for  and make sure that your data is parsed correctly for your algorithm. using the  example, if you use recordio wrapping, the following stream of data is sent to the queue:  images that aren't wrapped with recordio format, are streamed with the corresponding  attribute value as one record. this can cause a problem because the algorithm might not delimit the images and attributes correctly. with augmented manifest files and pipe mode in general, size limits of the ebs volume do not apply. this includes settings that otherwise must be within the ebs volume size limit such as . for more information about pipe mode and how to use it, see . to complete this procedure, you need: the url of the s3 bucket where you've stored the augmented manifest file.to store the data that is listed in the augmented manifest file in an s3 bucket.the url of the s3 bucket where you want to store the output of the job.to use an augmented manifest file in a training job (console) open the amazon sagemaker console at . in the navigation pane, choose training, then choose training jobs.  choose create training job. provide a name for the training job. the name must be unique within an aws region in an aws account. it can have 1 to 63 characters. valid characters: a-z, a-z, 0-9, and . : + = @ _ % - (hyphen). choose the algorithm that you want to use. for information about supported built-in algorithms, see . if you want to use a custom algorithm, make sure that it is compatible with pipe mode. (optional) for resource configuration, either accept the default values or, to reduce computation time, increase the resource consumption. (optional) for instance type, choose the ml compute instance type that you want to use. in most cases, ml.m4.xlarge is sufficient.  for instance count, use the default, . (optional) for additional volume per instance (gb), choose the size of the ml storage volume that you want to provision. in most cases, you can use the default, . if you are using a large dataset, use a larger size. provide information about the input data for the training dataset. for channel name, either accept the default (train) or enter a more meaningful name, such as training-augmented-manifest-file. for inputmode, choose pipe. for s3 data distribution type, choose fullyreplicated. when training incrementally, fully replicating causes each ml compute instance to use a complete copy of the expanded dataset. for neural-based algorithms, such as , choose . if the data specified in the augmented manifest file is uncompressed, set the compression type to none. if the data is compressed using gzip, set it to gzip. (optional) for content type, specify the appropriate mime type. content type is the multipurpose internet mail extension (mime) type of the data. for record wrapper, if the dataset specified in the augmented manifest file is saved in recordio format, choose recordio. if your dataset is not saved as a recordio-formatted file, choose none. for s3 data type, choose augmentedmanifestfile. for s3 location, provide the path to the bucket where you stored the augmented manifest file. for augmentedmanifestfile attribute names, specify the name of an attribute that you want to use. the attribute name must be present within the augmented manifest file, and is case-sensitive. (optional) to add more attribute names, choose add row and specify another attribute name for each attribute. (optional) to adjust the order of attribute names, choose the up or down buttons next to the names. when using an augmented manifest file, the order of the specified attribute names is important. choose done. for output data configuration, provide the following information: for s3 location, type the path to the s3 bucket where you want to store the output data. (optional) you can use your aws key management service (aws kms) encryption key to encrypt the output data at rest. for encryption key, provide the key id or its amazon resource number (arn). for more information, see . (optional) for tags, add one or more tags to the training job. a tag is metadata that you can define and assign to aws resources. in this case, you can use tags to help you manage your training jobs. a tag consists of a key and a value, which you define. for example, you might want to create a tag with project as a key and a value that refers to a project that is related to the training job, such as home value forecasts. choose create training job. amazon sagemaker creates and runs the training job. after the training job has finished, amazon sagemaker stores the model artifacts in the bucket whose path you provided for s3 output path in the output data configuration field. to deploy the model to get predictions, see . the following shows how to train a model with an augmented manifest file using the amazon sagemaker high-level python library: after the training job has finished, amazon sagemaker stores the model artifacts in the bucket whose path you provided for s3 output path in the output data configuration field. to deploy the model to get predictions, see . 
import libraries and get a boto3 client, which you use to call the hyperparameter tuning apis. in the new jupyter notebook, type the following code:  
monitoring is important for maintaining the reliability, availability, and performance of amazon sagemaker resources. to monitor and troubleshoot inference pipeline performance, use amazon cloudwatch logs and error messages. for information about the monitoring tools that amazon sagemaker provides, see . to monitor the multi-container models in inference pipelines, use amazon cloudwatch. cloudwatch collects raw data and processes it into readable, near real-time metrics. amazon sagemaker training jobs and endpoints write cloudwatch metrics and logs in the  namespace.  the following tables list the metrics and dimensions for the following: endpoint invocationstraining jobs, batch transform jobs, and endpoint instancesa dimension is a name/value pair that uniquely identifies a metric. you can assign up to 10 dimensions to a metric. for more information on monitoring with cloudwatch, see .  endpoint invocation metrics the  namespace includes the following request metrics from calls to . metrics are reported at a 1-minute intervals. dimensions for endpoint invocation metrics for an inference pipeline endpoint, cloudwatch lists per-container latency metrics in your account as endpoint container metrics and endpoint variant metrics in the sagemaker namespace, as follows. the  metric appears only for inferences pipelines.  for each endpoint and each container, latency metrics display names for the container, endpoint, variant, and metric.  training job, batch transform job, and endpoint instance metrics the namespaces , , and  include the following metrics for training jobs and endpoint instances. metrics are reported at a 1-minute intervals. dimensions for training job, batch transform job, and endpoint instance metrics to help you debug your training jobs, endpoints, and notebook instance lifecycle configurations, amazon sagemaker also sends anything an algorithm container, a model container, or a notebook instance lifecycle configuration sends to  or  to amazon cloudwatch logs. you can use this information for debugging and to analyze progress. the following table lists the log groups and log streams amazon sagemaker. sends to amazon cloudwatch  a log stream is a sequence of log events that share the same source. each separate source of logs into cloudwatch makes up a separate log stream. a log group is a group of log streams that share the same retention, monitoring, and access control settings. logs  noteamazon sagemaker creates the  log group when you create a notebook instance with a lifecycle configuration. for more information, see . for more information about amazon sagemaker logging, see .  
an amazon sagemaker notebook instance is a fully managed ml compute instance running the jupyter notebook app. amazon sagemaker manages creating the instance and related resources. use jupyter notebooks in your notebook instance to prepare and process data, write code to train models, deploy models to amazon sagemaker hosting, and test or validate your models. amazon sagemaker also provides sample notebooks that contain complete code walkthroughs. these walkthroughs show how to use amazon sagemaker to perform common machine learning tasks. for more information, see . topics 
cloud security at aws is the highest priority. as an aws customer, you benefit from a data center and network architecture that is built to meet the requirements of the most security-sensitive organizations. security is a shared responsibility between aws and you. the  describes this as security of the cloud and security in the cloud: security of the cloud – aws is responsible for protecting the infrastructure that runs aws services in the aws cloud. aws also provides you with services that you can use securely. third-party auditors regularly test and verify the effectiveness of our security as part of the . to learn about the compliance programs that apply to amazon sagemaker, see .security in the cloud – your responsibility is determined by the aws service that you use. you are also responsible for other factors including the sensitivity of your data, your company’s requirements, and applicable laws and regulations. this documentation helps you understand how to apply the shared responsibility model when using amazon sagemaker. the following topics show you how to configure amazon sagemaker to meet your security and compliance objectives. you also learn how to use other aws services that help you to monitor and secure your amazon sagemaker resources.  topics 
typically, a neural network for sequence-to-sequence modeling consists of a few layers, including:  an embedding layer. in this layer, the input matrix, which is input tokens encoded in a sparse way (for example, one-hot encoded) are mapped to a dense feature layer. this is required because a high-dimensional feature vector is more capable of encoding information regarding a particular token (word for text corpora) than a simple one-hot-encoded vector. it is also a standard practice to initialize this embedding layer with a pre-trained word vector like  or  or to initialize it randomly and learn the parameters during training. an encoder layer. after the input tokens are mapped into a high-dimensional feature space, the sequence is passed through an encoder layer to compress all the information from the input embedding layer (of the entire sequence) into a fixed-length feature vector. typically, an encoder is made of rnn-type networks like long short-term memory (lstm) or gated recurrent units (gru). ( explains lstm in a great detail.) a decoder layer. the decoder layer takes this encoded feature vector and produces the output sequence of tokens. this layer is also usually built with rnn architectures (lstm and gru). the whole model is trained jointly to maximize the probability of the target sequence given the source sequence. this model was first introduced by  in 2014.  attention mechanism. the disadvantage of an encoder-decoder framework is that model performance decreases as and when the length of the source sequence increases because of the limit of how much information the fixed-length encoded feature vector can contain. to tackle this problem, in 2015, bahdanau et al. proposed the . in an attention mechanism, the decoder tries to find the location in the encoder sequence where the most important information could be located and uses that information and previously decoded words to predict the next token in the sequence.  for more in details, see the whitepaper  by luong, et al. that explains and simplifies calculations for various attention mechanisms. additionally, the whitepaper  by wu, et al. describes google's architecture for machine translation, which uses skip connections between encoder and decoder layers. 
amazon sagemaker provides several kernels for jupyter that provide support for python 2 and 3, apache mxnet, tensorflow, and pyspark. to set a kernel for a new notebook in the jupyter notebook dashboard, choose new, and then choose the kernel from the list.  you can also create a custom kernel that you can use in your notebook instance. for information, see . 
in the  request, you specify the training algorithm. you can also specify algorithm-specific hyperparameters as string-to-string maps. the following table lists the hyperparameters for the lda training algorithm provided by amazon sagemaker. for more information, see . 
the amazon sagemaker model monitor platform invokes your container code according to a specified schedule. if you chose to write your own container code, the following environment variables are available for your container code. in this context, you can analyze the current dataset or evaluate the constraints if you chose to and emit metrics, if applicable. table: parameters   
automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many jobs that test a range of hyperparameters on your dataset. you choose the tunable hyperparameters, a range of values for each, and an objective metric. you choose the objective metric from the metrics that the algorithm computes. automatic model tuning searches the hyperparameters chosen to find the combination of values that result in the model that optimizes the objective metric. for more information about model tuning, see . the image classification algorithm is a supervised algorithm. it reports an accuracy metric that is computed during training. when tuning the model, choose this metric as the objective metric. tune an image classification model with the following hyperparameters. the hyperparameters that have the greatest impact on image classification objective metrics are: , , and . tune the optimizer-related hyperparameters, such as , , , , , and , based on the selected . for example, use  and  only when  is the . for more information about which hyperparameters are used in each optimizer, see . 
amazon sagemaker ip insights is an unsupervised algorithm that consumes observed data in the form of (entity, ipv4 address) pairs that associates entities with ip addresses. ip insights determines how likely it is that an entity would use a particular ip address by learning latent vector representations for both entities and ip addresses. the distance between these two representations can then serve as the proxy for how likely this association is. the ip insights algorithm uses a neural network to learn the latent vector representations for entities and ip addresses. entities are first hashed to a large but fixed hash space and then encoded by a simple embedding layer. character strings such as user names or account ids can be fed directly into ip insights as they appear in log files. you don't need to preprocess the data for entity identifiers. you can provide entities as an arbitrary string value during both training and inference. the hash size should be configured with a value that is high enough to insure that the number of collisions, which occur when distinct entities are mapped to the same latent vector, remain insignificant. for more information about how to select appropriate hash sizes, see . for representing ip addresses, on the other hand, ip insights uses a specially designed encoder network to uniquely represent each possible ipv4 address by exploiting the prefix structure of ip addresses. during training, ip insights automatically generates negative samples by randomly pairing entities and ip addresses. these negative samples represent data that is less likely to occur in reality. the model is trained to discriminate between positive samples that are observed in the training data and these generated negative samples. more specifically, the model is trained to minimize the cross entropy, also known as the log loss, defined as follows:   yn is the label that indicates whether the sample is from the real distribution governing observed data (yn=1) or from the distribution generating negative samples (yn=0). pn is the probability that the sample is from the real distribution, as predicted by the model. generating negative samples is an important process that is used to achieve an accurate model of the observed data. if negative samples are extremely unlikely, for example, if all of the ip addresses in negative samples are 10.0.0.0, then the model trivially learns to distinguish negative samples and fails to accurately characterize the actual observed dataset. to keep negative samples more realistic, ip insights generates negative samples both by randomly generating ip addresses and randomly picking ip addresses from training data. you can configure the type of negative sampling and the rates at which negative samples are generated with the  and  hyperparameters. given an nth (entity, ip address pair), the ip insights model outputs a score, sn , that indicates how compatible the entity is with the ip address. this score corresponds to the log odds ratio for a given (entity, ip address) of the pair coming from a real distribution as compared to coming from a negative distribution. it is defined as follows:  the score is essentially a measure of the similarity between the vector representations of the nth entity and ip address. it can be interpreted as how much more likely it would be to observe this event in reality than in a randomly generated dataset. during training, the algorithm uses this score to calculate an estimate of the probability of a sample coming from the real distribution, pn, to use in the cross entropy minimization, where:  
this section explains how amazon sagemaker interacts with a docker container that runs your custom training algorithm. use this information to write training code and create a docker image for your training algorithms.  topics 
you can create a jupyter notebook in the notebook instance you created in , and create a cell that gets the iam role that your notebook needs to run amazon sagemaker apis and specifies the name of the amazon s3 bucket that you will use to store the datasets that you use for your training data and the model artifacts that a amazon sagemaker training job outputs. to create a jupyter notebook open the notebook instance. sign in to the amazon sagemaker console at . open the notebook instances, and then open the notebook instance you created by choosing either open jupyter for classic juypter view or open jupyterlab for jupyterlab view next to the name of the notebook instance. noteif you see pending to the right of the notebook instance in the status column, your notebook is still being created. the status will change to inservice when the notebook is ready for use.  create a notebook.  if you opened the notebook in jupyter classic view, on the files tab, choose new, and conda_python3. this preinstalled environment includes the default anaconda installation and python 3. if you opened the notebook in jupyterlab view, on the file menu, choose new, and then choose notebook. for select kernel, choose conda_python3. this preinstalled environment includes the default anaconda installation and python 3. in the jupyter notebook, choose file and save as, and name the notebook. next step 
amazon sagemaker provides a built-in container sagemaker-model-monitor-analyzer that provides you with a range of model monitoring capabilities, including constraint suggestion, statistics generation, constraint validation against a baseline, and emitting amazon cloudwatch metrics. this container is based on spark and is built with . the prebuilt container for sagemaker model monitor can be accessed at:  for example:  the following table lists the supported values for account ids and corresponding aws region names. to write your own analysis container, see the container contract described in . 
amazon sagemaker hosts models in an amazon virtual private cloud by default. however, models access aws resources—such as the amazon s3 buckets where you store training data and model artifacts—over the internet. to avoid making your data and model containers accessible over the internet, we recommend that you create a private vpc and configure it to control access to them. for information about creating and configuring a vpc, see  in the amazon vpc user guide. using a vpc helps to protect your training containers and data because you can configure your vpc so that it is not connected to the internet. using a vpc also allows you to monitor all network traffic in and out of your training containers by using vpc flow logs. for more information, see  in the amazon vpc user guide. you specify your private vpc configuration when you create a model by specifying subnets and security groups. when you specify the subnets and security groups, amazon sagemaker creates elastic network interfaces (enis) that are associated with your security groups in one of the subnets. enis allow your model containers to connect to resources in your vpc. for information about enis, see  in the amazon vpc user guide. to specify subnets and security groups in your private vpc, use the  request parameter of the  api, or provide this information when you create a model in the amazon sagemaker console. amazon sagemaker uses this information to create enis and attach them to your model containers. the enis provide your model containers with a network connection within your vpc that is not connected to the internet. they also enable your model to connect to resources in your private vpc. noteyou must create at least two subnets in different availability zones in your private vpc, even if you have only one hosting instance. the following is an example of the  parameter that you include in your call to : when configuring the private vpc for your amazon sagemaker models, use the following guidelines. for information about setting up a vpc, see  in the amazon vpc user guide. topics your vpc subnets should have at least two private ip addresses for each model instance. for more information, see  in the amazon vpc user guide. if you configure your vpc so that model containers don't have access to the internet, they can't connect to the amazon s3 buckets that contain your data unless you create a vpc endpoint that allows access. by creating a vpc endpoint, you allow your model containers to access the buckets where you store your data and model artifacts . we recommend that you also create a custom policy that allows only requests from your private vpc to access to your s3 buckets. for more information, see . to create an amazon s3 vpc endpoint: open the amazon vpc console at . in the navigation pane, choose endpoints, then choose create endpoint for service name, choose com.amazonaws.region.s3, where region is the name of the aws region where your vpc resides. for vpc, choose the vpc that you want to use for this endpoint. for configure route tables, choose the route tables that the endpoint will use. the vpc service automatically adds a route to each route table that you choose that points amazon s3 traffic to the new endpoint. for policy, choose full access to allow full access to the amazon s3 service by any user or service within the vpc. to restrict access further, choose custom. for more information, see . the default endpoint policy allows full access to amazon simple storage service (amazon s3) for any user or service in your vpc. to further restrict access to amazon s3, create a custom endpoint policy. for more information, see .  you can also use a bucket policy to restrict access to your s3 buckets to only traffic that comes from your amazon vpc. for information, see . the default endpoint policy allows users to install packages from the amazon linux and amazon linux 2 repositories on the model container. if you don't want users to install packages from those repositories, create a custom endpoint policy that explicitly denies access to the amazon linux and amazon linux 2 repositories. the following is an example of a policy that denies access to these repositories: the  managed policy includes the permissions that you need to use models configured for amazon vpc access with an endpoint. these permissions allow amazon sagemaker to create an elastic network interface and attach it to model containers running in a vpc. if you use your own iam policy, you must add the following permissions to that policy to use models configured for vpc access.  for more information about the  managed policy, see .  use default dns settings for your endpoint route table, so that standard amazon s3 urls (for example, ) resolve. if you don't use default dns settings, ensure that the urls that you use to specify the locations of the data in your models resolve by configuring the endpoint route tables. for information about vpc endpoint route tables, see  in the amazon vpc user guide. if you configure your vpc so that it doesn't have internet access, models that use that vpc do not have access to resources outside your vpc. if your model needs access to resources outside your vpc, provide access with one of the following options: if your model needs access to an aws service that supports interface vpc endpoints, create an endpoint to connect to that service. for a list of services that support interface endpoints, see  in the amazon vpc user guide. for information about creating an interface vpc endpoint, see  in the amazon vpc user guide.if your model needs access to an aws service that doesn't support interface vpc endpoints or to a resource outside of aws, create a nat gateway and configure your security groups to allow outbound connections. for information about setting up a nat gateway for your vpc, see  in the amazon virtual private cloud user guide.
to get inference for an entire dataset, use batch transform. amazon sagemaker stores the results in amazon s3. for information about batch transforms, see . for an example that uses batch transform, see the batch transform sample notebook at . topics the following code creates a  object from the model that you trained in . then it calls that object's  method to create a transform job. when you create the  object, you specify the number and type of ml instances to use to perform the batch transform job, and the location in amazon s3 where you want to store the inferences.  paste the following code in a cell in the jupyter notebook you created in  and run the cell.  for more information, see . next step to run a batch transform job, call the . method using the model that you trained in . to create a batch transform job (sdk for python (boto 3)) for each of the following steps, paste the code in a cell in the jupyter notebook you created in  and run the cell. name the batch transform job and specify where the input data (the test dataset) is stored and where to store the job's output. configure the parameters that you pass when you call the  method.  for more information about the parameters, see . call the  method, passing in the parameters that you configured in the previous step. then call the  method in a loop until it completes. paste the following code in a cell in the jupyter notebook you created in  and run the cell. next step 
to access your amazon sagemaker notebook instances, choose one of the following options:  use the console. choose notebook instances. the console displays a list of notebook instances in your account. to open a notebook instance with a standard jupyter interface, choose open jupyter for that instance. to open a notebook instance with a jupyterlab interface, choose open jupyterlab for that instance. the console uses your sign-in credentials to send a  api request to amazon sagemaker. amazon sagemaker returns the url for your notebook instance, and the console opens the url in another browser tab and displays the jupyter notebook dashboard.  notethe url that you get from a call to  is valid only for 5 minutes. if you try to use the url after the 5-minute limit expires, you are directed to the aws management console sign-in page. use the api. to get the url for the notebook instance, call the  api and use the url that the api returns to open the notebook instance. use the jupyter notebook dashboard to create and manage notebooks and to write code. for more information about jupyter notebooks, see . 
image segmentation is the process of dividing an image into multiple segments, or sets of labeled pixels. in amazon sagemaker ground truth, the process of identifying all pixels that fall under a given label involves applying a colored filler, or "mask", over those pixels. some labeling job tasks contain images with a large numbers of objects that need to be segmented. to help workers label these objects in less time and with greater accuracy, ground truth provides an auto-segmentation tool for segmentation tasks assigned to private and vendor workforces. this tool uses a machine learning model to automatically segment individual objects in the image with minimal worker input. workers can refine the mask generated by the auto-segmentation tool using other tools found in the worker console. this helps workers complete image segmentation tasks faster and more accurately, resulting in lower cost and higher label quality.  notethe auto-segmentation tool is available for segmentation tasks that are sent to a private workforce or vendor workforce. it isn't available for tasks sent to the public workforce (amazon mechanical turk).  when workers are assigned a labeling job that provides the auto-segmentation tool, they are provided with detailed instructions on how to use the tool. for example, a worker might see the following in the worker console:   workers can use view full instructions to learn how to use the tool. workers will need to place a point on four extreme-points ( top-most, bottom-most, left-most, and right-most points ) of the object of interest, and the tool will automatically generate a mask for the object. workers can further-refine the mask using the other tools provided, or by using the auto-segment tool on smaller portions of the object that were missed.  the auto-segmentation tool automatically appears in your workers' consoles if you create a semantic segmentation labeling job using the amazon sagemaker console. while creating a semantic segmentation job in the amazon sagemaker console, you will be able to preview the tool while creating worker instructions. to learn how to create a semantic segmentation labeling job in the amazon sagemaker console, see .  if you are creating a custom instance segmentation labeling job in the amazon sagemaker console or creating an instance- or semantic-segmentation labeling job using the ground truth api, you need to create a custom task template to design your worker console and instructions. to include the auto-segmentation tool in your worker console, ensure that the following conditions are met in your custom task template: for semantic segmentation labeling jobs created using the api, the  is present in the task template. for custom instance segmentation labeling jobs, the  tag is present in the task template.the task is assigned to a private workforce or vendor workforce. the images to be labeled are amazon simple storage service amazon s3) objects that have been pre-signed for the worker so that they can access it. this is true if the task template includes the  filter. for information about the  filter, see .the following is an example of a custom task template for a custom instance segmentation labeling job, which includes the  tag and the  liquid filter. 
to test and evaluate inference performance using ei, you can attach ei to a notebook instance when you create or update a notebook instance. you can then use ei in local mode to host a model at an endpoint hosted on the notebook instance. you should test various sizes of notebook instances and ei accelerators to evaluate the configuration that works best for your use case. to use ei locally in a notebook instance, create a notebook instance with an ei instance. to create a notebook instance with an ei instance open the amazon sagemaker console at  in the navigation pane, choose notebook instances. choose create notebook instance. for notebook instance name, provide a unique name for your notebook instance. for notebook instance type, choose a cpu instance such as ml.t2.medium. for elastic inference (ei), choose an instance from the list, such as ml.eia2.medium. for iam role, choose an iam role that has the required permissions to use amazon sagemaker and ei. (optional) for vpc - optional, if you want the notebook instance to use a vpc, choose one from the available list. otherwise, leave it as no vpc. if you use a vpc follow the instructions at . (optional) for lifecycle configuration - optional, either leave it as no configuration or choose a lifecycle configuration. for more information, see . (optional) for encryption key - optional, optional) if you want amazon sagemaker to use an aws key management service (aws kms) key to encrypt data in the ml storage volume attached to the notebook instance, specify the key. (optional) for volume size in gb - optional, leave the default value of 5. (optional) for tags, add tags to the notebook instance. a tag is a label you assign to help manage your notebook instances. a tag consists of a key and a value, both of which you define. choose create notebook instance. after you create your notebook instance with ei attached, you can create a jupyter notebook and set up an ei endpoint that is hosted locally on the notebook instance. topics to use ei locally in an endpoint hosted on a notebook instance, use local mode with the  versions of either the tensorflow, mxnet, or pytorch estimators or models. for more information about local mode support in the amazon sagemaker python sdk, see . topics to use ei with tensorflow in local mode, specify  for  and  for  when you call the  method of an estimator or a model object. for more information about  tensorflow estimators and models, see . the following code shows how to use local mode with an estimator object. to call the  method, you must have previously either: trained the model by calling the  method of an estimator.pass a model artifact when you initialize the model object.to use ei with mxnet in local mode, specify  for  and  for  when you call the  method of an estimator or a model object. for more information about  mxnet estimators and models, see .  the following code shows how to use local mode with an estimator object. you must have previously called the  method of the estimator to train the model. for a complete example of using ei in local mode with mxnet, see the sample notebook at . to use ei with pytorch in local mode, when you call the  method of an estimator or a model object, specify  for  and  for . for more information about  pytorch estimators and models, see .  the following code shows how to use local mode with an estimator object. you must have previously called the  method of the estimator to train the model. 
to help you debug your processing jobs, training jobs, endpoints, transform jobs, notebook instances, and notebook instance lifecycle configurations, anything an algorithm container, a model container, or a notebook instance lifecycle configuration sends to  or  is also sent to amazon cloudwatch logs. in addition to debugging, you can use these for progress analysis. logs the following table lists all of the logs provided by amazon sagemaker. logs  note1. the  log stream is created when you create a notebook instance with a lifecycle configuration. for more information, see .2. for inference pipelines, if you don't provide container names, the platform uses **container-1, container-2**, and so on, corresponding to the order provided in the amazon sagemaker model. for more information about logging events with cloudwatch logging, see  in the amazon cloudwatch user guide. 
generates a tool to select and annotate key points on an image. the following is an example of an liquid template that uses the  element. copy the following code and save it in a file with the extenion . open the file in any browser to preview and interact with this template.  the following attributes are supported by this element. the text to display above the image. this is typically a question or simple instruction for the worker. an array, in json format, of keypoints to be applied to the image on start. for example: noteplease note that label values used in this attribute must have a matching value in the  attribute or the point will not be rendered. an array, in json format, of strings to be used as keypoint annotation labels. a string used to identify the answer submitted by the worker. this value will match a key in the json object that specifies the answer. the source uri of the image to be annotated. this element has the following parent and child elements. parent elements: child elements: , the following regions are required by this element. general instructions about how to annotate the image. important task-specific instructions that are displayed in a prominent place. the following output is supported by this element. a json object that specifies the dimensions of the image that is being annotated by the worker. this object contains the following properties. height – the height, in pixels, of the image.width – the width, in pixels, of the image.an array of json objects containing the coordinates and label of a keypoint. each object contains the following properties. label – the assigned label for the keypoint.x – the x coordinate, in pixels, of the keypoint on the image.y – the y coordinate, in pixels, of the keypoint on the image.notex and y coordinates are based on 0,0 being the top left corner of the image. example : sample element outputsthe following is a sample output from using this element.   you may have many labels available, but only the ones that are used appear in the output. for more information, see the following. 
the prediction task for a factorization machine model is to estimate a function ŷ from a feature set xi to a target domain. this domain is real-valued for regression and binary for classification. the factorization machine model is supervised and so has a training dataset (xi,yj) available. the advantages this model presents lie in the way it uses a factorized parametrization to capture the pairwise feature interactions. it can be represented mathematically as follows:    the three terms in this equation correspond respectively to the three components of the model:  the w0 term represents the global bias.the wi linear terms model the strength of the ith variable.the <vi,vj> factorization terms model the pairwise interaction between the ith and jth variable.the global bias and linear terms are the same as in a linear model. the pairwise feature interactions are modeled in the third term as the inner product of the corresponding factors learned for each feature. learned factors can also be considered as embedding vectors for each feature. for example, in a classification task, if a pair of features tends to co-occur more often in positive labeled samples, then the inner product of their factors would be large. in other words, their embedding vectors would be close to each other in cosine similarity. for more information about the factorization machine model, see . for regression tasks, the model is trained by minimizing the squared error between the model prediction ŷn and the target value yn. this is known as the square loss:  for a classification task, the model is trained by minimizing the cross entropy loss, also known as the log loss:   where:   for more information about loss functions for classification, see . 
amazon cloudwatch events monitors status changes in sagemaker labeling, training, hyperparameter tuning, and inference jobs. every time the job status changes, cloudwatch events invokes targets to react to the change. for example, a cloudwatch events rule can monitor a sagemaker training job and trigger an aws lambda function to automatically terminate the job when its status changes. to create a cloudwatch events rule, open the  and follow instructions provided at . you can set an event pattern to monitor a sagemaker job and a target to invoke. the cloudwatch console assists you to generate rules providing a sagemaker event pattern preview code depending on your choice of event type. for more information about the status values and their meanings for amazon sagemaker jobs, see the following links: for more information about the format of the amazon sagemaker events that cloudwatch events monitors, see . to learn more about creating a lambda function, see . 
